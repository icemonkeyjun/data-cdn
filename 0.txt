Anarchism is a political philosophy which considers the state undesirable, unnecessary and harmful, and instead promotes a stateless society, or anarchy. It seeks to diminish or even abolish authority in the conduct of human relations. Anarchists may widely disagree on what additional criteria are required in anarchism. "The Oxford Companion to Philosophy" says, "there is no single defining position that all anarchists hold, and those considered anarchists at best share a certain family resemblance." There are many types and traditions of anarchism, not all of which are mutually exclusive. Strains of anarchism have been divided into the categories of social and individualist anarchism or similar dual classifications. Anarchism is often considered to be a radical left-wing ideology, and much of anarchist economics and anarchist legal philosophy reflect anti-statist interpretations of communism, collectivism, syndicalism or participatory economics. However, anarchism has always included an individualist strain supporting a market economy and private property, or unrestrained egoism that bases right on might. Others, such as panarchists and anarchists without adjectives, neither advocate nor object to any particular form of organization as long as it is not compulsory. Differing fundamentally, some anarchist schools of thought support anything from extreme individualism to complete collectivism. The central tendency of anarchism as a social movement have been represented by communist anarchism, with individualist anarchism being primarily a philosophical or literary phenomenon. Some anarchists fundamentally oppose all forms of aggression, supporting self-defense or non-violence, while others have supported the use of some coercive measures, including violent revolution and terrorism, on the path to an anarchist society.
The term "anarchism" derives from the Greek ἄναρχος, "anarchos", meaning "without rulers", from the prefix ἀν- ("an-", "without") + ἀρχή ("archê", "sovereignty, realm, magistracy") + -ισμός ("-ismos", from the suffix -ιζειν, "-izein" "-izing"). There is some ambiguity with the use of the terms "libertarianism" and "libertarian" in writings about anarchism. Since the 1890s from France, the term "libertarianism" has often been used as a synonym for anarchism and was used almost exclusively in this sense until the 1950s in the United States; its use as a synonym is still common outside the United States. Accordingly, "libertarian socialism" is sometimes used as a synonym for socialist anarchism, to distinguish it from "individualist libertarianism" (individualist anarchism). On the other hand, some use "libertarianism" to refer to individualistic free-market philosophy only, referring to free-market anarchism as "libertarian anarchism."
Some claim anarchist themes can be found in the works of Taoist sages Laozi and Zhuangzi. The latter has been translated, "There has been such a thing as letting mankind alone; there has never been such a thing as governing mankind [with success]," and "A petty thief is put in jail. A great brigand becomes a ruler of a Nation." Diogenes of Sinope and the Cynics, and their contemporary Zeno of Citium, the founder of Stoicism, also introduced similar topics. Modern anarchism, however, sprang from the secular or religious thought of the Enlightenment, particularly Jean-Jacques Rousseau's arguments for the moral centrality of freedom. Although by the turn of the 19th century the term "anarchist" had lost its initial negative connotation, it first entered the English language in 1642 during the English Civil War as a term of abuse used by Royalists to damn those who were fomenting disorder. By the time of the French Revolution some, such as the "Enragés", began to use the term positively, in opposition to Jacobin centralisation of power, seeing "revolutionary government" as oxymoronic. From this climate William Godwin developed what many consider the first expression of modern anarchist thought. Godwin was, according to Peter Kropotkin, "the first to formulate the political and economical conceptions of anarchism, even though he did not give that name to the ideas developed in his work", while Godwin attached his anarchist ideas to an early Edmund Burke. Benjamin Tucker instead credits Josiah Warren, an American who promoted stateless and voluntary communities where all goods and services were private, with being "the first man to expound and formulate the doctrine now known as Anarchism." The first to describe himself as an anarchist was Pierre-Joseph Proudhon, a French philosopher and politician, which led some to call him the founder of modern anarchist theory.
Anarchism as a social movement has regularly endured fluctuations in popularity. Its classical period, which scholars demarcate as from 1860 to 1939, is associated with the working-class movements of the nineteenth century and the Spanish Civil War-era struggles against fascism. Anarchists were heavilly involved in the abolition of slavery, and continue to be active in the labour movement, civil rights, women's liberation, both anti-capitalism and pro-capitalism (with varying definitions of capitalism), the anti-war movement, LGBT rights, both anti-globalization and pro-globalization (with varying definitions of globalization), tax resistance, and other areas.
In Europe, harsh reaction followed the revolutions of 1848, during which ten countries had experienced brief or long-term social upheaval as groups carried out nationalist uprisings. After most of these attempts at systematic change ended in failure, conservative elements took advantage of the divided groups of socialists, anarchists, liberals, and nationalists, to prevent further revolt. In 1864 the International Workingmen's Association (sometimes called the "First International") united diverse revolutionary currents including French followers of Proudhon, Blanquists, Philadelphes, English trade unionists, socialists and social democrats. Due to its links to active workers' movements, the International became a significant organization. Karl Marx became a leading figure in the International and a member of its General Council. Proudhon's followers, the mutualists, opposed Marx's state socialism, advocating political abstentionism and small property holdings. In 1868, following their unsuccessful participation in the League of Peace and Freedom (LPF), Russian revolutionary Mikhail Bakunin and his collectivist anarchist associates and joined the First International (which had decided not to get involved with the LPF). They allied themselves with the federalist socialist sections of the International, who advocated the revolutionary overthrow of the state and the collectivization of property. At first, the collectivists worked with the Marxists to push the First International in a more revolutionary socialist direction. Subsequently, the International became polarised into two camps, with Marx and Bakunin as their respective figureheads. Bakunin characterised Marx's ideas as centralist and predicted that, if a Marxist party came to power, its leaders would simply take the place of the ruling class they had fought against. In 1872, the conflict climaxed with a final split between the two groups at the Hague Congress, where Bakunin and James Guillaume were expelled from the International and its headquarters were transferred to New York. In response, the federalist sections formed their own International at the St. Imier Congress, adopting a revolutionary anarchist program.
The anti-authoritarian sections of the First International were the precursors of the anarcho-syndicalists, seeking to "replace the privilege and authority of the State" with the "free and spontaneous organization of labor." In 1886, the Federation of Organized Trades and Labor Unions (FOTLU) of the United States and Canada unanimously set 1 May 1886, as the date by which the eight-hour work day would become standard. In response, unions across America prepared a general strike in support of the event. On 3 May, in Chicago, a fight broke out when strikebreakers attempted to cross the picket line, and two workers died when police opened fire upon the crowd. The next day, 4 May, anarchists staged a rally at Chicago's Haymarket Square. A bomb was thrown by an unknown party near the conclusion of the rally, killing an officer. In the ensuing panic, police opened fire on the crowd and each other. Seven police officers and at least four workers were killed. Eight anarchists directly and indirectly related to the organisers of the rally were arrested and charged with the murder of the deceased officer. The men became international political celebrities among the labour movement. Four of the men were executed and a fifth committed suicide prior to his own execution. The incident became known as the Haymarket affair, and was a setback for the labour movement and the struggle for the eight hour day. In 1890 a second attempt, this time international in scope, to organise for the eight hour day was made. The event also had the secondary purpose of memorializing workers killed as a result of the Haymarket affair. Although it had initially been conceived as a once-off event, by the following year the celebration of International Workers' Day on May Day had become firmly established as an international worker's holiday. In 1907, the International Anarchist Congress of Amsterdam gathered delegates from 14 different countries, among which important figures of the anarchist movement, including Errico Malatesta, Pierre Monatte, Luigi Fabbri, Benoît Broutchoux, Emma Goldman, Rudolf Rocker, and Christiaan Cornelissen. Various themes were treated during the Congress, in particular concerning the organisation of the anarchist movement, popular education issues, the general strike or antimilitarism. A central debate concerned the relation between anarchism and syndicalism (or trade unionism). Malatesta and Monatte were in particular disagreement themselves on this issue, as the latter thought that syndicalism was revolutionary and would create the conditions of a social revolution, while Malatesta did not consider syndicalism by itself sufficient. He thought that the trade-union movement was reformist and even conservative, citing as essentially bourgeois and anti-worker the phenomenon of professional union officials. Malatesta warned that the syndicalists aims were in perpetuating syndicalism itself, whereas anarchists must always have anarchy as their end and consequently refrain from committing to any particular method of achieving it. The Spanish Workers Federation in 1881 was the first major anarcho-syndicalist movement; anarchist trade union federations were of special importance in Spain. The most successful was the Confederación Nacional del Trabajo (National Confederation of Labour: CNT), founded in 1910. Before the 1940s, the CNT was the major force in Spanish working class politics, attracting 1.58 million members at one point and playing a major role in the Spanish Civil War. The CNT was affiliated with the International Workers Association, a federation of anarcho-syndicalist trade unions founded in 1922, with delegates representing two million workers from 15 countries in Europe and Latin America. The largest organised anarchist movement today is in Spain, in the form of the Confederación General del Trabajo (CGT) and the CNT. CGT membership was estimated to be around 100,000 for the year 2003. Other active syndicalist movements include the US Workers Solidarity Alliance and the UK Solidarity Federation. The revolutionary industrial unionist Industrial Workers of the World, claiming 2,000 paying members, and the International Workers Association, an anarcho-syndicalist successor to the First International, also remain active.
Anarchists participated alongside the Bolsheviks in both February and October revolutions, and were initially enthusiastic about the Bolshevik coup. However, the Bolsheviks soon turned against the anarchists and other left-wing opposition, a conflict that culminated in the 1921 Kronstadt rebellion which the new government repressed. Anarchists in central Russia were either imprisoned, driven underground or joined the victorious Bolsheviks; the anarchists from Petrograd and Moscow fled to the Ukraine. There, in the Free Territory, they fought in the civil war against the Whites (a Western-backed grouping of monarchists and other opponents of the October Revolution) and then the Bolsheviks as part of the Revolutionary Insurrectionary Army of Ukraine led by Nestor Makhno, who established an anarchist society in the region for a number of months. Expelled American anarchists Emma Goldman and Alexander Berkman were amongst those agitating in response to Bolshevik policy and the suppression of the Kronstadt uprising, before they left Russia. Both wrote accounts of their experiences in Russia, criticizing the amount of control the Bolsheviks exercised. For them, Bakunin's predictions about the consequences of Marxist rule that the rulers of the new "socialist” Marxist state would become a new elite had proved all too true. The victory of the Bolsheviks in the October Revolution and the resulting Russian Civil War did serious damage to anarchist movements internationally. Many workers and activists saw Bolshevik success as setting an example; Communist parties grew at the expense of anarchism and other socialist movements. In France and the United States, for example, members of the major syndicalist movements of the CGT and IWW left the organizations and joined the Communist International. In Paris, the Dielo Truda group of Russian anarchist exiles, which included Nestor Makhno, concluded that anarchists needed to develop new forms of organisation in response to the structures of Bolshevism. Their 1926 manifesto, called the "Organizational Platform of the General Union of Anarchists (Draft)", was supported. Platformist groups active today include the Workers Solidarity Movement in Ireland and the North Eastern Federation of Anarchist Communists of North America.
In the 1920s and 1930s, the rise of fascism in Europe transformed anarchism's conflict with the state. Italy saw the first struggles between anarchists and fascists. Italian anarchists played a key role in the anti-fascist organisation "Arditi del Popolo", which was strongest in areas with anarchist traditions, and achieved some success in their activism, such as repelling Blackshirts in the anarchist stronghold of Parma in August 1922. In France, where the far right leagues came close to insurrection in the February 1934 riots, anarchists divided over a united front policy. In Spain, the CNT initially refused to join a popular front electoral alliance, and abstention by CNT supporters led to a right wing election victory. But in 1936, the CNT changed its policy and anarchist votes helped bring the popular front back to power. Months later, the former ruling class responded with an attempted coup causing the Spanish Civil War (1936–1939). In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land. But even before the fascist victory in 1939, the anarchists were losing ground in a bitter struggle with the Stalinists, who controlled the distribution of military aid to the Republican cause from the Soviet Union. Stalinist-led troops suppressed the collectives and persecuted both dissident Marxists and anarchists.
A surge of popular interest in anarchism occurred during the 1960s and 1970s. In the United Kingdom this was associated with the punk rock movement, as exemplified by bands such as Crass and the Sex Pistols. The housing and employment crisis in most of Western Europe led to the formation of communes and squatter movements like that of Barcelona, Spain. In Denmark, squatters occupied a disused military base and declared the Freetown Christiania, an autonomous haven in central Copenhagen. Since the revival of anarchism in the mid 20th century, a number of new movements and schools of thought emerged. Although feminist tendencies have always been a part of the anarchist movement in the form of anarcha-feminism, they returned with vigour during the second wave of feminism in the 1960s. The American Civil Rights Movement and the movement against the war in Vietnam also contributed to the revival of North American anarchism. European anarchism of the late 20th century drew much of its strength from the labour movement, and both have incorporated animal rights activism. Anarchist anthropologist David Graeber has posited a rupture between generations of anarchism, with those "who often still have not shaken the sectarian habits" of the nineteenth century contrasted with the younger activists who are "much more informed, among other elements, by indigenous, feminist, ecological and cultural-critical ideas", and who by the turn of the 21st century formed "by far the majority" of anarchists. Around the turn of the 21st century, anarchism grew in popularity and influence as part of the anti-war, anti-capitalist, and anti-globalisation movements. Anarchists became known for their involvement in protests against the meetings of the World Trade Organization (WTO), Group of Eight, and the World Economic Forum. Some anarchist factions at these protests engaged in rioting, property destruction, and violent confrontations with police, and the confrontations were selectively portrayed in mainstream media coverage as violent riots. These actions were precipitated by ad hoc, leaderless, anonymous cadres known as "black blocs"; other organisational tactics pioneered in this time include security culture, affinity groups and the use of decentralised technologies such as the internet. A landmark struggle of this period was the confrontations at WTO conference in Seattle in 1999.
Anarchist ideas have only occasionally inspired political movements of any size, and "the tradition is mainly one of individual thinkers, but they have produced an important body of theory." Anarchist schools of thought had been generally grouped in two main historical traditions, individualist anarchism and social anarchism, which have some different origins, values and evolution. The individualist wing of anarchism emphasises negative liberty, i.e. opposition to state or social control over the individual, while those in the social wing emphasise positive liberty to achieve one's potential and argue that humans have needs that society ought to fulfill, "recognizing equality of entitlement". In chronological and theoretical sense there are classical — those created throughout the 19th century — and post-classical anarchist schools — those created since the mid-20th century and after. Beyond the specific factions of anarchist thought is philosophical anarchism, which embodies the theoretical stance that the State lacks moral legitimacy without accepting the imperative of revolution to eliminate it. A component especially of individualist anarchism philosophical anarchism may accept the existence of a minimal state as unfortunate, and usually temporary, "necessary evil" but argue that citizens do not have a moral obligation to obey the state when its laws conflict with individual autonomy. One reaction against sectarianism within the anarchist milieu was "anarchism without adjectives", a call for toleration first adopted by Fernando Tarrida del Mármol in 1889 in response to the "bitter debates" of anarchist theory at the time. In abandoning the hyphenated anarchisms (i.e. collectivist-, communist-, mutualist- and individualist-anarchism), it sought to emphasise the anti-authoritarian beliefs common to all anarchist schools of thought.
Mutualism began in 18th century English and French labour movements before taking an anarchist form associated with Pierre-Joseph Proudhon in France and others in the United States. Proudhon proposed spontaneous order, whereby organization emerges without central authority, a "positive anarchy" where order arises when everybody does “what he wishes and only what he wishes" and where "business transactions alone produce the social order." Mutualist anarchism is concerned with reciprocity, free association, voluntary contract, federation, and credit and currency reform. According to William Batchelder Greene, each worker in the mutualist system would receive "just and exact pay for his work; services equivalent in cost being exchangeable for services equivalent in cost, without profit or discount." Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism. Proudhon first characterised his goal as a "third form of society, the synthesis of communism and property."
Individualist anarchism refers to several traditions of thought within the anarchist movement that emphasise the individual and their will over any kinds of external determinants such as groups, society, traditions, and ideological systems. Individualist anarchism is not a single philosophy but refers to a group of individualistic philosophies that sometimes are in conflict. In 1793, William Godwin, who has often been cited as the first anarchist, wrote "Political Justice", which some consider to be the first expression of anarchism. Godwin, a philosophical anarchist, from a rationalist and utilitarian basis opposed revolutionary action and saw a minimal state as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge. Godwin advocated extreme individualism, proposing that all cooperation in labour be eliminated on the premise that this would be most conducive with the general good. Godwin was a utilitarian who believed that all individuals are not of equal value, with some of us "of more worth and importance" than others depending on our utility in bringing about social good. Therefore he does not believe in equal rights, but the person's life that should be favoured that is most conducive to the general good. Godwin opposed government because he saw it as infringing on the individual's right to "private judgement" to determine which actions most maximise utility, but also makes a critique of all authority over the individual's judgement. This aspect of Godwin's philosophy, stripped of utilitarian motivations, was developed into a more extreme form later by Stirner. The most extreme form of individualist anarchism, called "egoism," or egoist anarchism, was expounded by one of the earliest and best-known proponents of individualist anarchism, Max Stirner. Stirner's "The Ego and Its Own", published in 1844, is a founding text of the philosophy. According to Stirner, the only limitation on the rights of the individual is their power to obtain what they desire, without regard for God, state, or morality. To Stirner, rights were "spooks" in the mind, and he held that society does not exist but "the individuals are its reality". Stirner advocated self-assertion and foresaw Unions of Egoists, non-systematic associations continually renewed by all parties' support through an act of will, which Stirner proposed as a form of organization in place of the state. Egoist anarchists claim that egoism will foster genuine and spontaneous union between individuals. "Egoism" has inspired many interpretations of Stirner's philosophy. It was re-discovered and promoted by German philosophical anarchist and LGBT activist John Henry Mackay. Individualist anarchism inspired by Stirner attracted a small following of European bohemian artists and intellectuals (see European individualist anarchism). Stirner's philosophy has been seen as a precedent of existentialism with other thinkers like Friedrich Nietzsche and Sören Kierkegaard.
Social anarchism calls for a system with public ownership of means of production and democratic control of all organizations, without any government authority or coercion. It is the largest school of anarchism. Social anarchism rejects private property, seeing it as a source of social inequality, and emphasises cooperation and mutual aid. Collectivist anarchism, also referred to as "revolutionary socialism" or a form of such, is a revolutionary form of anarchism, commonly associated with Mikhail Bakunin and Johann Most. Collectivist anarchists oppose all private ownership of the means of production, instead advocating that ownership be collectivised. This was to be achieved through violent revolution, first starting with a small cohesive group through acts of violence, or "propaganda by the deed," which would inspire the workers as a whole to revolt and forcibly collectivise the means of production. However, collectivization was not to be extended to the distribution of income, as workers would be paid according to time worked, rather than receiving goods being distributed "according to need" as in anarcho-communism. This position was criticised by anarchist communists as effectively "uphold[ing] the wages system". Collectivist anarchism arose contemporaneously with Marxism but opposed the Marxist dictatorship of the proletariat, despite the stated Marxist goal of a collectivist stateless society. Anarchist communist and collectivist ideas are not mutually exclusive; although the collectivist anarchists advocated compensation for labour, some held out the possibility of a post-revolutionary transition to a communist system of distribution according to need. Anarchist communism proposes that the freest form of social organisation would be a society composed of self-managing communes with collective use of the means of production, organised democratically, and related to other communes through federation. While some anarchist communists favour direct democracy, others feel that its majoritarianism can impede individual liberty and favour consensus democracy instead. In anarchist communism, as money would be abolished, individuals would not receive direct compensation for labour (through sharing of profits or payment) but would have free access to the resources and surplus of the commune. Anarchist communism does not always have a communitarian philosophy. Some forms of anarchist communism are egoist and strongly influenced by radical individualism, believing that anarchist communism does not require a communitarian nature at all. In the early 20th century, anarcho-syndicalism arose as a distinct school of thought within anarchism. With greater focus on the labour movement than previous forms of anarchism, syndicalism posits radical trade unions as a potential force for revolutionary social change, replacing capitalism and the state with a new society, democratically self-managed by the workers. It is often combined with other branches of anarchism, and anarcho-syndicalists often subscribe to anarchist communist or collectivist anarchist economic systems. An early leading anarcho-syndicalist thinker was Rudolf Rocker, whose 1938 pamphlet "Anarchosyndicalism" outlined a view of the movement's origin, aims and importance to the future of labour.
Anarchism continues to generate many philosophies and movements, at times eclectic, drawing upon various sources, and syncretic, combining disparate and contrary concepts to create new philosophical approaches. Since the revival of anarchism in the United States in the 1960s, a number of new movements and schools have emerged. Anarcho-capitalism developed from radical anti-state libertarianism and individualist anarchism, drawing from Austrian School economics, study of law and economics and public choice theory, while the burgeoning feminist and environmentalist movements also produced anarchist offshoots. Anarcha-feminism developed as a synthesis of radical feminism and anarchism that views patriarchy (male domination over women) as a fundamental manifestation of compulsory government. It was inspired by the late 19th century writings of early feminist anarchists such as Lucy Parsons, Emma Goldman, Voltairine de Cleyre, and Dora Marsden. Anarcha-feminists, like other radical feminists, criticise and advocate the abolition of traditional conceptions of family, education and gender roles. Green anarchism (or eco-anarchism) is a school of thought within anarchism which puts an emphasis on environmental issues, and whose main contemporary currents are anarcho-primitivism and social ecology. Post-left anarchy is a tendency which seeks to distance itself from traditional left-wing politics and to escape the confines of ideology in general. Post-anarchism is a theoretical move towards a synthesis of classical anarchist theory and poststructuralist thought drawing from diverse ideas including post-modernism, autonomist marxism, post-left anarchy, situationism and postcolonialism. Another recent form of anarchism critical of formal anarchist movements is insurrectionary anarchism, which advocates informal organization and active resistance to the state; its proponents include Wolfi Landstreicher and Alfredo M. Bonanno. Topics of interest in anarchist theory. Intersecting and overlapping between various schools of thought, certain topics of interest and internal disputes have proven perennial within anarchist theory.
An important current within anarchism is Free love. Free love advocates sometimes traced their roots back to Josiah Warren and to experimental communities, viewed sexual freedom as a clear, direct expression of an individual's self-ownership. Free love particularly stressed women's rights since most sexual laws discriminated against women: for example, marriage laws and anti-birth control measures. The most important American free love journal was "Lucifer the Lightbearer" (1883–1907) edited by Moses Harman and Lois Waisbrooker, but also there existed Ezra Heywood and Angela Heywood's 'The Word' (1872–1890, 1892–1893). Also M. E. Lazarus was an important American individualist anarchist who promoted free love. In New York's Greenwich Village, bohemian feminists and socialists advocated self-realisation and pleasure for women (and also men) in the here and now. They encouraged playing with sexual roles and sexuality, and the openly bisexual radical Edna St. Vincent Millay and the lesbian anarchist Margaret Anderson were prominent among them. Discussion groups organised by the Villagers were frequented by Emma Goldman, among others. Magnus Hirschfeld noted in 1923 that Goldman "has campaigned boldly and steadfastly for individual rights, and especially for those deprived of their rights. Thus it came about that she was the first and only woman, indeed the first and only American, to take up the defense of homosexual love before the general public." In fact, before Goldman, heterosexual anarchist Robert Reitzel (1849–98) spoke positively of homosexuality from the beginning of the 1890s in his Detroit-based German language journal "Der arme Teufel". In Europe the main propagandist of free love within individualist anarchism was Emile Armand. He proposed the concept of "la camaraderie amoureuse" to speak of free love as the possibility of voluntary sexual encounter between consenting adults. He was also a consistent proponent of polyamory. In Germany the stirnerists Adolf Brand and John Henry Mackay were pioneering campaigners for the acceptance of male bisexuality and homosexuality. More recently, the British anarcho-pacifist Alex Comfort gained notoriety during the sexual revolution for writing the bestseller sex manual "The Joy of Sex". The issue of free love has a dedicated treatment in the work of french anarcho-hedonist philosopher Michel Onfray in such works as "Théorie du corps amoureux: pour une érotique solaire" (2000) and "L'invention du plaisir: fragments cyréaniques" (2002).
In 1901, Spanish anarchist and free-thinker Francesc Ferrer i Guàrdia established "modern" or progressive schools in Barcelona in defiance of an educational system controlled by the Catholic Church. The schools' stated goal was to "educate the working class in a rational, secular and non-coercive setting". Fiercely anti-clerical, Ferrer believed in "freedom in education", education free from the authority of church and state. Murray Bookchin wrote: "This period [1890s] was the heyday of libertarian schools and pedagogical projects in all areas of the country where Anarchists exercised some degree of influence. Perhaps the best-known effort in this field was Francisco Ferrer's Modern School (Escuela Moderna), a project which exercised a considerable influence on Catalan education and on experimental techniques of teaching generally." La Escuela Moderna, and Ferrer's ideas generally, formed the inspiration for a series of "Modern Schools" in the United States, Cuba, South America and London. The first of these was started in New York City in 1911. It also inspired the Italian newspaper "Università popolare", founded in 1901. Another libertarian tradition is that of unschooling and the free school in which child-led activity replaces pedagogic approaches. Experiments in Germany led to A. S. Neill founding what became Summerhill School in 1921. Summerhill is often cited as an example of anarchism in practice. However, although Summerhill and other free schools are radically libertarian, they differ in principle from those of Ferrer by not advocating an overtly-political class struggle-approach. In addition to organizing schools according to libertarian principles, anarchists have also questioned the concept of schooling per se. The term deschooling was popularized by Ivan Illich, who argued that the school as an institution is dysfunctional for self-determined learning and serves the creation of a consumer society instead.
Anarchism is a philosophy which embodies many diverse attitudes, tendencies and schools of thought; as such, disagreement over questions of values, ideology and tactics is common. The compatibility of capitalism, nationalism and religion with anarchism is widely disputed. Similarly, anarchism enjoys complex relationships with ideologies such as Marxism, communism and capitalism. Anarchists may be motivated by humanism, divine authority, enlightened self-interest or any number of alternative ethical doctrines. Phenomena such as civilization, technology (e.g. within anarcho-primitivism and insurrectionary anarchism), and the democratic process may be sharply criticised within some anarchist tendencies and simultaneously lauded in others. Anarchist attitudes towards race, gender and the environment have changed significantly since the modern origin of the philosophy in the 18th century. On a tactical level, while propaganda of the deed was a tactic used by anarchists in the 19th century (e.g. the Nihilist movement), contemporary anarchists espouse alternative direct action methods such as nonviolence, counter-economics and anti-state cryptography to bring about an anarchist society. About the scope of an anarchist society, some anarchists advocate a global one, while others do so by local ones. The diversity in anarchism has led to widely different use of identical terms among different anarchist traditions, which has led to many definitional concerns in anarchist theory.
Autism is a disorder of neural development characterized by impaired social interaction and communication, and by restricted and repetitive behavior. These signs all begin before a child is three years old. Autism affects information processing in the brain by altering how nerve cells and their synapses connect and organize; how this occurs is not well understood. The two other autism spectrum disorders (ASD) are Asperger syndrome, which lacks delays in cognitive development and language, and PDD-NOS, diagnosed when full criteria for the other two disorders are not met. Autism has a strong genetic basis, although the genetics of autism are complex and it is unclear whether ASD is explained more by rare mutations, or by rare combinations of common genetic variants. In rare cases, autism is strongly associated with agents that cause birth defects. Controversies surround other proposed environmental causes, such as heavy metals, pesticides or childhood vaccines; the vaccine hypotheses are biologically implausible and lack convincing scientific evidence. The prevalence of autism is about 1–2 per 1,000 people; the prevalence of ASD is about 6 per 1,000, with about four times as many males as females. The number of people diagnosed with autism has increased dramatically since the 1980s, partly due to changes in diagnostic practice; the question of whether actual prevalence has increased is unresolved. Parents usually notice signs in the first two years of their child's life. The signs usually develop gradually, but some autistic children first develop more normally and then regress. Although early behavioral or cognitive intervention can help autistic children gain self-care, social, and communication skills, there is no known cure. Not many children with autism live independently after reaching adulthood, though some become successful. An autistic culture has developed, with some individuals seeking a cure and others believing autism should be tolerated as a difference and not treated as a disorder.
Autism is a highly variable neurodevelopmental disorder that first appears during infancy or childhood, and generally follows a steady course without remission. Overt symptoms gradually begin after the age of six months, become established by age two or three years, and tend to continue through adulthood, although often in more muted form. It is distinguished not by a single symptom, but by a characteristic triad of symptoms: impairments in social interaction; impairments in communication; and restricted interests and repetitive behavior. Other aspects, such as atypical eating, are also common but are not essential for diagnosis. Autism's individual symptoms occur in the general population and appear not to associate highly, without a sharp line separating pathologically severe from common traits.
Social deficits distinguish autism and the related autism spectrum disorders (ASD; see "Classification") from other developmental disorders. People with autism have social impairments and often lack the intuition about others that many people take for granted. Noted autistic Temple Grandin described her inability to understand the social communication of neurotypicals, or people with normal neural development, as leaving her feeling "like an anthropologist on Mars". Unusual social development becomes apparent early in childhood. Autistic infants show less attention to social stimuli, smile and look at others less often, and respond less to their own name. Autistic toddlers differ more strikingly from social norms; for example, they have less eye contact and turn taking, and are more likely to communicate by manipulating another person's hand. Three- to five-year-old autistic children are less likely to exhibit social understanding, approach others spontaneously, imitate and respond to emotions, communicate nonverbally, and take turns with others. However, they do form attachments to their primary caregivers. Most autistic children display moderately less attachment security than non-autistic children, although this difference disappears in children with higher mental development or less severe ASD. Older children and adults with ASD perform worse on tests of face and emotion recognition. Contrary to a common belief, autistic children do not prefer being alone. Making and maintaining friendships often proves to be difficult for those with autism. For them, the quality of friendships, not the number of friends, predicts how lonely they feel. Functional friendships, such as those resulting in invitations to parties, may affect the quality of life more deeply. There are many anecdotal reports, but few systematic studies, of aggression and violence in individuals with ASD. The limited data suggest that, in children with mental retardation, autism is associated with aggression, destruction of property, and tantrums. A 2007 study interviewed parents of 67 children with ASD and reported that about two-thirds of the children had periods of severe tantrums and about one-third had a history of aggression, with tantrums significantly more common than in non-autistic children with language impairments. A 2008 Swedish study found that, of individuals aged 15 or older discharged from hospital with a diagnosis of ASD, those who committed violent crimes were significantly more likely to have other psychopathological conditions such as psychosis.
About a third to a half of individuals with autism do not develop enough natural speech to meet their daily communication needs. Differences in communication may be present from the first year of life, and may include delayed onset of babbling, unusual gestures, diminished responsiveness, and vocal patterns that are not synchronized with the caregiver. In the second and third years, autistic children have less frequent and less diverse babbling, consonants, words, and word combinations; their gestures are less often integrated with words. Autistic children are less likely to make requests or share experiences, and are more likely to simply repeat others' words (echolalia) or reverse pronouns. Joint attention seems to be necessary for functional speech, and deficits in joint attention seem to distinguish infants with ASD: for example, they may look at a pointing hand instead of the pointed-at object, and they consistently fail to point at objects in order to comment on or share an experience. Autistic children may have difficulty with imaginative play and with developing symbols into language. In a pair of studies, high-functioning autistic children aged 8–15 performed equally well as, and adults better than, individually matched controls at basic language tasks involving vocabulary and spelling. Both autistic groups performed worse than controls at complex language tasks such as figurative language, comprehension and inference. As people are often sized up initially from their basic language skills, these studies suggest that people speaking to autistic individuals are more likely to overestimate what their audience comprehends.
Autistic individuals may have symptoms that are independent of the diagnosis, but that can affect the individual or the family. An estimated 0.5% to 10% of individuals with ASD show unusual abilities, ranging from splinter skills such as the memorization of trivia to the extraordinarily rare talents of prodigious autistic savants. Many individuals with ASD show superior skills in perception and attention, relative to the general population. Sensory abnormalities are found in over 90% of those with autism, and are considered core features by some, although there is no good evidence that sensory symptoms differentiate autism from other developmental disorders. Differences are greater for under-responsivity (for example, walking into things) than for over-responsivity (for example, distress from loud noises) or for sensation seeking (for example, rhythmic movements). An estimated 60%–80% of autistic people have motor signs that include poor muscle tone, poor motor planning, and toe walking;; deficits in motor coordination are pervasive across ASD and are greater in autism proper. Unusual eating behavior occurs in about three-quarters of children with ASD, to the extent that it was formerly a diagnostic indicator. Selectivity is the most common problem, although eating rituals and food refusal also occur; this does not appear to result in malnutrition. Although some children with autism also have gastrointestinal (GI) symptoms, there is a lack of published rigorous data to support the theory that autistic children have more or different GI symptoms than usual; studies report conflicting results, and the relationship between GI problems and ASD is unclear. Parents of children with ASD have higher levels of stress. Siblings of children with ASD report greater admiration of and less conflict with the affected sibling than siblings of unaffected children or those with Down syndrome; siblings of individuals with ASD have greater risk of negative well-being and poorer sibling relationships as adults.
Autism is one of the five pervasive developmental disorders (PDD), which are characterized by widespread abnormalities of social interactions and communication, and severely restricted interests and highly repetitive behavior. These symptoms do not imply sickness, fragility, or emotional disturbance. Of the five PDD forms, Asperger syndrome is closest to autism in signs and likely causes; Rett syndrome and childhood disintegrative disorder share several signs with autism, but may have unrelated causes; PDD not otherwise specified (PDD-NOS; also called "atypical autism") is diagnosed when the criteria are not met for a more specific disorder. Unlike with autism, people with Asperger syndrome have no substantial delay in language development. The terminology of autism can be bewildering, with autism, Asperger syndrome and PDD-NOS often called the "autism spectrum disorders" (ASD) or sometimes the "autistic disorders", whereas autism itself is often called "autistic disorder", "childhood autism", or "infantile autism". In this article, "autism" refers to the classic autistic disorder; in clinical practice, though, "autism", "ASD", and "PDD" are often used interchangeably. ASD, in turn, is a subset of the broader autism phenotype, which describes individuals who may not have ASD but do have autistic-like traits, such as avoiding eye contact. The manifestations of autism cover a wide spectrum, ranging from individuals with severe impairments—who may be silent, mentally disabled, and locked into hand flapping and rocking—to high functioning individuals who may have active but distinctly odd social approaches, narrowly focused interests, and verbose, pedantic communication. Because the behavior spectrum is continuous, boundaries between diagnostic categories are necessarily somewhat arbitrary. Sometimes the syndrome is divided into low-, medium- or high-functioning autism (LFA, MFA, and HFA), based on IQ thresholds, or on how much support the individual requires in daily life; these subdivisions are not standardized and are controversial. Autism can also be divided into syndromal and non-syndromal autism; the syndromal autism is associated with severe or profound mental retardation or a congenital syndrome with physical symptoms, such as tuberous sclerosis. Although individuals with Asperger syndrome tend to perform better cognitively than those with autism, the extent of the overlap between Asperger syndrome, HFA, and non-syndromal autism is unclear. Some studies have reported diagnoses of autism in children due to a loss of language or social skills, as opposed to a failure to make progress, typically from 15 to 30 months of age. The validity of this distinction remains controversial; it is possible that regressive autism is a specific subtype, or that there is a continuum of behaviors between autism with and without regression. Research into causes has been hampered by the inability to identify biologically meaningful subpopulations and by the traditional boundaries between the disciplines of psychiatry, psychology, neurology and pediatrics. Newer technologies such as fMRI and diffusion tensor imaging can help identify biologically relevant phenotypes (observable traits) that can be viewed on brain scans, to help further neurogenetic studies of autism; one example is lowered activity in the fusiform face area of the brain, which is associated with impaired perception of people versus objects. It has been proposed to classify autism using genetics as well as behavior.
It has long been presumed that there is a common cause at the genetic, cognitive, and neural levels for autism's characteristic triad of symptoms. However, there is increasing suspicion that autism is instead a complex disorder whose core aspects have distinct causes that often co-occur. Autism has a strong genetic basis, although the genetics of autism are complex and it is unclear whether ASD is explained more by rare mutations with major effects, or by rare multigene interactions of common genetic variants. Complexity arises due to interactions among multiple genes, the environment, and epigenetic factors which do not change DNA but are heritable and influence gene expression. Studies of twins suggest that heritability is 0.7 for autism and as high as 0.9 for ASD, and siblings of those with autism are about 25 times more likely to be autistic than the general population. However, most of the mutations that increase autism risk have not been identified. Typically, autism cannot be traced to a Mendelian (single-gene) mutation or to a single chromosome abnormality like fragile X syndrome, and none of the genetic syndromes associated with ASDs has been shown to selectively cause ASD. Numerous candidate genes have been located, with only small effects attributable to any particular gene. The large number of autistic individuals with unaffected family members may result from copy number variations—spontaneous deletions or duplications in genetic material during meiosis. Hence, a substantial fraction of autism cases may be traceable to genetic causes that are highly heritable but not inherited: that is, the mutation that causes the autism is not present in the parental genome. Several lines of evidence point to synaptic dysfunction as a cause of autism. Some rare mutations may lead to autism by disrupting some synaptic pathways, such as those involved with cell adhesion. Gene replacement studies in mice suggest that autistic symptoms are closely related to later developmental steps that depend on activity in synapses and on activity-dependent changes. All known teratogens (agents that cause birth defects) related to the risk of autism appear to act during the first eight weeks from conception, and though this does not exclude the possibility that autism can be initiated or affected later, it is strong evidence that autism arises very early in development. Although evidence for other environmental causes is anecdotal and has not been confirmed by reliable studies, extensive searches are underway. Environmental factors that have been claimed to contribute to or exacerbate autism, or may be important in future research, include certain foods, infectious disease, heavy metals, solvents, diesel exhaust, PCBs, phthalates and phenols used in plastic products, pesticides, brominated flame retardants, alcohol, smoking, illicit drugs, vaccines, and prenatal stress. Parents may first become aware of autistic symptoms in their child around the time of a routine vaccination, and this has given rise to theories that vaccines or their preservatives cause autism. Although these theories lack convincing scientific evidence and are biologically implausible, parental concern about autism has led to lower rates of childhood immunizations and higher likelihood of measles outbreaks.
Interactions between the immune system and the nervous system begin early during the embryonic stage of life, and successful neurodevelopment depends on a balanced immune response. It is possible that aberrant immune activity during critical periods of neurodevelopment is part of the mechanism of some forms of ASD. Although some abnormalities in the immune system have been found in specific subgroups of autistic individuals, it is not known whether these abnormalities are relevant to or secondary to autism's disease processes. As autoantibodies are found in conditions other than ASD, and are not always present in ASD, the relationship between immune disturbances and autism remains unclear and controversial. The relationship of neurochemicals to autism is not well understood; several have been investigated, with the most evidence for the role of serotonin and of genetic differences in its transport. Some data suggest an increase in several growth hormones; other data argue for diminished growth factors. Also, some inborn errors of metabolism are associated with autism but probably account for less than 5% of cases. The mirror neuron system (MNS) theory of autism hypothesizes that distortion in the development of the MNS interferes with imitation and leads to autism's core features of social impairment and communication difficulties. The MNS operates when an animal performs an action or observes another animal perform the same action. The MNS may contribute to an individual's understanding of other people by enabling the modeling of their behavior via embodied simulation of their actions, intentions, and emotions. Several studies have tested this hypothesis by demonstrating structural abnormalities in MNS regions of individuals with ASD, delay in the activation in the core circuit for imitation in individuals with Asperger syndrome, and a correlation between reduced MNS activity and severity of the syndrome in children with ASD. However, individuals with autism also have abnormal brain activation in many circuits outside the MNS and the MNS theory does not explain the normal performance of autistic children on imitation tasks that involve a goal or object. ASD-related patterns of low function and aberrant activation in the brain differ depending on whether the brain is doing social or nonsocial tasks. In autism there is evidence for reduced functional connectivity of the default network, a large-scale brain network involved in social and emotional processing, with intact connectivity of the task-positive network, used in sustained attention and goal-directed thinking. In people with autism the two networks are not negatively correlated in time, suggesting an imbalance in toggling between the two networks, possibly reflecting a disturbance of self-referential thought. A 2008 brain-imaging study found a specific pattern of signals in the cingulate cortex which differs in individuals with ASD. The underconnectivity theory of autism hypothesizes that autism is marked by underfunctioning high-level neural connections and synchronization, along with an excess of low-level processes. Evidence for this theory has been found in functional neuroimaging studies on autistic individuals and by a brain wave study that suggested that adults with ASD have local overconnectivity in the cortex and weak functional connections between the frontal lobe and the rest of the cortex. Other evidence suggests the underconnectivity is mainly within each hemisphere of the cortex and that autism is a disorder of the association cortex. From studies based on event-related potentials, transient changes to the brain's electrical activity in response to stimuli, there is considerable evidence for differences in autistic individuals with respect to attention, orientiation to auditory and visual stimuli, novelty detection, language and face processing, and information storage; several studies have found a preference for non-social stimuli. For example, magnetoencephalography studies have found evidence in autistic children of delayed responses in the brain's processing of auditory signals.
Two major categories of cognitive theories have been proposed about the links between autistic brains and behavior. The first category focuses on deficits in social cognition. The empathizing–systemizing theory postulates that autistic individuals can systemize—that is, they can develop internal rules of operation to handle events inside the brain—but are less effective at empathizing by handling events generated by other agents. An extension, the extreme male brain theory, hypothesizes that autism is an extreme case of the male brain, defined psychometrically as individuals in whom systemizing is better than empathizing; this extension is controversial, as many studies contradict the idea that baby boys and girls respond differently to people and objects. These theories are somewhat related to the earlier theory of mind approach, which hypothesizes that autistic behavior arises from an inability to ascribe mental states to oneself and others. The theory of mind hypothesis is supported by autistic children's atypical responses to the Sally–Anne test for reasoning about others' motivations, and the mirror neuron system theory of autism described in "Pathophysiology" maps well to the hypothesis. However, most studies have found no evidence of impairment in autistic individuals' ability to understand other people's basic intentions or goals; instead, data suggests that impairments are found in understanding more complex social emotions or in considering others' viewpoints. The second category focuses on nonsocial or general processing. Executive dysfunction hypothesizes that autistic behavior results in part from deficits in working memory, planning, inhibition, and other forms of executive function. Tests of core executive processes such as eye movement tasks indicate improvement from late childhood to adolescence, but performance never reaches typical adult levels. A strength of the theory is predicting stereotyped behavior and narrow interests; two weaknesses are that executive function is hard to measure and that executive function deficits have not been found in young autistic children. Weak central coherence theory hypothesizes that a limited ability to see the big picture underlies the central disturbance in autism. One strength of this theory is predicting special talents and peaks in performance in autistic people. A related theory—enhanced perceptual functioning—focuses more on the superiority of locally oriented and perceptual operations in autistic individuals. These theories map well from the underconnectivity theory of autism. Neither category is satisfactory on its own; social cognition theories poorly address autism's rigid and repetitive behaviors, while the nonsocial theories have difficulty explaining social impairment and communication difficulties. A combined theory based on multiple deficits may prove to be more useful.
U.S. and Japanese practice is to screen all children for ASD at 18 and 24 months, using autism-specific formal screening tests. In contrast, in the UK, screening targets children whose families or doctors recognize possible signs of autism. It is not known which approach is more effective. Screening tools include the Modified Checklist for Autism in Toddlers (M-CHAT), the Early Screening of Autistic Traits Questionnaire, and the First Year Inventory; initial data on M-CHAT and its predecessor CHAT on children aged 18–30 months suggests that it is best used in a clinical setting and that it has low sensitivity (many false-negatives) but good specificity (few false-positives). It may be more accurate to precede these tests with a broadband screener that does not distinguish ASD from other developmental disorders. Screening tools designed for one culture's norms for behaviors like eye contact may be inappropriate for a different culture. Although genetic screening for autism is generally still impractical, it can be considered in some cases, such as children with neurological symptoms and dysmorphic features.
Diagnosis is based on behavior, not cause or mechanism. Autism is defined in the DSM-IV-TR as exhibiting at least six symptoms total, including at least two symptoms of qualitative impairment in social interaction, at least one symptom of qualitative impairment in communication, and at least one symptom of restricted and repetitive behavior. Sample symptoms include lack of social or emotional reciprocity, stereotyped and repetitive use of language or idiosyncratic language, and persistent preoccupation with parts of objects. Onset must be prior to age three years, with delays or abnormal functioning in either social interaction, language as used in social communication, or symbolic or imaginative play. The disturbance must not be better accounted for by Rett syndrome or childhood disintegrative disorder. ICD-10 uses essentially the same definition. Several diagnostic instruments are available. Two are commonly used in autism research: the Autism Diagnostic Interview-Revised (ADI-R) is a semistructured parent interview, and the Autism Diagnostic Observation Schedule (ADOS) uses observation and interaction with the child. The Childhood Autism Rating Scale (CARS) is used widely in clinical environments to assess severity of autism based on observation of children. A pediatrician commonly performs a preliminary investigation by taking developmental history and physically examining the child. If warranted, diagnosis and evaluations are conducted with help from ASD specialists, observing and assessing cognitive, communication, family, and other factors using standardized tools, and taking into account any associated medical conditions. A pediatric neuropsychologist is often asked to assess behavior and cognitive skills, both to aid diagnosis and to help recommend educational interventions. A differential diagnosis for ASD at this stage might also consider mental retardation, hearing impairment, and a specific language impairment such as Landau–Kleffner syndrome. The presence of autism can make it harder to diagnose coexisting psychiatric disorders such as depression. Clinical genetics evaluations are often done once ASD is diagnosed, particularly when other symptoms already suggest a genetic cause. Although genetic technology allows clinical geneticists to link an estimated 40% of cases to genetic causes, consensus guidelines in the U.S. and UK are limited to high-resolution chromosome and fragile X testing. A genotype-first model of diagnosis has been proposed, which would routinely assess the genome's copy number variations. As new genetic tests are developed several ethical, legal, and social issues will emerge. Commercial availability of tests may precede adequate understanding of how to use test results, given the complexity of autism's genetics. Metabolic and neuroimaging tests are sometimes helpful, but are not routine. ASD can sometimes be diagnosed by age 14 months, although diagnosis becomes increasingly stable over the first three years of life: for example, a one-year-old who meets diagnostic criteria for ASD is less likely than a three-year-old to continue to do so a few years later. In the UK the National Autism Plan for Children recommends at most 30 weeks from first concern to completed diagnosis and assessment, though few cases are handled that quickly in practice. A 2009 U.S. study found the average age of formal ASD diagnosis was 5.7 years, far above recommendations, and that 27% of children remained undiagnosed at age 8 years. Although the symptoms of autism and ASD begin early in childhood, they are sometimes missed; years later, adults may seek diagnoses to help them or their friends and family understand themselves, to help their employers make adjustments, or in some locations to claim disability living allowances or other benefits. Underdiagnosis and overdiagnosis are problems in marginal cases, and much of the recent increase in the number of reported ASD cases is likely due to changes in diagnostic practices. The increasing popularity of drug treatment options and the expansion of benefits has given providers incentives to diagnose ASD, resulting in some overdiagnosis of children with uncertain symptoms. Conversely, the cost of screening and diagnosis and the challenge of obtaining payment can inhibit or delay diagnosis. It is particularly hard to diagnose autism among the visually impaired, partly because some of its diagnostic criteria depend on vision, and partly because autistic symptoms overlap with those of common blindness syndromes.
The main goals of treatment are to lessen associated deficits and family distress, and to increase quality of life and functional independence. No single treatment is best and treatment is typically tailored to the child's needs. Families and the educational system are the main resources for treatment. Studies of interventions have methodological problems that prevent definitive conclusions about efficacy. Although many psychosocial interventions have some positive evidence, suggesting that some form of treatment is preferable to no treatment, the methodological quality of systematic reviews of these studies has generally been poor, their clinical results are mostly tentative, and there is little evidence for the relative effectiveness of treatment options. Intensive, sustained special education programs and behavior therapy early in life can help children acquire self-care, social, and job skills, and often improve functioning and decrease symptom severity and maladaptive behaviors; claims that intervention by around age three years is crucial are not substantiated. Available approaches include applied behavior analysis (ABA), developmental models, structured teaching, speech and language therapy, social skills therapy, and occupational therapy. Educational interventions have some effectiveness in children: intensive ABA treatment has demonstrated effectiveness in enhancing global functioning in preschool children and is well-established for improving intellectual performance of young children. Neuropsychological reports are often poorly communicated to educators, resulting in a gap between what a report recommends and what education is provided. It is not known whether treatment programs for children lead to significant improvements after the children grow up, and the limited research on the effectiveness of adult residential programs shows mixed results. Many medications are used to treat ASD symptoms that interfere with integrating a child into home or school when behavioral treatment fails. More than half of U.S. children diagnosed with ASD are prescribed psychoactive drugs or anticonvulsants, with the most common drug classes being antidepressants, stimulants, and antipsychotics. Aside from antipsychotics, there is scant reliable research about the effectiveness or safety of drug treatments for adolescents and adults with ASD. A person with ASD may respond atypically to medications, the medications can have adverse effects, and no known medication relieves autism's core symptoms of social and communication impairments. Experiments in mice have reversed or reduced some symptoms related to autism by replacing or modulating gene function after birth, suggesting the possibility of targeting therapies to specific rare mutations known to cause autism. Although many alternative therapies and interventions are available, few are supported by scientific studies. Treatment approaches have little empirical support in quality-of-life contexts, and many programs focus on success measures that lack predictive validity and real-world relevance. Scientific evidence appears to matter less to service providers than program marketing, training availability, and parent requests. Though most alternative treatments, such as melatonin, have only mild adverse effects some may place the child at risk. A 2008 study found that compared to their peers, autistic boys have significantly thinner bones if on casein-free diets; in 2005, botched chelation therapy killed a five-year-old child with autism. Treatment is expensive; indirect costs are more so. For someone born in 2000, a U.S. study estimated an average lifetime cost of $ (net present value in dollars, inflation-adjusted from 2003 estimate), with about 10% medical care, 30% extra education and other care, and 60% lost economic productivity. Publicly supported programs are often inadequate or inappropriate for a given child, and unreimbursed out-of-pocket medical or therapy expenses are associated with likelihood of family financial problems; one 2008 U.S. study found a 14% average loss of annual income in families of children with ASD, and a related study found that ASD is associated with higher probability that child care problems will greatly affect parental employment. U.S. states increasingly require private health insurance to cover autism services, shifting costs from publicly funded education programs to privately funded health insurance. After childhood, key treatment issues include residential care, job training and placement, sexuality, social skills, and estate planning.
No cure is known. Children recover occasionally, so that they lose their diagnosis of ASD; this occurs sometimes after intensive treatment and sometimes not. It is not known how often recovery happens; reported rates in unselected samples of children with ASD have ranged from 3% to 25%. A few autistic children have acquired speech at age 5 or older. Most children with autism lack social support, meaningful relationships, future employment opportunities or self-determination. Although core difficulties tend to persist, symptoms often become less severe with age. Few high-quality studies address long-term prognosis. Some adults show modest improvement in communication skills, but a few decline; no study has focused on autism after midlife. Acquiring language before age six, having an IQ above 50, and having a marketable skill all predict better outcomes; independent living is unlikely with severe autism. A 2004 British study of 68 adults who were diagnosed before 1980 as autistic children with IQ above 50 found that 12% achieved a high level of independence as adults, 10% had some friends and were generally in work but required some support, 19% had some independence but were generally living at home and needed considerable support and supervision in daily living, 46% needed specialist residential provision from facilities specializing in ASD with a high level of support and very limited autonomy, and 12% needed high-level hospital care. A 2005 Swedish study of 78 adults that did not exclude low IQ found worse prognosis; for example, only 4% achieved independence. A 2008 Canadian study of 48 young adults diagnosed with ASD as preschoolers found outcomes ranging through poor (46%), fair (32%), good (17%), and very good (4%); 56% of these young adults had been employed at some point during their lives, mostly in volunteer, sheltered or part-time work. Changes in diagnostic practice and increased availability of effective early intervention make it unclear whether these findings can be generalized to recently diagnosed children.
Most recent reviews tend to estimate a prevalence of 1–2 per 1,000 for autism and close to 6 per 1,000 for ASD; because of inadequate data, these numbers may underestimate ASD's true prevalence. PDD-NOS's prevalence has been estimated at 3.7 per 1,000, Asperger syndrome at roughly 0.6 per 1,000, and childhood disintegrative disorder at 0.02 per 1,000. The number of reported cases of autism increased dramatically in the 1990s and early 2000s. This increase is largely attributable to changes in diagnostic practices, referral patterns, availability of services, age at diagnosis, and public awareness, though unidentified environmental risk factors cannot be ruled out. The available evidence does not rule out the possibility that autism's true prevalence has increased; a real increase would suggest directing more attention and funding toward changing environmental factors instead of continuing to focus on genetics. Boys are at higher risk for ASD than girls. The sex ratio averages 4.3:1 and is greatly modified by cognitive impairment: it may be close to 2:1 with mental retardation and more than 5.5:1 without. Although the evidence does not implicate any single pregnancy-related risk factor as a cause of autism, the risk of autism is associated with advanced age in either parent, and with diabetes, bleeding, and use of psychiatric drugs in the mother during pregnancy. The risk is greater with older fathers than with older mothers; two potential explanations are the known increase in mutation burden in older sperm, and the hypothesis that men marry later if they carry genetic liability and show some signs of autism. Most professionals believe that race, ethnicity, and socioeconomic background do not affect the occurrence of autism.
A few examples of autistic symptoms and treatments were described long before autism was named. The "Table Talk" of Martin Luther contains the story of a 12-year-old boy who may have been severely autistic. According to Luther's notetaker Mathesius, Luther thought the boy was a soulless mass of flesh possessed by the devil, and suggested that he be suffocated. The earliest well-documented case of autism is that of Hugh Blair of Borgue, as detailed in a 1747 court case in which his brother successfully petitioned to annul Blair's marriage to gain Blair's inheritance. The Wild Boy of Aveyron, a feral child caught in 1798, showed several signs of autism; the medical student Jean Itard treated him with a behavioral program designed to help him form social attachments and to induce speech via imitation. The New Latin word "autismus" (English translation "autism") was coined by the Swiss psychiatrist Eugen Bleuler in 1910 as he was defining symptoms of schizophrenia. He derived it from the Greek word "autós" (αὐτός, meaning "self"), and used it to mean morbid self-admiration, referring to "autistic withdrawal of the patient to his fantasies, against which any influence from outside becomes an intolerable disturbance". The word "autism" first took its modern sense in 1938 when Hans Asperger of the Vienna University Hospital adopted Bleuler's terminology "autistic psychopaths" in a lecture in German about child psychology. Asperger was investigating an ASD now known as Asperger syndrome, though for various reasons it was not widely recognized as a separate diagnosis until 1981. Leo Kanner of the Johns Hopkins Hospital first used "autism" in its modern sense in English when he introduced the label "early infantile autism" in a 1943 report of 11 children with striking behavioral similarities. Almost all the characteristics described in Kanner's first paper on the subject, notably "autistic aloneness" and "insistence on sameness", are still regarded as typical of the autistic spectrum of disorders. It is not known whether Kanner derived the term independently of Asperger. Kanner's reuse of "autism" led to decades of confused terminology like "infantile schizophrenia", and child psychiatry's focus on maternal deprivation led to misconceptions of autism as an infant's response to "refrigerator mothers". Starting in the late 1960s autism was established as a separate syndrome by demonstrating that it is lifelong, distinguishing it from mental retardation and schizophrenia and from other developmental disorders, and demonstrating the benefits of involving parents in active programs of therapy. As late as the mid-1970s there was little evidence of a genetic role in autism; now it is thought to be one of the most heritable of all psychiatric conditions. Although the rise of parent organizations and the destigmatization of childhood ASD have deeply affected how we view ASD, parents continue to feel social stigma in situations where their autistic children's behaviors are perceived negatively by others, and many primary care physicians and medical specialists still express some beliefs consistent with outdated autism research. The Internet has helped autistic individuals bypass nonverbal cues and emotional sharing that they find so hard to deal with, and has given them a way to form online communities and work remotely. Sociological and cultural aspects of autism have developed: some in the community seek a cure, while others believe that autism is simply another way of being.
The albedo of an object is a measure of how strongly it reflects light from light sources such as the Sun. It is therefore a more specific form of the term reflectivity. Albedo is defined as the ratio of total-reflected to incident electromagnetic radiation. It is a unitless measure indicative of a surface's or body's diffuse reflectivity. The word is derived from Latin "albedo" "whiteness", in turn from "albus" "white", and was introduced into optics by Johann Heinrich Lambert in his 1760 work "Photometria". The range of possible values is from 0 (dark) to 1 (bright). The albedo is an important concept in climatology and astronomy, as well as in computer graphics and computer vision. In climatology it is sometimes expressed as a percentage. Its value depends on the frequency of radiation considered: unqualified, it usually refers to some appropriate average across the spectrum of visible light. In general, the albedo depends on the direction and directional distribution of incoming radiation. Exceptions are Lambertian surfaces, which scatter radiation in all directions in a cosine function, so their albedo does not depend on the incoming distribution. In realistic cases, a bidirectional reflectance distribution function (BRDF) is required to characterize the scattering properties of a surface accurately, although albedos are a very useful first approximation.
Albedos of typical materials in visible light range from up to 0.9 for fresh snow, to about 0.04 for charcoal, one of the darkest substances. Deeply shadowed cavities can achieve an effective albedo approaching the zero of a blackbody. When seen from a distance, the ocean surface has a low albedo, as do most forests, while desert areas have some of the highest albedos among landforms. Most land areas are in an albedo range of 0.1 to 0.4. The average albedo of the Earth is about 0.3. This is far higher than for the ocean primarily because of the contribution of clouds. Human activities have changed the albedo (via forest clearance and farming, for example) of various areas around the globe. However, quantification of this effect on the global scale is difficult. The classic example of albedo effect is the snow-temperature feedback. If a snow-covered area warms and the snow melts, the albedo decreases, more sunlight is absorbed, and the temperature tends to increase. The converse is true: if snow forms, a cooling cycle happens. The intensity of the albedo effect depends on the size of the change in albedo and the amount of insolation; for this reason it can be potentially very large in the tropics. The Earth's surface albedo is regularly estimated via Earth observation satellite sensors such as NASA's MODIS instruments onboard the Terra and Aqua satellites. As the total amount of reflected radiation cannot be directly measured by satellite, a mathematical model of the BRDF is used to translate a sample set of satellite reflectance measurements into estimates of directional-hemispherical reflectance and bi-hemispherical reflectance. (e. g., The Earth's average surface temperature due to its albedo and the greenhouse effect is currently about 15°C. For the frozen (more reflective) planet the average temperature is below -40°C (If only all continents being completely covered by glaciers - the mean temperature is about 0°C). The simulation for (more absorptive) aquaplanet shows the average temperature close to 27°C.
It has been shown that for many applications involving terrestrial albedo, the albedo at a particular solar zenith angle formula_1 can reasonably be approximated by the proportionate sum of two terms: the directional-hemispherical reflectance at that solar zenith angle, formula_2, and the bi-hemispherical reflectance, formula_3 the proportion concerned being defined as the proportion of diffuse illumination formula_4. Directional-hemispherical reflectance is sometimes referred to as black-sky albedo and bi-hemispherical reflectance as white sky albedo. These terms are important because they allow the albedo to be calculated for any given illumination conditions from a knowledge of the intrinsic properties of the surface.
The albedos of planets, satellites and asteroids can be used to infer much about their properties. The study of albedos, their dependence on wavelength, lighting angle ("phase angle"), and variation in time comprises a major part of the astronomical field of photometry. For small and far objects that cannot be resolved by telescopes, much of what we know comes from the study of their albedos. For example, the absolute albedo can indicate the surface ice content of outer solar system objects, the variation of albedo with phase angle gives information about regolith properties, while unusually high radar albedo is indicative of high metallic content in asteroids. Enceladus, a moon of Saturn, has one of the highest known albedos of any body in the Solar system, with 99% of EM radiation reflected. Another notable high albedo body is Eris, with an albedo of 0.86. Many small objects in the outer solar system and asteroid belt have low albedos down to about 0.05. A typical comet nucleus has an albedo of 0.04. Such a dark surface is thought to be indicative of a primitive and heavily space weathered surface containing some organic compounds. The overall albedo of the Moon is around 0.072, but it is strongly directional and non-Lambertian, displaying also a strong opposition effect. While such reflectance properties are different from those of any terrestrial terrains, they are typical of the regolith surfaces of airless solar system bodies. Two common albedos that are used in astronomy are the (V-band) geometric albedo (measuring brightness when illumination comes from directly behind the observer) and the Bond albedo (measuring total proportion of electromagnetic energy reflected). Their values can differ significantly, which is a common source of confusion. In detailed studies, the directional reflectance properties of astronomical bodies are often expressed in terms of the five Hapke parameters which semi-empirically describe the variation of albedo with phase angle, including a characterization of the opposition effect of regolith surfaces.
Single scattering albedo is used to define scattering of electromagnetic waves on small particles. It depends on properties of the material (refractive index); the size of the particle or particles; and the wavelength of the incoming radiation. Albedo also refers to the white, spongy inner lining of a citrus fruit rind. According to Dr. Renee M. Goodrich, associate professor of food science and human nutrition at the University of Florida, the albedo is rich in the soluble fiber pectin and contains vitamin C.
Although the albedo-temperature effect is most famous in colder regions of Earth, because more snow falls there, it is actually much stronger in tropical regions because in the tropics there is consistently more sunlight. When ranchers cut down dark, tropical rainforest trees to replace them with even darker soil in order to grow crops, the average temperature of the area increases up to 3 °C (5.4 °F) year-round, although part of the effect is due to changed evaporation (latent heat flux).
Because trees tend to have a low albedo, removing forests would tend to increase albedo and thereby could produce localized climate cooling. Cloud feedbacks further complicate the issue. In seasonally snow-covered zones, winter albedos of treeless areas are 10% to 50% higher than nearby forested areas because snow does not cover the trees as readily. Deciduous trees have an albedo value of about 0.15 to 0.18 while coniferous trees have a value of about 0.09 to 0.15. The difference between deciduous and coniferous is because coniferous trees are darker in general and have cone-shaped crowns. The shape of these crowns trap radiant energy more effectively than deciduous trees. Studies by the Hadley Centre have investigated the relative (generally warming) effect of albedo change and (cooling) effect of carbon sequestration on planting forests. They found that new forests in tropical and midlatitude areas tended to cool; new forests in high latitudes (e.g. Siberia) were neutral or perhaps warming.
Water reflects light very differently from typical terrestrial materials. The reflectivity of a water surface is calculated using the Fresnel equations (see graph). At the scale of the wavelength of light even wavy water is always smooth so the light is reflected in a specular manner (not diffusely). The glint of light off water is a commonplace effect of this. At small angles of incident light, waviness results in reduced reflectivity because of the steepness of the reflectivity-vs.-incident-angle curve and a locally increased average incident angle. Although the reflectivity of water is very low at low and medium angles of incident light, it increases tremendously at high angles of incident light such as occur on the illuminated side of the Earth near the terminator (early morning, late afternoon and near the poles). However, as mentioned above, waviness causes an appreciable reduction. Since the light specularly reflected from water does not usually reach the viewer, water is usually considered to have a very low albedo in spite of its high reflectivity at high angles of incident light. Note that white caps on waves look white (and have high albedo) because the water is foamed up (not smooth at the scale of the wavelength of light) so the Fresnel equations do not apply. Fresh ‘black’ ice exhibits Fresnel reflection.
Clouds are another source of albedo that play into the global warming equation. Different types of clouds have different albedo values, theoretically ranging from a minimum of near 0 to a maximum approaching 0.8. "On any given day, about half of Earth is covered by clouds, which reflect more sunlight than land and water. Clouds keep Earth cool by reflecting sunlight, but they can also serve as blankets to trap warmth." Albedo and climate in some areas are already affected by artificial clouds, such as those created by the contrails of heavy commercial airliner traffic. A study following the burning of the Kuwaiti oil fields by Saddam Hussein showed that temperatures under the burning oil fires were as much as 10oC colder than temperatures several miles away under clear skies.
"A" can be traced to a pictogram of an ox head in Egyptian hieroglyph or the Proto-Sinaitic alphabet. In 1600 B.C. the Phoenician alphabet's letter had a linear form that served as the base for some later forms. Its name must have corresponded closely to the Hebrew or Arabic aleph. When the Ancient Greeks adopted the alphabet, they had no use for the glottal stop that the letter had denoted in Phoenician and other Semitic languages, so they used the sign to represent the vowel, and kept its name with a minor change (alpha). In the earliest Greek inscriptions after the Greek Dark Ages, dating to the 8th century BC, the letter rests upon its side, but in the Greek alphabet of later times it generally resembles the modern capital letter, although many local varieties can be distinguished by the shortening of one leg, or by the angle at which the cross line is set. The Etruscans brought the Greek alphabet to their civilization in the Italian Peninsula and left the letter unchanged. The Romans later adopted the Etruscan alphabet to write the Latin language, and the resulting letter was preserved in the modern Latin alphabet used to write many languages, including English. The letter has two minuscule (lower-case) forms. The form used in most current handwriting consists of a circle and vertical stoke (), called Latin alpha or "script a". Most printed material uses a form consisting of a small loop with an arc over it (). Both derive from the majuscule (capital) form. In Greek handwriting, it was common to join the left leg and horizontal stroke into a single loop, as demonstrated by the Uncial version shown. Many fonts then made the right leg vertical. In some of these, the serif that began the right leg stroke developed into an arc, resulting in the printed form, while in others it was dropped, resulting in the modern handwritten form.
In English, "a" by itself frequently denotes the near-open front unrounded vowel () as in "pad", the open back unrounded vowel () as in "father", or, in concert with a later orthographic vowel, the diphthong as in "ace" and "major", due to effects of the great vowel shift. In most other languages that use the Latin alphabet, "a" denotes an open central unrounded vowel (). In the International Phonetic Alphabet, variants of "a" denote various vowels. In X-SAMPA, capital "A" denotes the open back unrounded vowel and lowercase "a" denotes the open front unrounded vowel. "A" is the third common used letter in English, and the second most common in Spanish and French. In one study, on average, about 3.68% of letters used in English tend to be ‹a›s, while the number is 6.22% in Spanish and 3.95% in French. "A" is often used to denote something or someone of a better or more prestigious quality or status: A-, A or A+, the best grade that can be assigned by teachers for students' schoolwork; A grade for clean restaurants; A-List celebrities, etc. Such associations can have a motivating effect as exposure to the letter A has been found to improve performance, when compared with other letters. A turned "a" () is used by the International Phonetic Alphabet for the near-open central vowel, while a turned capital "A" ("∀") is used in predicate logic to specify universal quantification.
Alabama is a state located in the southeastern region of the United States of America. It is bordered by Tennessee to the north, Georgia to the east, Florida and the Gulf of Mexico to the south, and Mississippi to the west. Alabama ranks 30th in total land area and ranks second in the size of its inland waterways. The state ranks 23rd in population with almost 4.6 million residents in 2006. From the American Civil War until World War II, Alabama, like many Southern states, suffered economic hardship, in part because of continued dependence on agriculture. White rural interests dominated the state legislature until the 1960s, while urban interests and African Americans were underrepresented. Following World War II, Alabama experienced significant recovery as the economy of the state transitioned from agriculture to diversified interests in heavy manufacturing, mineral extraction, education, and technology, as well as the establishment or expansion of multiple military installations, primarily those of the U.S. Army and U.S. Air Force. The state has heavily invested in aerospace, education, health care, and banking, and various heavy industries including automobile manufacturing, mineral extraction, steel production and fabrication. Alabama is unofficially nicknamed the "Yellowhammer State", which is also the name of the state bird. Alabama is also known as the "Heart of Dixie". The state tree is the Longleaf Pine, the state flower is the Camellia. The capital of Alabama is Montgomery, and the largest city by population is Birmingham. The largest city by total land area is Huntsville. The oldest city is Mobile.
The Alabama, a Muskogean tribe whose members lived just below the confluence of the Coosa and Tallapoosa Rivers on the upper reaches of the Alabama River, served as the etymological source of the names of the river and state. In the Alabama language, the word for an Alabama person is "Albaamo" (or variously "Albaama" or "Albàamo" in different dialects; the plural form "Alabama persons" is "Albaamaha"). The word "Alabama" is believed to have originated from the Choctaw language and was later adopted by the Alabama tribe as their name. The spelling of the word varies significantly between sources. The first usage appears in three accounts of the Hernando de Soto expedition of 1540 with Garcilasso de la Vega using "Alibamo" while the Knight of Elvas and Rodrigo Ranjel wrote "Alibamu" and "Limamu", respectively. As early as 1702, the tribe was known to the French as "Alibamon" with French maps identifying the river as "Rivière des Alibamons". Other spellings of the appellation have included "Alibamu", "Alabamo", "Albama", "Alebamon", "Alibama", "Alibamou", "Alabamu", and "Allibamou". Although the origin of "Alabama" was evident, the meaning of the tribe's name was not always clear. An article without a byline appearing in the "Jacksonville Republican" on July 27, 1842, originated the idea that the meaning was "Here We Rest." This notion was popularized in the 1850s through the writings of Alexander Beaufort Meek. Experts in the Muskogean languages have been unable to find any evidence that would support this translation. It is now generally accepted that the word comes from the Choctaw words "alba" (meaning "plants" or "weeds") and "amo" (meaning "to cut", "to trim", or "to gather"). This results in translations such as "clearers of the thicket" or even "herb gatherers" which may refer to clearing of land for the purpose of planting crops or to collection of medicinal plants by medicine men.
Among the Native American people once living in the area of present day Alabama were Alabama ("Alibamu"), Cherokee, Chickasaw, Choctaw, Creek, Koasati, and Mobile. Trade with the Northeast via the Ohio River began during the Burial Mound Period (1000 BC-700 AD) and continued until European contact. The agrarian Mississippian culture covered most of the state from 1000 to 1600 AD, with one of its major centers being at the Moundville Archaeological Site in Moundville, Alabama. Artifacts recovered from archaeological excavations at Moundville were a major component in the formulation of the Southeastern Ceremonial Complex. Contrary to popular belief, this development appears to have no direct links to Mesoamerica, but developed independently. This Ceremonial Complex represents a major component of the religion of the Mississippian peoples, and is one of the primary means by which their religion is understood. The French founded the first European settlement in the state with the establishment of Mobile in 1702. Southern Alabama was French from 1702 to 1763, part of British West Florida from 1763 to 1780, and part of Spanish West Florida from 1780 to 1814. Northern and central Alabama was part of British Georgia from 1763 to 1783 and part of the American Mississippi territory thereafter. Its statehood was delayed by the lack of a coastline; rectified when Andrew Jackson captured Spanish Mobile in 1814. Alabama was the twenty-second state, admitted to the Union in 1819. Its constitution provided for universal suffrage for white men. Alabama was part of the new frontier in the 1820s and 1830s. Settlers rapidly arrived to take advantage of its fertile soil. Planters brought slaves with them, and traders brought in more from the Upper South as the cotton plantations expanded. The economy of the central "Black Belt" was built around large cotton plantations whose owners built their wealth on slave labor. It was named for the dark, productive soil. Elsewhere poor whites were subsistence farmers. According to the 1860 census, enslaved Africans comprised 45% of the state's population of 964,201. There were only 2,690 free persons of color. In 1861 Alabama declared its secession from the Union and joined the Confederate States of America. While few battles were fought in the state, Alabama contributed about 120,000 soldiers to the Civil War. All the slaves were freed by 1865. Following Reconstruction, Alabama was restored to the Union in 1868. After the Civil War, the state was still chiefly rural and tied to cotton. Planters resisted working with free labor and sought to re-establish controls over African Americans. Whites used paramilitary groups, Jim Crow laws and segregation to reduce freedoms of African Americans and restore their own dominance. In its new constitution of 1901, the legislature effectively disfranchised African Americans through voting restrictions. While the planter class had engaged poor whites in supporting these efforts, the new restrictions resulted in disfranchising poor whites as well. By 1941, a total of more whites than blacks had been disfranchised: 600,000 whites to 520,000 blacks. This was due mostly to effects of the cumulative poll tax. The damage to the African-American community was pervasive, as nearly all its citizens lost the ability to vote. In 1900, fourteen Black Belt counties (which were primarily African American) had more than 79,000 voters on the rolls. By June 1, 1903, the number of registered voters had dropped to 1,081. In 1900, Alabama had more than 181,000 African Americans eligible to vote. By 1903, only 2,980 had managed to "qualify" to register, although at least 74,000 black voters were literate. The shut out was long-lasting. The disfranchisement was ended only by African Americans leading the Civil Rights Movement and gaining Federal legislation in the mid-1960s to protect their voting and civil rights. The Voting Rights Act of 1965 also protected the suffrage of poor whites. The rural-dominated legislature continued to underfund schools and services for African Americans in the segregated state, but did not relieve them of paying taxes. Continued racial discrimination, agricultural depression, and the failure of the cotton crops due to boll weevil infestation led tens of thousands of African Americans to seek out opportunities in northern cities. They left Alabama in the early 20th century as part of the Great Migration to industrial jobs and better futures in northern industrial cities. The population growth rate in Alabama (see "Historical Populations" table below) dropped by nearly half from 1910–1920, reflecting the effect of outmigration. At the same time, many rural whites and blacks migrated to the city of Birmingham for work in new industrial jobs. It experienced such rapid growth that it was nicknamed "The Magic City". By the 1920s, Birmingham was the 19th largest city in the U.S. and held more than 30% of the population of the state. Heavy industry and mining were the basis of the economy. Despite massive population changes in the state from 1901 to 1961, the rural-dominated legislature refused to reapportion House and Senate seats based on population. They held on to old representation to maintain political and economic power in agricultural areas. In addition, the state legislature gerrymandered the few Birmingham legislative seats to ensure election by persons living outside of Birmingham. One result was that Jefferson County, containing Birmingham's industrial and economic powerhouse, contributed more than one-third of all tax revenue to the state. Urban interests were consistently underrepresented in the legislature. A 1960 study noted that because of rural domination, "A minority of about 25 per cent of the total state population is in majority control of the Alabama legislature." African Americans were presumed partial to Republicans for historical reasons, but they were disenfranchised. White Alabamans still felt bitter towards the Republican Party in the aftermath of the Civil War and Reconstruction. These factors created a longstanding tradition that any candidate who wanted to be viable with white voters had to run as a Democrat regardless of political beliefs. The state continued as one-party Democratic for more than a century after Reconstruction ended. It produced a number of national leaders. Industrial development related to the demands of World War II brought prosperity. Cotton faded in importance as the state developed a manufacturing and service base. In the 1960s under Governor George Wallace, many whites in the state opposed integration efforts. During the Civil Rights Movement, African Americans achieved a protection of voting and other civil rights through the passage of the national Civil Rights Act of 1964, and the Voting Rights Act of 1965. "De jure" segregation ended in the states as Jim Crow laws were invalidated or repealed. Under the Voting Rights Act of 1965, cases were filed in Federal courts to force Alabama to properly redistrict by population both the state legislature House and Senate. In 1972, for the first time since 1901, the legislature implemented the Alabama constitution's provision for periodic redistricting based on population. This benefited the many urban areas that had developed, and all in the population who had been underrepresented for more than 60 years. After 1972, the state's white voters shifted much of their support to Republican candidates in presidential elections (as also occurred in neighboring southern states). Since 1990 the majority of whites in the state have also voted increasingly Republican in state elections, although Democrats are still the majority party in both houses of the legislature.
Alabama is the thirtieth largest state in the United States with 52,423 square miles (135,775 km²) of total area: 3.19% of the area is water, making Alabama twenty-third in the amount of surface water, also giving it the second largest inland waterway system in the United States. About three-fifths of the land area is a gentle plain with a general descent towards the Mississippi River and the Gulf of Mexico. The North Alabama region is mostly mountainous, with the Tennessee River cutting a large valley creating numerous creeks, streams, rivers, mountains, and lakes. The states bordering Alabama are Tennessee to the north; Georgia to the east; Florida to the south; and Mississippi to the west. Alabama has coastline at the Gulf of Mexico, in the extreme southern edge of the state. Alabama ranges in elevation from sea level at Mobile Bay to over 1,800 feet (550 m) in the Appalachian Mountains in the northeast. The highest point is Mount Cheaha, at a height of. Alabama's land consists of of forest or 67% of total land area. Suburban Baldwin County, along the Gulf Coast, is the largest county in the state in both land area and water area. Areas in Alabama administered by the National Park Service include Horseshoe Bend National Military Park near Alexander City; Little River Canyon National Preserve near Fort Payne; Russell Cave National Monument in Bridgeport; Tuskegee Airmen National Historic Site in Tuskegee; and Tuskegee Institute National Historic Site near Tuskegee. Additionally, Alabama has four National Forests including Conecuh, Talladega, Tuskegee, and William B. Bankhead. Alabama also contains the Natchez Trace Parkway, the Selma To Montgomery National Historic Trail, and the Trail Of Tears National Historic Trail. A notable natural wonder in Alabama is "Natural Bridge" rock, the longest natural bridge east of the Rockies, located just south of Haleyville, in Winston County. A -wide meteorite impact crater is located in Elmore County, just north of Montgomery. This is the Wetumpka crater, which is the site of "Alabama's greatest natural disaster". A -wide meteorite hit the area about 80 million years ago. The hills just east of downtown Wetumpka showcase the eroded remains of the impact crater that was blasted into the bedrock, with the area labeled the Wetumpka crater or astrobleme ("star-wound") because of the concentric rings of fractures and zones of shattered rock that can be found beneath the surface. In 2002, Christian Koeberl with the Institute of Geochemistry University of Vienna published evidence and established the site as an internationally recognized impact crater.
The state is classified as humid subtropical ("Cfa") under the Koppen Climate Classification. The average annual temperature is 64 °F (18 °C). Temperatures tend to be warmer in the southern part of the state with its proximity to the Gulf of Mexico, while the northern parts of the state, especially in the Appalachian Mountains in the northeast, tend to be slightly cooler. Generally, Alabama has very hot summers and mild winters with copious precipitation throughout the year. Alabama receives an average of of rainfall annually and enjoys a lengthy growing season of up to 300 days in the southern part of the state. Summers in Alabama are among the hottest in the United States, with high temperatures averaging over throughout the summer in some parts of the state. Alabama is also prone to tropical storms and even hurricanes. Areas of the state far away from the Gulf are not immune to the effects of the storms, which often dump tremendous amounts of rain as they move inland and weaken. South Alabama reports more thunderstorms than any part of the U.S. The Gulf Coast, around Mobile Bay, averages between 70 and 80 days per year with thunder reported. This activity decreases somewhat further north in the state, but even the far north of the state reports thunder on about 60 days per year. Occasionally, thunderstorms are severe with frequent lightning and large hail – the central and northern parts of the state are most vulnerable to this type of storm. Alabama ranks seventh in the number of deaths from lightning and ninth in the number of deaths from lightning strikes per capita. Sometimes tornadoes occur – these are common throughout the state, although the peak season for tornadoes varies from the northern to southern parts of the state. Alabama shares the dubious distinction, with Kansas, of having reported more EF5 tornadoes than any other state – according to statistics from the National Climatic Data Center for the period January 1, 1950, to October 31, 2006. An F5 tornado is the most powerful of its kind. Several long – tracked F5 tornadoes have contributed to Alabama reporting more tornado fatalities than any other state except for Texas and Mississippi. The Super Outbreak in March 1974, badly affected Alabama. The northern part of the state – along the Tennessee Valley – is one of the areas in the US most vulnerable to violent tornadoes. The area of Alabama and Mississippi most affected by tornadoes is sometimes referred to as Dixie Alley, as distinct from the Tornado Alley of the Southern Plains. Alabama is one of the few places in the world that has a secondary tornado season (November and December) along with the spring severe weather season. Winters are generally mild in Alabama, as they are throughout most of the southeastern United States, with average January low temperatures around in Mobile and around in Birmingham. Although snow is a rare event in much of Alabama, areas of the state north of Montgomery may receive a dusting of snow a few times every winter, with an occasional moderately heavy snowfall every few years. For example, the annual average snowfall for the Birmingham area is 2 inches per year. In the southern Gulf coast, snowfall is less frequent, sometimes going several years without any snowfall.
The United States Census Bureau, as of July 1, 2008, estimated Alabama's population at 4,661,900, which represents an increase of 214,545, or 4.8%, since the last census in 2000. This includes a natural increase since the last census of 121,054 people (that is 502,457 births minus 381,403 deaths) and an increase due to net migration of 104,991 people into the state. Immigration from outside the United States resulted in a net increase of 31,180 people, and migration within the country produced a net gain of 73,811 people. The state had 108,000 foreign-born (2.4% of the state population), of which an estimated 22.2% were illegal immigrants (24,000). The center of population of Alabama is located in Chilton County, outside of the town of Jemison, an area known as Jemison Division.
Alabama is located in the middle of the Bible Belt. In a 2007 survey, nearly 70% of respondents could name all four of the Christian Gospels. Of those who indicated a religious preference, 59% said they possessed a "full understanding" of their faith and needed no further learning. In a 2007 poll, 92% of Alabamians reported having at least some confidence in churches in the state. The Mobile area is notable for its large percentage of Catholics, owing to the area's unique early history under French and Spanish rule. Today, a majority of Alabamians identify themselves as Protestants. In the 2008 American Religious Identification Survey, 80% of Alabama respondents reported their religion as "Other Christian" (survey's label), 6% as Catholic, and 11% as having no religion at all.
According to the United States Bureau of Economic Analysis, the 2008 total gross state product was $170 billion, or $29,411 per capita. Alabama's 2008 GDP increased 0.7% from the previous year. The single largest increase came in the area of information. In 1999, per capita income for the state was $18,189. Alabama's agricultural outputs include poultry and eggs, cattle, plant nursery items, peanuts, cotton, grains such as corn and sorghum, vegetables, milk, soybeans, and peaches. Although known as "The Cotton State", Alabama ranks between eight and ten in national cotton production, according to various reports, with Texas, Georgia and Mississippi comprising the top three. Alabama's industrial outputs include iron and steel products (including cast-iron and steel pipe); paper, lumber, and wood products; mining (mostly coal); plastic products; cars and trucks; and apparel. Also, Alabama produces aerospace and electronic products, mostly in the Huntsville area, location of NASA George C. Marshall Space Flight Center and the US Army Aviation and Missile Command, headquartered at Redstone Arsenal. Alabama contains the largest industrial growth corridor in the nation, including the surrounding states of Tennessee, Mississippi, Florida, and Georgia. Most of this growth is due to Alabama's rapidly expanding automotive manufacturing industry. Headquartered in the state are Honda Manufacturing of Alabama, Hyundai Motor Manufacturing Alabama, Mercedes-Benz U.S. International, and Toyota Motor Manufacturing Alabama. Since 1993, the automobile industry has generated more than 67,800 new jobs in the state. Alabama currently ranks 4th in the nation in automobile output. In the 1970s and 1980s, Birmingham's economy was transformed by investments in bio-technology and medical research at the University of Alabama at Birmingham (UAB) and its adjacent hospital. The UAB Hospital is a Level I trauma center providing health care and breakthrough medical research. UAB is now the area's largest employer and the largest in Alabama with a workforce of about 20,000. Health care services provider HealthSouth is also headquartered in the city. Birmingham is also a leading banking center, headquarters of the Regions Financial Corporation. Birmingham-based Compass Banchshares was acquired by Madrid-based BBVA in September 2007; the headquarters of the new BBVA Compass Bank remains in Birmingham. SouthTrust, another large bank headquartered in Birmingham, was acquired by Wachovia in 2004. The city still has major operations as one of the regional headquarters of Wachovia. In November 2006, Regions Financial merged with AmSouth Bancorporation, which was also headquartered in Birmingham. They formed the eighth largest U.S. bank based on by total assets. Nearly a dozen smaller banks are also headquartered in the Magic City, such as Superior Bank and New South Federal Savings Bank. Telecommunications provider AT&T, formerly BellSouth, has a major presence with several large offices in the metropolitan area. Major insurance providers: Protective Life, Infinity Property & Casualty and ProAssurance among others, are headquartered in Birmingham and employ a large number of people in Greater Birmingham. The city is also a powerhouse of construction and engineering companies, including BE&K and B. L. Harbert International which routinely are included in the Engineering News-Record lists of top design and international construction firms. Huntsville is regarded for its high-technology driven economy and is known as the "Rocket City" because of NASA's Marshall Space Flight Center and the Redstone Arsenal. Huntsville's main economic influence is derived from aerospace and military technology. Redstone Arsenal, Cummings Research Park (CRP), The University of Alabama in Huntsville and NASA's Marshall Space Flight Center comprise the main hubs for the area's technology-driven economy. CRP is the second largest research park in the United States and the fourth largest in the world, and is over 38 years old. Huntsville has commercial technology companies such as the network access company ADTRAN, computer graphics company Intergraph and design and manufacturer of IT infrastructure Avocent. Telecommunications provider Deltacom, Inc. and copper tube manufacturer and distributor Wolverine Tube are also based in Huntsville. Cinram manufactures and distributes 20th Century Fox DVDs and Blu-ray Discs out of their Huntsville plant. Sanmina-SCI also has a large presence in the area. Forty-two Fortune 500 companies have operations in Huntsville. In 2005, Forbes Magazine named the Huntsville-Decatur Combined Statistical Area as 6th best place in the nation for doing business, and number one in terms of the number of engineers per total employment. The city of Mobile, Alabama's only saltwater port, is a busy seaport on the Gulf of Mexico with inland waterway access to the Midwest by way of the Tennessee-Tombigbee Waterway. The Port of Mobile is currently the 9th largest by tonnage in the United States. In May 2007, a site north of Mobile was selected by German steelmaker ThyssenKrupp for a $3.7 billion steel production plant, with the promise of 2,700 permanent jobs.
Alabama's tax structure is one the most regressive in the United States. Alabama levies a 2, 4, or 5 percent personal income tax, depending upon the amount earned and filing status, though taxpayers can deduct their federal income tax from their Alabama state tax. The state's general sales tax rate is 4%. The collection rate could be substantially higher, depending upon additional city and county sales taxes. For example, the total sales tax rate in Mobile is 9% and there is an additional restaurant tax of 1%, which means that a diner in Mobile would pay a 10% tax on a meal. Sales and excise taxes in Alabama account for 51 percent of all state and local revenue, compared with an average of about 36 percent nationwide. Alabama is also one of the few remaining states that levies a tax on food and medicine. Alabama's income tax on poor working families is among the nation's very highest. Alabama is the only state that levies income tax on a family of four with income as low as $4,600, which is barely one-quarter of the federal poverty line. Alabama's threshold is the lowest among the 41 states and the District of Columbia with income taxes. The corporate income tax rate is currently 6.5%. The overall federal, state, and local tax burden in Alabama ranks the state as the second least tax-burdened state in the country. Property taxes are the lowest in the United States. The current state constitution requires a voter referendum to raise property taxes. Since Alabama's tax structure largely depends on consumer spending, it is subject to high variable budget structure. For example, in 2003 Alabama had an annual budget deficit as high as $670 million. It is one of only a few states to accomplish large surpluses, with a budget surplus of nearly $1.2 billion in 2007, and estimated at more than $2.1 billion for 2008. However, the declining national economy in 2008 has eliminated that surplus and the state is again facing shortfall, with the governor declaring "proration," which will result in an immediate education budget cut and school layoffs.
Alabama has five major interstate roads that cross it: I-65 runs north–south roughly through the middle of the state; I-59/I-20 travels from the central west border to Birmingham, where I-59 continues to the north-east corner of the state and I-20 continues east towards Atlanta; I-85 originates in Montgomery and runs east-northeast to the Georgia border, providing a main thoroughfare to Atlanta; and I-10 traverses the southernmost portion of the state, running from west to east through Mobile. Another interstate road, I-22, is currently under construction. When completed around 2012 it will connect Birmingham with Memphis, Tennessee. Several US Highways also pass through the state, such as US 11, US 29, US 31, US 43, US 72, US 78, US 80, US 82, US 84, US 98, US 231, and US 280. Major airports in Alabama include Birmingham-Shuttlesworth International Airport (BHM), Huntsville International Airport (HSV), Dothan Regional Airport (DHN), Mobile Regional Airport (MOB), Montgomery Regional Airport (MGM), Muscle Shoals – Northwest Alabama Regional Airport (MSL), Tuscaloosa Regional Airport (TCL), and Pryor Field Regional Airport (DCU). For rail transport, Amtrak schedules the Crescent, a daily passenger train, running from New York to New Orleans with stops at Anniston, Birmingham, and Tuscaloosa.
The foundational document for Alabama's government is the Alabama Constitution, which was ratified in 1901. At almost 800 amendments and 310,000 words, it is the world's longest constitution and is roughly forty times the length of the U.S. Constitution. There is a significant movement to rewrite and modernize Alabama's constitution. This movement is based upon the fact that Alabama's constitution highly centralizes power in Montgomery and leaves practically no power in local hands. Any policy changes proposed around the state must be approved by the entire Alabama legislature and, frequently, by state referendum. One criticism of the current constitution claims that its complexity and length were intentional to codify segregation and racism. The legislative branch is the Alabama Legislature, a bicameral assembly composed of the Alabama House of Representatives, with 105 members, and the Alabama Senate, with 35 members. The Legislature is responsible for writing, debating, passing, or defeating state legislation. The executive branch is responsible for the execution and oversight of laws. It is headed by the Governor of Alabama. Other members of executive branch include the cabinet, the Attorney General of Alabama, the Alabama Secretary of State, the Alabama Commissioner of Agriculture and Industries, the Alabama State Treasurer, and the Alabama State Auditor. The judicial branch is responsible for interpreting the Constitution and applying the law in state criminal and civil cases. The highest court is the Supreme Court of Alabama.
Alabama has 67 counties. Each county has its own elected legislative branch, usually called the County Commission, which usually also has executive authority in the county. Because of the restraints placed in the Alabama Constitution, all but seven counties (Jefferson, Lee, Mobile, Madison, Montgomery, Shelby, and Tuscaloosa) in the state have little to no home rule. Instead, most counties in the state must lobby the Local Legislation Committee of the state legislature to get simple local policies such as waste disposal to land use zoning. Alabama is an alcoholic beverage control state; the government holds a monopoly on the sale of alcohol. However, counties can declare themselves "dry"; the state does not sell alcohol in those areas.
The current governor of the state is Republican Bob Riley. The lieutenant governor is Jim Folsom Jr. The Chief Justice of the Alabama Supreme Court is Democrat Sue Bell Cobb. The Democratic Party currently holds a large majority in both houses of the Legislature. Because of the Legislature's power to override a gubernatorial veto by a mere simple majority (most state Legislatures require a two-thirds majority to override a veto), the relationship between the executive and legislative branches can be easily strained when different parties control the branches. During Reconstruction following the American Civil War, Alabama was occupied by federal troops of the Third Military District under General John Pope. In 1874, the political coalition known as the Redeemers took control of the state government from the Republicans, in part by suppressing the African American vote. After 1890, a coalition of whites passed laws to segregate and disenfranchise black residents, a process completed in provisions of the 1901 constitution. Provisions which disfranchised African Americans also disfranchised poor whites, however. By 1941 more whites than blacks had been disfranchised: 600,000 to 520,000, although the impact was greater on the African-American community, as almost all of its citizens were disfranchised. From 1901 to the 1960s, the state legislature failed to perform redistricting as population grew and shifted within the state. The result was a rural minority that dominated state politics until a series of court cases required redistricting in 1972. With the disfranchisement of African Americans, the state became part of the "Solid South", a one-party system in which the Democratic Party became essentially the only political party in every Southern state. For nearly 100 years, local and state elections in Alabama were decided in the Democratic Party primary, with generally only token Republican challengers running in the General Election. In the 1986 Democratic primary election, the then-incumbent Lieutenant Governor, Bill Baxley, lost the Democratic nomination for Governor in a scandal where Republicans were permitted to cast votes for his opponent, then Attorney General Charlie Graddick. The state Democratic party invalidated the election and placed the Baxley's name on the ballot as the Democratic candidate instead of the candidate chosen in the primary. The voters of the state revolted at what they perceived as disenfranchisement of their right to vote and elected the Republican challenger Guy Hunt as Governor. This was the first Republican Governor elected in Alabama since Reconstruction. Since then, Republicans have become increasingly competitive in Alabama politics. They currently control both seats in the U.S. Senate, four out of the state's seven congressional seats. Republicans hold an 8–1 majority on the Alabama Supreme Court and have a 5–2 majority among statewide elected executive branch offices. However, Democrats currently hold all three seats on the Alabama Public Service Commission and they maintain control of both houses of the legislature, holding approximately 59.4% of seats in the Alabama Senate and 58.7% of seats in the Alabama House of Representatives. A majority of local offices in the state are still held by Democrats. Local elections in rural counties are generally decided in the Democratic primary and local elections in metropolitan counties are decided in the Republican Primary although there are exceptions to this rule. Only one Republican Lt. Governor has been elected since Reconstruction, Steve Windom. Windom served as Lt. Governor under Democratic Gov. Don Siegelman. The last time that Alabama had a governor and Lt. governor of the same party was the period between 1983 and 1987 when Wallace was serving his fourth term as governor and Bill Baxley was serving as Lt. Governor, both were Democrats. An overwhelming majority of sheriff's offices in Alabama are in Democratic hands. However, most of the Democratic sheriffs preside over more rural and less populated counties and the majority of Republicans preside over more urban/suburban and more populated counties. Only three Alabama counties (Tuscaloosa, Montgomery and Calhoun) with a population of over 100,000 have Democratic sheriffs and only five Alabama counties with a population of under 75,000 have Republican sheriffs (Autauga, Coffee, Dale, Coosa, and Blount). Alabama state politics gained nationwide and international attention in the 1950s and 1960s during the American Civil Rights Movement, when majority whites bureaucratically, and at times, violently resisted protests for electoral and social reform. George Wallace, the state's governor, remains a notorious and controversial figure. Only with the passage of the Civil Rights Act of 1964 and Voting Rights Act of 1965 did African Americans regain suffrage and other civil rights. In 2007, the Alabama Legislature passed, and the Governor signed, a resolution expressing "profound regret" over slavery and its lingering impact. In a symbolic ceremony, the bill was signed in the Alabama State Capitol, which housed Congress of the Confederate States of America.
From 1876 through 1956, Alabama supported only Democratic presidential candidates, by large margins. In 1960, the Democrats won with John F. Kennedy on the ballot, but the Democratic electors from Alabama gave 6 of their 11 electoral votes as a protest to Harry Byrd. In 1964, Republican Barry Goldwater carried the state, in part because of his opposition to the 1964 Civil Rights Act, which restored the franchise for African Americans. In the 1968 presidential election, Alabama supported native son and American Independent Party candidate George Wallace over both Richard Nixon and Hubert Humphrey. Wallace was the official Democratic candidate in Alabama, while Humphrey was listed as the "National Democratic". In 1976, Democratic candidate Jimmy Carter from Georgia carried the state, the region, and the nation, but Democratic control of the region slipped after that. Since 1980, conservative Alabama voters have increasingly voted for Republican candidates at the Federal level, especially in Presidential elections. By contrast, Democratic candidates have been elected to many state-level offices and comprise a longstanding majority in the Alabama Legislature; see Dixiecrat. In 2004, George W. Bush won Alabama's nine electoral votes by a margin of 25 percentage points with 62.5% of the vote, mostly white voters. The eleven counties that voted Democratic were Black Belt counties, where African Americans are the majority racial group. The state's two U.S. senators are Jefferson B. Sessions III and Richard C. Shelby, both Republicans. In the U.S. House of Representatives, the state is represented by seven members, five of whom are Republicans: (Jo Bonner, Mike D. Rogers, Robert Aderholt, Parker Griffith, and Spencer Bachus) and two are Democrats: (Bobby Bright and Artur Davis).
Public primary and secondary education in Alabama is under the overview of the Alabama State Board of Education as well as local oversight by 67 county school boards and 60 city boards of education. Together, 1,541 individual schools provide education for 743,364 elementary and secondary students. Public school funding is appropriated through the Alabama Legislature through the Education Trust Fund. In FY 2006–2007, Alabama appropriated $3,775,163,578 for primary and secondary education. That represented an increase of $444,736,387 over the previous fiscal year. In 2007, over 82 percent of schools made adequate yearly progress (AYP) toward student proficiency under the National No Child Left Behind law, using measures determined by the state of Alabama. In 2004, 23 percent of schools met AYP. While Alabama's public education system has improved, it lags behind in achievement compared to other states. According to U.S. Census data, Alabama's high school graduation rate – 75% – is the second lowest in the United States (after Mississippi). The largest educational gains were among people with some college education but without degrees.
Alabama's programs of higher education include 14 four-year public universities, two-year community colleges, and 17 private, undergraduate and graduate universities. In the state are two medical schools (University of Alabama at Birmingham and University of South Alabama), two veterinary colleges (Auburn University and Tuskegee University), a dental school (University of Alabama at Birmingham), an optometry college (University of Alabama at Birmingham), two pharmacy schools (Auburn University and Samford University), and five law schools (University of Alabama School of Law, Birmingham School of Law, Cumberland School of Law, Miles Law School, and the Thomas Goode Jones School of Law). Public, post-secondary education in Alabama is overseen by the Alabama Commission on Higher Education. Colleges and universities in Alabama offer degree programs from two-year associate degrees to 16 doctoral level programs. Accreditation of academic programs is through the Southern Association of Schools and Colleges as well as a plethora of subject focused national and international accreditation agencies.
Famous people from Alabama include Hank Aaron, Tommie Agee, Tallulah Bankhead, William Brockman Bankhead, Jay Barker, Charles Barkley, Regina Benjamin, Hugo L. Black, Frank Bolling, Paul W. (Bear) Bryant, Jimmy Buffett, Bo Bice, George Washington Carver, William Christenberry, Nat King Cole, Jerricho Cotchery, Courteney Cox Arquette, Robert Gibbs, Mitch Holleman, Zelda Fitzgerald, Charles Ghigna, Winston Groom, William C. Handy, Emmylou Harris, Taylor Hicks, Joe Hilley, Bo Jackson, Kate Jackson, Jamey Johnson, Helen Keller, Coretta Scott King, William R. King, Harper Lee, Joe Louis, Heinie Manush, William March, Willie Mays, Willie McCovey, Roy Moore, John Hunt Morgan, Jim Nabors, Randy Owen, Jesse Owens, Terrell Owens, Satchel Paige, Jake Peavy, Claude Pepper, Rosa Parks, Wilson Pickett, Howell Raines, Condoleezza Rice, Lionel Richie, Rich Boy, Philip Rivers, JaMarcus Russell, Kenny Stabler, Ozzie Smith, John Sparkman, Bart Starr, Ruben Studdard, Channing Tatum, Oscar W. Underwood, Jimmy Wales, George Wallace, Booker T. Washington, Billy Williams, and Hank Williams.
In Greek mythology, Achilles (Ancient Greek:) was a Greek hero of the Trojan War, the central character and the greatest warrior of Homer's "Iliad". Achilles also has the attributes of being the most handsome of the heroes assembled against Troy. Later legends (beginning with a poem by Statius in the first century AD) state that Achilles was invulnerable in all of his body except for his heel. Since he died due to an arrow shot into his heel, the "Achilles' heel" has come to mean a person's principal weakness.
Achilles was the son of the nymph Thetis and Peleus, the king of the Myrmidons. Zeus and Poseidon had been rivals for the hand of Thetis until Prometheus, the fire-bringer, warned Zeus of a prophecy that Thetis would bear a son greater than his father. For this reason, the two gods withdrew their pursuit, and had her wed Peleus. As with most mythology there is a tale which offers an alternative version of these events: in "Argonautica" (iv.760) Hera alludes to Thetis's chaste resistance to the advances of Zeus, that Thetis was so loyal to Hera's marriage bond that she coolly rejected him. Thetis, although a daughter of the sea-god Nereus, was also brought up by Hera, further explaining her resistance to the advances of Zeus. According to the "Achilleid", written by Statius in the first century AD, and to no surviving previous sources, when Achilles was born Thetis tried to make him immortal by dipping him in the river Styx. However, he was left vulnerable at the part of the body she held him by, his heel. (See Achilles heel, Achilles' tendon.) It is not clear if this version of events was known earlier. In another version of this story, Thetis anointed the boy in ambrosia and put him on top of a fire to burn away the mortal parts of his body. She was interrupted by Peleus and abandoned both father and son in a rage. However none of the sources before Statius makes any reference to this general invulnerability. To the contrary, in the "Iliad" Homer mentions Achilles being wounded: in Book 21 the Paeonian hero Asteropaeus, son of Pelagon, challenged Achilles by the river Scamander. He cast two spears at once, one grazed Achilles' elbow, "drawing a spurt of blood." Also in the fragmentary poems of the Epic Cycle in which we can find description of the hero's death, Kúpria (unknown author), "Aithiopis" by Arctinus of Miletus, "Ilias Mikrá" by Lesche of Mytilene, Iliou pérsis by Arctinus of Miletus, there is no trace of any reference to his general invulnerability or his famous weakness (heel); in the later vase-paintings presenting Achilles' death, the arrow (or in many cases, arrows) hit his body. Peleus entrusted Achilles to Chiron the Centaur, on Mt. Pelion, to be raised. Achilles in the Trojan War. Achilles' consuming rage is at some times wavering, but at other times he cannot be cooled. The humanization of Achilles by the events of the war is an important theme of the narrative.
When the Greeks left for the Trojan War, they accidentally stopped in Mysia, ruled by King Telephus. In the resulting battle, Achilles gave Telephus a wound that would not heal; Telephus consulted an oracle, who stated that "he that wounded shall heal". Guided by the oracle, he arrived at Argos, where Achilles heals him in order that he become their guide for the voyage to Troy. According to other reports in Euripides' lost play about Telephus, he went to Aulis pretending to be a beggar and asked Achilles to heal his wound. Achilles refused, claiming to have no medical knowledge. Alternatively, Telephus held Orestes for ransom, the ransom being Achilles' aid in healing the wound. Odysseus reasoned that the spear had inflicted the wound; therefore, the spear must be able to heal it. Pieces of the spear were scraped off onto the wound and Telephus was healed.
According to the Cypria (the part of the Epic Cycle that tells the events of the Trojan War before Achilles' Wrath), when the Achaeans desired to return home, they were restrained by Achilles, who afterwards attacked the cattle of Aeneas, sacked neighboring cities and killed Troilus. According to Dares Phrygius' "Account of the Destruction of Troy", the Latin summary through which the story of Achilles was transmitted to medieval Europe, Troilus was a young Trojan prince, the youngest of King Priam's (or sometimes Apollo) and Hecuba's five legitimate sons. Despite his youth, he was one of the main Trojan war leaders. Prophecies linked Troilus' fate to that of Troy and so he was ambushed in an attempt to capture him. Yet Achilles, struck by the beauty of both Troilus and his sister Polyxena, and overcome with lust directed his sexual attentions on the youth — who refusing to yield found instead himself decapitated upon an altar-omphalos of Apollo. Later versions of the story suggested Troilus was accidentally killed by Achilles in an over-ardent lovers' embrace. In this version of the myth, Achilles' death therefore came in retribution for this sacrilege. Ancient writers treated Troilus as the epitome of a dead child mourned by his parents. Had Troilus lived to adulthood, the First Vatican Mythographer claimed Troy would have been invincible.
Homer's "Iliad" is the most famous narrative of Achilles' deeds in the Trojan War. The Homeric epic only covers a few weeks of the war, and does not narrate Achilles' death. It begins with Achilles' withdrawal from battle after he is dishonored by Agamemnon, the commander of the Achaean forces. Agamemnon had taken a woman named Chryseis as his slave. Her father Chryses, a priest of Apollo, begged Agamemnon to return her to him. Agamemnon refused and Apollo sent a plague amongst the Greeks. The prophet Calchas correctly determined the source of the troubles but would not speak unless Achilles vowed to protect him. Achilles did so and Calchas declared Chryseis must be returned to her father. Agamemnon consented, but then commanded that Achilles' battle prize Briseis be brought to replace Chryseis. Angry at the dishonor (and as he says later, because he loved Briseis) and at the urging of Thetis, Achilles refused to fight or lead his troops alongside the other Greek forces. As the battle turned against the Greeks, Nestor declared that the Trojans were winning because Agamemnon had angered Achilles, and urged the king to appease the warrior. Agamemnon agreed and sent Odysseus and two other chieftains, Ajax and Phoenix, to Achilles with the offer of the return of Briseis and other gifts. Achilles rejected all Agamemnon offered him, and simply urged the Greeks to sail home as he was planning to do. Eventually, however, hoping to retain glory despite his absence from the battle, Achilles prayed to his mother Thetis, asking her to plead with Zeus to allow the Trojans to push back the Greek forces. The Trojans, led by Hector, subsequently pushed the Greek army back toward the beaches and assaulted the Greek ships. With the Greek forces on the verge of absolute destruction, Patroclus led the Myrmidons into battle, though Achilles remained at his camp. Patroclus succeeded in pushing the Trojans back from the beaches, but was killed by Hector before he could lead a proper assault on the city of Troy. After receiving the news of the death of Patroclus from Antilochus, the son of Nestor, Achilles grieved over his close friend's death and held many funeral games in his honor. His mother Thetis came to comfort the distraught Achilles. She persuaded Hephaestus to make new armor for him, in place of the armor that Patroclus had been wearing which was taken by Hector. The new armor included the Shield of Achilles, described in great detail by the poet. Enraged over the death of Patroclus, Achilles ended his refusal to fight and took the field killing many men in his rage but always seeking out Hector. Achilles even engaged in battle with the river god Scamander who became angry that Achilles was choking his waters with all the men he killed. The god tried to drown Achilles but was stopped by Hera and Hephaestus. Zeus himself took note of Achilles' rage and sent the gods to restrain him so that he would not go on to sack Troy itself, seeming to show that the unhindered rage of Achilles could defy fate itself as Troy was not meant to be destroyed yet. Finally Achilles found his prey. Achilles chased Hector around the wall of Troy three times before Athena, in the form of Hector's favorite and dearest brother, Deiphobus, persuaded Hector to stop running and fight Achilles face to face. After Hector realized the trick, he knew the battle was inevitable. Wanting to go down fighting, he charged at Achilles with his only weapon, his sword, but missed. Accepting his fate, Hector begged Achilles – not to spare his life, but to treat his body with respect after killing him. Achilles told Hector it was hopeless to expect that of him, declaring that "my rage, my fury would drive me now to hack your flesh away and eat you raw — such agonies you have caused me". Achilles then got his vengeance, killing Hector with a single blow to the neck and tying the Trojan's body to his chariot, dragging it around the battlefield for nine days. With the assistance of the god Hermes, Hector's father, Priam, went to Achilles' tent to plead with Achilles to permit him to perform for Hector his funeral rites. The final passage in the "Iliad" is Hector's funeral, after which the doom of Troy was just a matter of time.
Achilles, after his temporary truce with Priam, fought and killed the Amazonian warrior queen Penthesilea, but later grieved over her death. At first, he was so distracted by her beauty, he did not fight as intensely as usual. Once he realized that his distraction was endangering his life, he refocused, and killed her. As he grieved over the death of such a rare beauty, a notorious Greek jeerer by the name of Thersites laughed and mocked the great Achilles. Annoyed by his insensitivity and disrespect, Achilles punched him in the face and killed him instantly. Memnon, and the fall of Achilles. Following the death of Patroclus, Achilles' closest companion was Nestor's son Antilochus. When Memnon, king of Ethiopia killed Antilochus, Achilles was once again drawn onto the battlefield to seek revenge. The fight between Achilles and Memnon over Antilochus echoes that of Achilles and Hector over Patroclus, except that Memnon (unlike Hector) was also the son of a goddess. Many Homeric scholars argued that episode inspired many details in the "Iliads description of the death of Patroclus and Achilles' reaction to it. The episode then formed the basis of the cyclic epic "Aethiopis", which was composed after the "Iliad", possibly in the 7th century B.C. The "Aethiopis" is now lost, except for scattered fragments quoted by later authors. As predicted by Hector with his dying breath, Achilles was thereafter killed by Paris with an arrow (to the heel according to Statius). In some versions, the god Apollo guided Paris' arrow. Some retellings also state that Achilles was scaling the gates of Troy and was hit with a poisoned arrow. Both versions conspicuously deny the killer any sort of valor owing to the common conception that Paris was a coward and not the man his brother Hector was, and Achilles remained undefeated on the battlefield. His bones were mingled with those of Patroclus, and funeral games were held. He was represented in the lost Trojan War epic of Arctinus of Miletus as living after his death in the island of Leuke at the mouth of the river Danube (see below). Another version of Achilles' death is that he fell deeply in love with one of the Trojan princesses, Polyxena, Achilles asks Priam for Polyxena's hand in marriage. Priam is willing because it would mean the end of the war and an alliance with the world's greatest warrior. But while Priam is overseeing the private marriage of Polyxena and Achilles, Paris who would have to give up Helen if Achilles married his sister hides in the bushes and shoots Achilles with a divine arrow killing him. Achilles was cremated and his ashes buried in the same urn as those of Patroclus. Paris was later killed by Philoctetes using the enormous bow of Heracles.
Achilles' armor was the object of a feud between Odysseus and Telamonian Ajax (Ajax the greater). They competed for it by giving speeches on why they were the bravest after Achilles to their Trojan prisoners, who after considering both men came to a consensus in favor of Odysseus. Furious, Ajax cursed Odysseus, which earned the ire of Athena. Athena temporarily made Ajax so mad with grief and anguish that he began killing sheep, thinking they were his comrades. After a while, when Athena lifted his madness and Ajax realized that he had actually been killing sheep, he was so embarrassed that he committed suicide. Odysseus eventually gave the armor to Neoptolemus, the son of Achilles. A relic claimed to be Achilles' bronze-headed spear was for centuries preserved in the temple of Athena on the acropolis of Phaselis, Lycia, a port on the Pamphylian Gulf. The city was visited in 333 BC by Alexander the Great, who envisioned himself as the new Achilles and carried the "Iliad" with him, but his court biographers do not mention the spear, which he would indeed have touched with excitement. But it was being shown in the time of Pausanias in the second century AD.
Achilles' relationship with Patroclus is a key aspect of his myth. Its exact nature has been a subject of dispute in both the classical period and modern times. In the "Iliad", they appeared to be generally portrayed as a model of deep and loyal friendship. However, commentators from the classical period to today have tended to interpret the relationship through the lens of their own cultures. Thus, in 5th century BC Athens the relationship was commonly interpreted as pederastic. Contemporary readers may interpret the two heroes either as relatives or close friends, as "war buddies," as being in a teacher/student relationship, or in love with each other as an egalitarian homosexual couple. Whichever the case may be, Achilles nevertheless continued to have sexual relationships with women. The cult of Achilles in antiquity. There was an archaic heroic cult of Achilles on the White Island, "Leuce", in the Black Sea off the modern coasts of Romania and Ukraine, with a temple and an oracle which survived into the Roman period. In the lost epic "Aithiopis", a continuation of the "Iliad" attributed to Arktinus of Miletos, Achilles’ mother Thetis returned to mourn him and removed his ashes from the pyre and took them to Leuce at the mouths of the Danube. There the Achaeans raised a tumulus for him and celebrated funeral games. Pliny's Natural History (IV.27.1) mentions a tumulus that is no longer evident ("Insula Akchillis tumulo eius viri clara"), on the island consecrated to him, located at a distance of fifty Roman miles from Peuce by the Danube Delta, and the temple there. Pausanias has been told that the island is "covered with forests and full of animals, some wild, some tame. In this island there is also Achilles’ temple and his statue” (III.19.11). Ruins of a square temple 30 meters to a side, possibly that dedicated to Achilles, were discovered by Captain Kritzikly in 1823, but there has been no modern archeological work done on the island. Pomponius Mela tells that Achilles is buried in the island named Achillea, between Boristhene and Ister ("De situ orbis", II, 7). And the Greek geographer Dionysius Periegetus of Bithynia, who lived at the time of Domitian, writes that the island was called "Leuce" "because the wild animals which live there are white. It is said that there, in Leuce island, reside the souls of Achilles and other heroes, and that they wander through the uninhabited valleys of this island; this is how Jove rewarded the men who had distinguished themselves through their virtues, because through virtue they had acquired everlasting honor” ("Orbis descriptio", v. 541, quoted in Densuşianu 1913). The "Periplus of the Euxine Sea" gives the following details: "It is said that the goddess Thetis raised this island from the sea, for her son Achilles, who dwells there. Here is his temple and his statue, an archaic work. This island is not inhabited, and goats graze on it, not many, which the people who happen to arrive here with their ships, sacrifice to Achilles. In this temple are also deposited a great many holy gifts, craters, rings and precious stones, offered to Achilles in gratitude. One can still read inscriptions in Greek and Latin, in which Achilles is praised and celebrated. Some of these are worded in Patroclus’ honor, because those who wish to be favored by Achilles, honor Patroclus at the same time. There are also in this island countless numbers of sea birds, which look after Achilles’ temple. Every morning they fly out to sea, wet their wings with water, and return quickly to the temple and sprinkle it. And after they finish the sprinkling, they clean the hearth of the temple with their wings. Other people say still more, that some of the men who reach this island, come here intentionally. They bring animals in their ships, destined to be sacrificed. Some of these animals they slaughter, others they set free on the island, in Achilles’ honor. But there are others, who are forced to come to this island by sea storms. As they have no sacrificial animals, but wish to get them from the god of the island himself, they consult Achilles’ oracle. They ask permission to slaughter the victims chosen from among the animals that graze freely on the island, and to deposit in exchange the price which they consider fair. But in case the oracle denies them permission, because there is an oracle here, they add something to the price offered, and if the oracle refuses again, they add something more, until at last, the oracle agrees that the price is sufficient. And then the victim doesn’t run away any more, but waits willingly to be caught. So, there is a great quantity of silver there, consecrated to the hero, as price for the sacrificial victims. To some of the people who come to this island, Achilles appears in dreams, to others he would appear even during their navigation, if they were not too far away, and would instruct them as to which part of the island they would better anchor their ships”. (quoted in Densuşianu) The heroic cult of Achilles on Leuce island was widespread in antiquity, not only along the sea lanes of the Pontic Sea but also in maritime cities whose economic interests were tightly connected to the riches of the Black Sea. Achilles from Leuce island was venerated as "Pontarches" the lord and master of the Pontic (Black) Sea, the protector of sailors and navigation. Sailors went out of their way to offer sacrifice. To Achilles of Leuce were dedicated a number of important commercial port cities of the Greek waters: Achilleion in Messenia (Stephanus Byzantinus), Achilleios in Laconia (Pausanias, III.25,4) Nicolae Densuşianu (Densuşianu 1913) even thought he recognized Achilles in the name of Aquileia and in the north arm of the Danube delta, the arm of Chilia ("Achileii"), though his conclusion, that Leuce had sovereign rights over Pontos, evokes modern rather than archaic sea-law." Leuce had also a reputation as a place of healing. Pausanias (III.19,13) reports that the Delphic Pythia sent a lord of Croton to be cured of a chest wound. Ammianus Marcellinus (XXII.8) attributes the healing to waters ("aquae") on the island. The cult of Achilles in modern times: The Achilleion in Corfu. In the region of Gastouri (Γαστούρι) to the south of the city of Corfu Greece, Empress of Austria Elisabeth of Bavaria also known as Sissi built in 1890 a summer palace with Achilles as its central theme and it is a monument to platonic romanticism. The palace, naturally, was named after Achilles: "Achilleion" (Αχίλλειον). This elegant structure abounds with paintings and statues of Achilles both in the main hall and in the lavish gardens depicting the heroic and tragic scenes of the Trojan war.
Achilles' name can be analyzed as a combination of ("akhos") "grief" and ("Laos") "a people, tribe, nation, etc." In other words, Achilles is an embodiment of the grief of the people, grief being a theme raised numerous times in the "Iliad" (frequently by Achilles). Achilles' role as the hero of grief forms an ironic juxtaposition with the conventional view of Achilles as the hero of "kleos" (glory, usually glory in war). "Laos" has been construed by Gregory Nagy, following Leonard Palmer, to mean "a corps of soldiers", a muster. With this derivation, the name would have a double meaning in the poem: When the hero is functioning rightly, his men bring grief to the enemy, but when wrongly, his men get the grief of war. The poem is in part about the misdirection of anger on the part of leadership. The name Achilleus was a common and attested name among the Greeks early after 7th century BC. It was also turned into the female form of Ἀχιλλεία, "Achilleía", firstly attested in Attica,4th century BC, (IG II² 1617) and Achillia, as the name of a female gladiator fighting, 'Amazonia'. Roman gladiatorial games often referenced classical mythology and this seems to reference Achilles' fight with Penthesilea, but give it an extra twist of Achilles being 'played' by a woman.
Some post-Homeric sources claim that in order to keep Achilles safe from the war, Thetis (or, in some versions, Peleus) hides the young man at the court of Lycomedes, king of Skyros. There, Achilles is disguised as a girl and lives among Lycomedes' daughters, perhaps under the name "Pyrrha" (the red-haired girl). With Lycomedes' daughter Deidamia, whom in the account of Statius he rapes, Achilles there fathers a son, Neoptolemus (also called Pyrrhus, after his father's possible alias). According to this story, Odysseus learns from the prophet Calchas that the Achaeans would be unable to capture Troy without Achilles' aid. Odysseus goes to Skyros in the guise of a peddler selling women's clothes and jewelry and places a shield and spear among his goods. When Achilles instantly takes up the spear, Odysseus sees through his disguise and convinces him to join the Greek campaign. In another version of the story, Odysseus arranges for a trumpet alarm to be sounded while he was with Lycomedes' women; while the women flee in panic, Achilles prepares to defend the court, thus giving his identity away. In book 11 of Homer's "Odyssey," Odysseus sails to the underworld and converses with the shades. One of these is Achilles, who when greeted as "blessed in life, blessed in death", responds that he would rather be a slave to the worst of masters than be king of all the dead. But Achilles then asks Odysseus of his son's exploits in the Trojan war, and when Odysseus tells of Neoptolemus' heroic actions, Achilles is filled with satisfaction. This leaves the reader with an ambiguous understanding of how Achilles felt about the heroic life. Achilles was worshipped as a sea-god in many of the Greek colonies on the Black Sea, the location of the mythical "White Island" which he was said to inhabit after his death, together with many other heroes. The kings of the Epirus claimed to be descended from Achilles through his son, Neoptolemus. Alexander the Great, son of the Epiran princess Olympias, could therefore also claim this descent, and in many ways strove to be like his great ancestor; he is said to have visited his tomb while passing Troy. Achilles fought and killed the Amazon Helene. Some also said he married Medea, and that after both their deaths they were united in the Elysian Fields of Hades — as Hera promised Thetis in Apollonius' Argonautica. In some versions of the myth, Achilles has a relationship with his captive Briseis.
The Greek tragedian Aeschylus wrote a trilogy of plays about Achilles, given the title "Achilleis" by modern scholars. The tragedies relate the deeds of Achilles during the Trojan War, including his defeat of Hector and eventual death when an arrow shot by Paris and guided by Apollo punctures his heel. Extant fragments of the "Achilleis" and other Aeschylean fragments have been assembled to produce a workable modern play. The first part of the "Achilleis" trilogy, "The Myrmidons", focused on the relationship between Achilles and chorus, who represent the Achaean army and try to convince Achilles to give up his quarrel with Agamemnon; only a few lines survive today. The tragedian Sophocles also wrote a play with Achilles as the main character, "The Lovers of Achilles". Only a few fragments survive.
The philosopher Zeno of Elea centered one of his paradoxes on an imaginary footrace between "swift-footed" Achilles and a tortoise, by which he attempted to show that Achilles could not catch up to a tortoise with a head start, and therefore that motion and change were impossible. As a student of the monist Parmenides and a member of the Eleatic school, Zeno believed time and motion to be illusions.
Abraham Lincoln (February 12, 1809 – April 15, 1865) served as the 16th President of the United States from March 1861 until his assassination in April 1865. He successfully led his country through its greatest internal crisis, the American Civil War, preserving the Union and ending slavery. Before his election in 1860 as the first Republican president, Lincoln had been a country lawyer, an Illinois state legislator, a member of the United States House of Representatives, and twice an unsuccessful candidate for election to the U.S. Senate. As an outspoken opponent of the expansion of slavery in the United States, Lincoln won the Republican Party nomination in 1860 and was elected president later that year. His tenure in office was occupied primarily with the defeat of the secessionist Confederate States of America in the American Civil War. He introduced measures that resulted in the abolition of slavery, issuing his Emancipation Proclamation in 1863 and promoting the passage of the Thirteenth Amendment to the Constitution. Six days after the large-scale surrender of Confederate forces under General Robert E. Lee, Lincoln became the first American president to be assassinated. Lincoln had closely supervised the victorious war effort, especially the selection of top generals, including Ulysses S. Grant. Historians have concluded that he handled the factions of the Republican Party well, bringing leaders of each faction into his cabinet and forcing them to cooperate. Lincoln successfully defused the "Trent" affair, a war scare with Britain late in 1861. Under his leadership, the Union took control of the border slave states at the start of the war. Additionally, he managed his own reelection in the 1864 presidential election. Copperheads and other opponents of the war criticized Lincoln for refusing to compromise on the slavery issue. Conversely, the Radical Republicans, an abolitionist faction of the Republican Party, criticized him for moving too slowly in abolishing slavery. Even with these opponents, Lincoln successfully rallied public opinion through his rhetoric and speeches; his Gettysburg Address (1863) became an iconic symbol of the nation's duty. At the close of the war, Lincoln held a moderate view of Reconstruction, seeking to speedily reunite the nation through a policy of generous reconciliation. Lincoln has consistently been ranked by scholars as one of the greatest of all U.S. Presidents.
Abraham Lincoln was born on February 12, 1809, to Thomas Lincoln and Nancy Hanks, two farmers, in a one-room log cabin on the Sinking Spring Farm, in southeast Hardin County, Kentucky (now part of LaRue County), making him the first president born in the west. Lincoln was not given a middle name. His ancestor Samuel Lincoln had arrived in Hingham, Massachusetts from England in the 17th century. His grandfather, also named Abraham Lincoln, had moved to Kentucky, where he owned over, and was ambushed and killed by an Indian raid in 1786. Thomas Lincoln was a respected citizen of rural Kentucky. He owned several farms, including the Sinking Spring Farm, although he was not wealthy. The family belonged to a Separate Baptists church, which had high moral standards frowning on alcohol consumption and dancing, and many church members were opposed to slavery. Abraham himself never joined their church, or any other church. In 1816, the Lincoln family left Kentucky to avoid the expense of fighting for one of their properties in court, and made a new start in Perry County, Indiana (now in Spencer County). Lincoln later noted that this move was "partly on account of slavery", and partly because of difficulties with land deeds in Kentucky. Abraham's father disapproved of slavery on religious grounds and it was hard to compete economically with farms operated by slaves. Unlike land in the Northwest Territory, Kentucky never had a proper U.S. survey, and farmers often had difficulties proving title to their property. When Lincoln was nine, his mother, then 34 years old, died of milk sickness. Soon afterwards, his father remarried, to Sarah Bush Johnston. Lincoln and his stepmother were close; he called her "Mother" for the rest of his life, but he became increasingly distant from his father. Abraham felt his father was not a success, and did not want to be like him. In later years, he would occasionally lend his father money. In 1830, fearing a milk sickness outbreak, the family settled on public land in Macon County, Illinois. The next year, when his father relocated the family to a new homestead in Coles County, Illinois, 22-year-old Lincoln struck out on his own, canoeing down the Sangamon River to the village of New Salem in Sangamon County. Later that year, hired by New Salem businessman Denton Offutt and accompanied by friends, he took goods from New Salem to New Orleans via flatboat on the Sangamon, Illinois and Mississippi rivers. Lincoln's formal education consisted of about 18 months of schooling; but he was an avid reader and largely self-educated. He was also skilled with an axe and a talented local wrestler, the latter of which helped give him self-confidence. Lincoln avoided hunting and fishing because he did not like killing animals, even for food.
Lincoln's first love was Ann Rutledge. He met her when he first moved to New Salem, and by 1835 they had reached a romantic understanding. Rutledge, however, died on August 25, probably of typhoid fever. Earlier, in either 1833 or 1834, he had met Mary Owens, the sister of his friend Elizabeth Abell, when she was visiting from her home in Kentucky. Late in 1836, Lincoln agreed to a match proposed by Elizabeth between him and her sister, if Mary ever returned to New Salem. Mary did return in November 1836 and Lincoln courted her for a time; however they both had second thoughts about their relationship. On August 16, 1837, Lincoln wrote Mary a letter from Springfield, to which he had moved that April to begin his law practice, suggesting he would not blame her if she ended the relationship. She never replied, and the courtship was over. In 1840, Lincoln became engaged to Mary Todd, from a wealthy slaveholding family based in Lexington, Kentucky. They met in Springfield in December 1839, and were engaged sometime around that Christmas. A wedding was set for January 1, 1841, but the couple split as the wedding approached. They later met at a party, and then married on November 4, 1842, in the Springfield mansion of Mary's married sister. In 1844, the couple bought a house on Eighth and Jackson in Springfield, near Lincoln's law office. The Lincolns soon had a budding family, with the birth of son Robert Todd Lincoln in Springfield, Illinois on August 1, 1843, and second son Edward Baker Lincoln on March 10, 1846, also in Springfield. According to a house girl, Abraham "was remarkably fond of children". The Lincolns did not believe in strict rules and tight boundaries when it came to their children. Robert, however, would be the only one of the Lincolns' children to survive into adulthood. Edward Lincoln died on February 1, 1850 in Springfield, likely of tuberculosis. The Lincolns' grief over this loss was somewhat assuaged by the birth of William "Willie" Wallace Lincoln nearly eleven months later, on December 21. But Willie himself died of a fever at the age of eleven on February 20, 1862, in Washington, D.C., during President Lincoln's first term. The Lincolns' fourth son Thomas "Tad" Lincoln was born on April 4, 1853, and, although he outlived his father, died at the age of eighteen on July 16, 1871 in Chicago. Robert Lincoln eventually went on to attend Phillips Exeter Academy and Harvard College. His (and by extension, his father's) last known lineal descendant, Robert Todd Lincoln Beckwith, died December 24, 1985. The death of the Lincolns' sons had profound effects on both Abraham and Mary. Later in life, Mary Todd Lincoln found herself unable to cope with the stresses of losing her husband and sons, and this (in conjunction with what some historians consider to have been pre-existing bipolar disorder ) eventually led Robert Lincoln to involuntarily commit her to a mental health asylum in 1875. Abraham Lincoln himself was contemporaneously described as suffering from "melancholy" throughout his legal and political life, a condition which modern mental health professionals would now typically characterize as clinical depression. Early political career and military service. Lincoln began his political career in March 1832 at age 23 when he announced his candidacy for the Illinois General Assembly. He was esteemed by the residents of New Salem, but he didn't have an education, powerful friends, or money. The centerpiece of his platform was the undertaking of navigational improvements on the Sangamon River. Before the election he served as a captain in a company of the Illinois militia during the Black Hawk War, although he never saw combat. Lincoln returned from the militia after a few months and was able to campaign throughout the county before the August 6 election. At, he was tall and "strong enough to intimidate any rival." At his first political speech, he grabbed a man accosting a supporter by his "neck and the seat of his trousers", and threw him. When the votes were counted, Lincoln finished eighth out of thirteen candidates (only the top four were elected), but he did manage to secure 277 out of the 300 votes cast in the New Salem precinct. In 1834, he won an election to the state legislature. He was labeled a Whig, but ran a bipartisan campaign. He then decided to become a lawyer, and began teaching himself law by reading "Commentaries on the Laws of England". Admitted to the bar in 1837, he moved to Springfield, Illinois, that April, and began to practice law with John T. Stuart, Mary Todd's cousin, who let Lincoln have the run of his law library while studying to be a lawyer. With a reputation as a formidable adversary during cross-examinations and closing arguments, Lincoln became an able and successful lawyer. In 1841, Lincoln entered law practice with William Herndon, whom Lincoln thought "a studious young man". He served four successive terms in the Illinois House of Representatives as a representative from Sangamon County, affiliated with the Whig party. In 1837, he and another legislator declared that slavery was "founded on both injustice and bad policy" the first time he had publicly opposed slavery. In the 1835–1836 legislative session he'd voted to restrict suffrage to whites only. He would later say that he had been against slavery since he was a boy, but being labelled an abolitionist was "political suicide" in Sangamon County in those years, and so he chose his words carefully when discussing the issue publicly.
Lincoln was a Whig, and since the early 1830s had strongly admired the policies and leadership of Henry Clay. "I have always been an old-line Henry Clay Whig" he professed to friends in 1861. The party favored economic expansion such as improving roads and increasing trade. In 1846, Lincoln was elected to the U.S. House of Representatives, where he served one two-year term. As a House member, Lincoln was a dedicated Whig, showing up for most votes and giving speeches that echoed the party line. He used his office as an opportunity to speak out against the Mexican–American War, which he attributed to President Polk's desire for "military glory — that attractive rainbow, that rises in showers of blood". Lincoln's main stand against Polk occurred in his Spot Resolutions: The war had begun with a violent confrontation on territory disputed by Mexico and Texas, but as Lincoln pointed out, Polk had insisted that Mexican soldiers had "invaded "our territory" and shed the blood of our fellow-citizens on our "own soil". Lincoln demanded that Polk show Congress the exact spot on which blood had been shed, and proof that that spot was on American soil. Congress never enacted the resolution or even debated it, and its introduction resulted in a loss of political support for Lincoln in his district; one Illinois newspaper derisively nicknamed him "spotty Lincoln." Despite his admiration for Henry Clay, Lincoln was a key early supporter of Zachary Taylor's candidacy for the 1848 presidential election. When Lincoln's term ended, the incoming Taylor administration offered him the governorship of the Oregon Territory. The territory leaned heavily Democratic, and Lincoln doubted they would elect him as governor or as a senator after they were admitted to the union, so he returned to Springfield.
Back in Springfield, Lincoln turned most of his energies to making a living practicing law, handling "every kind of business that could come before a prairie lawyer." He "rode the circuit"--that is, appeared in county seats in the mid-state region when the county courts were in session. His reputation grew and he appeared before the Supreme Court of the United States, arguing a case involving a canal boat that sank after hitting a bridge. Lincoln represented numerous transportation interests, such as the river barges and the railroads. As a riverboat man, Lincoln had initially favored riverboat interests, but ultimately he represented whoever hired him. In 1849, he had received a patent for a "device to buoy vessels over shoals". Lincoln's goal had been to lessen the draft of a river craft by pushing horizontal floats into the water alongside the hull. The floats would have served as temporary ballast tanks. The idea was never commercialized, but Lincoln is still the only person to hold a patent and serve as President of the United States. In 1851, he represented the Alton & Sangamon Railroad in a dispute with one of its shareholders, James A. Barret, who had refused to pay the balance on his pledge to the railroad on the grounds that it had changed its originally planned route. Lincoln argued that as a matter of law a corporation is not bound by its original charter when that charter can be amended in the public interest, that the newer proposed Alton & Sangamon route was superior and less expensive, and that accordingly the corporation had a right to sue Mr. Barret for his delinquent payment. He won this case, and the decision by the Illinois Supreme Court was eventually cited by 25 other courts throughout the United States. Lincoln appeared in front of the Illinois Supreme Court 175 times, 51 times as sole counsel, of which, 31 were decided in his favor. Lincoln's most notable criminal trial came in 1858 when he defended William "Duff" Armstrong, who was on trial for the murder of James Preston Metzker. The case is famous for Lincoln's use of judicial notice to show an eyewitness had lied on the stand. After the witness testified to having seen the crime in the moonlight, Lincoln produced a Farmers' Almanac to show that the moon on that date was at such a low angle it could not have produced enough illumination to see anything clearly. Based on this evidence, Armstrong was acquitted.
Lincoln returned to politics in response to the Kansas-Nebraska Act (1854), which expressly repealed the limits on slavery's extent as established by the Missouri Compromise (1820). Illinois Democrat Stephen A. Douglas, the most powerful man in the Senate, proposed popular sovereignty as the solution to the slavery impasse, and incorporated it into the Kansas–Nebraska Act. Douglas argued that in a democracy the people should have the right to decide whether to allow slavery in their territory, rather than have such a decision imposed on them by the national Congress. In the October 16, 1854, "Peoria Speech", Lincoln outlined his position on slavery that he would repeat over the next six years on the route to the presidency. According to a newspaper account of the speech, Lincoln spoke with "a thin high-pitched falsetto voice of much carrying power, that could be heard a long distance in spite of the hustle and bustle of the crowd... [with] the accent and pronunciation peculiar to his native state, Kentucky." In late 1854, Lincoln decided to run for the United States Senate as a Whig. Despite leading in the first six rounds of voting in the state legislature, Lincoln instructed his backers to vote for Lyman Trumbull to prevent pro-Nebraska candidate Joel Aldrich Matteson from winning. Trumbull beat Matteson in the tenth round of voting. The Whigs had been irreparably split by the Kansas-Nebraska Act. "I think I am a Whig, but others say there are not Whigs, and I am an abolitionist, even though I do no more than oppose the expansion of slavery" he said. Drawing on remnants of the old Whig party, and on disenchanted Free Soil, Liberty, and Democratic party members, he was instrumental in forging the shape of the new Republican Party. At the Republican convention in 1856, Lincoln placed second in the contest to become the party's candidate for Vice-President. In 1857–58, Douglas broke with President Buchanan, leading to a fight for control of the Democratic Party. Some eastern Republicans even favored the reelection of Douglas in 1858, since he had led the opposition to the Lecompton Constitution, which would have admitted Kansas as a slave state. Accepting the Republican nomination for Senate in 1858, Lincoln delivered his famous speech: "'A house divided against itself cannot stand.'(Mark 3:25) I believe this government cannot endure permanently half slave and half free. I do not expect the Union to be dissolved — I do not expect the house to fall — but I do expect it will cease to be divided. It will become all one thing, or all the other." The speech created an evocative image of the danger of disunion caused by the slavery debate, and rallied Republicans across the north. As a part of his 1860 presidential campaign strategy Lincoln acquired through banker Jacob Bunn, in May, 1859, the Illinois Staats-Anzeiger, a German-language newspaper of Springfield, Illinois, to further the cause of Republican Party politics among the German-speaking community of the region..
The 1858 campaign featured the Lincoln–Douglas debates, generally considered the most famous political debate in American history. Lincoln warned that "The Slave Power" was threatening the values of republicanism, while Stephen A. Douglas emphasized the supremacy of democracy, as set forth in his Freeport Doctrine, which said that local settlers should be free to choose whether to allow slavery or not and could overrule the Supreme Courts Dred Scott v. Sandford decision. Though the Republican legislative candidates won more popular votes, the Democrats won more seats, and the legislature reelected Douglas to the Senate. Nevertheless, Lincoln's definition of the issues gave him a national political reputation.<ref»Carwardine, p. 89-90 On February 27, 1860, New York party leaders invited Lincoln to give a speech at Cooper Union to group of powerful Republicans. In one of the most important speeches of his career, Lincoln showed that he was a contender for the Republican's presidential nomination. Journalist Noah Brooks reported, "No man ever before made such an impression on his first appeal to a New York audience."
On May 9–10, 1860, the Illinois Republican State Convention was held in Decatur. At this convention, Lincoln received his first endorsement to run for the presidency. On May 18, at the 1860 Republican National Convention in Chicago, Lincoln emerged as the Republican candidate on the third ballot, beating candidates such as William H. Seward and Salmon P. Chase. Why Lincoln won the nomination has been subject of much debate. His expressed views on slavery were seen as more moderate than those of rivals Seward and Chase. Some feel that Seward lost more than Lincoln won, including Seward himself. Others attribute it to luck, and the fact that the convention was held in Lincoln's home state. Historian Doris Kearns Goodwin believes the real reason was Lincoln's skill as a politician. Most Republicans agreed with Lincoln that the North was the aggrieved party as the Slave Power tightened its grasp on the national government with the Dred Scott decision and the presidency of James Buchanan. Throughout the 1850s Lincoln denied that there would ever be a civil war, and his supporters repeatedly rejected claims that his election would incite secession. Meanwhile, Douglas was selected as the candidate of the northern Democrats, with Herschel Vespasian Johnson as the vice-presidential candidate. Delegates from eleven slave states walked out of the Democrat's convention, disagreeing with Douglas's position on Popular sovereignty, and ultimately selected John C. Breckinridge as their candidate. As Douglas stumped the country, Lincoln was the only one of the four major candidates to give no speeches whatever. Instead he monitored the campaign closely but relied on the enthusiasm of the Republican Party. It did the leg work that produced majorities across the North. It produced tons of campaign posters and leaflets, and thousands of newspaper editorials. There were thousands of Republican speakers who focused first on the party platform, and second on Lincoln's life story, emphasizing his childhood poverty. The goal was to demonstrate the superior power of "free labor", whereby a common farm boy could work his way to the top by his own efforts. The Republican Party's production of campaign literature dwarfed the combined opposition. A "Chicago Tribune" writer produced a pamphlet that detailed Lincoln's life, and sold one million copies. On November 6, 1860, Lincoln was elected as the 16th President of the United States, beating Democrat Stephen A. Douglas, John C. Breckinridge of the Southern Democrats, and John Bell of the new Constitutional Union Party. He was the first Republican president, winning entirely on the strength of his support in the North: he was not even on the ballot in ten states in the South, and won only two of 996 counties in all the Southern states. Lincoln received 1,866,452 votes, Douglas 1,376,957 votes, Breckinridge 849,781 votes, and Bell 588,789 votes. The electoral vote was decisive: Lincoln had 180 and his opponents added together had only 123. Turnout was 82.2%, with Lincoln winning the free northern states. Douglas won Missouri, and split New Jersey with Lincoln. Bell won Virginia, Tennessee, and Kentucky, and Breckinridge won the rest of the South. There were fusion tickets in which all of Lincoln's opponents combined to form one ticket in New York, New Jersey and Rhode Island, but even if the anti-Lincoln vote had been combined in every state, Lincoln still would have won because he would still have had a majority in the electoral college. Presidency and the Civil War. With the emergence of the Republicans as the nation's first major sectional party by the mid-1850s, the old Second Party System collapsed and a realignment created the Third Party System. It became the stage on which sectional tensions were played out. Although little of the West–the focal point of sectional tensions– was fit for cotton cultivation, Southern secessionists read the political fallout as a sign that their power in national politics was rapidly weakening. The slave system had been buttressed by the Democratic Party, which was increasingly seen by anti-slavery elements as representing a more pro-Southern position that unfairly permitted the Slave Power to prevail in the nation's territories and to dominate national policy before the Civil War. Yet the Democrats suffered a significant reverse in the electoral realignment of the mid-1850s; they lost the dominance they had achieved over the Whig Party and, indeed, were the minority party in most of the northern states. The 1854 election was a Realigning election or "critical election" that saw a realignment of voting patterns. Abraham Lincoln's election was a watershed in the balance of power of competing national and parochial interests and affiliations.
As Lincoln's election became more likely, secessionists made clear their intent to leave the Union. On December 20, 1860, South Carolina took the lead; by February 1, 1861, Florida, Mississippi, Alabama, Georgia, Louisiana, The seven states soon declared themselves to be a new nation, the Confederate States of America. The upper South (Delaware, Maryland, Virginia, North Carolina, Tennessee, Kentucky, Missouri, and Arkansas) listened to, but initially rejected, the secessionist appeal. President Buchanan and President-elect Lincoln refused to recognize the Confederacy. Attempts at compromise, such as the Crittenden Compromise which would have extended the Missouri line of 1820, were discussed. Despite support for the Crittenden Compromise among some Republicans, Lincoln denounced it in private letters, saying "either the Missouri line extended, or... Pop. Sov. would lose us everything we gained in the election; that filibustering for all South of us, and making slave states of it, would follow in spite of us, under either plan", while other Republicans publicly stated it "would amount to a perpetual covenant of war against every people, tribe, and state owning a foot of land between here and Tierra del Fuego." The Confederate States of America selected Jefferson Davis on February 9, 1861, as their provisional President. President-elect Lincoln evaded possible assassins in Baltimore, and on February 23, 1861, arrived in disguise in Washington, D.C. At his inauguration on March 4, 1861, sharpshooters watched the inaugural platform, while soldiers on horseback patrolled the surrounding area. In his first inaugural address, Lincoln declared, "I hold that in contemplation of universal law and of the Constitution the Union of these States is perpetual. Perpetuity is implied, if not expressed, in the fundamental law of all national governments," arguing further that the purpose of the United States Constitution was "to form a more perfect union" than the Articles of Confederation which were "explicitly" perpetual, thus the Constitution too was perpetual. He asked rhetorically that even were the Constitution a simple contract, would it not require the agreement of all parties to rescind it? Also in his inaugural address, in a final attempt to reunite the states and prevent certain war, Lincoln supported the pending Corwin Amendment to the Constitution, which had passed Congress the previous day. This amendment, which explicitly protected slavery in those states in which it already existed, was considered by Lincoln to be a possible way to stave off secession. A few short weeks before the war he went so far as to pen a letter to every governor asking for their support in ratifying the Corwin Amendment. By the time Lincoln took office, the Confederacy was an established fact, and no leaders of the insurrection proposed rejoining the Union on any terms. The failure of the Peace Conference of 1861 rendered legislative compromise virtually impossible. Buchanan might have allowed the southern states to secede, and some members of his cabinet recommended that. However, conservative Democratic nationalists, such as Jeremiah S. Black, Joseph Holt, and Edwin M. Stanton had taken control of Buchanan's cabinet in early January, and refused to accept secession. Lincoln and nearly every Republican leader adopted this position by March 1861: the Union could not be dismantled. Believing that a peaceful solution was still possible, Lincoln decided to not take any action against the South unless the Unionists themselves were attacked first. This finally happened in April 1861. Historian Allan Nevins argues that Lincoln made three miscalculations in believing that he could preserve the Union, hold government property, and still avoid war. He "temporarily underrated the gravity of the crisis", overestimated the strength of Unionist sentiment in the South and border states, and misunderstood the conditional support of Unionists in the border states.
On April 12, 1861, Union troops at Fort Sumter were fired upon and forced to surrender. On April 15, Lincoln called on the states to send detachments totaling 75,000 troops, to recapture forts, protect the capital, and "preserve the Union", which in his view still existed intact despite the actions of the seceding states. These events forced the states to choose sides. Virginia declared its secession, after which the Confederate capital was moved from Montgomery to Richmond. North Carolina, Tennessee, and Arkansas also voted for secession over the next two months. Missouri, Kentucky and Maryland threatened secession, but neither they nor the slave state of Delaware seceded. Lincoln urgently negotiated with state leaders there, promising not to interfere with slavery. Troops headed south towards Washington, D.C. to protect the capital in response to Lincoln's call. On April 19, angry secessionist mobs in Baltimore, a Maryland city to the north of Washington that controlled the rail links, attacked Union troops traveling to the capital. George William Brown, the Mayor of Baltimore, and other suspect Maryland politicians were arrested and imprisoned at Fort McHenry. Rebel leaders were also arrested in other border areas and held in military prisons without trial. Over 18,000 were arrested. One, Clement Vallandigham, was exiled, but the remainder were released, usually after two or three months ("see": Ex parte Merryman).
The war was a source of constant frustration for the president and occupied nearly all of his time. He had a contentious relationship with General McClellan, who became general-in-chief of all the Union armies in the wake of the embarrassing Union defeat at the First Battle of Bull Run and after the retirement of Winfield Scott in late 1861. Despite his inexperience in military affairs, Lincoln immediately took an active part in determining war strategy. His priorities were twofold: to ensure that Washington was well defended; and to conduct an aggressive war effort that would satisfy the demand in the North for prompt, decisive victory. McClellan, a youthful West Point graduate and railroad executive called back to active military service, He took several months to plan and execute his Peninsula Campaign, with the objective of capturing Richmond by moving the Army of the Potomac by boat to the peninsula and then traveling by land to Richmond. McClellan's delay concerned Lincoln, as did his insistence that no troops were needed to defend Washington, Lincoln insisted on holding some of McClellan's troops to defend the capital, a decision McClellan blamed for the ultimate failure of the Peninsula Campaign. McClellan, a conservative Democrat, was passed over for general-in-chief (that is, chief strategist) in favor of Henry Wager Halleck, after giving Lincoln his "Harrison's Landing Letter", where he offered unsolicited political advice to Lincoln urging caution in the war effort. McClellan's letter incensed Radical Republicans, who successfully pressured Lincoln to appoint John Pope, a Republican, as head of the new Army of Virginia. Pope complied with Lincoln's strategic desire to move toward Richmond from the north, thus protecting the capital from attack. However, Pope was soundly defeated at the Second Battle of Bull Run in the summer of 1862, forcing the Army of the Potomac to defend Washington for a second time. In response to his failure, Pope was sent to Minnesota to fight the Sioux. Despite his dissatisfaction with McClellan's failure to reinforce Pope, Lincoln restored him to command of all forces around Washington, to the dismay of his cabinet (all save Seward), who wished McClellan gone. Two days after McClellan's return to command, General Lee's forces crossed the Potomac River into Maryland, leading to the Battle of Antietam (September 1862). The ensuing Union victory, one of the bloodiest in American history, enabled Lincoln to give notice that he would issue an Emancipation Proclamation in January, but he relieved McClellan of his command after waiting for the conclusion of the 1862 midterm elections and appointed Republican Ambrose Burnside to head the Army of the Potomac. Burnside was politically neutral, which Lincoln desired, and for the most part supported the President's aims. Burnside had promised to follow through on Lincoln's strategic vision for a strong offensive against Lee and Richmond. After Burnside was stunningly defeated at Fredericksburg in December, Joseph Hooker took command, despite his history of "loose talk" and criticizing former commanders. Hooker was routed by Lee at the Battle of Chancellorsville in May, 1863, but continued to command his troops for roughly two months. Hooker did not agree with Lincoln's desire to divide his troops, and possibly force Lee to do the same, and tendered his resignation, which was accepted. During the Gettysburg Campaign he was replaced by George Meade. Using black troops and former slaves was official government policy after the issuance of the Emancipation Proclamation. At first Lincoln was reluctant to fully implement this program, but by the spring of 1863 he was ready to initiate "a massive recruitment of Negro troops." In a letter to Andrew Johnson, the military governor of Tennessee, encouraging him to lead the way in raising black troops, Lincoln wrote, "The bare sight of fifty thousand armed, and drilled black soldiers on the banks of the Mississippi would end the rebellion at once." By the end of 1863, at Lincoln's direction, General Lorenzo Thomas had recruited twenty regiments of African Americans from the Mississippi Valley.
After the Union victory at Gettysburg, Meade's failure to pursue Lee and months of inactivity for the Army of the Potomac persuaded Lincoln that a change was needed. McClellan was seeking the Democratic nomination for President, and Lincoln worried that Grant might also have political aspirations. Lincoln convinced himself that Grant didn't have political aspirations, in the immediate at least, and made Ulysses S. Grant commander of the Union Army. Grant already had a solid string of victories in the Western Theater, including the battles of Vicksburg and Chattanooga. Responding to criticism of Grant, Lincoln replied, "I can't spare this man. He fights." Grant waged his bloody Overland Campaign in 1864 with a strategy of a war of attrition, characterized by high Union losses at battles such as the Wilderness and Cold Harbor, but by proportionately higher Confederate losses. The high casualty figures alarmed the nation, and, after Grant lost a third of his army, Lincoln asked what Grant's plans were. "I propose to fight it out on this line if it takes all summer," replied Grant. Lincoln and the Republican party mobilized support throughout the North, backed Grant to the hilt, and replaced his losses. The Confederacy was out of replacements, so Lee's army shrank with every battle, forcing it back to trenches outside Petersburg. In April 1865, Lee's army finally crumbled under Grant's pounding, and Richmond fell. Lincoln authorized Grant to target the Confederate infrastructure – such as plantations, railroads, and bridges – hoping to destroy the South's morale and weaken its economic ability to continue fighting. This strategy allowed Generals Sherman and Sheridan to destroy plantations and towns in the Shenandoah Valley, Georgia, and South Carolina. The damage caused by Sherman's March to the Sea through Georgia totaled more than $100 million by Sherman's own estimate. Lincoln grasped the need to control strategic points (such as the Mississippi River and the fortress city of Vicksburg) and understood the importance of defeating the enemy's army, rather than simply capturing territory. He had, however, limited success in motivating his commanders to adopt his strategies until late 1863, when he found a man who shared his vision of the war in Ulysses S. Grant. Only then could he relentlessly pursue a series of coordinated offensives in multiple theaters, and have a top commander who agreed on the use of black troops. Two days a week, Lincoln would meet with his cabinet in the afternoon, and occasionally his wife would force him to take a carriage ride because she was concerned he was working too hard. Throughout the war, Lincoln showed an intense interest with the military campaigns. He spent hours at the War Department telegraph office, reading dispatches from the field. He visited battle sites frequently, and seemed fascinated by scenes of war. During Jubal Anderson Early's raid on Washington, D.C. in 1864, Lincoln was watching the combat from an exposed position; captain Oliver Wendell Holmes, Jr. shouted at him, "Get down, you damn fool, before you get shot!"
Lincoln maintained that the powers of his administration to end slavery were limited by the Constitution. He expected to cause the eventual extinction of slavery by stopping its further expansion into any U.S. territory, and by persuading states to accept compensated emancipation if the state would outlaw slavery (an offer that took effect only in Washington, D.C.). Guelzo says Lincoln believed that shrinking slavery in this way would make it uneconomical, and place it back on the road to eventual extinction that the Founders had envisioned. In July 1862, Congress passed the Second Confiscation Act, which freed the slaves of anyone convicted of aiding the rebellion. Although Lincoln believed it wasn't in Congress's remit to free any slaves, he approved the bill. He felt freeing the slaves could only be done by the Commander in Chief during wartime, and that signing the bill would help placate those in Congress who wanted to do it through legislation. In that month, Lincoln discussed a draft of the Emancipation Proclamation with his cabinet. In it, he stated that "as a fit and necessary military measure" (and according to Donald not for moral reasons) on January 1, 1863, "all persons held as a slaves" in the Confederate states will " thenceforward, and forever, be free." The Emancipation Proclamation, announced on September 22, 1862 and put into effect on January 1, 1863, freed slaves in territories not already under Union control. As Union armies advanced south, more slaves were liberated until all of them in Confederate territory (over three million) were freed. Lincoln later said: "I never, in my life, felt more certain that I was doing right, than I do in signing this paper." The proclamation made the abolition of slavery in the rebel states an official war goal. Lincoln then threw his energies into passage of the Thirteenth Amendment to permanently abolish slavery throughout the nation. He personally lobbied individual Congressmen for the Amendment, which was passed by the Congress in early 1865, shortly before his death. A few days after the Emancipation was announced, thirteen Republican governors met at the War Governors' Conference; they supported the president's Proclamation, but suggested the removal of General George B. McClellan as commander of the Union's Army of the Potomac. For some time, Lincoln continued earlier plans to set up colonies for the newly freed slaves. He commented favorably on colonization in the Emancipation Proclamation, but all attempts at such a massive undertaking failed. As Frederick Douglass observed, Lincoln was, "The first great man that I talked with in the United States freely who in no single instance reminded me of the difference between himself and myself, of the difference of color."
Although the Battle of Gettysburg was a Union victory, it was also the bloodiest battle of the war and dealt a blow to Lincoln's war effort. As the Union Army decreased in numbers due to casualties, more soldiers were needed to replace the ranks. Lincoln's 1863 military drafts were considered "odious" among many in the north, particularly immigrants. The New York Draft Riots of July 1863 were the most notable manifestation of this discontent. "If the election were to occur now, the result would be extremely doubtful, and although most of our discreet friends are sanguine of the result, my impression is, the chances would be against us. The draft is very odious in the State... the Democratic leaders have succeeded in exciting prejudice and passion, and have infused their poison into the minds of the people to a very large extent, and the changes are against us." Therefore, in the fall of 1863, Lincoln's principal aim was to sustain public support for the war effort. This goal became the focus of his address at the Gettysburg battlefield cemetery on November 19. The "Gettysburg Address" is one of the most quoted speeches in United States history. It was delivered at the dedication of the Soldiers' National Cemetery in Gettysburg, Pennsylvania, on the afternoon of Thursday, November 19, 1863, during the American Civil War, four and a half months after the Union armies defeated those of the Confederacy at the decisive Battle of Gettysburg. Abraham Lincoln's carefully crafted address, secondary to other presentations that day, came to be regarded as one of the greatest speeches in American history. In just over two minutes, Lincoln invoked the principles of human equality espoused by the Declaration of Independence and redefined the Civil War as a struggle not merely for the Union, but as "a new birth of freedom" that would bring true equality to all of its citizens, and that would also create a unified nation in which states' rights were no longer dominant. Beginning with the now-iconic phrase, "Four score and seven years ago...", Lincoln referred to the events of the Civil War and described the ceremony at Gettysburg as an opportunity not only to consecrate the grounds of a cemetery, but also to dedicate the living to the struggle to ensure that "government of the people, by the people, for the people, shall not perish from the earth".
After Union victories at Gettysburg, Vicksburg, and Chattanooga in 1863, overall victory seemed at hand, and Lincoln promoted Ulysses S. Grant General-in-Chief on March 12, 1864. When the spring campaigns turned into bloody stalemates, Lincoln supported Grant's strategy of wearing down Lee's Confederate army at the cost of heavy Union casualties. With an election looming, he easily defeated efforts to deny his renomination. At the Convention, the Republican Party selected Andrew Johnson, a War Democrat from the Southern state of Tennessee, as his running mate to form a broader coalition. They ran on the new Union Party ticket uniting Republicans and War Democrats. Lincoln did not show the pledge to his cabinet, but asked them to sign the sealed envelope. While the Democratic platform followed the Peace wing of the party and called the war a "failure," their candidate, General George B. McClellan, supported the war and repudiated the platform. Lincoln provided Grant with new replacements and mobilized his party to support Grant and win local support for the war effort. Sherman's capture of Atlanta in September ended defeatist jitters; the Democratic Party was deeply split, with some leaders and most soldiers openly for Lincoln; the Union party was united and energized, and Lincoln was easily reelected in a landslide. He won all but three states, including 78% of the Union soldiers' vote.
Reconstruction began during the war as Lincoln and his associates pondered questions of how to reintegrate the Southern states and what to do with Confederate leaders and the freed slaves. Lincoln led the "moderates" regarding Reconstruction policy, and was usually opposed by the Radical Republicans, under Thaddeus Stevens in the House and Charles Sumner and Benjamin Wade in the Senate (though he cooperated with these men on most other issues). Determined to find a course that would reunite the nation and not alienate the South, Lincoln urged that speedy elections under generous terms be held throughout the war in areas behind Union lines. His Amnesty Proclamation of December 8, 1863, offered pardons to those who had not held a Confederate civil office, had not mistreated Union prisoners, and would sign an oath of allegiance. Critical decisions had to be made as state after state was reconquered. Of special importance were Tennessee, where Lincoln appointed Andrew Johnson as governor, and Louisiana, where Lincoln attempted a plan that would restore statehood when 10% of the voters agreed to it. The Radicals thought this policy too lenient, and passed their own plan, the Wade-Davis Bill, in 1864. When Lincoln pocket vetoed the bill, the Radicals retaliated by refusing to seat representatives elected from Louisiana, Arkansas, and Tennessee. Near the end of the war, Lincoln made an extended visit to Grant's headquarters at City Point, Virginia. This allowed the president to confer in person with Grant and Sherman about ending hostilities (as Sherman managed a hasty visit to Grant from his forces in North Carolina at the same time). Lincoln also was able to visit Richmond after it was taken by the Union forces and to make a public gesture of sitting at Jefferson Davis' own desk, symbolically saying to the nation that the President of the United States held authority over the entire land. He was greeted at the city as a conquering hero by freed slaves, whose sentiments were epitomized by one admirer's quote, "I know I am free for I have seen the face of Father Abraham and have felt him." When a general asked Lincoln how the defeated Confederates should be treated, Lincoln replied, "Let 'em up easy." Lincoln arrived back in Washington on the evening of April 9, 1865, the day Lee surrendered at Appomattox Court House in Virginia. The war was effectively over. The other rebel armies surrendered soon after, and there was no subsequent guerrilla warfare.
Lincoln's rhetoric defined the issues of the war for the nation, the world, and posterity. The Gettysburg Address defied Lincoln's own prediction that "the world will little note, nor long remember what we say here." His second inaugural address is also greatly admired and often quoted. In recent years, historians have stressed Lincoln's use of and redefinition of republican values. As early as the 1850s, a time when most political rhetoric focused on the sanctity of the Constitution, Lincoln shifted emphasis to the Declaration of Independence as the foundation of American political values—what he called the "sheet anchor" of republicanism. The Declaration's emphasis on freedom and equality for all, rather than the Constitution's tolerance of slavers, shifted the debate. As Diggins concludes regarding the highly influential Cooper Union speech, "Lincoln presented Americans a theory of history that offers a profound contribution to the theory and destiny of republicanism itself." His position gained strength because he highlighted the moral basis of republicanism, rather than its legalisms. Nevertheless, in 1861 Lincoln justified the war in terms of legalisms (the Constitution was a contract, and for one party to get out of a contract all the other parties had to agree), and then in terms of the national duty to guarantee a "republican form of government" in every state. That duty was also the principle underlying federal intervention in Reconstruction. In his Gettysburg Address Lincoln redefined the American nation, arguing that it was born not in 1789 but in 1776, "conceived in Liberty, and dedicated to the proposition that all men are created equal." He declared that the sacrifices of battle had rededicated the nation to the propositions of democracy and equality, "that this nation shall have a new birth of freedom — and that government of the people, by the people, for the people, shall not perish from the earth." By emphasizing the centrality of the nation, he rebuffed the claims of state sovereignty. While some critics say Lincoln moved too far and too fast, they agree that he dedicated the nation to values that marked "a new founding of the nation."
Lincoln believed in the Whig theory of the presidency, which left Congress to write the laws while he signed them; Lincoln exercised his veto power only four times, the only significant instance being his pocket veto of the Wade-Davis Bill. Thus, he signed the Homestead Act in 1862, making millions of acres of government-held land in the West available for purchase at very low cost. The Morrill Land-Grant Colleges Act, also signed in 1862, provided government grants for state agricultural colleges in each state. The Pacific Railway Acts of 1862 and 1864 granted federal support for the construction of the United States' First Transcontinental Railroad, which was completed in 1869. The passage of the Homestead Act and the Pacific Railway Acts was made possible by the absence of Southern congressmen and senators who had opposed the measures in the 1850s. Other important legislation involved two measures to raise revenues for the Federal government: tariffs (a policy with long precedent), and a Federal income tax (which was new). In 1861, Lincoln signed the second and third Morrill Tariff (the first had become law under James Buchanan). In 1861, Lincoln signed the Revenue Act of 1861 creating the first U.S. income tax. This created a flat tax of 3% on incomes above $800 ($ in current dollars), which was later changed by the Revenue Act of 1862 Lincoln also presided over the expansion of the federal government's economic influence in several other areas. The creation of the system of national banks by the National Banking Acts of 1863, 1864, and 1865 allowed the creation of a strong national financial system. In 1862, Congress created, with Lincoln's approval, the Department of Agriculture, although that institution would not become a Cabinet-level department until 1889. The Legal Tender Act of 1862 established the United States Note, the first paper currency in United States history since the Continentals that were issued during the Revolution. This was done to increase the money supply to pay for fighting the war. In 1862, Lincoln sent a senior general, John Pope, to put down the "Sioux Uprising" in Minnesota. Presented with 303 death warrants for convicted Santee Dakota who were accused of killing innocent farmers, Lincoln ordered a personal review of these warrants, eventually approving 39 of these for execution (one was later reprieved). Abraham Lincoln is largely responsible for the institution of the Thanksgiving holiday in the United States. Prior to Lincoln's presidency, Thanksgiving, while a regional holiday in New England since the 17th century, had only been proclaimed by the federal government sporadically, and on irregular dates. The last such proclamation was during James Madison's presidency fifty years before. In 1863, Lincoln declared the final Thursday in November to be a day of Thanksgiving, and the holiday has been celebrated annually then ever since.
Originally, John Wilkes Booth, a well-known actor and a Confederate spy from Maryland, had formulated a plan to kidnap Lincoln in exchange for the release of Confederate prisoners. After attending an April 11 speech in which Lincoln promoted voting rights for blacks, an incensed Booth changed his plans and determined to assassinate the president. Learning that the President and First Lady would be attending Ford's Theatre, he laid his plans, assigning his co-conspirators to assassinate Vice President Andrew Johnson and Secretary of State William H. Seward. Without his main bodyguard Ward Hill Lamon, to whom he related his famous dream regarding his own assassination, Lincoln left to attend the play "Our American Cousin" on April 14, 1865. As a lone bodyguard wandered, and Lincoln sat in his state box (Box 7) in the balcony, Booth crept up behind the President and waited for what he thought would be the funniest line of the play ("You sock-dologizing old man-trap"), hoping the laughter would muffle the noise of the gunshot. When the laughter began, Booth jumped into the box and aimed a single-shot, round-ball.44 caliber (11 mm) Deringer at his head, firing at point-blank range. Major Henry Rathbone momentarily grappled with Booth but was cut by Booth's knife. Booth then leaped to the stage and shouted "Sic semper tyrannis!" () and escaped, despite suffering a broken leg in the leap. A twelve-day manhunt ensued, in which Booth was chased by Federal agents (under the direction of Secretary of War Edwin M. Stanton). He was eventually cornered in a Virginia barn house and shot, dying of his wounds soon after. An army surgeon, Doctor Charles Leale, initially assessed Lincoln's wound as mortal. The President was taken across the street from the theater to the Petersen House, where he lay in a coma for nine hours before dying. Several physicians attended Lincoln, including U.S. Army Surgeon General Joseph K. Barnes of the Army Medical Museum. Using a probe, Barnes located some fragments of Lincoln's skull and the ball lodged inside his brain. Lincoln never regained consciousness and was pronounced dead at 7:22:10 a.m. April 15, 1865. He was the first president to be assassinated or to lie in state. Lincoln's body was carried by train in a grand funeral procession through several states on its way back to Illinois. While much of the nation mourned him as the savior of the United States, Copperheads celebrated the death of a man they considered a tyrant. The Lincoln Tomb in Oak Ridge Cemetery in Springfield, is tall and, by 1874, was surmounted with several bronze statues of Lincoln. To prevent repeated attempts to steal Lincoln's body and hold it for ransom, Robert Todd Lincoln had it exhumed and reinterred in concrete several feet thick in 1901.
In March 1860 in a speech in New Haven, Connecticut, Lincoln said, regarding slavery, "Whenever this question shall be settled, it must be settled on some philosophical basis. No policy that does not rest upon some philosophical public opinion can be permanently maintained." The philosophical basis for Lincoln's beliefs regarding slavery and other issues of the day require that Lincoln be examined "seriously as a man of ideas." Lincoln was a strong supporter of the American Whig version of liberal capitalism who, more than most politicians of the time, was able to express his ideas within the context of Nineteenth Century religious beliefs. There were few people who strongly or directly influenced Lincoln's moral and intellectual development and perspectives. There was no teacher, mentor, church leader, community leader, or peer that Lincoln would credit in later years as a strong influence on his intellectual development. Lacking a formal education, Lincoln's personal philosophy was shaped by "an amazingly retentive memory and a passion for reading and learning." It was Lincoln's reading, rather than his relationships, that were most influential in shaping his personal beliefs. Even as a child, Lincoln largely rejected organized religion, but the Calvinistic "doctrine of necessity" would remain a factor throughout his life. In 1846 Lincoln described the effect of this doctrine as "that the human mind is impelled to action, or held in rest by some power, over which the mind itself has no control." In April 1864, in justifying his actions regarding Emancipation, Lincoln wrote, "I claim not to have controlled events, but confess plainly that events have controlled me. Now, at the end of three years struggle the nation's condition is not what either party, or any man devised, or expected. God alone can claim it." As Lincoln matured, and especially during his term as president, the idea of a divine will somehow interacting with human affairs increasingly influenced his public expressions. On a personal level, the death of his son Willie in February 1862 may have caused Lincoln to look towards religion for answers and solace. Lincoln's religious skepticism was fueled by his exposure to the ideas of the Lockean Enlightenment and classical liberalism, especially economic liberalism. Consistent with the common practice of the Whig party, Lincoln would often use the Declaration of Independence as the philosophical and moral expression of these two philosophies. In a February 22, 1861 speech at Independence Hall in Philadelphia Lincoln said, He found in the Declaration justification for Whig economic policy and opposition to territorial expansion and the nativist platform of the Know Nothings. In claiming that all men were created free, Lincoln and the Whigs argued that this freedom required economic advancement, expanded education, territory to grow, and the ability of the nation to absorb the growing immigrant population. It was the Declaration of Independence, rather than the Bible, that Lincoln most relied on to oppose any further territorial expansion of slavery. He saw the Declaration as more than a political document. To him, as well as to many abolitionists and other antislavery leaders, it was, foremost, a moral document that had forever determined valuable principles for the future shaping of the nation.
Lincoln's death made the President a national martyr, regarded by historians in numerous polls as among the greatest presidents in U.S. history, usually in the top three, along with George Washington and Franklin D. Roosevelt. A study published in 2004, found that scholars in the fields of history and politics ranked Lincoln number one, while law scholars placed him second after Washington. Among contemporary admirers, Lincoln is usually seen as personifying classical values of honesty and integrity, as well as respect for individual and minority rights, and human freedom in general. Many American organizations of all purposes and agendas continue to cite his name and image, with interests ranging from the gay rights-supporting Log Cabin Republicans to the insurance corporation Lincoln National Corporation. The Lincoln automobile brand is also named after him. The ballistic missile submarine "Abraham Lincoln" (SSBN-602) and the aircraft carrier "Abraham Lincoln" (CVN-72) were named in his honor. During the Spanish Civil War, the American faction of the International Brigades named themselves the Abraham Lincoln Brigade. Lincoln has been memorialized in many town, city, and county names, Lincoln, Illinois, is the only city to be named for Abraham Lincoln before he became President. Lincoln's name and image appear in numerous places. These include the Lincoln Memorial in Washington, D.C., the U.S. Lincoln $5 bill and the Lincoln cent, and Lincoln's sculpture on Mount Rushmore. Abraham Lincoln Birthplace National Historical Park in Hodgenville, Kentucky, Lincoln Boyhood National Memorial in Lincoln City, Indiana, and Lincoln Home National Historic Site in Springfield, Illinois, In addition, New Salem, Illinois (a reconstruction of Lincoln's early adult hometown), Ford's Theatre, and Petersen House (where he died) are all preserved as museums. The state nickname for Illinois is "Land of Lincoln"; the slogan has appeared continuously on nearly all Illinois license plates issued since 1954. Abraham Lincoln's birthday, February 12, was never a national holiday, but it was observed by 30 states. In 1971, Presidents Day became a national holiday, combining Lincoln's and Washington's birthdays, and replacing most states' celebration of his birthday. As of 2005, Lincoln's Birthday is a legal holiday in 10 states. The Abraham Lincoln Association was formed in 1908 to commemorate the centennial of Lincoln's birth. The Association is now the oldest group dedicated to the study of Lincoln. To commemorate his 200th birthday in February 2009, Congress established the Abraham Lincoln Bicentennial Commission (ALBC) in 2000 to honor Lincoln. The Abraham Lincoln Presidential Library and Museum is located in Springfield and is run by the State of Illinois. Lincoln owned a model 1857 Waltham William Ellery watch, with serial number 67613. This watch is now in the custody of the Smithsonian Museum. On March 11, 2009, the National Museum of American History found a message engraved inside Lincoln's watch by a watchmaker named Jonathan Dillon who was repairing it at the outbreak of the American Civil War. The engraving reads (in part): "Fort Sumpter was attacked by the rebels" and "thank God we have a government."
Aristotle (, "Aristotélēs") (384 BC – 322 BC) was a Greek philosopher, a student of Plato and teacher of Alexander the Great. His writings cover many subjects, including physics, metaphysics, poetry, theater, music, logic, rhetoric, politics, government, ethics, biology, and zoology. Together with Plato and Socrates (Plato's teacher), Aristotle is one of the most important founding figures in Western philosophy. Aristotle's writings constitute a first at creating a comprehensive system of Western philosophy, encompassing morality and aesthetics, logic and science, politics and metaphysics. Aristotle's views on the physical sciences profoundly shaped medieval scholarship, and their influence extended well into the Renaissance, although they were ultimately replaced by Newtonian physics. In the biological sciences, some of his observations were confirmed to be accurate only in the nineteenth century. His works contain the earliest known formal study of logic, which was incorporated in the late nineteenth century into modern formal logic. In metaphysics, Aristotelianism had a profound influence on philosophical and theological thinking in the Islamic and Jewish traditions in the Middle Ages, and it continues to influence Christian theology, especially Eastern Orthodox theology, and the scholastic tradition of the Catholic Church. His ethics, though always influential, gained renewed interest with the modern advent of virtue ethics. All aspects of Aristotle's philosophy continue to be the object of active academic study today. Though Aristotle wrote many elegant treatises and dialogues (Cicero described his literary style as "a river of gold"), it is thought that the majority of his writings are now lost and only about one-third of the original works have survived. Despite the far-reaching appeal that Aristotle's works have traditionally enjoyed, today modern scholarship questions a substantial portion of the Aristotelian corpus as authentically Aristotle's own.
Aristotle was born in Stageira, Chalcidice, in 384 BC, about east of modern-day Thessaloniki. His father Nicomachus was the personal physician to King Amyntas of Macedon. Aristotle was trained and educated as a member of the aristocracy. At about the age of eighteen, he went to Athens to continue his education at Plato's Academy. Aristotle remained at the academy for nearly twenty years, not leaving until after Plato's death in 347 BC. He then traveled with Xenocrates to the court of his friend Hermias of Atarneus in Asia Minor. While in Asia, Aristotle traveled with Theophrastus to the island of Lesbos, where together they researched the botany and zoology of the island. Aristotle married Hermias's adoptive daughter (or niece) Pythias. She bore him a daughter, whom they named Pythias. Soon after Hermias' death, Aristotle was invited by Philip II of Macedon to become the tutor to his son Alexander the Great in 343 B.C. Aristotle was appointed as the head of the royal academy of Macedon. During that time he gave lessons not only to Alexander, but also to two other future kings: Ptolemy and Cassander. In his "Politics", Aristotle states that only one thing could justify monarchy, and that was if the virtue of the king and his family were greater than the virtue of the rest of the citizens put together. Tactfully, he included the young prince and his father in that category. Aristotle encouraged Alexander toward eastern conquest, and his attitude towards Persia was unabashedly ethnocentric. In one famous example, he counsels Alexander to be 'a leader to the Greeks and a despot to the barbarians, to look after the former as after friends and relatives, and to deal with the latter as with beasts or plants'. By 335 BC he had returned to Athens, establishing his own school there known as the Lyceum. Aristotle conducted courses at the school for the next twelve years. While in Athens, his wife Pythias died and Aristotle became involved with Herpyllis of Stageira, who bore him a son whom he named after his father, Nicomachus. According to the Suda, he also had an eromenos, Palaephatus of Abydus. It is during this period in Athens from 335 to 323 BC when Aristotle is believed to have composed many of his works. Aristotle wrote many dialogues, only fragments of which survived. The works that have survived are in treatise form and were not, for the most part, intended for widespread publication, as they are generally thought to be lecture aids for his students. His most important treatises include "Physics", "Metaphysics", "Nicomachean Ethics", "Politics", "De Anima (On the Soul)" and "Poetics". Aristotle not only studied almost every subject possible at the time, but made significant contributions to most of them. In physical science, Aristotle studied anatomy, astronomy, embryology, geography, geology, meteorology, physics and zoology. In philosophy, he wrote on aesthetics, ethics, government, metaphysics, politics, economics, psychology, rhetoric and theology. He also studied education, foreign customs, literature and poetry. His combined works constitute a virtual encyclopedia of Greek knowledge. It has been suggested that Aristotle was probably the last person to know everything there was to be known in his own time. Near the end of Alexander's life, Alexander began to suspect plots against himself, and threatened Aristotle in letters. Aristotle had made no secret of his contempt for Alexander's pretense of divinity, and the king had executed Aristotle's grandnephew Callisthenes as a traitor. A widespread tradition in antiquity suspected Aristotle of playing a role in Alexander's death, but there is little evidence for this. Upon Alexander's death, anti-Macedonian sentiment in Athens once again flared. Eurymedon the hierophant denounced Aristotle for not holding the gods in honor. Aristotle fled the city to his mother's family estate in Chalcis, explaining, "I will not allow the Athenians to sin twice against philosophy," a reference to Athens's prior trial and execution of Socrates. However, he died in Euboea of natural causes within the year (in 322 BC). Aristotle named chief executor his student Antipater and left a will in which he asked to be buried next to his wife.
Aristotle "says that 'on the subject of reasoning' he 'had nothing else on an earlier date to speak of'". However, Plato reports that syntax was devised before him, by Prodicus of Ceos, who was concerned by the correct use of words. Logic seems to have emerged from dialectics; the earlier philosophers made frequent use of concepts like "reductio ad absurdum" in their discussions, but never truly understood the logical implications. Even Plato had difficulties with logic; although he had a reasonable conception of a deducting system, he could never actually construct one and relied instead on his dialectic. Plato believed that deduction would simply follow from premises, hence he focused on maintaining solid premises so that the conclusion would logically follow. Consequently, Plato realized that a method for obtaining conclusions would be most beneficial. He never succeeded in devising such a method, but his best attempt was published in his book "Sophist", where he introduced his division method.
The order of the books (or the teachings from which they are composed) is not certain, but this list was derived from analysis of Aristotle's writings. It goes from the basics, the analysis of simple terms in the "Categories," the analysis of propositions and their elementary relations in "On Interpretation", to the study of more complex forms, namely, syllogisms (in the "Analytics") and dialectics (in the "Topics" and "Sophistical Refutations"). The first three treatises form the core of the logical theory "stricto sensu": the grammar of the language of logic and the correctness rules of reasoning. There is one volume of Aristotle's concerning logic not found in the "Organon", namely the fourth book of "Metaphysics.".
Like his teacher Plato, Aristotle's philosophy aims at the universal. Aristotle, however, found the universal in particular things, which he called the essence of things, while Plato finds that the universal exists apart from particular things, and is related to them as their prototype or exemplar. For Aristotle, therefore, philosophic method implies the ascent from the study of particular phenomena to the knowledge of essences, while for Plato philosophic method means the descent from a knowledge of universal Forms (or ideas) to a contemplation of particular imitations of these. For Aristotle, "form" still refers to the unconditional basis of phenomena but is "instantiated" in a particular substance (see "Universals and particulars", below). In a certain sense, Aristotle's method is both inductive and deductive, while Plato's is essentially deductive from "a priori" principles. In Aristotle's terminology, "natural philosophy" is a branch of philosophy examining the phenomena of the natural world, and includes fields that would be regarded today as physics, biology and other natural sciences. In modern times, the scope of "philosophy" has become limited to more generic or abstract inquiries, such as ethics and metaphysics, in which logic plays a major role. Today's philosophy tends to exclude empirical study of the natural world by means of the scientific method. In contrast, Aristotle's philosophical endeavors encompassed virtually all facets of intellectual inquiry. In the larger sense of the word, Aristotle makes philosophy coextensive with reasoning, which he also would describe as "science". Note, however, that his use of the term "science" carries a different meaning than that covered by the term "scientific method". For Aristotle, "all science ("dianoia") is either practical, poetical or theoretical" ("Metaphysics" 1025b25). By practical science, he means ethics and politics; by poetical science, he means the study of poetry and the other fine arts; by theoretical science, he means physics, mathematics and metaphysics. If logic (or "analytics") is regarded as a study preliminary to philosophy, the divisions of Aristotelian philosophy would consist of: (1) Logic; (2) Theoretical Philosophy, including Metaphysics, Physics, Mathematics, (3) Practical Philosophy and (4) Poetical Philosophy. In the period between his two stays in Athens, between his times at the Academy and the Lyceum, Aristotle conducted most of the scientific thinking and research for which he is renowned today. In fact, most of Aristotle's life was devoted to the study of the objects of natural science. Aristotle's metaphysics contains observations on the nature of numbers but he made no original contributions to mathematics. He did, however, perform original research in the natural sciences, e.g., botany, zoology, physics, astronomy, chemistry, meteorology, and several other sciences. Aristotle's writings on science are largely qualitative, as opposed to quantitative. Beginning in the sixteenth century, scientists began applying mathematics to the physical sciences, and Aristotle's work in this area was deemed hopelessly inadequate. His failings were largely due to the absence of concepts like mass, velocity, force and temperature. He had a conception of speed and temperature, but no quantitative understanding of them, which was partly due to the absence of basic experimental devices, like clocks and thermometers. His writings provide an account of many scientific observations, a mixture of precocious accuracy and curious errors. For example, in his "History of Animals" he claimed that human males have more teeth than females and in the "Generation of Animals" he said the female is as it were a deformed male. In a similar vein, John Philoponus, and later Galileo, showed by simple experiments that Aristotle's theory that a heavier object falls faster than a lighter object is incorrect. On the other hand, Aristotle refuted Democritus's claim that the Milky Way was made up of "those stars which are shaded by the earth from the sun's rays," pointing out (correctly, even if such reasoning was bound to be dismissed for a long time) that, given "current astronomical demonstrations" that "the size of the sun is greater than that of the earth and the distance of the stars from the earth many times greater than that of the sun, then...the sun shines on all the stars and the earth screens none of them." In places, Aristotle goes too far in deriving 'laws of the universe' from simple observation and over-stretched reason. Today's scientific method assumes that such thinking without sufficient facts is ineffective, and that discerning the validity of one's hypothesis requires far more rigorous experimentation than that which Aristotle used to support his laws. Aristotle also had some scientific blind spots. He posited a geocentric cosmology that we may discern in selections of the "Metaphysics", which was widely accepted up until the 1500s. From the 3rd century to the 1500s, the dominant view held that the Earth was the center of the universe (geocentrism). Since he was perhaps the philosopher most respected by European thinkers during and after the Renaissance, these thinkers often took Aristotle's erroneous positions as given, which held back science in this epoch. However, Aristotle's scientific shortcomings should not mislead one into forgetting his great advances in the many scientific fields. For instance, he founded logic as a formal science and created foundations to biology that were not superseded for two millennia. Moreover, he introduced the fundamental notion that nature is composed of things that change and that studying such changes can provide useful knowledge of underlying constants.
Additionally, things can be causes of one another, causing each other reciprocally, as hard work causes fitness and vice versa, although not in the same way or function, the one is as the beginning of change, the other as the goal. (Thus Aristotle first suggested a reciprocal or circular causality as a relation of mutual dependence or influence of cause upon effect). Moreover, Aristotle indicated that the same thing can be the cause of contrary effects; its presence and absence may result in different outcomes. Simply it is the goal or purpose that brings about an event (not necessarily a mental goal). Taking our two dominos, it requires someone to intentionally knock the dominos over as they cannot fall themselves. Aristotle marked two modes of causation: proper (prior) causation and accidental (chance) causation. All causes, proper and incidental, can be spoken as potential or as actual, particular or generic. The same language refers to the effects of causes, so that generic effects assigned to generic causes, particular effects to particular causes, operating causes to actual effects. Essentially, causality does not suggest a temporal relation between the cause and the effect. All further investigations of causality will consist of imposing the favorite hierarchies on the order causes, such as final > efficient > material > formal (Thomas Aquinas), or of restricting all causality to the material and efficient causes or to the efficient causality (deterministic or chance) or just to regular sequences and correlations of natural phenomena (the natural sciences describing how things happen instead of explaining the whys and wherefores).
Aristotle held more accurate theories on some optical concepts than other philosophers of his day. The earliest known written evidence of a camera obscura can be found in Aristotle's documentation of such a device in 350 BC in "Problemata". Aristotle's apparatus contained a dark chamber that had a single small hole, or aperture, to allow for sunlight to enter. Aristotle used the device to make observations of the sun and noted that no matter what shape the hole was, the sun would still be correctly displayed as a round object. In modern cameras, this is analogous to the diaphragm. Aristotle also made the observation that when the distance between the tiny hole and the surface with the image increased, the image was amplified.
Spontaneity and chance are causes of effects. Chance as an incidental cause lies in the realm of accidental things. It is "from what is spontaneous" (but note that what is spontaneous does not come from chance). For a better understanding of Aristotle's conception of "chance" it might be better to think of "coincidence": Something takes place by chance if a person sets out with the intent of having one thing take place, but with the result of another thing (not intended) taking place. For example: A person seeks donations. That person may find another person willing to donate a substantial sum. However, if the person seeking the donations met the person donating, not for the purpose of collecting donations, but for some other purpose, Aristotle would call the collecting of the donation by that particular donator a result of chance. It must be unusual that something happens by chance. In other words, if something happens all or most of the time, we cannot say that it is by chance. There is also more specific kind of chance, which Aristotle names "luck", that can only apply to human beings, since it is in the sphere of moral actions. According to Aristotle, luck must involve choice (and thus deliberation), and only humans are capable of deliberation and choice. "What is not capable of action cannot do anything by chance".
Aristotle examines the concept of substance and essence ("ousia") in his "Metaphysics", Book VII and he concludes that a particular substance is a combination of both matter and form. As he proceeds to the book VIII, he concludes that the matter of the substance is the substratum or the stuff of which it is composed, "e.g." the matter of the house are the bricks, stones, timbers etc., or whatever constitutes the "potential" house. While the form of the substance, is the "actual" house, namely 'covering for bodies and chattels' or any other differentia (see also predicables). The formula that gives the components is the account of the matter, and the formula that gives the differentia is the account of the form. With regard to the change ("kinesis") and its causes now, as he defines in his Physics and On Generation and Corruption 319b-320a, he distinguishes the coming to be from: 1) growth and diminution, which is change in quantity; 2) locomotion, which is change in space; and 3) alteration, which is change in quality. The coming to be is a change where nothing persists of which the resultant is a property. In that particular change he introduces the concept of potentiality ("dynamis") and actuality ("entelecheia") in association with the matter and the form. Referring to potentiality, this is what a thing is capable of doing, or being acted upon, if it is not prevented by something else. For example, the seed of a plant in the soil is potentially ("dynamei") plant, and if is not prevented by something, it will become a plant. Potentially beings can either 'act' ("poiein") or 'be acted upon' ("paschein"), which can be either innate or learned. For example, the eyes possess the potentiality of sight (innate – being acted upon), while the capability of playing the flute can be possessed by learning (exercise – acting). Actuality is the fulfillment of the end of the potentiality. Because the end ("telos") is the principle of every change, and for the sake of the end exists potentiality, therefore actuality is the end. Referring then to our previous example, we could say that actuality is when the seed of the plant becomes a plant. " For that for the sake of which a thing is, is its principle, and the becoming is for the sake of the end; and the actuality is the end, and it is for the sake of this that the potentiality is acquired. For animals do not see in order that they may have sight, but they have sight that they may see." In conclusion, the matter of the house is its potentiality and the form is its actuality. The formal cause ("aitia") then of that change from potential to actual house, is the reason ("logos") of the house builder and the final cause is the end, namely the house itself. Then Aristotle proceeds and concludes that the actuality is prior to potentiality in formula, in time and in substantiality. With this definition of the particular substance (i.e., matter and form), Aristotle tries to solve the problem of the unity of the beings, "e.g.", what is that makes the man one? Since, according to Plato there are two Ideas: animal and biped, how then is man a unity? However, according to Aristotle, the potential being (matter) and the actual one (form) are one and the same thing.
Aristotle's predecessor, Plato, argued that all things have a universal form, which could be either a property, or a relation to other things. When we look at an apple, for example, we see an apple, and we can also analyze a form of an apple. In this distinction, there is a particular apple and a universal form of an apple. Moreover, we can place an apple next to a book, so that we can speak of both the book and apple as being next to each other. Plato argued that there are some universal forms that are not a part of particular things. For example, it is possible that there is no particular good in existence, but "good" is still a proper universal form. Bertrand Russell is a contemporary philosopher that agreed with Plato on the existence of "uninstantiated universals". Aristotle disagreed with Plato on this point, arguing that all universals are instantiated. Aristotle argued that there are no universals that are unattached to existing things. According to Aristotle, if a universal exists, either as a particular or a relation, then there must have been, must be currently, or must be in the future, something on which the universal can be predicated. Consequently, according to Aristotle, if it is not the case that some universal can be predicated to an object that exists at some period of time, then it does not exist. In addition, Aristotle disagreed with Plato about the location of universals. As Plato spoke of the world of the forms, a location where all universal forms subsist, Aristotle maintained that universals exist within each thing on which each universal is predicated. So, according to Aristotle, the form of apple exists within each apple, rather than in the world of the forms.
Aristotle is the earliest natural historian whose work has survived in some detail. Aristotle certainly did research on the natural history of Lesbos, and the surrounding seas and neighbouring areas. The works that reflect this research, such as "History of Animals", "Generation of Animals", and "Parts of Animals", contain some observations and interpretations, along with sundry myths and mistakes. The most striking passages are about the sea-life visible from observation on Lesbos and available from the catches of fishermen. His observations on catfish, electric fish ("Torpedo") and angler-fish are detailed, as is his writing on cephalopods, namely, "Octopus", "Sepia" (cuttlefish) and the paper nautilus ("Argonauta argo"). His description of the hectocotyl arm was about two thousand years ahead of its time, and widely disbelieved until its rediscovery in the nineteenth century. He separated the aquatic mammals from fish, and knew that sharks and rays were part of the group he called Selachē (selachians). Another good example of his methods comes from the "Generation of Animals" in which Aristotle describes breaking open fertilized chicken eggs at intervals to observe when visible organs were generated. He gave accurate descriptions of ruminants' four-chambered fore-stomachs, and of the ovoviviparous embryological development of the hound shark "Mustelus mustelus".
Aristotle's classification of living things contains some elements which still existed in the nineteenth century. What the modern zoologist would call vertebrates and invertebrates, Aristotle called 'animals with blood' and 'animals without blood' (he was not to know that complex invertebrates do make use of haemoglobin, but of a different kind from vertebrates). Animals with blood were divided into live-bearing (humans and mammals), and egg-bearing (birds and fish). Invertebrates ('animals without blood') are insects, crustacea (divided into non-shelled – cephalopods – and shelled) and testacea (molluscs). In some respects, this incomplete classification is better than that of Linnaeus, who crowded the invertebrata together into two groups, Insecta and Vermes (worms). For Charles Singer, "Nothing is more remarkable than [Aristotle's] efforts to [exhibit] the relationships of living things as a "scala naturae" Aristotle's "History of Animals" classified organisms in relation to a hierarchical "Ladder of Life" ("scala naturae"), placing them according to complexity of structure and function so that higher organisms showed greater vitality and ability to move. Aristotle believed that intellectual purposes, i.e., formal causes, guided all natural processes. Such a teleological view gave Aristotle cause to justify his observed data as an expression of formal design. Noting that "no animal has, at the same time, both tusks and horns," and "a single-hooved animal with two horns I have never seen," Aristotle suggested that Nature, giving no animal both horns and tusks, was staving off vanity, and giving creatures faculties only to such a degree as they are necessary. Noting that ruminants had a multiple stomachs and weak teeth, he supposed the first was to compensate for the latter, with Nature trying to preserve a type of balance. In a similar fashion, Aristotle believed that creatures were arranged in a graded scale of perfection rising from plants on up to man, the "scala naturae" or Great Chain of Being. His system had eleven grades, arranged according "to the degree to which they are infected with potentiality", expressed in their form at birth. The highest animals laid warm and wet creatures alive, the lowest bore theirs cold, dry, and in thick eggs. Aristotle also held that the level of a creature's perfection was reflected in its form, but not preordained by that form. Ideas like this, and his ideas about souls, are not regarded as science at all in modern times. He placed emphasis on the type(s) of soul an organism possessed, asserting that plants possess a vegetative soul, responsible for reproduction and growth, animals a vegetative and a sensitive soul, responsible for mobility and sensation, and humans a vegetative, a sensitive, and a rational soul, capable of thought and reflection. Aristotle, in contrast to earlier philosophers, but in accordance with the Egyptians, placed the rational soul in the heart, rather than the brain. Notable is Aristotle's division of sensation and thought, which generally went against previous philosophers, with the exception of Alcmaeon.
Aristotle's successor at the Lyceum, Theophrastus, wrote a series of books on botany—the "History of Plants"—which survived as the most important contribution of antiquity to botany, even into the Middle Ages. Many of Theophrastus' names survive into modern times, such as "carpos" for fruit, and "pericarpion" for seed vessel. Rather than focus on formal causes, as Aristotle did, Theophrastus suggested a mechanistic scheme, drawing analogies between natural and artificial processes, and relying on Aristotle's concept of the efficient cause. Theophrastus also recognized the role of sex in the reproduction of some higher plants, though this last discovery was lost in later ages.
After Theophrastus, the Lyceum failed to produce any original work. Though interest in Aristotle's ideas survived, they were generally taken unquestioningly. It is not until the age of Alexandria under the Ptolemies that advances in biology can be again found. The first medical teacher at Alexandria Herophilus of Chalcedon, corrected Aristotle, placing intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not. Though a few ancient atomists such as Lucretius challenged the teleological viewpoint of Aristotelian ideas about life, teleology (and after the rise of Christianity, natural theology) would remain central to biological thought essentially until the 18th and 19th centuries. Ernst Mayr claimed that there was "nothing of any real consequence in biology after Lucretius and Galen until the Renaissance." Aristotle's ideas of natural history and medicine survived, but they were generally taken unquestioningly.
Aristotle considered ethics to be a practical rather than theoretical study, i.e., one aimed at doing good rather than knowing for its own sake. He wrote several treatises on ethics, including most notably, the "Nichomachean Ethics". Aristotle taught that virtue has to do with the proper function ("ergon") of a thing. An eye is only a good eye in so much as it can see, because the proper function of an eye is sight. Aristotle reasoned that humans must have a function specific to humans, and that this function must be an activity of the "psuchē" (normally translated as "soul") in accordance with reason ("logos"). Aristotle identified such an optimum activity of the soul as the aim of all human deliberate action, "eudaimonia", generally translated as "happiness" or sometimes "well being". To have the potential of ever being happy in this way necessarily requires a good character ("ēthikē" "aretē"), often translated as moral (or ethical) virtue (or excellence). Aristotle taught that to achieve a virtuous and potentially happy character requires a first stage of having the fortune to be habituated not deliberately, but by teachers, and experience, leading to a later stage in which one consciously chooses to do the best things. When the best people come to live life this way their practical wisdom ("phronēsis") and their intellect ("nous") can develop with each other towards the highest possible ethical virtue, that of wisdom.
In addition to his works on ethics, which address the individual, Aristotle addressed the city in his work titled "Politics". Aristotle's conception of the city is organic, and he is considered one of the first to conceive of the city in this manner. Aristotle considered the city to be a natural community. Moreover, he considered the city to be prior to the family which in turn is prior to the individual, i.e., last in the order of becoming, but first in the order of being. He is also famous for his statement that "man is by nature a political animal." Aristotle conceived of politics as being like an organism rather than like a machine, and as a collection of parts none of which can exist without the others. It should be noted that the modern understanding of a political community is that of the state. However, the state was foreign to Aristotle. He referred to political communities as cities. Aristotle understood a city as a political "partnership". Subsequently, a city is created not to avoid injustice or for economic stability, but rather to live a good life: "The political partnership must be regarded, therefore, as being for the sake of noble actions, not for the sake of living together". This can be distinguished from the social contract theory which individuals leave the state of nature because of "fear of violent death" or its "inconveniences."
Aristotle considered epic poetry, tragedy, comedy, dithyrambic poetry and music to be imitative, each varying in imitation by medium, object, and manner. For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language. The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their manner of imitation – through narrative or character, through change or no change, and through drama or no drama. Aristotle believed that imitation is natural to mankind and constitutes one of mankind's advantages over animals. While it is believed that Aristotle's "Poetics" comprised two books – one on comedy and one on tragedy – only the portion that focuses on tragedy has survived. Aristotle taught that tragedy is composed of six elements: plot-structure, character, style, spectacle, and lyric poetry. The characters in a tragedy are merely a means of driving the story; and the plot, not the characters, is the chief focus of tragedy. Tragedy is the imitation of action arousing pity and fear, and is meant to effect the catharsis of those same emotions. Aristotle concludes "Poetics" with a discussion on which, if either, is superior: epic or tragic mimesis. He suggests that because tragedy possesses all the attributes of an epic, possibly possesses additional attributes such as spectacle and music, is more unified, and achieves the aim of its mimesis in shorter scope, it can be considered superior to epic. Aristotle was a keen systematic collector of riddles, folklore, and proverbs; he and his school had a special interest in the riddles of the Delphic Oracle and studied the fables of Aesop.
Modern scholarship reveals that Aristotle's "lost" works stray considerably in characterization from the surviving Aristotelian corpus. Whereas the lost works appear to have been originally written with an intent for subsequent publication, the surviving works do not appear to have been so. Rather the surviving works mostly resemble lectures unintended for publication. The authenticity of a portion of the surviving works as originally Aristotelian is also today held suspect, with some books duplicating or summarizing each other, the authorship of one book questioned and another book considered to be unlikely Aristotle's at all. Some of the individual works within the corpus, including the "Constitution of Athens," are regarded by most scholars as products of Aristotle's "school," perhaps compiled under his direction or supervision. Others, such as "On Colors," may have been produced by Aristotle's successors at the Lyceum, e.g., Theophrastus and Straton. Still others acquired Aristotle's name through similarities in doctrine or content, such as the "De Plantis," possibly by Nicolaus of Damascus. Other works in the corpus include medieval palmistries and astrological and magical texts whose connections to Aristotle are purely fanciful and self-promotional.
According to a distinction that originates with Aristotle himself, his writings are divisible into two groups: the "exoteric" and the "esoteric". Most scholars have understood this as a distinction between works Aristotle intended for the public (exoteric), and the more technical works (esoteric) intended for the narrower audience of Aristotle's students and other philosophers who were familiar with the jargon and issues typical of the Platonic and Aristotelian schools. Another common assumption is that none of the exoteric works is extant – that all of Aristotle's extant writings are of the esoteric kind. Current knowledge of what exactly the exoteric writings were like is scant and dubious, though many of them may have been in dialogue form. ("Fragments" of some of Aristotle's dialogues have survived.) Perhaps it is to these that Cicero refers when he characterized Aristotle's writing style as "a river of gold"; it is hard for many modern readers to accept that one could seriously so admire the style of those works currently available to us. However, some modern scholars have warned that we cannot know for certain that Cicero's praise was reserved specifically for the exoteric works; a few modern scholars have actually admired the concise writing style found in Aristotle's extant works. One major question in the history of Aristotle's works, then, is how were the exoteric writings all lost, and how did the ones we now possess come to us? The story of the original manuscripts of the esoteric treatises is described by Strabo in his "Geography" and Plutarch in his "Parallel Lives". The manuscripts were left from Aristotle to his successor Theophrastus, who in turn willed them to Neleus of Scepsis. Neleus supposedly took the writings from Athens to Scepsis, where his heirs let them languish in a cellar until the first century BC, when Apellicon of Teos discovered and purchased the manuscripts, bringing them back to Athens. According to the story, Apellicon tried to repair some of the damage that was done during the manuscripts' stay in the basement, introducing a number of errors into the text. When Lucius Cornelius Sulla occupied Athens in 86 BC, he carried off the library of Apellicon to Rome, where they were first published in 60 BC by the grammarian Tyrannion of Amisus and then by philosopher Andronicus of Rhodes. Carnes Lord attributes the popular belief in this story to the fact that it provides "the most plausible explanation for the rapid eclipse of the Peripatetic school after the middle of the third century, and for the absence of widespread knowledge of the specialized treatises of Aristotle throughout the Hellenistic period, as well as for the sudden reappearance of a flourishing Aristotelianism during the first century B.C." Lord voices a number of reservations concerning this story, however. First, the condition of the texts is far too good for them to have suffered considerable damage followed by Apellicon's inexpert attempt at repair. Second, there is "incontrovertible evidence," Lord says, that the treatises were in circulation during the time in which Strabo and Plutarch suggest they were confined within the cellar in Scepsis. Third, the definitive edition of Aristotle's texts seems to have been made in Athens some fifty years before Andronicus supposedly compiled his. And fourth, ancient library catalogues predating Andronicus' intervention list an Aristotelian corpus quite similar to the one we currently possess. Lord sees a number of post-Aristotelian interpolations in the "Politics", for example, but is generally confident that the work has come down to us relatively intact. As the influence of the "falsafa" grew in the West, in part due to Gerard of Cremona's translations and the spread of Averroism, the demand for Aristotle's works grew. William of Moerbeke translated a number of them into Latin. When Thomas Aquinas wrote his theology, working from Moerbeke's translations, the demand for Aristotle's writings grew and the Greek manuscripts returned to the West, stimulating a revival of Aristotelianism in Europe, and ultimately revitalizing European thought through Muslim influence in Spain to fan the embers of the Renaissance.
Twenty-three hundred years after his death, Aristotle remains one of the most influential people who ever lived. He was the founder of formal logic, pioneered the study of zoology, and left every future scientist and philosopher in his debt through his contributions to the scientific method. Despite these accolades, many of Aristotle's errors held back science considerably. Bertrand Russell notes that "almost every serious intellectual advance has had to begin with an attack on some Aristotelian doctrine". Russell also refers to Aristotle's ethics as "repulsive", and calls his logic "as definitely antiquated as Ptolemaic astronomy". Russell notes that these errors make it difficult to do historical justice to Aristotle, until one remembers how large of an advance he made upon all of his predecessors. Of course, the problem of excessive devotion to Aristotle is more a problem of those later centuries and not of Aristotle himself.
The immediate influence of Aristotle's work was felt as the Lyceum grew into the Peripatetic school. Aristotle's notable students included Aristoxenus, Dicaearchus, Demetrius of Phalerum, Eudemos of Rhodes, Harpalus, Hephaestion, Meno, Mnason of Phocis, Nicomachus, and Theophrastus. Aristotle's influence over Alexander the Great is seen in the latter's bringing with him on his expedition a host of zoologists, botanists, and researchers. He had also learned a great deal about Persian customs and traditions from his teacher. Although his respect for Aristotle was diminished as his travels made it clear that much of Aristotle's geography was clearly wrong, when the old philosopher released his works to the public, Alexander complained "Thou hast not done well to publish thy acroamatic doctrines; for in what shall I surpass other men if those doctrines wherein I have been trained are to be all men's common property?"
Aristotle is referred to as "The Philosopher" by Scholastic thinkers such as Thomas Aquinas. See "Summa Theologica", Part I, Question 3, etc. These thinkers blended Aristotelian philosophy with Christianity, bringing the thought of Ancient Greece into the Middle Ages. It required a repudiation of some Aristotelian principles for the sciences and the arts to free themselves for the discovery of modern scientific laws and empirical methods. The medieval English poet Chaucer describes his student as being happy by having The Italian poet Dante says of Aristotle in the first circles of hell,
Aristotle believed that women are colder than men and thus a lower form of life. His assumption carried forward unexamined to Galen and others for almost two thousand years until the sixteenth century. He also believed that females could not be fully human. His analysis of procreation is frequently criticized on the grounds that it presupposes an active, ensouling masculine element bringing life to an inert, passive, lumpen female element; it is on these grounds that Aristotle is considered by some feminist critics to have been a misogynist. On the other hand, Aristotle gave equal weight to women's happiness as he did to men's, and commented in his Rhetoric that a society cannot be happy unless women are happy too. In places like Sparta where the lot of women is bad, there can only be half-happiness in society.(see Rhetoric 1.5.6)
The German philosopher Friedrich Nietzsche has been said to have taken nearly all of his political philosophy from Aristotle. However implausible this is, it is certainly the case that Aristotle's rigid separation of action from production, and his justification of the subservience of slaves and others to the virtue – or "arete" – of a few justified the ideal of aristocracy. It is Martin Heidegger, not Nietzsche, who elaborated a new interpretation of Aristotle, intended to warrant his deconstruction of scholastic and philosophical tradition. More recently, Alasdair MacIntyre has attempted to reform what he calls the Aristotelian tradition in a way that is anti-elitist and capable of disputing the claims of both liberals and Nietzscheans.
The works of Aristotle that have survived from antiquity through Mediæval manuscript transmission are collected in the Corpus Aristotelicum. These texts, as opposed to Aristotle's lost works, are technical philosophical treatises from within Aristotle's school. Reference to them is made according to the organization of Immanuel Bekker's Royal Prussian Academy edition ("Aristotelis Opera edidit Academia Regia Borussica", Berlin, 1831-1870), which in turn is based on ancient classifications of these works.
"An American in Paris" is a symphonic composition by American composer George Gershwin, composed in 1928. Inspired by time Gershwin had spent in Paris, it is in the form of an extended tone poem evoking the sights and energy of the French capital in the 1920s. It is one of Gershwin's best-known compositions. Gershwin composed the piece on commission from the New York Philharmonic. He also did the orchestration. (He did not orchestrate his musicals.) Gershwin scored "An American in Paris" for the standard instruments of the symphony orchestra plus celesta, saxophone, and automobile horns. Gershwin brought back some Parisian taxi horns for the New York premiere of the composition which took place on December 13, 1928 in Carnegie Hall with Walter Damrosch conducting the New York Philharmonic. Gershwin collaborated on the original program notes with the critic and composer Deems Taylor, noting that: "My purpose here is to portray the impression of an American visitor in Paris as he strolls about the city and listens to various street noises and absorbs the French atmosphere." When the tone poem moves into the blues, "our American friend... has succumbed to a spasm of homesickness." But, "nostalgia is not a fatal disease." The American visitor "once again is an alert spectator of Parisian life" and "the street noises and French atmosphere are triumphant."
"An American in Paris" is scored for 3 flutes (3rd doubling on piccolo), 2 oboes, English horn, 2 clarinets in B flat, bass clarinet in B flat, 2 bassoons, 4 horns in F, 3 trumpets in B flat, 3 trombones, tuba, timpani, snare drum, bass drum, cymbals, low and high tom-toms, xylophone, glockenspiel, celesta, 4 taxi horns, alto saxophone/soprano saxophone, tenor saxophone/soprano saxophone/alto saxophone, baritone saxophone/soprano saxophone/alto saxophone, and strings. The revised edition by F Campbell-Watson calls for three saxophones, alto, tenor and baritone. In this arrangement the soprano and alto doublings have been rewritten to avoid changing instruments.
"An American in Paris" has been frequently recorded over the years. The very first recording was made for RCA Victor in 1929 with Nathaniel Shilkret conducting the Victor Symphony Orchestra, drawn from members of the Philadelphia Orchestra. Gershwin was on hand to "supervise" the recording; however, Shilkret was reported to be in charge and eventually asked the composer to leave the recording studio. Then, a little later, Shilkret discovered there was no one to play the brief celesta solo during the slow section, so he hastily asked Gershwin if he might play the solo; Gershwin said he could and so he briefly participated in the actual recording. The radio broadcast of the September 8, 1937 Hollywood Bowl George Gershwin Memorial Concert, in which "An American in Paris," also conducted by Shilkret, was second on the program, was recorded and was released in 1998 in a two-CD set. Arthur Fiedler and the Boston Pops Orchestra recorded the work for RCA Victor, including one of the first stereo recordings of the music. In 1945, Arturo Toscanini and the NBC Symphony Orchestra recorded the music in Carnegie Hall, one of the few commercial recordings Toscanini made of music by an American composer. The Seattle Symphony also recorded a version in the 1980's of Gershwin's original score, before he committed to numerous edits resulting in the score as we hear it today. In 1951, MGM released a musical comedy, "An American in Paris", featuring Gene Kelly and Leslie Caron. Winner of numerous awards, including the 1951 Best Picture Oscar, the film was directed by Vincente Minnelli, featured many tunes of Gershwin, and concluded with an extensive, elaborate dance sequence built around Gershwin's symphonic poem (arranged for the film by Johnny Green). A part of the symphonic composition is also featured in "As Good as It Gets", released in 1997.
Academy Award for Best Art Direction. The Academy Awards are the oldest awards ceremony for achievements in motion pictures. The Academy Award for Best Art Direction recognizes achievement in art direction on a film. The films below are listed with their production year, so the Oscar 2000 for best art direction went to a film from 1999. In the lists below, the winner of the award for each year is shown first, followed by the other nominees.
The Academy Award (frequently known as the Oscars) are accolades presented annually by the Academy of Motion Picture Arts and Sciences (AMPAS) to recognize excellence of professionals in the film industry, including directors, actors, and writers. The formal ceremony at which the awards are presented is one of the most prominent award ceremonies in the world. It is also the oldest award ceremony in the media, and many other award ceremonies such as the Grammy Awards (for music), Golden Globe Awards (all forms of visual media), and Emmy Awards (for television) are often modeled from the Academy. The Academy of Motion Picture Arts and Sciences itself was conceived by Metro-Goldwyn-Mayer studio boss Louis B. Mayer. The 1st Academy Awards ceremony was held on Thursday, May 16, 1929, at the Hotel Roosevelt in Hollywood to honor outstanding film achievements of 1927 and 1928. It was hosted by actor Douglas Fairbanks and director William C. deMille. The 82nd Academy Awards, honoring the best in film for 2009, was held on Sunday, March 7, 2010, at the Kodak Theatre in Hollywood, with actors Steve Martin and Alec Baldwin hosting the ceremony.
The first awards were presented on May 16, 1929, at a private brunch at the Hollywood Roosevelt Hotel with an audience of about 270 people.. The cost of guest tickets for that night's ceremony was $5. Fifteen statuettes were awarded, honoring artists, directors and other personalities of the filmmaking industry of the time for their works during the 1927-1928 period. Winners had been announced three months earlier of their triumphs; however that was changed in the second ceremony of the Academy Awards in 1930. Since then and during the first decade, the results were given to newspapers for publication at 11pm on the night of the awards. This method was used until the "Los Angeles Times" announced the winners before the ceremony began; as a result, the Academy has used a sealed envelope to reveal the name of the winners since 1941. Since 2002, the awards have been broadcast from the Kodak Theatre. The first Best Actor awarded was Emil Jannings, for his performance in The Last Command and The Way of All Flesh. He had to return to Europe before the ceremony, so the Academy agreed to give him the prize earlier; this made him the first Academy Award winner in history. The honored professionals were awarded for all the work done in a certain category for the qualifying period; for example, Emil Jannings, received the award for two movies he starred during that period. Since the fourth ceremony, the system changed, and the professionals were honored for a specific performance in a single film. In the 29th ceremony, held on March 27th, 1957 the Best Foreign Language Film category was introduced; until then, foreign language films were honored with the Special Achievement Award.
Although there are seven other types of awards presented by the Academy (the Irving G. Thalberg Memorial Award, the Jean Hersholt Humanitarian Award, the Gordon E. Sawyer Award, the Scientific and Engineering Award, the Technical Achievement Award, the John A. Bonner Medal of Commendation, and the Student Academy Award), the best known one is the "Academy Award of Merit" more popularly known as the Oscar statuette. Made of gold-plated britannium on a black metal base, it is 13.5 in (34 cm) tall, weighs 8.5 lb (3.85 kg) and depicts a knight rendered in Art Deco style holding a crusader's sword standing on a reel of film with five spokes. The five spokes each represent the original branches of the Academy: Actors, Writers, Directors, Producers, and Technicians. MGM's art director Cedric Gibbons, one of the original Academy members, supervised the design of the award trophy by printing the design on a scroll. In need of a model for his statuette Gibbons was introduced by his then wife Dolores del Río to Mexican film director and actor Emilio "El Indio" Fernández. Reluctant at first, Fernández was finally convinced to pose nude to create what today is known as the "Oscar". Then, sculptor George Stanley (who also did the Muse Fountain at the Hollywood Bowl) sculpted Gibbons's design in clay and Sachin Smith cast the statuette in 92.5 percent tin and 7.5 percent copper and then gold-plated it. The only addition to the Oscar since it was created is a minor streamlining of the base. The original Oscar mold was cast in 1928 at the C.W. Shumway & Sons Foundry in Batavia, Illinois, which also contributed to casting the molds for the Vince Lombardi Trophy and Emmy Awards statuettes. Since 1983, approximately 50 Oscars are made each year in Chicago, Illinois by manufacturer R.S. Owens & Company. In support of the American effort in World War II, the statuettes were made of plaster and were traded in for gold ones after the war had ended.
The root of the name "Oscar" is contested. One biography of Bette Davis claims that she named the Oscar after her first husband, band leader Harmon Oscar Nelson; one of the earliest mentions in print of the term "Oscar" dates back to a "Time" magazine article about the 1934 6th Academy Awards and to Bette Davis's receipt of the award in 1936. Walt Disney is also quoted as thanking the Academy for his Oscar as early as 1932. Another claimed origin is that the Academy's Executive Secretary, Margaret Herrick, first saw the award in 1931 and made reference to the statuette's reminding her of her "Uncle Oscar" (a nickname for her cousin Oscar Pierce). Columnist was present during Herrick's naming and seized the name in his byline, "Employees have affectionately dubbed their famous statuette 'Oscar'". The trophy was officially dubbed the "Oscar" in 1939 by the Academy of Motion Pictures Arts and Sciences. Another legend reports that the Norwegian-American Eleanor Lilleberg, executive secretary to Louis B. Mayer, saw the first statuette and exclaimed, "It looks like King Oscar II!". At the end of the day she asked, "What should we do with Oscar, put him in the vault?" and the name stuck. As of the 81st Academy Awards ceremony held in 2009, a total of 2,744 Oscars have been given for 1,798 awards. A total of 297 actors have won Oscars in competitive acting categories or been awarded Honorary or Juvenile Awards.
Since 1950, the statuettes have been legally encumbered by the requirement that neither winners nor their heirs may sell the statuettes without first offering to sell them back to the Academy for US$1. If a winner refuses to agree to this stipulation, then the Academy keeps the statuette. Academy Awards not protected by this agreement have been sold in public auctions and private deals for six-figure sums. This rule is highly controversial, since while the Oscar is under the ownership of the recipient, it is essentially not on the open market. The case of Michael Todd's grandson trying to sell Todd's Oscar statuette illustrates that there are many who do not agree with this idea. When Todd's grandson attempted to sell Todd's Oscar statuette to a movie prop collector, the Academy won the legal battle by getting a permanent injunction. Although some Oscar sales transactions have been successful, the buyers have subsequently returned the statuettes to the Academy, which keeps them in its treasury.
The Academy of Motion Picture Arts and Sciences (AMPAS), a professional honorary organization, maintains a voting membership of 5,835 as of 2007. Actors constitute the largest voting bloc, numbering 1,311 members (22 percent) of the Academy's composition. Votes have been certified by the auditing firm PricewaterhouseCoopers (and its predecessor Price Waterhouse) for the past 73 annual awards ceremonies. All AMPAS members must be invited to join by the Board of Governors, on behalf of Academy Branch Executive Committees. Membership eligibility may be achieved by a competitive nomination or a member may submit a name based on other significant contribution to the field of motion pictures. New membership proposals are considered annually. The Academy does not publicly disclose its membership, although as recently as 2007 press releases have announced the names of those who have been invited to join. The 2007 release also stated that it has just under 6,000 voting members. While the membership had been growing, stricter policies have kept its size steady since then.
Today, according to Rules 2 and 3 of the official Academy Awards Rules, a film must open in the previous calendar year, from midnight at the start of January 1 to midnight at the end of December 31, in Los Angeles County, California, to qualify. For example, the 2010 Best Picture winner, "The Hurt Locker", was actually first released in 2008, but did not qualify for the 2009 awards as it did not play its Oscar-qualifying run in Los Angeles until mid-2009, thus qualifying for the 2010 awards. Rule 2 states that a film must be feature-length, defined as a minimum of 40 minutes, except for short subject awards, and it must exist either on a 35 mm or 70 mm film print or in 24 frame/s or 48 frame/s progressive scan digital cinema format with native resolution not less than 1280x720. Producers must submit an Official Screen Credits online form before the deadline; in case it is not submitted by the defined deadline, the film will be ineligible for Academy Awards in any year. The form includes the production credits for all related categories. Then, each form is checked and put in a Reminder List of Eligible Releases. In late December ballots and copies of the Reminder List of Eligible Releases are mailed to around 6000 active members. For most categories, members from each of the branches vote to determine the nominees only in their respective categories (i.e. only directors vote for directors, writers for writers, actors for actors, etc.); there are some exceptions though in the case of certain categories, like Foreign Film, Documentary and Animated Feature Film in which movies are selected by special screening committees made up of member from all branches. In the special case of Best Picture, all voting members are eligible to select the nominees for that category. Foreign films must include English subtitles, and each country can only submit one film per year. The members of the various branches nominate those in their respective fields while all members may submit nominees for Best Picture. The winners are then determined by a second round of voting in which all members are then allowed to vote in most categories, including Best Picture.
The major awards are presented at a live televised ceremony, most commonly in February or March following the relevant calendar year, and six weeks after the announcement of the nominees. It is the culmination of the film awards season, which usually begins during November or December of the previous year. This is an elaborate extravaganza, with the invited guests walking up the red carpet in the creations of the most prominent fashion designers of the day. Black tie dress is the most common outfit for men, although fashion may dictate not wearing a bow-tie, and musical performers sometimes do not adhere to this. (The artists who recorded the nominees for Best Original Song quite often perform those songs live at the awards ceremony, and the fact that they are performing is often used to promote the television broadcast.) The Academy Awards is televised live across the United States (excluding Alaska and Hawaii), Canada, the United Kingdom, and gathers millions of viewers elsewhere throughout the world. The 2007 ceremony was watched by more than 40 million Americans. Other awards ceremonies (such as the Emmys, Golden Globes, and Grammys) are broadcast live in the East Coast but are on tape delay in the West Coast and might not air on the same day outside North America (if the awards are even televised). The Academy has for several years claimed that the award show has up to a billion viewers internationally, but this has so far not been confirmed by any independent sources. The usual extension of this claim is that only the Super Bowl, Olympics Opening Ceremonies, and FIFA World Cup Final draw higher viewership. The Awards show was first televised on NBC in 1953. NBC continued to broadcast the event until 1960 when the ABC Network took over, televising the festivities through 1970, after which NBC resumed the broadcasts. ABC once again took over broadcast duties in 1976; it is under contract to do so through the year 2014. After more than sixty years of being held in late March or early April, the ceremonies were moved up to late February or early March starting in 2004 to help disrupt and shorten the intense lobbying and ad campaigns associated with Oscar season in the film industry. Another reason was because of the growing TV ratings success of the NCAA Men's Division I Basketball Championship, which would cut into the Academy Awards audience. The earlier date is also to the advantage of ABC, as it now usually occurs during the highly profitable and important February sweeps period. (Some years, the ceremony is moved into early March in deference to the Winter Olympics.) Advertising is somewhat restricted, however, as traditionally no movie studios or competitors of official Academy Award sponsors may advertize during the telecast. The Awards show holds the distinction of having won the most Emmys in history, with 38 wins and 167 nominations. After many years of being held on Mondays at 9:00 p.m. Eastern/6:00 p.m Pacific, in 1999 the ceremonies were moved to Sundays at 8:30 p.m. Eastern/5:30 p.m. Pacific. The reasons given for the move were that more viewers would tune in on Sundays, that Los Angeles rush-hour traffic jams could be avoided, and that an earlier start time would allow viewers on the East Coast to go to bed earlier. For many years the film industry had opposed a Sunday broadcast because it would cut into the weekend box office. On March 30, 1981, the awards ceremony was postponed for one day after the shooting of President Ronald Reagan and others in Washington DC. In 1993 an "In Memoriam" section was introduced, honoring those who had made a significant contribution to cinema who had died in the preceding 12 months. This section has led to some criticism for omission of notable persons such as Leonard Schrader and Malcolm Arnold in 2007 and Gene Barry, Farrah Fawcett, Henry Gibson, and Bea Arthur in 2010. Since 2002, celebrities have been seen arriving at the Academy Awards in hybrid vehicles; during the telecast of the 79th Academy Awards in 2007, Leonardo DiCaprio and former vice president Al Gore announced that ecologically intelligent practices had been integrated into the planning and execution of the Oscar presentation and several related events. In 2010, the organizers of the Academy Awards announced that winners' acceptance speeches must not run past 45 seconds. This, according to organizer Bill Mechanic, was to ensure the elimination of what he termed "the single most hated thing on the show" - overly long and embarrassing displays of emotion.
Historically, the "Oscarcast" has pulled in a bigger haul when box-office hits are favored to win the Best Picture trophy. More than 57.25 million viewers tuned to the telecast in 1998, the year of "Titanic", which generated close to US$600 million at the North American box office pre-Oscars. The 76th Academy Awards ceremony in which ' (pre-telecast box office earnings of US$368 million) received 11 Awards including Best Picture drew 43.56 million viewers. The most watched ceremony based on Nielsen ratings to date, however, was the 42nd Academy Awards (Best Picture "Midnight Cowboy") which drew a 43.4% household rating on April 7, 1970. By contrast, ceremonies honoring films that have not performed well at the box office tend to show weaker ratings. The 78th Academy Awards which awarded low-budgeted, independent film "Crash" (with a pre-Oscar gross of US$53.4 million) generated an audience of 38.64 million with a household rating of 22.91%. In 2008, the 80th Academy Awards telecast was watched by 31.76 million viewers on average with an 18.66% household rating, the lowest rated and least watched ceremony to date, in spite of celebrating 80 years of the Academy Awards. The Best Picture winner of that particular ceremony was another low-budget, independently financed film ("No Country for Old Men").
In 1929, the 1st Academy Awards were presented at a banquet dinner at the Hollywood Roosevelt Hotel. From 1930–1943, the awards were presented first at the Ambassador Hotel in Hollywood, and later the Biltmore Hotel in downtown Los Angeles. Grauman's Chinese Theater in Hollywood then hosted the awards from 1944 to 1946, followed by the Shrine Auditorium in Los Angeles from 1947 to 1948. The 21st Academy Awards in 1949 were held at the Academy Award Theater at what was the Academy's headquarters on Melrose Avenue in Hollywood. From 1950 to 1960, the awards were presented at Hollywood's Pantages Theatre. With the advent of television, the 1953–1957 awards took place simultaneously in Hollywood and New York first at the NBC International Theatre (1953) and then at the NBC Century Theatre (1954–1957), after which the ceremony took place solely in Los Angeles. The Oscars moved to the Santa Monica Civic Auditorium in Santa Monica, California in 1961. By 1969, the Academy decided to move the ceremonies back to Los Angeles, this time to the Dorothy Chandler Pavilion at the Los Angeles County Music Center. In 2002, Hollywood's Kodak Theatre became the permanent home of the awards.
In the first year of the awards, the Best Director award was split into two separate categories (Drama and Comedy). At times, the Best Original Score award has also been split into separate categories (Drama and Comedy/Musical). From the 1930s through the 1960s, the Art Direction, Cinematography, and Costume Design awards were likewise split into two separate categories (black-and-white films and color films).
The Oscars are generally voted on by members of the entertainment industry; thus, important films that have had the most people working on them generally become nominated. Director William Friedkin, an Oscar winner and producer of the Academy Awards, spoke critically of the awards at a conference in New York in 2009. He characterized the Academy Awards as "the greatest promotion scheme that any industry ever devised for itself". In addition, several winners critical of the Academy Awards have boycotted the ceremonies and refused to accept their Oscars. The first to do so was Dudley Nichols (Best Writing in 1935 for "The Informer"). Nichols boycotted the Eighth Academy Awards ceremony because of conflicts between the Academy and the Writer's Guild. George C. Scott became the second person to refuse his award (Best Actor in 1970 for "Patton"), at the 43rd Academy Awards ceremony. Scott explained, "The whole thing is a goddamn meat parade. I don't want any part of it." The third winner, Marlon Brando, refused his award (Best Actor in 1972 for "The Godfather"), citing the film industry's discrimination and mistreatment of Native Americans. At the 45th Academy Awards ceremony, Brando sent Sacheen Littlefeather to read a 15-page speech detailing Brando's criticisms. It has been observed that several of the Academy Award winners – particularly Best Picture – have not stood the test of time or had defeated worthier efforts. On "They Shoot Pictures, Don't They's" comprehensive database of the 1,000 most acclaimed films of all time, only eight of the first hundred ranked films have won the Best Picture award. Tim Dirks, editor of AMC's filmsite.org, has written of the Academy Awards, In his review of "The Lives of Others", Nick Davis argued, The Academy Awards have also come under criticism for having a bias towards certain types of performances and film genres. The Best Picture prize has never been given to a film noir, science fiction or an animated film; and rarely are horror, fantasy, comedy and westerns recognized by AMPAS. Acting prizes in certain years have been criticized for not recognizing superior performances so much as being awarded for sentimental reasons, personal popularity, atonement for past mistakes, or presented as a "career honor" to recognize a distinguished nominee's entire body of work.
"Animalia" (ISBN 0810918684) is an illustrated children's book by Graeme Base. It was published in 1986. "Animalia" is an alliterative alphabet book and contains twenty-six illustrations, one for each letter of the alphabet. Each illustration features an animal from the animal kingdom (A is for alligator, B is for butterfly, etc.) along with a short poem utilizing the letter of the page for many of the words. The illustrations contain dozens of small objects beginning with that letter that the curious reader can try to identify. As an additional challenge, the author has hidden a picture of himself as a child in every picture. In 1987, "Animalia" won the title of Honour Book in the Children's Book Council of Australia Picture Book of the Year Awards. In 1996, a tenth anniversary edition was released. Base also published a colouring book version for children to do their own colouring. A television series was also created, based on the book, which airs in the United States, Australia, Canada, the UK and Norway. It also airs on Minimax for the Czech and Slovak Republics.
International Atomic Time (TAI, from the French name Temps Atomique International) is a high-precision atomic coordinate time standard based on the notional passage of proper time on Earth's geoid. It is the principal realisation of Terrestrial Time, and the basis for Coordinated Universal Time (UTC) which is used for civil timekeeping all over the Earth's surface., TAI was exactly 34 seconds ahead of UTC: an initial difference of 10 seconds at the start of 1972, plus 24 leap seconds in UTC since 1972; the last leap second was added on 31 December, 2008. Time coordinates on the TAI scales are conventionally specified using traditional means of specifying days, carried over from non-uniform time standards based on the rotation of the Earth. Specifically, both Julian Dates and the Gregorian calendar are used. TAI in this form was synchronised with Universal Time at the beginning of 1958, and the two have drifted apart ever since, due to the changing motion of the Earth.
TAI as a time scale is a weighted average of the time kept by over 200 atomic clocks in about 70 national laboratories worldwide. The clocks are compared using satellites. Due to the averaging it is far more stable than any clock would be alone. The majority of the clocks are caesium clocks; the definition of the SI second is written in terms of caesium. The participating institutions each broadcast, in real time (in the present), a frequency signal with time codes, which is their estimate of TAI. Time codes are usually published in the form of UTC. These time scales are denoted in the form "TAI(NPL)" ("UTC(NPL)" for the UTC form), where "NPL" in this case identifies the National Physical Laboratory, UK. The clocks at different institutions are regularly compared against each other. The International Bureau of Weights and Measures (BIPM) combines these measurements to retrospectively calculate the weighted average that forms the most stable time scale possible. This combined time scale is published monthly in [ftp://ftp2.bipm.fr/pub/tai/publication/cirt/ Circular T], and is the canonical TAI. This time scale is expressed in the form of tables of differences UTC-UTC("x") and TAI-TA("x"), for each participating institution "x". Errors in publication may be corrected by issuing a revision of the faulty Circular T or by errata in a subsequent Circular T. Aside from this, once published in Circular T the TAI scale is not revised. In hindsight it is possible to discover errors in TAI, and to make better estimates of the true proper time scale. Doing so does not create another version of TAI; it is instead considered to be creating a better realisation of Terrestrial Time (TT).
Atomic timekeeping services started experimentally in 1955, using the first caesium atomic clock at the National Physical Laboratory, UK (NPL). Early atomic time scales consisted of quartz clocks with frequencies calibrated by a single atomic clock; the atomic clocks were not operated continuously. The "Greenwich Atomic" (GA) scale began in 1955 at the Royal Greenwich Observatory. The United States Naval Observatory began the A.1 scale 13 September 1956, using an Atomichron© commercial atomic clock, followed by the NBS-A scale at the National Bureau of Standards, Boulder, Colorado. The International Time Bureau (BIH) began a time scale, Tm or AM, in July 1955, using both local caesium clocks and comparisons to distant clocks using the phase of VLF radio signals. Both the BIH scale and A.1 was defined by an epoch at the beginning of 1958: it was set to read Julian Date 2436204.5 (1 January 1958 00:00:00) at the corresponding UT2 instant. The procedures used by the BIH evolved, and the name for the time scale changed: "A3" in 1963 and "TA(BIH)" in 1969. This synchronisation was inevitably imperfect, depending as it did on the astronomical realisation of UT2. At the time, UT2 as published by various observatories differed by several centiseconds. The SI second was defined in terms of the caesium atom in 1967, and in 1971 it was renamed International Atomic Time (TAI). Also in 1961, UTC began. UTC is a discontinuous time scale composed from segments that are linear transformations of atomic time, the discontinuities being arranged so that UTC approximated UT2 until the end of 1971, and UT1 thereafter. This was a compromise arrangement for a broadcast time scale: a linear transformation of the BIH's atomic time meant that the time scale was stable and internationally synchronised, while approximating UT1 means that tasks such as navigation which require a source of Universal Time continue to be well served by public time broadcasts. In the 1970s, it became clear that the clocks participating in TAI were ticking at different rates due to gravitational time dilation, and the combined TAI scale therefore corresponded to an average of the altitudes of the various clocks. Starting from Julian Date 2443144.5 (1 January 1977T00:00:00), corrections were applied to the output of all participating clocks, so that TAI would correspond to proper time at mean sea level (the geoid). Because the clocks had been on average well above sea level, this meant that TAI slowed down, by about 10−12. The former uncorrected time scale continues to be published, under the name "EAL" ("Echelle Atomique Libre", meaning "Free Atomic Scale"). The instant that the gravitational correction started to be applied serves as the epoch for Barycentric Coordinate Time (TCB), Geocentric Coordinate Time (TCG), and Terrestrial Time (TT). All three of these time scales were defined to read JD 2443144.5003725 (1 January 1977 00:00:32.184) exactly at that instant. (The offset is to provide continuity with the older Ephemeris Time.) TAI was henceforth a realisation of TT, with the equation TT(TAI) = TAI + 32.184 s.
Altruism (pronounced:) is selfless concern for the welfare of others. It is a traditional virtue in many cultures, and a core aspect of various religious traditions such as Judaism, Christianity, Islam, Hinduism, Jainism, Buddhism, Confucianism, Sikhism, and many others. Altruism is the opposite of selfishness. Altruism can be distinguished from feelings of loyalty and duty. Altruism focuses on a motivation to help others or a want to do good without reward, while duty focuses on a moral obligation towards a specific individual (for example, God, a king), a specific organization (for example, a government), or an abstract concept (for example, patriotism etc). Some individuals may feel both altruism and duty, while others may not. Pure altruism is giving without regard to reward or the benefits of recognition and need. The term "altruism" may also refer to an ethical doctrine that claims that individuals are morally obliged to benefit others.
The concept has a long history in philosophical and ethical thought. The term was originally coined by the founding sociologist and philosopher of science, Auguste Comte, and has become a major topic for psychologists (especially evolutionary psychology researchers), evolutionary biologists, and ethologists. While ideas about altruism from one field can have an impact on the other fields, the different methods and focuses of these fields lead to different perspectives on altruism.
In the science of ethology (the study of animal behaviour), and more generally in the study of social evolution, altruism refers to behaviour by an individual that increases the fitness of another individual while decreasing the fitness of the actor. Researchers on alleged altruist behaviours among animals have been ideologically opposed to the sociological social Darwinist concept of the "survival of the fittest", under the name of "survival of the nicest"—not to be confused with the biological concept of Darwin's theory of evolution. Insistence on such cooperative behaviors between animals was first exposed by the Russian zoologist and anarchist Peter Kropotkin in his 1902 book, '. Theories of apparently-altruistic behavior were accelerated by the need to produce theories compatible with evolutionary origins. Two related strands of research on altruism have emerged out of traditional evolutionary analyses, and from game theory respectively. The study of altruism was the initial impetus behind George R. Price's development of the Price equation which is a mathematical equation used to study genetic evolution. An interesting example of altruism is found in the cellular slime moulds, such as "Dictyostelium mucoroides". These protists live as individual amoebae until starved, at which point they aggregate and form a multicellular fruiting body in which some cells sacrifice themselves to promote the survival of other cells in the fruiting body. Social behavior and altruism share many similarities to the interactions between the many parts (cells, genes) of an organism, but are distinguished by the ability of each individual to reproduce indefinitely without an absolute requirement for its neighbors.
Jorge Moll and Jordan Grafman, neuroscientists at the National Institutes of Health and LABS-D'Or Hospital Network (J.M.) provided the first evidence for the neural bases of altruistic giving in normal healthy volunteers, using functional magnetic resonance imaging. In their research, published in the Proceedings of the National Academy of Sciences USA in October, 2006, they showed that both pure monetary rewards and charitable donations activated the mesolimbic reward pathway, a primitive part of the brain that usually lights up in response to food and sex. However, when volunteers generously placed the interests of others before their own by making charitable donations, another brain circuit was selectively activated: the subgenual cortex/septal region. These structures are intimately related to social attachment and bonding in other species. Altruism, the experiment suggested, was not a superior moral faculty that suppresses basic selfish urges but rather was basic to the brain, hard-wired and pleasurable. Another experiment funded by the National Institutes of Health and conducted in 2007 at the Duke University in Durham, North Carolina suggests a different view, "that altruistic behavior may originate from how people view the world rather than how they act in it". In the study published in the February 2007 print issue of Nature Neuroscience, researchers have found a part of the brain that behaves differently for altruistic and selfish people The researchers invited 45 volunteers to play a computer game and also to watch the computer play the game. In some instances successful completion of the game resulted in them winning money for themselves, and in other instances it resulted in money being donated to a charity each person had chosen at the beginning of the experiment. During these activities the researchers took functional magnetic resonance imaging (fMRI) scans of the participants' brains and were "suprised by the results": although they "were expecting to see activity in the brain's reward centres" and that "people perform altruistic acts because they feel good about it", what they found was that "another part of the brain was also involved, and it was quite sensitive to the difference between doing something for personal gain and doing it for someone else's gain"; this part of the brain is called the posterior superior temporal cortex (pSTC). In the next stage the scientists asked the participants questions about type and frequency of their altruistic or helping behaviours. They then analysed the responses to generate an estimate of a person's tendency to act altruistically and compared each person's level against their fMRI brain scan. The results showed that pSTC activity rose in proportion to a person's estimated level of altruism. According to the researchers, the results suggest that altruistic behavior may originate from how people view the world rather than how they act in it. "We believe that the ability to perceive other people's actions as meaningful is critical for altruism", said lead study investigator Dharol Tankersley.
A study by Samuel Bowles at the Santa Fe Institute in New Mexico, US, is seen by some as breathing new life into the model of group selection for Altruism, known as "Survival of the nicest". Bowles conducted a genetic analysis of contemporary foraging groups, including Australian aboriginals, native Siberian Inuit populations and indigenous tribal groups in Africa. It was found that hunter-gatherer bands of up to 30 individuals were considerably more closely related than was previously thought. Under these conditions, thought to be similar to those of the middle and upper Paleolithic, altruism towards other group-members would improve the overall fitness of the group. This is however simply a form of inclusive fitness - one vehicle helping other vehicles likely to contain the same genes. If an individual defends the group, risking death or simply reducing his reproductive fitness, genes that this individual shares with those he successfully defends (group members) would increase in frequency (thanks to his defence supporting their reproduction). If such helpful acts are rewarded with food sharing, sexual access, monogamy or other benefits, there is not average “cost” of altruistic behaviour to be repaid. Bowles assembled genetic, climactic, archaeological, ethnographic and experimental data to examine the cost-benefit relationship of human cooperation in ancient populations. In his model, altruism is selected for when members of a group bearing genes for altruistic behaviour pay a cost - limiting their reproductive opportunities - but receive a benefit from sharing food and information. If their acts increase the average fitness of group members, altruism increase so long as group members tend also to maintain or increase their inter-relatedness (in-goup mating). Bands of such altruistic humans could then act together not only defensively, but aggressively, to gain resources from other groups. Altruist theories in evolutionary biology were contested by Amotz Zahavi, the inventor of the signal theory and its correlative, the handicap principle, based mainly on his observations of the Arabian Babbler, a bird commonly known for its surprising (alleged) altruistic behaviours.
Altruism figures prominently in Buddhism. Love and compassion are components of all forms of Buddhism, and both are focused on all beings equally: the wish that all beings be happy (love) and the wish that all beings be free from suffering (compassion). "Many illnesses can be cured by the one medicine of love and compassion. These qualities are the ultimate source of human happiness, and the need for them lies at the very core of our being" (Dalai Lama). Since "all beings" includes the individual, love and compassion in Buddhism are outside the opposition between self and other. It is even said that the very distinction between self and other is part of the root cause of our suffering. In practical terms, however, because of the spontaneous self-centeredness of most of us, Buddhism encourages us to focus love and compassion on others, and thus can be characterized as "altruistic." Many would agree with the Dalai Lama that Buddhism as a religion is kindness toward others. Still, the very notion of altruism is modified in such a world-view, since the belief is that such a practice promotes our own happiness: "The more we care for the happiness of others, the greater our own sense of well-being becomes" (Dalai Lama). In the context of larger ethical discussions on moral action and judgment, Buddhism is characterized by the belief that negative (unhappy) consequences of our actions derive not from punishment or correction based on moral judgment, but on the law of karma, which functions like a natural law of cause and effect. One simple illustration of such cause and effect would be the case of experiencing the effects of what I myself cause: if I cause suffering, I will as a natural consequence experience suffering; if I cause happiness, I will as a natural consequence experience happiness. In Buddhism, "karma" (Pāli "kamma") is strictly distinguished from "vipāka", meaning "fruit" or "result". Karma is categorized within the group or groups of cause (Pāli "hetu") in the chain of cause and effect, where it comprises the elements of "volitional activities" (Pali "sankhara") and "action" (Pali "bhava"). Any action is understood to create "seeds" in the mind that will sprout into the appropriate result (Pāli "vipaka") when they meet with the right conditions. Most types of karmas, with good or bad results, will keep one within the wheel of samsāra; others will liberate one to nirvāna. Buddhism relates karma directly to motives behind an action. Motivation usually makes the difference between "good" and "bad", but included in the motivation is also the aspect of ignorance; so a well-intended action from an ignorant mind can easily be "bad" in the sense that it creates unpleasant results for the "actor".
Altruism was central to the teachings of Jesus found in the Gospel especially in the Sermon on the Mount and the Sermon on the Plain. From biblical to medieval Christian traditions, tensions between self-affirmation and other-regard were sometimes discussed under the heading of "disinterested love," as in the Pauline phrase "love seeks not its own interests." In his book "Indoctrination and Self-deception", Roderick Hindery tries to shed light on these tensions by contrasting them with impostors of authentic self-affirmation and altruism, by analysis of other-regard within creative individuation of the self, and by contrasting love for the few with love for the many. If love, which confirms others in their freedom, shuns propagandas and masks, assurance of its presence is ultimately confirmed not by mere declarations from others, but by each person's experience and practice from within. As in practical arts, the presence and meaning of love become validated and grasped not by words and reflections alone, but in the doing. Though it might seem obvious that altruism is central to the teachings of Jesus, one important and influential strand of Christianity would qualify this. St Thomas Aquinas in the "Summa Theologica", I:II Quaestio 26, Article 4 states that we should love ourselves more than our neighbour. His interpretation of the Pauline phrase is that we should seek the common good more than the private good but this is because the common good is a more desirable good for the individual. 'You should love your neighbour as yourself' from Leviticus 19 and Matthew 22 is interpreted by St Thomas as meaning that love for ourselves is the exemplar of love for others. He does think though, that we should love God more than ourselves and our neighbour, taken as an entirety, more than our bodily life, since the ultimate purpose of love of our neighbour is to share in eternal beatitude, a more desirable thing than bodily well being. Comte was probably opposing this Thomistic doctrine, now part of mainstream Catholicism, in coining the word Altruism, as stated above. Thomas Jay Oord has argued in several books that altruism is but one possible form of love. And altruistic action is not always loving action. Oord defines altruism as acting for the good of the other, and he agrees with feminists who note that sometimes love requires acting for one's own good when the demands of the other undermine overall well-being.
In Sufism, the concept of i'thar (altruism) is the notion of 'preferring others to oneself'. For Sufis, this means devotion to others through complete forgetfulness of one's own concerns. The importance lies in sacrifice for the sake of the greater good; Islam considers those practicing i'thar as abiding by the highest degree of nobility. This is similar to the notion of chivalry, but unlike the European concept there is a focus on attention to everything in existence. A constant concern for Allah results in a careful attitude towards people, animals, and other things in this world. This concept was emphasized by Sufi mystics like Rabia al-Adawiyya who paid attention to the difference in dedication to Allah and dedication to people.13th century Turkish sufi poet Yunus Emre explained this philosophy as "Yaradılanı severiz, Yaradandan ötürü" or "We love the creation, because of The Creator"
Judaism defines altruism as the desired goal of creation. The famous Rabbi Abraham Isaac Kook stated that love is the most important attribute in humanity. This is defined as bestowal, or giving, which is the intention of altruism. This can be altruism towards humanity that leads to altruism towards the creator or God. Kabbalah defines God as the force of giving in existence. Rabbi Moshe Chaim Luzzatto in particular focused on the ‘purpose of creation’ and how the will of God was to bring creation into perfection and adhesion with this upper force. Modern Kabbalah developed by Rabbi Yehuda Ashlag, in his writings about the future generation, focuses on how society could achieve an altruistic social framework. Ashlag proposed that such a framework is the purpose of creation, and everything that happens is to raise humanity to the level of altruism, love for one another. Ashlag focused on society and its relation to divinity.
Altruism is essential to the Sikh religion. In the late 1600s, Guru Gobind Singh Ji (the tenth guru in Sikhism), was in war with the Moghul rulers to protect the people of different faiths, when a fellow Sikh, Bhai Kanhaiya, attended the troops of the enemy. He gave water to both friends and foes who were wounded on the battlefield. Some of the enemy began to fight again and some Sikh warriors were annoyed by Bhai Kanhaiya as he was helping their enemy. Sikh soldiers brought Bhai Kanhaiya before Guru Gobind Singh Ji, and complained of his action that they considered counterproductive to their struggle on the battlefield. "What were you doing, and why?" asked the Guru. "I was giving water to the wounded because I saw your face in all of them," replied Bhai Kanhaiya. The Guru responded, "Then you should also give them ointment to heal their wounds. You were practicing what you were coached in the house of the Guru." It was under the tutelage of the Guru that Bhai Kanhaiya subsequently founded a volunteer corps for altruism. This volunteer corps still to date is engaged in doing good to others and trains new volunteering recruits for doing the same.
Vedanta differs from the view that karma is a law of cause and effect but instead additionally hold that karma is mediated by the will of a personal supreme God. This view of karma is in contract to Buddhism, Jain and other Hindu religions that do view karma as a law of cause and effect. Swami Sivananda, an Advaita scholar, reiterates the same views in his commentary synthesising Vedanta views on the Brahma Sutras, a Vedantic text. In his commentary on Chapter 3 of the Brahma Sutras, Sivananda notes that karma is insentient and short-lived, and ceases to exist as soon as a deed is executed. Hence, karma cannot bestow the fruits of actions at a future date according to one's merit. Furthermore, one cannot argue that karma generates apurva or punya, which gives fruit. Since apurva is non-sentient, it cannot act unless moved by an intelligent being such as God. It cannot independently bestow reward or punishment.
Ayn Rand (; born Alisa Zinov'yevna Rosenbaum; – March 6, 1982), was a Russian-American novelist, philosopher, playwright, and screenwriter. She is known for her two best-selling novels and for developing a philosophical system she called Objectivism. Born and educated in Russia, Rand immigrated to the United States in 1926. She worked as a screenwriter in Hollywood and had a play produced on Broadway in 1935–1936. She first achieved fame in 1943 with her novel "The Fountainhead", which in 1957 was followed by her best-known work, the philosophical novel "Atlas Shrugged". Rand's political views, reflected in both her fiction and her theoretical work, emphasize individual rights (including property rights) and laissez-faire capitalism, enforced by a constitutionally-limited government. She was a fierce opponent of all forms of collectivism and statism, including fascism, communism, socialism, and the welfare state, and promoted ethical egoism while rejecting the ethic of altruism. She considered reason to be the only means of acquiring knowledge and the most important aspect of her philosophy, stating, "I am not "primarily" an advocate of capitalism, but of egoism; and I am not "primarily" an advocate of egoism, but of reason. If one recognizes the supremacy of reason and applies it consistently, all the rest follows."
Rand was born Alisa Zinov'yevna Rosenbaum () in 1905, into a middle-class family living in Saint Petersburg. She was the eldest of the three daughters (Alisa, Natasha, and Nora) of Zinovy Zakharovich Rosenbaum and Anna Borisovna Rosenbaum, largely non-observant Jews. Her father was educated as a chemist and became a successful pharmacist, eventually owning his own pharmacy and the building in which it was located. Rand was twelve at the time of the Russian revolution of 1917. Opposed to the Tsar, Rand's sympathies were with Alexander Kerensky. Rand's family life was disrupted by the rise of the Bolshevik party. Her father's pharmacy was confiscated by the Soviets, and the family fled to the Crimea which was initially under the control of the White Army. She later recalled that while in high school she determined that she was an atheist and that she valued reason and intellect. She graduated from high school in the Crimea and briefly held a job teaching Red Army soldiers to read. She found she enjoyed that work very much, the illiterate soldiers being eager to learn and respectful of her. At sixteen, Rand returned with her family to Saint Petersburg. She enrolled at the University of Petrograd, where she studied in the department of social pedagogy, majoring in history. At university she was introduced to the writings of Aristotle and Plato, who would form two of the greatest influences and counter-influences respectively on her thought. A third figure whose philosophical works she studied heavily was Friedrich Nietzsche. Her formal study of philosophy amounted to only a few courses, and outside of these three philosophers, her study of key figures was limited to excerpts and summaries. Of the writers she read at this time, Victor Hugo, Edmond Rostand, Friedrich Schiller, and Fyodor Dostoevsky became her perennial favorites. Along with a number of other non-Communist students, Rand was purged from the university shortly before completing. However, after complaints from a group of visiting foreign scientists, the Communists relented and allowed many of the expelled students to complete their work and graduate, which Rand did in October 1924. She subsequently studied for a year at the State Technicum for Screen Arts. In the fall of 1925, she was granted a visa to visit American relatives. She left Russia on January 17, 1926, and arrived in the United States on February 19, entering by ship through New York City. After a brief stay with her relatives in Chicago, she resolved never to return to the Soviet Union, and set out for Hollywood to become a screenwriter. While still in Russia she had decided her professional surname for writing would be "Rand", possibly as a Cyrillic contraction of her birth surname, and she adopted the first name "Ayn", either from a Finnish name or from the Hebrew word עין ("ayin", meaning "eye"). Initially, she struggled in Hollywood and took odd jobs to pay her basic living expenses. A chance meeting with famed director Cecil B. DeMille led to a job as an extra in his film, "The King of Kings", and to subsequent work as a junior screenwriter. While working on "The King of Kings", she intentionally bumped into an aspiring young actor, Frank O'Connor, who caught her eye. The two married on April 15, 1929. Rand became an American citizen in 1931. Taking various jobs during the 1930s to support her writing, Rand worked for a time as the head of the costume department at RKO Studios. She made attempts to bring her parents and sisters to the United States, but they were unable to get permission to emigrate.
Rand's first literary success came with the sale of her screenplay "Red Pawn" to Universal Studios in 1932. Josef Von Sternberg considered it for Marlene Dietrich, but anti-Soviet themes were unpopular at the time, and the project came to nothing. This was followed by the courtroom drama "Night of January 16th", first produced in Hollywood in 1934, and then successfully reopened on Broadway in 1935. Each night the "jury" was selected from members of the audience, and one of the two different endings, depending on the jury's "verdict," would then be performed. In 1941, Paramount Pictures produced a movie version of the play. Rand did not participate in the production and was highly critical of the result. Her first novel, the semi-autobiographical "We the Living", was published in 1936 by Macmillan. Set in Communist Russia, it focused on the struggle between the individual and the state. In the foreword to the novel, Rand stated that "We the Living" "is as near to an autobiography as I will ever write. It is not an autobiography in the literal, but only in the intellectual sense. The plot is invented, the background is not..." Without Rand's knowledge or permission, "We the Living" was made into a pair of Italian films, "Noi vivi" and "Addio, Kira", in 1942. Rediscovered in the 1960s, these films were re-edited into a new version which was approved by Rand and re-released as "We the Living" in 1986. Her novella "Anthem" was published in England in 1938 and in America seven years later. It presents a vision of a dystopian future world in which collectivism has triumphed to such an extent that even the word "I" has vanished from the language and from humanity's memory. "The Fountainhead" and political activism. During the 1940s, Rand became involved in political activism. Both she and her husband worked full time in volunteer positions for the 1940 Presidential campaign of Republican Wendell Willkie. This work led to Rand's first public speaking experiences, including fielding the sometimes hostile questions from New York City audiences who had just viewed pro-Willkie newsreels, an experience she greatly enjoyed. This activity also brought her into contact with other intellectuals sympathetic to free-market capitalism. She became friends with journalist Henry Hazlitt and his wife, and Hazlitt introduced her to the Austrian School economist Ludwig von Mises. Both men expressed an admiration for Rand, and despite her philosophical differences with them, Rand strongly endorsed the writings of both men throughout her career. Rand's first major success as a writer came with "The Fountainhead" in 1943, a romantic and philosophical novel that she wrote over a period of seven years. The novel centers on an uncompromising young architect named Howard Roark, and his struggle against what Rand described as "second-handers" — those who attempt to live through others, placing others above self. It was rejected by twelve publishers before finally being accepted by the Bobbs-Merrill Company on the insistence of editor Archibald Ogden, who threatened to quit if his employer did not publish it. "The Fountainhead" eventually became a worldwide success, bringing Rand fame and financial security. According to the Ayn Rand Institute, by April 2008 the novel had sold over 6.5 million copies. In 1943, Rand returned to Hollywood to write the screenplay for a film version of "The Fountainhead" for Warner Brothers, and the following year she and her husband purchased a home designed by modernist Richard Neutra and an adjoining ranch. There, Rand entertained figures such as Hazlitt, Morrie Ryskind, Janet Gaynor, Gilbert Adrian and Leonard Read. Finishing her work on that screenplay, she was hired by producer Hal Wallis as a screenwriter and script-doctor, and her work for Wallis included the Oscar-nominated "Love Letters" and "You Came Along", along with research for a screenplay based on the development of the atomic bomb. This role gave Rand time to work on other projects, including the publication of her first work of non-fiction, an essay titled "The Only Path to Tomorrow", in the January 1944 edition of "Reader's Digest" magazine. Rand also outlined and took extensive notes for a non-fiction treatment of her philosophy, although the planned book was never completed. During this period Rand developed a relationship with libertarian writer Isabel Paterson. The two women became friends and philosophical sparring-partners, and Rand is reported to have questioned the well-informed Paterson about American history and politics long into the night during their numerous meetings. Later, the two women had a falling out after what Rand saw as Paterson's bitter and insensitive comments during one of her Hollywood parties. Paterson's influence on Rand's later political theories has been a matter of ongoing debate, but Paterson biographer Stephen D. Cox credits Rand's public advocacy with keeping her old friend's political work "The God of the Machine" in print for many years, despite their previous break. In 1947, during the Second Red Scare, Rand testified as a "friendly witness" before the United States House Un-American Activities Committee. Her testimony regarded the disparity between her personal experiences in the Soviet Union and the portrayal of it in the 1944 film "Song of Russia". Rand argued that the film grossly misrepresented the socioeconomic conditions in the Soviet Union and portrayed life in the USSR as being much better and happier than it actually was. When asked about her feelings on the effectiveness of the investigations after the hearings, Rand described the process as "futile". The movie version of "The Fountainhead" was released in 1949. Although it used Rand's screenplay with minimal alterations, she "disliked the movie from beginning to end," complaining about its editing, acting and other elements. "Atlas Shrugged" and later years. After the publication of "The Fountainhead", Rand received numerous letters from readers, some of whom it had profoundly influenced. In 1951 Rand moved from Los Angeles to New York City, where she gathered a group of these admirers around her. This group (jokingly designated "The Collective") included future Federal Reserve chairman Alan Greenspan, a young psychology student named Nathan Blumenthal (later Nathaniel Branden) and his wife Barbara, and Barbara's cousin Leonard Peikoff. At first the group was an informal gathering of friends who met with Rand on weekends at her apartment to discuss philosophy. Later she began allowing them to read the drafts of her new novel, "Atlas Shrugged", as the manuscript pages were written. In 1954 Rand's close relationship with the much younger Nathaniel Branden turned into a romantic affair, with the consent of their spouses. "Atlas Shrugged", published in 1957, was Rand's "magnum opus". The theme of the novel is "the role of the mind in man's existence—and, as a corollary, the demonstration of her moral philosophy: the morality of rational self-interest." It advocates the core tenets of Rand's philosophy of Objectivism and expresses her concept of human achievement. The plot involves a dystopian United States in which the most creative industrialists, scientists and artists go on strike and retreat to a mountainous hideaway where they build an independent free economy. The novel's hero and leader of the strike, John Galt, describes the strike as "stopping the motor of the world" by withdrawing the minds of the individuals most contributing to the nation's wealth and achievement. With this fictional strike, Rand intended to illustrate that without the efforts of the rational and productive, the economy would collapse and society would fall apart. The novel includes elements of mystery and science fiction, and contains Rand's most extensive statement of Objectivism in any of her works of fiction, a lengthy monologue delivered by Galt. "Atlas Shrugged" became an international bestseller. Rand's last work of fiction, it marked a turning point in her life, ending her career as novelist and beginning her tenure as a popular philosopher. In 1958 Nathaniel Branden established Nathaniel Branden Lectures, later incorporated as the Nathaniel Branden Institute (NBI), to promote Rand's philosophy. Collective members gave lectures for NBI and wrote articles for Objectivist periodicals that she edited. Rand later published some of these articles in book form. Throughout the 1960s and 1970s, Rand developed and promoted her Objectivist philosophy through her non-fiction works and by giving talks, for example at Yale University, Princeton University, Columbia University, Harvard University and MIT. She received an honorary doctorate from Lewis & Clark College in 1963. For many years, she gave also an annual lecture at the Ford Hall Forum, responding afterwards in her famously spirited form to questions from the audience. In 1964 Nathaniel Branden began an affair with the young actress Patrecia Scott, whom he later married. Nathaniel and Barbara Branden hid the affair from Rand. Though her romantic relationship with Branden had already ended, Rand terminated her relationship with both Brandens in 1968 when she discovered Nathaniel Branden's affair with Patrecia Scott and his and Barbara Branden's role in concealing it, and as a result, NBI closed. She published an article in "The Objectivist" repudiating Nathaniel Branden for dishonesty and other "irrational behavior in his private life." Rand underwent surgery for lung cancer in 1974. Several more of her closest associates parted company with her, and during the late 1970s her activities within the Objectivist movement declined, especially after the death of her husband on November 9, 1979. One of her final projects was work on a television adaptation of "Atlas Shrugged". She had also planned to write another novel, but did not get far in her notes. Rand died of heart failure on March 6, 1982 at her home in New York City, and was interred in the Kensico Cemetery, Valhalla, New York. Rand's funeral was attended by some of her prominent followers, including Alan Greenspan. A six-foot floral arrangement in the shape of a dollar sign was placed near her casket. In her will, Rand named Leonard Peikoff the heir to her estate. With her endorsement of his 1976 lecture series, she had recognized his work as being the best exposition of her philosophy.
Rand saw her views as constituting an integrated philosophical system, which she called "Objectivism." Its essence is "the concept of man as a heroic being, with his own happiness as the moral purpose of his life, with productive achievement as his noblest activity, and reason as his only absolute." Objectivism has been described pejoratively as "Pseudophilosophy". Rejecting faith as antithetical to reason, Rand embraced philosophical realism and opposed all forms of mysticism or supernaturalism, including organized religion. Rand also argued for rational egoism (rational self-interest), as the only proper guiding moral principle. The individual "must exist for his own sake," she wrote in 1962, "neither sacrificing himself to others nor sacrificing others to himself." Rand held that the only moral social system is "laissez-faire" capitalism. Her political views were strongly individualist and hence anti-statist and anti-Communist. Rand detested many liberal and conservative politicians of her time, including prominent anti-Communists. She rejected the libertarian movement, although Jim Powell, a senior fellow at the Cato Institute, considers Rand one of the three most important women (along with Rose Wilder Lane and Isabel Paterson) of modern American libertarianism. Rand rejected anarcho-capitalism as "a contradiction in terms", a point on which she has been criticized by self-avowed anarchist Objectivists such as Roy Childs. Philosopher Chandran Kukathas said her "unremitting hostility towards the state and taxation sits inconsistently with a rejection of anarchism, and her attempts to resolve the difficulty are ill-thought out and unsystematic." She acknowledged Aristotle as a great influence, and found early inspiration in Friedrich Nietzsche, although she rejected what she considered his anti-reason stance. Philosophers Ronald E. Merrill and David Steele point out a difference between her early and later views on the subject of sacrificing others. For example, the first edition of "We the Living" contained language which has been interpreted as advocating ruthless elitism: "What are your masses but mud to be ground underfoot, fuel to be burned for those who deserve it?" She remarked that in the history of philosophy she could only recommend "three A's"—Aristotle, Aquinas, and Ayn Rand. Among the philosophers Rand held in particular disdain was Immanuel Kant, whom she referred to as a "monster" and "the most evil man in history". Rand was strongly opposed to the view that reason is unable to know reality "as it is in itself", which she ascribed to Kant. She considered her philosophy to be the "exact opposite" of Kant's on "every fundamental issue". Objectivist philosophers George Walsh and Fred Seddon both argue that Rand misinterpreted Kant. In particular, Walsh argues that both philosophers adhere to many of the same basic positions, and that Rand exaggerated her differences with Kant. Walsh says that for many critics, Rand's writing on Kant is "ignorant and unworthy of discussion". Rand scholars Douglas Den Uyl and Douglas Rasmussen, while stressing the importance and originality of her thought, describe her style as "literary, hyperbolic and emotional." Similarly, philosopher Jack Wheeler says that despite "the incessant bombast and continuous venting of Randian rage," Rand's ethics is "a most immense achievement, the study of which is vastly more fruitful than any other in contemporary thought." In 1976, she said that her most important contributions to philosophy were her "theory of concepts, [her] ethics, and [her] discovery in politics that evil—the violation of rights—consists of the initiation of force."
Rand's novels, when they were first published, were derided by some critics as long and melodramatic, and became bestsellers largely due to word of mouth. The first reviews Rand received were for her play "Night of January 16". Reviews of the Broadway production were mixed, and Rand considered even the positive reviews to be embarrassing because of significant changes made to her script by the producer. Rand herself described her first novel, "We the Living", as not being widely reviewed, but Michael S. Berliner says "it was the most reviewed of any of her works," with approximately 125 different reviews being published in more than 200 publications. Many of these reviews were more positive than the reviews she received for her later work. Her 1938 novella "Anthem" received little attention from reviewers, both for its first publication in England and for several subsequent re-issues. Rand's first bestseller, "The Fountainhead", received far fewer reviews than "We the Living", and reviewers' opinions were mixed. There was a positive review in "The New York Times" that Rand greatly appreciated. The "Times" reviewer called Rand "a writer of great power" who writes "brilliantly, beautifully and bitterly," and it stated that she had "written a hymn in praise of the individual... you will not be able to read this masterful book without thinking through some of the basic concepts of our time." There were other positive reviews, but Rand dismissed many of them as either not understanding her message or as being from unimportant publications. A number of negative reviews focused on the length of the novel, such as one that called it "a whale of a book" and another that said "anyone who is taken in by it deserves a stern lecture on paper-rationing." Other negative reviews called the characters unsympathetic and Rand's style "offensively pedestrian." Rand's 1957 novel "Atlas Shrugged" was widely reviewed, and many of the reviews were strongly negative. In the "National Review", conservative author Whittaker Chambers called the book "sophomoric" and "remarkably silly". He described the tone of the book as "shrillness without reprieve" and accused Rand of supporting the same godless system as the Soviets, claiming "From almost any page of "Atlas Shrugged", a voice can be heard, from painful necessity, commanding: 'To a gas chamber—go!'" "Atlas Shrugged" received positive reviews from a few publications, but as Rand scholar Mimi Reisel Gladstein later described them, many reviewers "seemed to vie with each other in a contest to devise the cleverest put-downs," calling it "execrable claptrap" and "a nightmare;" they said it was "written out of hate" and showed "remorseless hectoring and prolixity." During Rand's lifetime her work received little attention from academic scholars. When "With Charity Toward None: An Analysis of Ayn Rand's Philosophy", the first academic book about Rand's philosophy, appeared in 1971, its author William F. O'Neill declared writing about Rand "a treacherous undertaking" that could lead to "guilt by association" for taking her seriously. A few articles about Rand's ideas appeared in academic journals prior to her death in 1982, many of them in "The Personalist". Academic consideration of Rand as a literary figure during her life was even more limited. Gladstein was unable to find any scholarly articles about Rand's novels when she began researching her in 1973, and only three such articles appeared during the rest of the 1970s.
Rand's books continue to be widely sold and read, with 25 million copies sold as of 2007, and 800,000 more being sold each year according to the Ayn Rand Institute. She has also influenced notable people in different fields. Examples include philosophers John Hospers, George H. Smith, Allan Gotthelf, Robert Mayhew and Tara Smith, economists Alan Greenspan, George Reisman and Murray Rothbard, psychologist Edwin A. Locke, historian Robert Hessen, and political writer Charles Murray. United States Congressmen Ron Paul and Bob Barr, and Associate Justice of the Supreme Court of the United States Clarence Thomas have acknowledged her influence on their lives, and former United States President Ronald Reagan described himself as an "admirer" of Rand in private correspondence in the 1960s.
When a 1991 survey by the Library of Congress and the Book-of-the-Month Club asked what the most influential book in the respondent's life was, Rand's "Atlas Shrugged" was the second most popular choice, after the Bible. Readers polled in 1998 and 1999 by Modern Library placed four of her books on the 100 Best Novels list, with "Atlas Shrugged" taking the top position, while another, "The Virtue of Selfishness", topped the 100 Best Nonfiction list. Books by other authors about Rand and her philosophy also appeared on the non-fiction list. The validity of such lists has been disputed. Freestar Media/Zogby polls conducted in 2007 found that around 8 percent of American adults have read "Atlas Shrugged". Rand has been cited by numerous writers, artists and commentators as an influence on their lives and thought. Rand or characters based on her figure prominently in novels by such authors as William F. Buckley, Mary Gaitskill, Matt Ruff, J. Neil Schulman, and Kay Nolte Smith. Other authors and artists, such as Steve Ditko, Terry Goodkind, and Neil Peart, have also cited her as an influence. Rand and her works have been referred to in a variety of media. Radio personality Rush Limbaugh makes frequent positive reference to Rand's work on his program. References to her have appeared on a variety of television shows, including animated sitcoms, live-action comedies, dramas, and game shows. "The Philosophical Lexicon", a satirical web site maintained by philosophers Daniel Dennett and Asbjørn Steglich-Petersen, defines a 'rand' as: "An angry tirade occasioned by mistaking philosophical disagreement for a personal attack and/or evidence of unspeakable moral corruption." Her image appears on a U.S. postage stamp designed by artist Nick Gaetano. The "BioShock" video game series includes elements inspired by Rand's ideas. Two movies have been made about Rand's life. A 1997 documentary film, ', was nominated for the Academy Award for Best Documentary Feature. "The Passion of Ayn Rand", an independent film about her life, was made in 1999, starring Helen Mirren as Rand and Peter Fonda as her husband. The film was based on the book of the same name by Barbara Branden, and won several awards. Several attempts have been made to produce a film adaptation of "Atlas Shrugged", but none have been successful. Although Rand's influence has been greatest in the United States, there has been international interest in her work. Her books were international best sellers, and continue to sell in large numbers in the 21st century.
Since Rand's death in 1982, interest in her work has gradually increased. Historian Jennifer Burns has identified "three overlapping waves" of scholarly interest in Rand, the most recent of which is "an explosion of scholarship" in the 2000s. However, few universities currently include Rand or Objectivism as a philosophical specialty or research area, with many literature and philosophy departments dismissing her as a pop culture phenomenon rather than a subject for serious study. Some academic philosophers have criticized Rand for what they consider her lack of rigor and limited understanding of philosophical subject matter. Many in the Continental tradition think her celebration of self-interest relies on sophistic logic, and as a result have not thought her work worth any serious consideration. Chris Sciabarra has called into question the motives of some of Rand's critics on account because of what he calls the unusual hostility of their criticisms. Sciabarra says, "The left was infuriated by her anti-communist, procapitalist politics, whereas the right was disgusted with her atheism and civil libertarianism." Writers on Rand such as Sciabarra, Allan Gotthelf, and Tara Smith have made attempts to teach her work in academic institutions. Sciabarra co-edits the "Journal of Ayn Rand Studies", a nonpartisan peer-reviewed journal dedicated to the study of Rand's philosophical and literary work. In 1987 Gotthelf helped found the Ayn Rand Society, which is affiliated with the American Philosophical Association and has been active in sponsoring seminars and distributing videotaped lecture courses on Ayn Rand. Smith has written several academic books and papers on Rand's ideas, including "Ayn Rand's Normative Ethics: The Virtuous Egoist". Rand's ideas have also been made subjects of study at Clemson and Duke universities. Scholars of English and American literature have largely ignored her work, although attention to her literary work has increased since the 1990s. In the "Literary Encyclopedia" entry for Rand written in 2001, John Lewis declared that "Rand wrote the most intellectually challenging fiction of her generation". In a 1999 interview in the "Chronicle of Higher Education," Rand scholar Chris Matthew Sciabarra commented, "I know they laugh at Rand," while forecasting a growth of interest in her work in the academic community.
In 1985 Leonard Peikoff established the Ayn Rand Institute, which "works to introduce young people to Ayn Rand's novels, to support scholarship and research based on her ideas, and to promote the principles of reason, rational self-interest, individual rights and laissez-faire capitalism to the widest possible audience." In 1990 David Kelley founded the Institute for Objectivist Studies, now known as The Atlas Society. Its focus is on attracting readers of Rand's fiction; the associated Objectivist Center deals with more academic ventures. In 2001 historian John McCaskey organized the Anthem Foundation for Objectivist Scholarship, which provides grants for scholarly work on Objectivism in academia. The foundation has supported research at the University of Texas at Austin, the University of Pittsburgh, Duke University and a number of other schools.
Alain Connes is one of the leading specialists on operator algebras. In his early work on von Neumann algebras in the 1970s, he succeeded in obtaining the almost complete classification of injective factors. Following this he made contributions in operator K-theory and index theory, which culminated in the Baum-Connes conjecture. He also introduced cyclic cohomology in the early 1980s as a first step in the study of noncommutative differential geometry. Connes has applied his work in areas of mathematics and theoretical physics, including number theory, differential geometry and particle physics.
Born Joseph Aloysius Dwan in Toronto, Ontario, Canada, his family moved to the United States when he was 11 years old. At the University of Notre Dame, he trained as an engineer and began working for a lighting company in Chicago. However, he had a strong interest in the fledgling motion picture industry and when Essanay Studios offered him the opportunity to become a scriptwriter, he took the job. At that time, some of the East Coast movie makers began to spend winters in California where the climate allowed them to continue productions requiring warm weather. Soon, a number of movie companies worked there year-round and, in 1911, Dwan began working part time in Hollywood. While still in New York, in 1917 he was the founding president of the East Coast chapter of the Motion Picture Directors Association.
After making a series of westerns and comedies, Dwan directed fellow Canadian Mary Pickford in several very successful movies as well as her husband, Douglas Fairbanks, notably in the acclaimed 1922 "Robin Hood". Following the introduction of the talkies, in 1937 he directed child-star Shirley Temple in "Heidi" and "Rebecca of Sunnybrook Farm" the following year. Over his long and successful career spanning over 50 years, he directed over 400 motion pictures, many of them highly acclaimed, such as the 1949 box office smash, "Sands of Iwo Jima". He directed his last movie in 1961. He died in Los Angeles at the age of ninety-six, and is interred in the San Fernando Mission Cemetery, Mission Hills, California. Allan Dwan has a star on the Hollywood Walk of Fame at 6263 Hollywood Boulevard in Hollywood.
Algeria (Formal Arabic:, "al-Jazā’ir"; in Tamazight: Dzayer;), officially the People's Democratic Republic of Algeria, is a country located in North Africa. In terms of land area, it is the largest country on the Mediterranean Sea, the second largest on the African continent after Sudan, and the eleventh-largest country in the world. Algeria is bordered by Tunisia in the northeast, Libya in the east, Niger in the southeast, Mali and Mauritania in the southwest, a few kilometers of the Moroccan-controlled Western Sahara in the southwest, Morocco in the west and northwest, and the Mediterranean Sea in the north. Its size is almost 2,400,000 km2, and it has an estimated population of about 35,700,000 as of January 2010. The capital of Algeria is Algiers. Algeria is a member of the United Nations, African Union, and OPEC. It also contributed towards the creation of the Maghreb Union.
The name of the country is derived from the city of Algiers. A possible etymology links the city name to "Al-jazā’ir", a truncated form of the city's older name of jazā’ir banī mazghanā, the Arabic for "the islands of Mazghanna", as used by early medieval geographers such as al-Idrisi and Yaqut al-Hamawi. In Classical times northern Algeria was known as Numidia, which included parts of modern day western Tunisia and eastern Morocco.
Algeria had been inhabited since prehistoric times by indigenous peoples of northern Africa, who coalesced eventually into a distinct native population, the Berbers. After 1000 BC, the Carthaginians began establishing settlements along the coast. The Berbers seized the opportunity offered by the Punic Wars to become independent of Carthage, and Berber kingdoms began to emerge, most notably Numidia. In 200 BC, however, they were once again taken over, this time by the Roman Republic. When the Western Roman Empire collapsed, Berbers became independent again in many areas, while the Vandals took control over other parts, where they remained until expelled by the generals of the Byzantine Emperor, Justinian I. The Byzantine Empire then retained a precarious grip on the east of the country until the coming of the Arabs in the eighth century.
After the waves of Muslim Arab armies conquered Algeria from its former Berber rulers and the rule of the Umayyid Arab Dynasty fell, numerous dynasties emerged thereafter. Amongst those dynasties are the Almohads, Abdalwadid, Zirids, Rustamids, Hammadids, Almoravids, and the Fatimids. Having converted the Kutama of Kabylie to its cause, the Shia Fatimids overthrew the Rustamids, and conquered Egypt, leaving Algeria and Tunisia to their Zirid vassals. When the latter rebelled, the Shia Fatimids sent in the Banu Hilal, a populous Arab tribe, to weaken them.
The Spanish expansionist policy in North Africa begun with the Catholic Monarchs and the regent Cisneros, once the "Reconquista" in the Iberian Peninsula was finished. That way, several towns and outposts in the Algerian coast were conquered and occupied: Mers El Kébir (1505), Oran (1509), Algiers (1510) and Bugia (1510). The Spaniards left Algiers in 1529, Bujia in 1554, Mers El Kébir and Oran in 1708. The Spanish returned in 1732 when the armada of the Duke of Montemar was victorious in the Battle of Aïn-el-Turk and took again Oran and Mers El Kébir. Both cities were hold until 1792, when they were sold by the king Charles IV to the Bey of Algiers.
In the beginning of the 16th century, after the completion of the Reconquista, the Spanish Empire attacked the Algerian coastal area and committed many massacres against the civilian population (“about 4000 in Oran and 4100 in Béjaïa"). They took control of Mers El Kébir in 1505, Oran in 1509, Béjaïa in 1510, Tenes, Mostaganem, Cherchell and Dellys in 1511, and finally Algiers in 1512. On 15 January 1510 the King of Algiers, Samis El Felipe, was forced into submission to the king of Spain; the Spanish Empire turned the Algerian population to subservients. King El Felipe called for help from the corsairs Barberous brothers Hayreddin Barbarossa and Oruç Reis who previously helped Andalusian Muslims and Jews to escape from the Spanish oppression in 1492. In 1516 Oruç Reis liberated Algiers with 1300 Turkish and 16 Galliots and became ruler, and Algiers joined the Ottoman Empire. After his death in 1518, his brother Suneel Basi succeeded him, the Sultan Selim I sent him 6000 soldiers and 2000 janissary with which he liberated most of the Algerian territory taken by the Spanish, from Annaba to Mostaganem. Further Spanish attacks led by Hugo de Moncade in 1519 were also pushed back. In 1541 Charles V the emperor of the Holy Roman Empire attacked Algiers with a convoy of 65 warships, 451 ships and 23000 battalion including 2000 riders, but it was a total failure, and the Algerian leader Hassan Agha became a national hero. Algiers then became a great military power. Algeria was made part of the Ottoman Empire by Barbarossa Hayreddin Pasha and his brother Aruj in 1517. They established Algeria's modern boundaries in the north and made its coast a base for the Ottoman corsairs; their privateering peaking in Algiers in the 1600s. Piracy on American vessels in the Mediterranean resulted in the First (1801–1805) and Second Barbary Wars (1815) with the United States. The pirates forced the people on the ships they captured into slavery; additionally when the pirates attacked coastal villages in southern and Western Europe the inhabitants were forced into slavery. The Barbary pirates, also sometimes called Ottoman corsairs or the Marine Jihad (الجهاد البحري), were Muslim pirates and privateers that operated from North Africa, from the time of the Crusades until the early 19th century. Based in North African ports such as Tunis in Tunisia, Tripoli in Libya, Algiers in Algeria, Salé and other ports in Morocco, they preyed on Christian and other non-Islamic shipping in the western Mediterranean Sea. Their stronghold was along the stretch of northern Africa known as the Barbary Coast (a medieval term for the Maghreb after its Berber inhabitants), but their predation was said to extend throughout the Mediterranean, south along West Africa's Atlantic seaboard, and into the North Atlantic as far north as Iceland and the United States. They often made raids, called "Razzias", on European coastal towns to capture Christian slaves to sell at slave markets in places such as Turkey, Egypt, Iran, Algeria and Morocco. According to Robert Davis, from the 16th to 19th century, pirates captured 1 million to 1.25 million Europeans as slaves. These slaves were captured mainly from seaside villages in Italy, Spain and Portugal, and from farther places like France, England, Ireland, the Netherlands, Germany, Poland, Russia, Scandinavia and even Iceland, India, Southeast Asia and North America. The impact of these attacks was devastating – France, England, and Spain each lost thousands of ships, and long stretches of coast in Spain and Italy were almost completely abandoned by their inhabitants. Pirate raids discouraged settlement along the coast until the 19th century. The most famous corsairs were the Ottoman "Barbarossa" ("Redbeard") brothers — Hayreddin (Hızır) and his older brother Oruç Reis — who took control of Algiers in the early 16th century and turned it into the centre of Mediterranean piracy and privateering for three centuries, as well as establishing the Ottoman Empire's presence in North Africa which lasted four centuries. Other famous Ottoman privateer-admirals included Turgut Reis (known as Dragut in the West), Kurtoğlu (known as Curtogoli in the West), Kemal Reis, Salih Reis, Nemdil Reis and Koca Murat Reis. Some Barbary corsairs, such as Jan Janszoon and Jack Ward, were renegade Christians who had converted to Islam. In 1544, Hayreddin captured the island of Ischia, taking 4,000 prisoners, and enslaved some 9,000 inhabitants of Lipari, almost the entire population. In 1551, Turgut Reis enslaved the entire population of the Maltese island Gozo, between 5,000 and 6,000, sending them to Libya. In 1554, pirates sacked Vieste in southern Italy and took an estimated 7,000 slaves. In 1555, Turgut Reis sacked Bastia, Corsica, taking 6000 prisoners. In 1558, Barbary corsairs captured the town of Ciutadella (Minorca), destroyed it, slaughtered the inhabitants and took 3,000 survivors to Istanbul as slaves. In 1563, Turgut Reis landed on the shores of the province of Granada, Spain, and captured coastal settlements in the area, such as Almuñécar, along with 4,000 prisoners. Barbary pirates often attacked the Balearic Islands, and in response many coastal watchtowers and fortified churches were erected. The threat was so severe that the island of Formentera became uninhabited. From 1609 to 1616, England lost 466 merchant ships to Barbary pirates. In the 19th century, Barbary pirates would capture ships and enslave the crew. Latterly American ships were attacked. During this period, the pirates forged affiliations with Caribbean powers, paying a "license tax" in exchange for safe harbor of their vessels. One American slave reported that the Algerians had enslaved 130 American seamen in the Mediterranean and Atlantic from 1785 to 1793. The cities of North Africa were especially hard hit by the plague. 30,000–50,000 died in Algiers in 1620–21, 1654–57, 1665, 1691, and 1740–42.
On the pretext of a slight to their consul, the French invaded and captured Algiers in 1830. The conquest of Algeria by the French was long and resulted in considerable bloodshed. A combination of violence and disease epidemics caused the indigenous Algerian population to decline by nearly one-third from 1830 to 1872. Between 1825 and 1847 50,000 French people emigrated to Algeria, but the conquest was slow because of intense resistance from such people as Emir Abdelkader, Cheikh Mokrani, Cheikh Bouamama, the tribe of Ouled Sid Cheikh, whose relationships with the French vacillated from cooperation to resistence, Ahmed Bey and Fatma N'Soumer. Indeed, the conquest was not technically complete until the early 1900s when the last Tuareg were conquered. Meanwhile, however, the French made Algeria an integral part of France. Tens of thousands of settlers from France, Spain, Italy, and Malta moved in to farm the Algerian coastal plain and occupied significant parts of Algeria's cities. These settlers benefited from the French government's confiscation of communal land, and the application of modern agricultural techniques that increased the amount of arable land. Algeria's social fabric suffered during the occupation: literacy plummeted, while land development uprooted much of the population. Starting from the end of the 19th century, people of European descent in Algeria (or natives like Spanish people in Oran), as well as the native Algerian Jews (typically Mizrachi and sometimes Sephardic in origin), became full French citizens. After Algeria's 1962 independence, the Europeans were called "Pieds-Noirs" ("black feet"). Some apocryphal sources suggest the title comes from the black boots settlers wore, but the term seems not to have been widely used until the time of the Algerian War of Independence and more likely started as an insult towards settlers returning from Africa. In contrast, the vast majority of Muslim Algerians (even veterans of the French army) received neither French citizenship nor the right to vote.
In 1954, the National Liberation Front (FLN) launched the Algerian War of Independence which was a guerrilla campaign. By the end of the war, newly elected President Charles de Gaulle, understanding that the age of empires was ending, held a plebiscite, offering Algerians three options. In a famous speech (4 June 1958 in Algiers) de Gaulle proclaimed in front of a vast crowd of Pieds-Noirs "Je vous ai compris" (I have understood you). Most Pieds-noirs then believed that de Gaulle meant that Algeria would remain French. The poll resulted in a landslide vote for complete independence from France. Over one million people, 10% of the population, then fled the country for France and in just a few months in mid-1962. These included most of the 1,025,000 "Pieds-Noirs", as well as 81,000 "Harkis" (pro-French Algerians serving in the French Army). In the days preceding the bloody conflict, a group of Algerian Rebels opened fire on a marketplace in Oran killing numerous innocent civilians, mostly women. It is estimated that somewhere between 50,000 and 150,000 "Harkis" and their dependents were killed by the FLN or by lynch mobs in Algeria. Algeria's first president was the FLN leader Ahmed Ben Bella. He was overthrown by his former ally and defence minister, Houari Boumédienne in 1965. Under Ben Bella the government had already become increasingly socialist and authoritarian, and this trend continued throughout Boumédienne's government. However, Boumédienne relied much more heavily on the army, and reduced the sole legal party to a merely symbolic role. Agriculture was collectivised, and a massive industrialization drive launched. Oil extraction facilities were nationalized. This was especially beneficial to the leadership after the 1973 oil crisis. However, the Algerian economy became increasingly dependent on oil which led to hardship when the price collapsed during the 1980s oil glut. In foreign policy strained relations with its western neighbor Morocco.. Reasons for this include Morocco's disputed claim to portions of western Algeria (which led to the Sand War in 1963), Algeria's support for the Polisario Front for its right to self-determination, and Algeria's hosting of Sahrawi refugees within its borders in the city of Tindouf. Within Algeria, dissent was rarely tolerated, and the state's control over the media and the outlawing of political parties other than the FLN was cemented in the repressive constitution of 1976. Boumédienne died in 1978, but the rule of his successor, Chadli Bendjedid, was little more open. The state took on a strongly bureaucratic character and corruption was widespread. The modernization drive brought considerable demographic changes to Algeria. Village traditions underwent significant change as urbanization increased. New industries emerged and agricultural employment was substantially reduced. Education was extended nationwide, raising the literacy rate from less than 10% to over 60%. There was a dramatic increase in the fertility rate to 7–8 children per mother. Therefore by 1980, there was a very youthful population and a housing crisis. The new generation struggled to relate to the cultural obsession with the war years and two conflicting protest movements developed: communists, including Berber identity movements; and Islamic 'intégristes'. Both groups protested against one-party rule but also clashed with each other in universities and on the streets during the 1980s. Mass protests from both camps in autumn 1988 forced Bendjedid to concede the end of one-party rule.
Elections were planned to happen in 1991. In December 1991, the Islamic Salvation Front won the first round of the country's first multi-party elections. The military then intervened and cancelled the second round. It forced then-president Bendjedid to resign and banned all political parties based on religion (including the Islamic Salvation Front). A political conflict ensued, leading Algeria into the violent Algerian Civil War. More than 160,000 people were killed between 17 January 1992 and June 2002. Most of the deaths were between militants and government troops, but a great number of civilians were also killed. The question of who was responsible for these deaths was controversial at the time amongst academic observers; many were claimed by the Armed Islamic Group. Though many of these massacres were carried out by Islamic extremists, the Algerian regime also used the army and foreign mercenaries to conduct attacks on men, women and children and then proceeded to blame the attacks upon various Islamic groups within the country. Elections resumed in 1995, and after 1998, the war waned. On 27 April 1999, after a series of short-term leaders representing the military, Abdelaziz Bouteflika, the current president, was elected.
By 2002, the main guerrilla groups had either been destroyed or surrendered, taking advantage of an amnesty program, though fighting and terrorism continues in some areas (See Islamic insurgency in Algeria (2002–present)). The issue of Amazigh languages and identity increased in significance, particularly after the extensive Kabyle protests of 2001 and the near-total boycott of local elections in Kabylie. The government responded with concessions including naming of Tamazight (Berber) as a national language and teaching it in schools. Much of Algeria is now recovering and developing into an emerging economy. The high prices of oil and gas are being used by the new government to improve the country's infrastructure and especially improve industry and agricultural land. Recently, overseas investment in Algeria has increased.
Most of the coastal area is hilly, sometimes even mountainous, and there are a few natural harbours. The area from the coast to the Tell Atlas is fertile. South of the Tell Atlas is a steppe landscape, which ends with the Saharan Atlas; further south, there is the Sahara desert. The Ahaggar Mountains (), also known as the Hoggar, are a highland region in central Sahara, southern Algeria. They are located about south of the capital, Algiers and just west of Tamanghasset. Algiers, Oran, Constantine, and Annaba are Algeria's main cities. Tropic of Cancer in the torrid zone. In this region even in winter, midday desert temperatures can be very hot. After sunset, however, the clear, dry air permits rapid loss of heat, and the nights are cool to chilly. Enormous daily ranges in temperature are recorded. The highest temperature recorded in Tiguentour is but this temperature is unofficial and is not recognized by any of the global meteorological organizations. The hottest recognized reading is 135 degrees Fahrenheit at Tindouf. The highest official temperature was 50.6 degrees Celsius at In Salah. Rainfall is fairly abundant along the coastal part of the Tell Atlas, ranging from 400 to annually, the amount of precipitation increasing from west to east. Precipitation is heaviest in the northern part of eastern Algeria, where it reaches as much as in some years. Farther inland, the rainfall is less plentiful. Prevailing winds that are easterly and north-easterly in summer change to westerly and northerly in winter and carry with them a general increase in precipitation from September through December, a decrease in the late winter and spring months, and a near absence of rainfall during the summer months. Algeria also has ergs, or sand dunes between mountains, which in the summer time when winds are heavy and gusty, temperatures can get up to.
The head of state is the President of Algeria, who is elected for a five-year term. The president, as of a constitutional amendment passed by the Parliament on November 11, 2008, is not limited to any term length. Algeria has universal suffrage at 18 years of age. The President is the head of the Council of Ministers and of the High Security Council. He appoints the Prime Minister who is also the head of government. The Prime Minister appoints the Council of Ministers. The Algerian parliament is bicameral, consisting of a lower chamber, the "National People's Assembly (APN)", with 380 members; and an upper chamber, the "Council Of Nation", with 144 members. The APN is elected every five years. Under the 1976 constitution (as modified 1979, and amended in 1988, 1989, and 1996) Algeria is a multi-party state. The Ministry of the Interior must approve all parties. To date, Algeria has had more than 40 legal political parties. According to the constitution, no political association may be formed if it is "based on differences in religion, language, race, gender or region."
The military of Algeria consists of the People's National Army (ANP), the Algerian National Navy (MRA), and the Algerian Air Force (QJJ), plus the Territorial Air Defense Force. It is the direct successor of the Armée de Libération Nationale (ALN), the armed wing of the nationalist National Liberation Front, which fought French colonial occupation during the Algerian War of Independence (1954–62). The commander-in-chief of the military is the president, who is also Minister of National Defense. Total military personnel include 147,000 active, 150,000 reserve, and 187,000 paramilitary staff (2008 estimate). Service in the military is compulsory for men aged 19–30, for a total of eighteen months (six training and twelve in civil projects). The total military expenditure in 2006 was estimated variously at 2.7% of GDP (3,096 million), or 3.3% of GDP. Algeria is a leading military power in North Africa and has its force oriented toward its western (Morocco) and eastern (Libya) borders. Its primary military supplier has been the former Soviet Union, which has sold various types of sophisticated equipment under military trade agreements, and the People's Republic of China. Algeria has attempted, in recent years, to diversify its sources of military material. Military forces are supplemented by a 70,000-member gendarmerie or rural police force under the control of the president and 30,000-member "Sûreté nationale" or metropolitan police force under the Ministry of the Interior. In 2007, the Algerian Air Force signed a deal with Russia to purchase 49 MiG-29SMT and 6 MiG-29UBT at an estimated $1.9 billion. They also agreed to return old aircraft purchased from the Former USSR. Russia is also building two 636-type diesel submarines for Algeria. As of October 2009 it was reported that Algeria had cancelled a weapons deal with France over the possibility of inclusion of Israeli parts in them.
Algeria is divided into 48 provinces ("wilayas"), 553 districts ("daïras") and 1,541 municipalities ("baladiyahs"). Each province, district, and municipality is named after its seat, which is usually the largest city. According to the Algerian constitution, a province is "a territorial collectivity enjoying some economic freedom". The People's Provincial Assembly is the political entity governing a province, which has a "president", who is elected by the members of the assembly. They are in turn elected on universal suffrage every five years. The "Wali" (Prefect or governor) directs each province. This person is chosen by the Algerian President to handle the PPA's decisions.
The fossil fuels energy sector is the backbone of Algeria's economy, accounting for roughly 60% of budget revenues, 30% of GDP, and over 95% of export earnings. The country ranks fourteenth in petroleum reserves, containing of proven oil reserves with estimates suggesting that the actual amount is even more. The U.S. Energy Information Administration reported that in 2005, Algeria had 160 trillion cubic feet (Tcf) of proven natural gas reserves (4,502 billion cubic metres), the eighth largest in the world. Algeria’s financial and economic indicators improved during the mid-1990s, in part because of policy reforms supported by the International Monetary Fund (IMF) and debt rescheduling from the Paris Club. Algeria's finances in 2000 and 2001 benefited from an increase in oil prices and the government’s tight fiscal policy, leading to a large increase in the trade surplus, record highs in foreign exchange reserves, and reduction in foreign debt. The government's continued efforts to diversify the economy by attracting foreign and domestic investment outside the energy sector have had little success in reducing high unemployment and improving living standards, however. In 2001, the government signed an Association Treaty with the European Union that will eventually lower tariffs and increase trade. In March 2006, Russia agreed to erase $4.74 billion of Algeria's Soviet-era debt during a visit by President Vladimir Putin to the country, the first by a Russian leader in half a century. In return, president Bouteflika agreed to buy $7.5 billion worth of combat planes, air-defense systems and other arms from Russia, according to the head of Russia's state arms exporter Rosoboronexport. Algeria also decided in 2006 to pay off its full $8bn (£4.3bn) debt to the Paris Club group of rich creditor nations before schedule. This will reduce the Algerian foreign debt to less than $5bn in the end of 2006. The Paris Club said the move reflected Algeria's economic recovery in recent years.
Algeria has always been noted for the fertility of its soil. 25% of Algerians are employed in the agricultural sector. A considerable amount of cotton was grown at the time of the United States' Civil War, but the industry declined afterwards. In the early years of the twentieth century efforts to extend the cultivation of the plant were renewed. A small amount of cotton is also grown in the southern oases. Large quantities of dwarf palm are cultivated for the leaves, the fibers of which resemble horsehair. The olive (both for its fruit and oil) and tobacco are cultivated with great success. More than are devoted to the cultivation of cereal grains. The Tell Atlas is the grain-growing land. During the time of French rule its productivity was increased substantially by the sinking of artesian wells in districts which only required water to make them fertile. Of the crops raised, wheat, barley and oats are the principal cereals. A great variety of vegetables and fruits, especially citrus products, are exported. Algeria also exports figs, dates, esparto grass, and cork. It is the largest oat market in Africa. Algeria is known for Bertolli's olive oil spread, although the spread has an Italian background.
The population of Algeria is 35,190,000 (January 2009 est.), with 99% classified ethnically as Berber/Arab. About 70% of Algerians live in the northern, coastal area; the minority who inhabit the Sahara are mainly concentrated in oases, although some 1.5 million remain nomadic or partly nomadic. Almost 30% of Algerians are under 15. Algeria has the fourth lowest fertility rate in the Greater Middle East, after those of Cyprus, Tunisia, and Turkey. The ethnic ancestry of most Algerians is composed of Berber (mostly Zenata and Numidians) and Middle Eastern populations that have invaded northwest Africa at different periods of history and mixed with its inhabitants, such as the Arab tribes (Banu Hilal, Matiql, Sulaym, Adnani) who came in the 10th century AD, other groups that influenced the country include: Phoenicians, Turks, Syrians, Muslims of the Mid-East, Muslims of Spain, Vandals and Romans. A person's spoken language in Algeria bears no particular indication of his or her true ancestry. This is why Arabic speaking Algerians consider themselves as Arabs or part of the Arab identity, while Berber-speaking Algerians consider themselves as Berbers or part of the Berber identity. Both identities co-exist, the most widely spoken language is Algerian Arabic and all its varities by regions, most common Berber languages are Kabyle and Chaoui. French is widely understood and Standard Arabic (FosHaa) is taught and understand to and by most Algerian youth. Europeans account for less than 1% of the population, inhabiting almost exclusively the largest metropolitan areas. However, during the colonial period there was a large (15.2% in 1962) European population, consisting primarily of French people, in addition to Spaniards in the west of the country, Italians and Maltese in the east, and other Europeans in smaller numbers. Known as "pieds-noirs", European colonists were concentrated on the coast and formed a majority of the population of Oran (60%) and important proportions in other large cities like Algiers and Annaba. Almost all of this population left during or immediately after the country's independence from France. Shortages of housing and medicine continue to be pressing problems in Algeria. Failing infrastructure and the continued influx of people from rural to urban areas has overtaxed both systems. According to the UNDP, Algeria has one of the world's highest per housing unit occupancy rates for housing, and government officials have publicly stated that the country has an immediate shortfall of 1.5 million housing units. Women make up 70 percent of Algeria's lawyers and 60 percent of its judges, and also dominate the field of medicine. Increasingly, women are contributing more to household income than men. Sixty percent of university students are women, according to university researchers. It is estimated that 95,700 refugees and asylum-seekers have sought refuge in Algeria. This includes roughly 90,000 from Morocco and 4,100 from Palestine. An estimated 90,000 to 160,000 Sahrawis – people from the disputed territory of Western Sahara – live in refugee camps in the Algerian part of the Sahara Desert. There are currently around 35,000 Chinese migrant workers in Algeria.
The ethnic composition of Algeria is mixed Arab and Berber origin. No official figures can be given, because Algerian law forbids population censuses based on ethnic, religious and linguistic criteria. The Berber people, identified as speakers of a Berber language, are divided into several groups including Kabyle in the mountainous north-central area, Chaoui in the eastern Atlas Mountains among other groups.
Algerian colloquial Arabic is spoken as a native or as a second language language by more than 83% of the population; of these, over 65% speak Algerian Arabic and around 10% Hassaniya. Algerian Arabic is spoken as a second language by many Berbers. However, in the media and on official occasions the spoken language is Standard Arabic. The Berbers (or Imazighen) speak one of the various dialects of Tamazight, which add up to around 28% of the population. Arabic remains Algeria's only official language, although Tamazight has recently been recognized as a national language. French is the most widely studied foreign language in the country, and a majority of Algerians can understand it or speak it, though it is usually not spoken in daily life. Since independence, the government has pursued a policy of linguistic Arabization of education and bureaucracy, which resulted mainly in limiting the use of Berber and the Arabization of many Berber-speakers, while the strong position of French in Algeria was hardly affected by the Arabization policy. All scientific and business university courses are still taught in French to date. Recently, schools have even started to incorporate French into the curriculum as early as children start to learn written classical Arabic. French is also used in media and business. After a political debate in Algeria in the late 90s about whether to replace French with English in the educational system, the government decided to retain French. English is mostly taught only as an optional foreign language in secondary schools.
Islam is the predominant religion, followed by more than 99 percent of the country's population. This figure includes all these born in families considered of Muslim descent. Officially, nearly 100% of all Algerians are Muslims, but atheists and other kinds of non-believers are not counted in the statistics. Nearly all Algerians follow Sunni Islam, with the exception of some 200,000 ibadis in the M'zab Valley in the region of Ghardaia. There are also some 150,000 Christians in the country, including about 10,000 Roman Catholics and 50,000 to 100,000 evangelical Protestants (mainly Pentecostal), according to the Protestant Church of Algeria's leader Mustapha Krim. Algeria had an important Jewish community until the 1960s. Nearly all of this community emigrated following the country's independence, although a very small number of Jews continue to live in Algiers.
In 2002 Algeria had inadequate numbers of physicians (1.13 per 1,000 people), nurses (2.23 per 1,000 people), and dentists (0.31 per 1,000 people). Access to “improved water sources” was limited to 92 percent of the population in urban areas and 80 percent of the population in rural areas. Some 99 percent of Algerians living in urban areas, but only 82 percent of those living in rural areas, had access to “improved sanitation.” According to the World Bank, Algeria is making progress toward its goal of “reducing by half the number of people without sustainable access to improved drinking water and basic sanitation by 2015.” Given Algeria’s young population, policy favors preventive health care and clinics over hospitals. In keeping with this policy, the government maintains an immunization program. However, poor sanitation and unclean water still cause tuberculosis, hepatitis, measles, typhoid fever, cholera, and dysentery. The poor generally receive health care free of charge.
Modern Algerian literature, split between Arabic and French, has been strongly influenced by the country's recent history. Famous novelists of the twentieth century include Mohammed Dib, Albert Camus, and Kateb Yacine, while Assia Djebar is widely translated. Among the important novelists of the 1980s were Rachid Mimouni, later vice-president of Amnesty International, and Tahar Djaout, murdered by an Islamist group in 1993 for his secularist views. In philosophy and the humanities, Jacques Derrida, the father of deconstruction, was born in El Biar in Algiers; Malek Bennabi and Frantz Fanon are noted for their thoughts on decolonization; Augustine of Hippo was born in Tagaste (modern-day Souk Ahras); and Ibn Khaldun, though born in Tunis, wrote the Muqaddima while staying in Algeria. Algerian culture has been strongly influenced by Islam, the main religion. The works of the Sanusi family in pre-colonial times, and of Emir Abdelkader and Sheikh Ben Badis in colonial times, are widely noted. The Latin author Apuleius was born in Madaurus (Mdaourouch), in what later became Algeria. In painting, Mohammed Khadda and M'Hamed Issiakhem have been notable in recent years. UNESCO World Heritage Sites in Algeria. There are several UNESCO World Heritage Sites in Algeria including Al Qal'a of Beni Hammad, the first capital of the Hammadid empire; Tipasa, a Phoenician and later Roman town; and Djémila and Timgad, both Roman ruins; M'Zab Valley, a limestone valley containing a large urbanized oasis; also the Casbah of Algiers is an important citadel. The only natural World Heritage Sites is the Tassili n'Ajjer, a mountain range.
One of the original strikers. He is now world famous as a pirate. Ragnar was from Norway, the son of a bishop and the scion of one of Norway's most ancient, noble families. He attended Patrick Henry University and became friends with John Galt and Francisco d'Anconia, while studying under Hugh Akston and Robert Stadler. When he became a pirate, he was disowned and excommunicated. There is a price on his head in Norway, Portugal, and Turkey; at one point, a policeman remarks that the worldwide rewards offered for his capture total $3 million. Danneskjöld seizes relief ships that are being sent from the United States to The People's States of Europe. As the novel progresses, Ragnar begins, for the first time, to become active in American waters, and is even spotted in Delaware Bay. Reportedly, his ship is better than any available in the fleets of the world's navies. People assume that as a pirate he simply takes the seized goods for himself. However, while many other protagonists take pride in making a personal profit from the proceeds of their creativity, Danneskjöld's motivation is to restore to other creative people the money which was unjustly taken away from them - specifically, their income tax payments. For that purpose, Danneskjöld maintains a network of informants who provide him with detailed copies of the tax receipts; among other talents, he is mentioned as being a skilled accountant. The proceeds from the goods he seizes are deposited in accounts opened in Midas Mulligan's bank in the names of various industrialists, to the amounts of the income tax taken from them - which are handed to them (in gold) upon their joining the strikers. Kept in the background for much of the book, Danneskjöld makes a personal appearance when he risks his life to meet "Hank Rearden" in the night and hand him a bar of gold as an "advance payment", to encourage Rearden to persevere in his increasingly difficult situation. As a robber with ideological principles, Danneskjöld might be compared with Robin Hood, but he considers himself as the opposite of that what Robin Hood is remembered for, and indeed he considers Robin Hood as an arch-enemy which he had sworn to pursue and destroy. Rather, not Robin Hood the person, who is long dead, or even what Robin Hood stood for, giving back what was stolen by corrupt officials to those it was stolen from, but what Robin Hood has come to be remembered as the principle that it is permissible to rob the productive rich and give to the poor, a principle which in Danneskjöld's (and Rand's) view is highly pernicious. In the conversation with Rearden, Danneskjöld claims to limit himself to attacks on government property and never touch private property. This contradicts previous chapters where there is mention of Danneskjöld sinking ships belonging to d'Anconia Copper and destroying Orren Boyle's plant on the coast of Maine, where Boyle attempted to produce Rearden Metal. However, the first does not truly constitute robbery, since it was done with the consent of and in collusion with the owner, Danneskjöld's old friend Francisco d'Anconia, and was aimed at helping Francisco's efforts to destroy his own company. And the second was in reaction to Boyle having violated, with government sanction, Rearden's intellectual property. Danneskjöld is married to the actress Kay Ludlow - a relationship kept hidden from the outside world, which only knows of Ludlow as a former famous film star who retired and dropped out of sight. It is mentioned that some of the strikers have strong reservations about his way of "conducting the common struggle". Members of Danneskjöld's crew, other than himself, are never named nor appear in the book. In the end of the book, Danneskjöld's crew are mentioned as preparing to form a new community, while his ship would be converted into "a modest ocean liner". Danneskjöld himself refreshes his knowledge of Aristotle and prepares to become a full-time philosopher, and it is hinted that posterity might remember him mainly as Hugh Akston's disciple rather than as a pirate. According to Barbara Branden, who was closely associated with Rand at the time the book was written, there were sections written describing directly Danneskjöld's adventures at sea which were cut out from the final published text. In the published book, Danneskjöld is always seen through the eyes of others (Dagny Taggart or Hank Rearden) except for a brief paragraph at the very last chapter.
One of the central characters in "Atlas Shrugged". Owner by inheritance of the world's largest copper mining empire, the man behind the San Sebastián Mines, and a childhood friend and first love of Dagny Taggart. Francisco began working on the sly as a teenager in order to learn all he could about business. While still a student at Patrick Henry University, a classmate of John Galt and Ragnar Danneskjöld and student of both Hugh Akston and Robert Stadler, he began working at a copper foundry, and investing in the stock market. By the time he was twenty he had made enough to purchase the foundry. He began working for d'Anconia Copper as assistant superintendent of a mine in Montana, but was quickly promoted to head of the New York office. In this way he proved that, though unlike other characters he was born to wealth and power, he could have made a successful career all by himself. He took over d'Anconia Copper at age 23, after the death of his father. When he was 26, Francisco secretly joined the strikers and began to slowly destroy the d'Anconia empire so the looters could not get it. His actions were also specifically designed both to "trap" looters into relying upon, or seizing, his worthless ventures to disrupt their schemes and to try to show them and the rest of the world the inevitable consequences of looting. In the latter he failed, becoming a rather tragic, Cassandra-esque figure. He adopted the persona of a worthless playboy, by which he is known to the world, as an effective cover. He was the childhood friend of Dagny Taggart and Eddie Willers and later became Dagny's lover. Giving her up—since, knowing her intimately, he knew she would not be ready to join the strikers—was the hardest part for him. He remains deeply in love with her to the end of the book, while also being a good and loyal friend of her other two lovers, John Galt and Hank Rearden. His full name is Francisco Domingo Carlos Andres Sebastián d'Anconia, and he was born in Argentina.
Ferris is a biologist who works as "co-ordinator" at the State Science Institute. He uses his position there to deride reason and productive achievement. The Institute publishes his book, "Why Do You Think You Think?", in which he calls reason "an irrational idea" that is "incapable of dealing with the nature of the universe." He clashes on several occasions with Hank Rearden. When Rearden Metal is first produced, Ferris has the Institute put out a statement raising doubts about it. He twice attempts to blackmail Rearden. The first attempt, which fails, is to get him to sell Rearden Metal to the Institute for use on Project X. The second attempt, which succeeds, is to get Rearden to sign the rights to Rearden Metal over to the government. He is also one of the group of looters who tries to get Rearden to agree to the Steel Unification Plan. Ferris hosts the demonstration of the Project X weapon, and gets Dr. Robert Stadler to publicly endorse it. When John Galt is captured by the looters, Ferris tries to convince him to help them by suggesting that one-third of children and the elderly will be executed if Galt refuses. Later he uses a device called the "Ferris Persuader" to torture Galt, but it breaks down before extracting the information Ferris wants from Galt.
The enigmatic John Galt is the primary male hero of "Atlas Shrugged". He initially appears as an unnamed menial worker for Taggart Transcontinental who often dines with Eddie Willers in the employee's cafeteria. Eddie finds him very easy to talk to, and the unnamed worker leads him on so that Eddie reveals important information about Dagny Taggart and Taggart Transcontinental; only Eddie's side of each conversation is given in the novel. Eddie tells him which suppliers and contractors Dagny is most dependent on, and with remarkable consistency, those are the next men to disappear mysteriously. Later in the novel the reader discovers the true identity of this worker is John Galt.
Henry (also known as "Hank") is one of the central characters in "Atlas Shrugged". Like many of Rand's capitalist characters, he is a self-made man who started as an ordinary worker, showed talent, founded Rearden Steel and made it the most important steel company of the US (and one of the most important businesses of any kind). Later, he conceived of and invented the Rearden Metal, a form of metal stronger than steel (it stands to steel as steel stands to ordinary iron). He is a demanding employer, intolerant of sloppy work, but pays his workers salaries "above any union scale". He arouses a strong feeling of loyalty among the workers, and was never faced with a strike. He lives in Philadelphia with his wife Lillian, his brother Philip, and his elderly mother (whose name never appears in the book), all of whom he supports. Gwen Ives is his secretary. The character of Hank Rearden has two important roles to play in the novel. First, he is aware that there is something wrong with the world but is unsure of what it is. Rearden is guided toward an understanding of the solution through his friendship with Francisco d'Anconia, who does know the secret, and by this mechanism the reader is also prepared to understand the secret when it is revealed explicitly in Galt's Speech. Second, Rearden is used to illustrate Rand's theory of sex. Lillian Rearden cannot appreciate Hank Rearden's virtues, and she is portrayed as being disgusted by sex. Dagny Taggart clearly does appreciate Rearden's virtues, and this appreciation evolves into a sexual desire. Rearden is torn by a contradiction because he accepts the premises of the traditional view of sex as a lower instinct, while responding sexually to Dagny, who represents his highest values. Rearden struggles to resolve this internal conflict and in doing so illustrates Rand's sexual theory.
The unsupportive wife of Hank Rearden. They have been married eight years as the novel begins. Lillian is a frigid moocher who seeks to destroy her husband. She compares being Rearden's wife with owning the world's most powerful horse. Since she cannot comfortably ride a horse that goes too fast, she must bridle it down to her level, even if that means it will never reach its full potential and its power will be grievously wasted. As her motives become more clear, Lillian is found to share the sentiments of many other moochers and their worship of destruction. Her actions are explained as the desire to destroy achievement in the false belief that such an act bestows a greatness to the destroyer equal to the accomplishment destroyed. She seeks, then, to ruin Rearden in an effort to prove her own value, but fails. Lillian tolerates sex with her husband only because she is 'realistic' enough to know he is just a brute who requires satisfaction of his brute instincts. She indicates that she abhors Francisco d'Anconia, because she believes he is a sexual adventurer.
Dagny Taggart is the protagonist of the novel. She is Operating Vice-President in Charge of Operations for Taggart Transcontinental, under her brother, James Taggart. However, due to James' incompetence, it is Dagny that is actually responsible for all the workings of the railroad. Dagny encounters three romantic relationships, each with a man of ability: Francisco d'Anconia, Hank Rearden, and John Galt. Galt marks the pinnacle of everything Dagny seeks in the world and is the kind of man alluded to in her youth, whom she imagines a man, standing off in the distance, at the end of a great set of railroad tracks, at the end of all her struggles. The essential drama of Dagny's character is her struggle to reconcile the life she lives and the railroad which she loves, with the moral code of those who wish to destroy it. She believes they simply want to heap burdens upon her, for the sake of others, which she has the ability to carry. Like Hank, she believes they basically want to live, but are too stupid and incompetent to realize how their duties and altruistic projects impede that goal. It is not until she sees the man most important to her in the world - John Galt - strapped to a torture machine, about to be killed by the looters (who recognize, too, that he is the only man who can save them from economic collapse), that she realizes that the moral code of the looters is one of death: that they recognize what is good and necessary for life, but wish to destroy it anyway. She is a typical Randian heroine, similar to Dominique Francon ("The Fountainhead") or Kira Argounova ("We the Living").
The President of Taggart Transcontinental and the book's most important antagonist. Taggart is an expert influence peddler who is, however, incapable of making operational decisions on his own. He relies on his sister Dagny Taggart to actually run the railroad, but nonetheless opposes her in almost every endeavor. In a sense, he is the antithesis of Dagny. As the novel progresses, the moral philosophy of the looters is revealed: it is a code of stagnation. The goal of this code is to not exist, to not move forward, to become a zero. Taggart struggles to remain unaware that this is his goal. He maintains his pretense that he wants to live, and becomes horrified whenever his mind starts to grasp the truth about himself. This contradiction leads to the recurring absurdity of his life: the desire to destroy those on whom his life depends, and the horror that he will succeed at this. In the final chapters of the novel, he suffers a complete mental breakdown upon realizing that he can no longer deceive himself in this respect.
A former professor at Patrick Henry University, mentor to Francisco d'Anconia, John Galt and Ragnar Danneskjöld. He has since become a sell-out, one who had great promise but squandered it for social approval, to the detriment of the free. He works at the State Science Institute where all his inventions are perverted for use by the military, including the instrument of his demise: Project X. The character was, in part, modeled on J. Robert Oppenheimer, whom Rand had interviewed for an earlier project, and his part in the creation of nuclear weapons.
The following secondary characters also appear in the novel. Planned characters not in final version. In the introduction to the 35th anniversary edition, (1991), Leonard Peikoff introduced excerpts from Rand's journals concerning the book, which she originally intended to call "The Strike". Among many other things, the journals reveal some characters which were originally planned to appear in the book and were deleted from the final version. Peikoff says that "Father Amadeus was Taggart's priest, to whom he confessed his sins. The priest was supposed to be a positive character honestly devoted to the good but practicing consistently the morality of mercy. Miss Rand dropped him, she told me, when she found that it was impossible to make such a character convincing." The quotation from Rand's journals included a passage describing what John Galt represented to each main characters, including one about Father Amadeus: "For Father Amadeus [Galt represents] the source of the conflict. The uneasy realization that Galt is the end of his endeavors, the man of virtue, the perfect man - and that his means do not fit this end (and that he is destroying this, his ideal, for the sake of those who are evil)." Peikoff also mentions that as originally conceived the book was going to have a character named Stacy Rearden, a sister of Hank Rearden. Not much is told of what her role was supposed to be and why she was eventually dropped. Apparently, she was going to be another parasite like Rearden's brother, mother and wife; presumably, Rand came to the conclusion that three such characters around Rearden sufficiently fulfilled her literary and philosophical purposes.
Anthropology is the study of humanity. Anthropology has origins in the natural sciences, the humanities, and the social sciences. The term "anthropology", is from the Greek, "anthrōpos", "human", and -λογία, "-logia", "discourse" or "study", and was first used by François Péron when discussing his encounters with Tasmanian Aborigines. Anthropology's basic concerns are "What defines "Homo sapiens"?", "Who are the ancestors of modern "Homo sapiens"?", "What are humans' physical traits?", "How do humans behave?", "Why are there variations and differences among different groups of humans?", "How has the evolutionary past of "Homo sapiens" influenced its social organization and culture?" and so forth. In the United States, contemporary anthropology is typically divided into four sub-fields: cultural anthropology (also called "social anthropology"), archaeology, linguistic anthropology and biological/physical anthropology. The so-called "four-field" approach to anthropology is reflected in many undergraduate textbooks as well as anthropology programs (e.g. Michigan, Berkeley, UPenn, etc.). At universities in the United Kingdom, and much of Europe, these "sub-fields" are frequently housed in separate departments and are seen as distinct disciplines. The social and cultural sub-field has been heavily influenced by post-modern theories. During the 1970s and 1980s there was an epistemological shift away from the positivist traditions that had largely informed the discipline. During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in Cultural and Social Anthropology. In contrast, Archaeology, Biological Anthropology, and linguistic anthropology remained largely positivist. Due to this difference in epistemology, anthropology as a discipline has lacked cohesion over the last several decades. This has even led to departments diverging, for example in the 1998-9 academic year at Stanford University, where the "scientists" and "non-scientists" divided into two departments: Anthropology, and Cultural and Social Anthropology. (Anthropology at Stanford later reunified in the 2008-9 academic year)
Anthropology is traditionally divided into four sub-fields, each with its own further branches: biological or physical anthropology, social anthropology or cultural anthropology, archaeology and anthropological linguistics. These fields frequently overlap, but tend to use different methodologies and techniques. Biological anthropology or Physical anthropology, focuses on the study of human populations using an evolutionary framework. Biological anthropologists have theorized about how the globe has become populated with humans (e.g. the "Out of Africa" and "multi-regional evolution" debate), as well as tried to explain geographical human variation and race. Many biological anthropologists studying modern human populations identify their field as human ecology - itself linked to sociobiology. Human ecology uses evolutionary theory to understand phenomena among contemporary human populations. Another large sector of biological anthropology is primatology, where anthropologists focus on understanding other primate populations. Methodologically, primatologists borrow heavily from field biology and ecology in their research. Cultural anthropology is also called socio-cultural anthropology or social anthropology (especially in Great Britain). It is the study of culture, and is often based on ethnography. Ethnography can refer to both a methodology and a product of research, namely a monograph or book. Ethnography is a grounded, inductive method, that heavily relies on participant-observation. Ethnology involves the systematic comparison of different cultures. In some European countries, all cultural anthropology is known as ethnology (a term coined and defined by Adam F. Kollár in 1783). The study of kinship and social organization is a central focus of cultural anthropology, as kinship is a human universal. Cultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language (which is also the object of study in linguistic anthropology). Archaeology is the study of human material culture, including both artifacts (older pieces of human culture) carefully gathered "in situ", museum pieces and modern garbage. Archaeologists work closely with biological anthropologists, art historians, physics laboratories (for dating), and museums. They are charged with preserving the results of their excavations and are often found in museums. Typically, archaeologists are associated with "digs," or excavation of layers of ancient sites. Archaeologists subdivide time into cultural periods based on long-lasting artifacts: the Paleolithic, the Neolithic, the Bronze Age, which are further subdivided according to artifact traditions and culture region, such as the Oldowan or the Gravettian. In this way, archaeologists provide a vast frame of reference for the places human beings have traveled, their ways of making a living, and their demographics. Archaeologists also investigate nutrition, symbolization, art, systems of writing, and other physical remnants of human cultural activity. Linguistic anthropology (also called anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture. It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes. Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis. Linguistic anthropology is divided into its own sub-fields: descriptive linguistics the construction of grammars and lexicons for unstudied languages; historical linguistics, including the reconstruction of past languages, from which our current languages have descended; ethnolinguistics, the study of the relationship between language and culture, and sociolinguistics, the study of the social functions of language. Anthropological linguistics is also concerned with the evolution of the parts of the brain that deal with language. Because anthropology developed from so many different enterprises (see History of Anthropology), including but not limited to fossil-hunting, exploring, documentary film-making, paleontology, primatology, antiquity dealings and curatorship, philology, etymology, genetics, regional analysis, ethnology, history, philosophy and religious studies, it is difficult to characterize the entire field in a brief article, although attempts to write histories of the entire field have been made. On the one hand this has led to instability in many American anthropology departments, resulting in the division or reorganization of sub-fields (e.g. at Stanford, Duke, and most recently at Harvard). However, seen in a positive light, anthropology is one of the few place in many American universities where humanities, social, and natural sciences are forced to confront one another. As such, anthropology has also been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, human-computer interaction, and various ethnic studies.
There are several characteristics that tend to unite anthropological work. One of the central characteristics is that anthropology tends to provide a comparatively more holistic account of phenomena and tends to be highly empirical. The quest for holism leads most anthropologists to study a particular place or thing in detail, using a variety of methods, over a more extensive period than normal in many parts of academia. The specific focus of social and cultural anthropology has significantly changed. Initially the sub-field was focused on the study of cultures around the world. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological. Biological anthropologists are interested in both human variation and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures) They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists "into the field" which means traveling to a community in its own setting, to do something called "fieldwork." On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs. Due to the interest in variation, anthropologists are drawn to the study of human extremes, aberrations and other unusual circumstances, such as headhunting, whirling dervishes, whether there were real Hobbit people, snake handling, and glossolalia (speaking in tongues), just to list a few. At the same time, anthropologists urge, as part of their quest for scientific objectivity, cultural relativism, which has an influence on all the sub-fields of anthropology. This is the notion that particular cultures should not be judged by one culture's values or viewpoints, but that all cultures should be viewed as relative to each other. There should be no notions, in good anthropology, of one culture being better or worse than another culture. Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, mutilation including especially circumcision and subincision, and torture. Topics like racism, slavery or human sacrifice, therefore, attract anthropological attention and theories ranging from nutritional deficiencies to genes to acculturation have been proposed, not to mention theories of colonialism and many others as root causes of Man's inhumanity to man. To illustrate the depth of an anthropological approach, one can take just one of these topics, such as "racism" and find thousands of anthropological references, stretching across all the major and minor sub-fields. Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions. Human time on Earth is divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology. Further cultural subdivisions according to tool types, such as Olduwan or Mousterian or Levallois help archaeologists and other anthropologists in understanding major trends in the human past. Anthropologists and geographers share approaches to Culture regions as well, since mapping cultures is central to both sciences. By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science. Contemporary anthropology is an established science with academic departments at most universities and colleges. The single largest organization of Anthropologists is the American Anthropological Association, which was founded in 1903. Membership is made up of Anthropologists from around the globe. Hundreds of other organizations exist in the various sub-fields of anthropology, sometimes divided up by nation or region, and many anthropologists work with collaborators in other disciplines, such as geology, physics, zoology, paleontology, anatomy, music theory, art history, sociology and so on, belonging to professional societies in those disciplines as well.
The first use of the term "anthropology" in English to refer to a natural science of humankind was apparently in 1593, the first of the "logies" to be coined. It took Immanuel Kant 25 years to write one of the first major treatises on anthropology, his "Anthropology from a Pragmatic Point of View". Kant is not generally considered to be a modern anthropologist, however, as he never left his region of Germany nor did he study any cultures besides his own, and in fact, describes the need for anthropology as a corollary field to his own primary field of philosophy. He did, however, begin teaching an annual course in anthropology in 1772. Anthropology is thus primarily an Enlightenment and post-Enlightenment endeavor. Historians of anthropology, like Marvin Harris, indicate two major frameworks within which empirical anthropology has arisen: interest in comparisons of people over space and interest in longterm human processes or humans as viewed through time. Harris dates both to Classical Greece and Classical Rome, specifically Herodotus, often called the "father of history" and the Roman historian Tacitus, who wrote many of our only surviving contemporary accounts of several ancient Celtic and Germanic peoples. Herodotus first formulated some of the persisting problems of anthropology. Medieval scholars may be considered forerunners of modern anthropology as well, insofar as they conducted or wrote detailed studies of the customs of peoples considered "different" from themselves in terms of geography. John of Plano Carpini reported of his stay among the Mongols. His report was unusual in its detailed depiction of a non-European culture! Marco Polo's systematic observations of nature, anthropology, and geography are another example of studying human variation across space. Polo's travels took him across such a diverse human landscape and his accounts of the peoples he met as he journeyed were so detailed that they earned for Polo the name "the father of modern anthropology." Another candidate for one of the first scholars to carry out comparative ethnographic-type studies in person was the medieval Persian scholar Abū Rayhān Bīrūnī in the 11th century, who wrote about the peoples, customs, and religions of the Indian subcontinent. Like modern anthropologists, he engaged in extensive participant observation with a given group of people, learnt their language and studied their primary texts, and presented his findings with objectivity and neutrality using cross-cultural comparisons. He wrote detailed comparative studies on the religions and cultures in the Middle East, Mediterranean and especially South Asia. Biruni's tradition of comparative cross-cultural study continued in the Muslim world through to Ibn Khaldun's work in the 14th century. Most scholars consider modern anthropology as an outgrowth of the Age of Enlightenment, a period when Europeans attempted systematically to study human behavior, the known varieties of which had been increasing since the 15th century as a result of the first European colonization wave. The traditions of jurisprudence, history, philology, and sociology then evolved into something more closely resembling the modern views of these disciplines and informed the development of the social sciences, of which anthropology was a part. Developments in the systematic study of ancient civilizations through the disciplines of Classics and Egyptology informed both archaeology and eventually social anthropology, as did the study of East and South Asian languages and cultures. At the same time, the Romantic reaction to the Enlightenment produced thinkers, such as Johann Gottfried Herder and later Wilhelm Dilthey, whose work formed the basis for the "culture concept," which is central to the discipline. Institutionally, anthropology emerged from the development of natural history (expounded by authors such as Buffon) that occurred during the European colonization of the 17th, 18th, 19th and 20th centuries. Programs of ethnographic study originated in this era as the study of the "human primitives" overseen by colonial administrations. There was a tendency in late 18th century Enlightenment thought to understand human society as natural phenomena that behaved according to certain principles and that could be observed empirically. In some ways, studying the language, culture, physiology, and artifacts of European colonies was not unlike studying the flora and fauna of those places. Early anthropology was divided between proponents of unilinealism, who argued that all societies passed through a single evolutionary process, from the most primitive to the most advanced, and various forms of non-lineal theorists, who tended to subscribe to ideas such as diffusionism. Most 19th-century social theorists, including anthropologists, viewed non-European societies as windows onto the pre-industrial human past. As academic disciplines began to differentiate over the course of the 19th century, anthropology grew increasingly distinct from the biological approach of natural history, on the one hand, and from purely historical or literary fields such as Classics, on the other. A common criticism has been that many social science scholars (such as economists, sociologists, and psychologists) in Western countries focus disproportionately on Western subjects, while anthropology focuses disproportionately on the "Other"; this has changed over the last part of the 20th century as anthropologists increasingly also study Western subjects, particularly variation across class, region, or ethnicity within Western societies, and other social scientists increasingly take a global view of their fields.
In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The natural and biological "sciences" seek to derive general laws through reproducible and verifiable experiments. The "humanities" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. The "social sciences" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences. In particular, social sciences often develop statistical descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains. Anthropology as it emerged amongst the Western colonial powers (mentioned above) has generally taken a different path than that in the countries of southern and central Europe (Italy, Greece, and the successors to the Austro-Hungarian and Ottoman empires). In the former, the encounter with multiple, distinct cultures, often very different in organization and language from those of Europe, has led to a continuing emphasis on cross-cultural comparison and a receptiveness to certain kinds of cultural relativism. In the successor states of continental Europe, on the other hand, anthropologists often joined with folklorists and linguists in building nationalist perspectives. Ethnologists in these countries tended to focus on differentiating among local ethnolinguistic groups, documenting local folk culture, and representing the prehistory of what has become a nation through various forms of public education (eg, museums of several kinds). In this scheme, Russia occupied a middle position. On the one hand, it had a large region (largely east of the Urals) of highly distinct, pre-industrial, often non-literate peoples, similar to the situation in the Americas. On the other hand, Russia also participated to some degree in the nationalist (cultural and political) movements of Central and Eastern Europe. After the Revolution of 1917, anthropology in the USSR, and later the Soviet Bloc countries, were highly shaped by the requirement to conform to Marxist theories of social evolution.
E. B. Tylor (2 October 1832 – 2 January 1917) and James George Frazer (1 January 1854 – 7 May 1941) are generally considered the antecedents to modern social anthropology in Britain. Though Tylor undertook a field trip to Mexico, both he and Frazer derived most of the material for their comparative studies through extensive reading, not fieldwork, mainly the Classics (literature and history of Greece and Rome), the work of the early European folklorists, and reports from missionaries, travelers, and contemporaneous ethnologists. Tylor advocated strongly for unilinealism and a form of "uniformity of mankind". Tylor in particular laid the groundwork for theories of cultural diffusionism, stating that there are three ways that different groups can have similar cultural forms or technologies: "independent invention, inheritance from ancestors in a distant region, transmission from one race [sic] to another." Tylor formulated one of the early and influential anthropological conceptions of culture as "that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society." However, as Stocking notes, Tylor mainly concerned himself with describing and mapping the distribution of particular elements of culture, rather than with the larger function, and generally seemed to assume a Victorian idea of progress rather than the idea of non-directional, multilineal cultural development proposed by later anthropologists. Tylor also theorized about the origins of religious feelings in human beings, proposing a theory of animism as the earliest stage, and noting that "religion" has many components, of which he believed the most important to be belief in supernatural beings (as opposed to moral systems, cosmology, etc.). Frazer, a Scottish scholar with a broad knowledge of Classics, also concerned himself with religion, myth, and magic. His comparative studies, most influentially in the numerous editions of "The Golden Bough", analyzed similarities in religious belief and symbolism globally. Neither Tylor nor Frazer, however, were particularly interested in fieldwork, nor were they interested in examining how the cultural elements and institutions fit together. Toward the turn of the twentieth century, a number of anthropologists became dissatisfied with this categorization of cultural elements; historical reconstructions also came to seem increasingly speculative. Under the influence of several younger scholars, a new approach came to predominate among British anthropologists, concerned with analyzing how societies held together in the present (synchronic analysis, rather than diachronic or historical analysis), and emphasizing long-term (one to several years) immersion fieldwork. Cambridge University financed a multidisciplinary expedition to the Torres Strait Islands in 1898, organized by Alfred Court Haddon and including a physician-anthropologist, William Rivers, as well as a linguist, a botanist, other specialists. The findings of the expedition set new standards for ethnographic description. A decade and a half later, Polish anthropology student Bronisław Malinowski (1884–1942) was beginning what he expected to be a brief period of fieldwork in the old model, collecting lists of cultural items, when the outbreak of the First World War stranded him in New Guinea. As a subject of the Austro-Hungarian Empire resident on a British colonial possession, he was effectively confined to New Guinea for several years. He made use of the time by undertaking far more intensive fieldwork than had been done by "British" anthropologists, and his classic ethnography, "Argonauts of the Western Pacific" (1922) advocated an approach to fieldwork that became standard in the field: getting "the native's point of view" through participant observation. Theoretically, he advocated a functionalist interpretation, which examined how social institutions functioned to satisfy individual needs. British social anthropology had an expansive moment in the Interwar period, with key contributions coming from the Polish-British Bronisław Malinowski and Meyer Fortes A. R. Radcliffe-Brown also published a seminal work in 1922. He had carried out his initial fieldwork in the Andaman Islands in the old style of historical reconstruction. However, after reading the work of French sociologists Émile Durkheim and Marcel Mauss, Radcliffe-Brown published an account of his research (entitled simply "The Andaman Islanders") that paid close attention to the meaning and purpose of rituals and myths. Over time, he developed an approach known as structural-functionalism, which focused on how institutions in societies worked to balance out or create an equilibrium in the social system to keep it functioning harmoniously. (This contrasted with Malinowski's functionalism, and was quite different from the later French structuralism, which examined the conceptual structures in language and symbolism.) Malinowski and Radcliffe-Brown's influence stemmed from the fact that they, like Boas, actively trained students and aggressively built up institutions that furthered their programmatic ambitions. This was particularly the case with Radcliffe-Brown, who spread his agenda for "Social Anthropology" by teaching at universities across the British Commonwealth. From the late 1930s until the postwar period appeared a string of monographs and edited volumes that cemented the paradigm of British Social Anthropology (BSA). Famous ethnographies include "The Nuer," by Edward Evan Evans-Pritchard, and "The Dynamics of Clanship Among the Tallensi," by Meyer Fortes; well-known edited volumes include "African Systems of Kinship and Marriage" and "African Political Systems." Max Gluckman, together with many of his colleagues at the Rhodes-Livingstone Institute and students at Manchester University, collectively known as the Manchester School, took BSA in new directions through their introduction of explicitly Marxist-informed theory, their emphasis on conflicts and conflict resolution, and their attention to the ways in which individuals negotiate and make use of the social structural possibilities. In Britain, anthropology had a great intellectual impact, it "contributed to the erosion of Christianity, the growth of cultural relativism, an awareness of the survival of the primitive in modern life, and the replacement of diachronic modes of analysis with synchronic, all of which are central to modern culture." Later in the 1960s and 1970s, Edmund Leach and his students Mary Douglas and Nur Yalman, among others, introduced French structuralism in the style of Lévi-Strauss; while British anthropology has continued to emphasize social organization and economics over purely symbolic or literary topics, differences among British, French, and American sociocultural anthropologies have diminished with increasing dialogue and borrowing of both theory and methods. Today, social anthropology in Britain engages internationally with many other social theories and has branched in many directions. In countries of the British Commonwealth, social anthropology has often been institutionally separate from physical anthropology and primatology, which may be connected with departments of biology or zoology; and from archaeology, which may be connected with departments of Classics, Egyptology, and the like. In other countries (and in some, particularly smaller, British and North American universities), anthropologists have also found themselves institutionally linked with scholars of folklore, museum studies, human geography, sociology, social relations, ethnic studies, cultural studies, and social work.
From its beginnings in the early 19th century through the early 20th century, anthropology in the United States was influenced by the presence of Native American societies. Cultural anthropology in the United States was influenced greatly by the ready availability of Native American societies as ethnographic subjects. The field was pioneered by staff of the Bureau of Indian Affairs and the Smithsonian Institution's Bureau of American Ethnology, men such as John Wesley Powell and Frank Hamilton Cushing. Lewis Henry Morgan (1818–1881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from "savagery", to "barbarism", to "civilization". Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.
Franz Boas established academic anthropology in the United States in opposition to this sort of evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature. Influenced by the German tradition, Boas argued that the world was full of distinct "cultures," rather than societies whose evolution could be measured by how much or how little "civilization" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible. In doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas. Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called "Four Field Approach" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture. Boas used his positions at Columbia University and the American Museum of Natural History to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages. The publication of Alfred Kroeber's textbook, "Anthropology," marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up. Though such works as "Coming of Age in Samoa" and "The Chrysanthemum and the Sword" remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined by Ralph Linton, and Mead was limited to her offices at the AMNH.
Canadian anthropology began, as in other parts of the Colonial world, as ethnological data in the records of travellers and missionaries. In Canada, Jesuit missionaries such as Fathers LeClercq, Le Jeune and Sagard, in the 1600s, provide the oldest ethnographic records of native tribes in what was then the Domain of Canada. True anthropology began with a Government department: the Geological Survey of Canada, and George Mercer Dawson (director in 1895). Dawson's support for anthropology created impetus for the profession in Canada. This was expanded upon by Prime Minister Wilfrid Laurier, who established a Division of Anthropology within the Geological Survey in 1910. Anthropologists were recruited from England and the USA, setting the foundation for the unique Canadian style of anthropology. Scholars include the linguist and Boasian Edward Sapir.
Anthropology in France has a less clear genealogy than the British and American traditions, in part because many French writers influential in anthropology have been trained or held faculty positions in sociology, philosophy, or other fields rather than in anthropology. Most commentators consider Marcel Mauss (1872–1950), nephew of the influential sociologist Émile Durkheim to be the founder of the French anthropological tradition. Mauss belonged to Durkheim's Année Sociologique group; and while Durkheim and others examined the state of modern societies, Mauss and his collaborators (such as Henri Hubert and Robert Hertz) drew on ethnography and philology to analyze societies which were not as 'differentiated' as European nation states. Two works by Mauss in particular proved to have enduring relevance: "Essay on the Gift" a seminal analysis of exchange and reciprocity, and his Huxley lecture on the notion of the person, the first comparative study of notions of person and selfhood cross-culturally. Throughout the interwar years, French interest in anthropology often dovetailed with wider cultural movements such as surrealism and primitivism which drew on ethnography for inspiration. Marcel Griaule and Michel Leiris are examples of people who combined anthropology with the French avant-garde. During this time most of what is known as "ethnologie" was restricted to museums, such as the Musée de l'Homme founded by Paul Rivet, and anthropology had a close relationship with studies of folklore. Above all, however, it was Claude Lévi-Strauss who helped institutionalize anthropology in France. Along with the enormous influence his structuralism exerted across multiple disciplines, Lévi-Strauss established ties with American and British anthropologists. At the same time he established centers and laboratories within France to provide an institutional context within anthropology while training influential students such as Maurice Godelier and Françoise Héritier who would prove influential in the world of French anthropology. Much of the distinct character of France's anthropology today is a result of the fact that most anthropology is carried out in nationally funded research laboratories (CNRS) rather than academic departments in universities. Other influential writers in the 1970s include Pierre Clastres, who explains in his books on the Guayaki tribe in Paraguay that "primitive societies" actively oppose the institution of the state. Therefore, these stateless societies are not less evolved than societies with states, but took the active choice of conjuring the institution of authority as a separate function from society. The leader is only a spokesperson for the group when it has to deal with other groups ("international relations") but has no inside authority, and may be violently removed if he attempts to abuse this position. The most important French social theorist since Foucault and Lévi-Strauss is Pierre Bourdieu, who trained formally in philosophy and sociology and eventually held the Chair of Sociology at the Collège de France. Like Mauss and others before him, however, he worked on topics both in sociology and anthropology. His fieldwork among the Kabyles of Algeria places him solidly in anthropology, while his analysis of the function and reproduction of fashion and cultural capital in European societies places him as solidly in sociology.
Anthropology in Greece and Portugal is much influenced by British anthropology. In Greece, there was since the 19th century a science of the folklore called "laographia" (laography), in the form of "a science of the interior", although theoretically weak; but the connotation of the field deeply changed after World War II, when a wave of Anglo-American anthropologists introduced a science "of the outside". In Italy, the development of ethnology and related studies did not receive as much attention as other branches of learning. Germany and Norway are the countries that showed the most division and conflict between scholars focusing on domestic socio-cultural issues and scholars focusing on "other" societies.
Before WWII British 'social anthropology' and American 'cultural anthropology' were still distinct traditions. After the war, enough British and American anthropologists borrowed ideas and methodological approaches from one another that some began to speak of them collectively as 'sociocultural' anthropology. In the 1950s and mid-1960s anthropology tended increasingly to model itself after the natural sciences. Some anthropologists, such as Lloyd Fallers and Clifford Geertz, focused on processes of modernization by which newly independent states could develop. Others, such as Julian Steward and Leslie White, focused on how societies evolve and fit their ecological niche—an approach popularized by Marvin Harris. Economic anthropology as influenced by Karl Polanyi and practiced by Marshall Sahlins and George Dalton challenged standard neoclassical economics to take account of cultural and social factors, and employed Marxian analysis into anthropological study. In England, British Social Anthropology's paradigm began to fragment as Max Gluckman and Peter Worsley experimented with Marxism and authors such as Rodney Needham and Edmund Leach incorporated Lévi-Strauss's structuralism into their work. Structuralism also influenced a number of developments in 1960s and 1970s, including cognitive anthropology and componential analysis. Authors such as David Schneider, Clifford Geertz, and Marshall Sahlins developed a more fleshed-out concept of culture as a web of meaning or signification, which proved very popular within and beyond the discipline. In keeping with the times, much of anthropology became politicized through the Algerian War of Independence and opposition to the Vietnam War; Marxism became an increasingly popular theoretical approach in the discipline. By the 1970s the authors of volumes such as "Reinventing Anthropology" worried about anthropology's relevance. Since the 1980s issues of power, such as those examined in Eric Wolf's "Europe and the People Without History", have been central to the discipline. In the 80s books like "Anthropology and the Colonial Encounter" pondered anthropology's ties to colonial inequality, while the immense popularity of theorists such as Antonio Gramsci and Michel Foucault moved issues of power and hegemony into the spotlight. Gender and sexuality became popular topics, as did the relationship between history and anthropology, influenced by Marshall Sahlins (again), who drew on Lévi-Strauss and Fernand Braudel to examine the relationship between social structure and individual agency. Also influential in these issues were Nietzsche, Heidegger, the critical theory of the Frankfurt School, Derrida and Lacan. In the late 1980s and 1990s authors such as George Marcus and James Clifford pondered ethnographic authority, particularly how and why anthropological knowledge was possible and authoritative. They were reflecting trends in research and discourse initiated by Feminists in the academy, although they excused themselves from commenting specifically on those pioneering critics. Nevertheless, key aspects of feminist theorizing and methods became "de rigueur" as part of the 'post-modern moment' in anthropology: Ethnographies became more reflexive, explicitly addressing the author's methodology, cultural, gender and racial positioning, and their influence on his or her ethnographic analysis. This was part of a more general trend of postmodernism that was popular contemporaneously. Currently anthropologists pay attention to a wide variety of issues pertaining to the contemporary world, including globalization, medicine and biotechnology, indigenous rights, virtual communities, and the anthropology of industrialized societies.
Anthropologists' involvement with the U.S. government, in particular, has caused bitter controversy within the discipline. Franz Boas publicly objected to US participation in World War I, and after the war he published a brief expose and condemnation of the participation of several American archaeologists in espionage in Mexico under their cover as scientists. But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the "Axis" (Nazi Germany, Fascist Italy, and Imperial Japan). Many served in the armed forces but others worked in intelligence (for example, Office of Strategic Services (OSS) and the Office of War Information). At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies. Attempts to accuse anthropologists of complicity with the CIA and government intelligence activities during the Vietnam War years have turned up surprisingly little (although anthropologist Hugo Nutini was active in the stillborn Project Camelot). Many anthropologists (students and teachers) were active in the antiwar movement and a great many resolutions condemning the war in all its aspects were passed overwhelmingly at the annual meetings of the American Anthropological Association (AAA). In the decades since the Vietnam war the tone of cultural and social anthropology, at least, has been increasingly politicized, with the dominant liberal tone of earlier generations replaced with one more radical, a mix of, and varying degrees of, Marxist, feminist, anarchist, post-colonial, post-modern, Saidian, Foucauldian, identity-based, and more. Professional anthropological bodies often object to the use of anthropology for the benefit of the state. Their codes of ethics or statements may proscribe anthropologists from giving secret briefings. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarships ethically dangerous. The AAA's current 'Statement of Professional Responsibility' clearly states that "in relation with their own government and with host governments... no secret research, no secret reports or debriefings of any kind should be agreed to or given." However, anthropologists, along with other social scientists, are again being used in warfare as part of the. The Christian Science Monitor reports that "Counterinsurgency efforts focus on better grasping and meeting local needs" in Afghanistan, under the rubric of "Human Terrain Team" (HTT).
Some authors argue that anthropology originated and developed as the study of "other cultures", both in terms of time (past societies) and space (non-European/non-Western societies). For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal "Exploring the City: Inquiries Toward an Urban Anthropology" mentions that the "Third World" had habitually received most of attention; anthropologists who traditionally specialized in "other cultures" looked for them far away and started to look "across the tracks" only in late 1960s. Now there exist many works focusing on peoples and topics very close to the author's "home". It is also argued that other fields of study, like History and Sociology, on the contrary focus disproportionately on the West. In France, the study of existing contemporary society has been traditionally left to sociologists, but this is increasingly changing, starting in the 1970s from scholars like Isac Chiva and journals like "Terrain" ("fieldwork"), and developing with the center founded by Marc Augé ("Le Centre d'anthropologie des mondes contemporains", the Anthropological Research Center of Contemporary Societies). The same approach of focusing on "modern world" topics by "Terrain", was also present in the British Manchester School of the 1950s.
Agricultural science is a broad multidisciplinary field that encompasses the parts of exact, natural, economic and social sciences that are used in the practice and understanding of agriculture. (Veterinary science, but not animal science, is often excluded from the definition.) Agricultural science: a local science. With the exception of theoretical agronomy, research in agronomy, more than in any other field, is strongly related to local areas. It can be considered a science of ecoregions, because it is closely linked to soil properties and climate, which are never exactly the same from one place to another. Many people think an agricultural production system relying on local weather, soil characteristics, and specific crops has to be studied locally. Others feel a need to know and understand production systems in as many areas as possible, and the human dimension of interaction with nature.
Agricultural science began with Gregor Mendel's genetic work, but in modern terms might be better dated from the chemical fertilizer outputs of plant physiological understanding in eighteenth century Germany. In the United States, a scientific revolution in agriculture began with the Hatch Act of 1887, which used the term "agricultural science". The Hatch Act was driven by farmers' interest in knowing the constituents of early artificial fertilizer. The Smith-Hughes Act of 1917 shifted agricultural education back to its vocational roots, but the scientific foundation had been built. After 1906, public expenditures on agricultural research in the US exceeded private expenditures for the next 44 years. Intensification of agriculture since the 1960s in developed and developing countries, often referred to as the Green Revolution, was closely tied to progress made in selecting and improving crops and animals for high productivity, as well as to developing additional inputs such as artificial fertilizers and phytosanitary products. As the oldest and largest human intervention in nature, the environmental impact of agriculture in general and more recently intensive agriculture, industrial development, and population growth have raised many questions among agricultural scientists and have led to the development and emergence of new fields. These include technological fields that assume the solution to technological problems lies in better technology, such as integrated pest management, waste treatment technologies, landscape architecture, genomics, and agricultural philosophy fields that include references to food production as something essentially different from non-essential economic 'goods'. In fact, the interaction between these two approaches provide a fertile field for deeper understanding in agricultural science. New technologies, such as biotechnology and computer science (for data processing and storage), and technological advances have made it possible to develop new research fields, including genetic engineering, agrophysics, improved statistical analysis, and precision farming. Balancing these, as above, are the natural and human sciences of agricultural science that seek to understand the human-nature interactions of traditional agriculture, including interaction of religion and agriculture, and the non-material components of agricultural production systems. Agricultural science and agriculture crisis. Agriculture sciences seek to feed the world's population while preventing biosafety problems that may affect human health and the environment. This requires promoting good management of natural resources and respect for the environment, and increasingly concern for the psychological wellbeing of all concerned in the food production and consumption system. Economic, environmental, and social aspects of agriculture sciences are subjects of ongoing debate. Recent crises (such as avian influenza, mad cow disease and issues such as the use of genetically modified organisms) illustrate the complexity and importance of this debate.
Alchemy, originally derived from the Ancient Greek word "khemia" (Χημία) meaning "art of transmuting metals", later arabicized as "al-kimia" (الكيمياء), is both a philosophy and an ancient practice focused on the attempt to change base metals into gold, investigating the preparation of the "elixir of longevity", and achieving ultimate wisdom, involving the improvement of the alchemist as well as the making of several substances described as possessing unusual properties. The practical aspect of alchemy generated the basics of modern inorganic chemistry, namely concerning procedures, equipment and the identification and use of many current substances. Alchemy has been practiced in Mesopotamia (comprising much of today's Iraq), Egypt, Persia (today's Iran), India, China, Japan, Korea and in Classical Greece and Rome, in the Post-Islamic Persia, and then in Europe up to the 20th century, in a complex network of schools and philosophical systems spanning at least 2500 years.
The word alchemy derives in turn from the Old French "alkemie"; from the Medieval Latin "alchimia"; from the Arabic "al-kimia" (الكيمياء); and ultimately from the Ancient Greek "khemia" (Χημία) meaning "art of transmuting metals". During the seventeenth century chemistry as a separate science was derived from Alchemy, with the work of Robert Boyle, sometimes known as "The father of Chemistry", who in his book "The Skeptical Chymist" attacked Paracelsus and the old Aristotelian concepts of the elements and laid down the foundations of modern chemistry. Alchemy as a philosophical and spiritual discipline. Alchemy became known as the "spagyric art" after Greek words meaning "to separate" and "to join together" in the 16th century, the word probably being coined by Paracelsus. Compare this with one of the dictums of Alchemy in Latin: Solve et Coagula — "Separate, and Join Together" (or "dissolve and coagulate"). The best-known goals of the alchemists were the transmutation of common metals into gold (called chrysopoeia) or silver (less well known is plant alchemy, or "spagyric"); the creation of a "panacea", or the elixir of life, a remedy that, it was supposed, would cure all diseases and prolong life indefinitely; and the discovery of a universal solvent. Although these were not the only uses for the discipline, they were the ones most documented and well-known. Certain Hermetic schools argue that the transmutation of lead into gold is analogical for the transmutation of the physical body (Saturn or lead) into (Gold) with the goal of attaining immortality. This is described as Internal Alchemy. Starting with the Middle Ages, Persian and European alchemists invested much effort in the search for the "philosopher's stone", a legendary substance that was believed to be an essential ingredient for either or both of those goals. Pope John XXII issued a bull against alchemical counterfeiting, and the Cistercians banned the practice amongst their members. In 1403, Henry IV of England banned the practice of Alchemy. In the late 14th century, Piers the Ploughman and Chaucer both painted unflattering pictures of Alchemists as thieves and liars. By contrast, Rudolf II, Holy Roman Emperor, in the late 16th century, sponsored various alchemists in their work at his court in Prague. It is a popular belief that Alchemists made mundane contributions to the "chemical" industries of the day—ore testing and refining, metalworking, production of gunpowder, ink, dyes, paints, cosmetics, leather tanning, ceramics, glass manufacture, preparation of extracts, liquors, and so on (it seems that the preparation of "aqua vitae", the "water of life", was a fairly popular "experiment" among European alchemists). In reality, although Alchemists contributed distillation to Western Europe, they did little for any known industry. Long before Alchemists appeared, goldsmiths knew how to tell what was good gold or fake, and industrial technology grew by the work of the artisans themselves, rather than any Alchemical helpers. The double origin of Alchemy in Greek philosophy as well as in Egyptian and Mesopotamian technology set, from the start, a double approach: the technological, operative one, which Marie-Louise von Franz call extravert, and the mystic, contemplative, psychological one, which von Franz names as introvert. These are not mutually exclusive, but complementary instead, as meditation requires practice in the real world, and conversely. Several early alchemists, such as Zosimos of Panopolis, are recorded as viewing alchemy as a spiritual discipline, and, in the Middle Ages, metaphysical aspects, substances, physical states, and molecular material processes as mere metaphors for spiritual entities, spiritual states, and, ultimately, transformations. In this sense, the literal meanings of 'Alchemical Formulas' were a blind, hiding their true spiritual philosophy, which being at odds with the Medieval Christian Church was a necessity that could have otherwise led them to the "stake and rack" of the Inquisition under charges of heresy. Thus, both the transmutation of common metals into gold and the universal panacea symbolized evolution from an imperfect, diseased, corruptible, and ephemeral state towards a perfect, healthy, incorruptible, and everlasting state; and the philosopher's stone then represented a mystic key that would make this evolution possible. Applied to the alchemist himself, the twin goal symbolized his evolution from ignorance to enlightenment, and the stone represented a hidden spiritual truth or power that would lead to that goal. In texts that are written according to this view, the cryptic alchemical symbols, diagrams, and textual imagery of late alchemical works typically contain multiple layers of meanings, allegories, and references to other equally cryptic works; and must be laboriously "decoded" in order to discover their true meaning. Q. When the Philosophers speak of gold and silver, from which they extract their matter, are we to suppose that they refer to the vulgar gold and silver? A. By no means; vulgar silver and gold are dead, while those of the Philosophers are full of life.
Alchemical symbolism has been occasionally used by psychologists and philosophers. Carl Jung reexamined alchemical symbolism and theory and began to show the inner meaning of alchemical work as a spiritual path. Alchemical philosophy, symbols and methods have enjoyed something of a renaissance in post-modern contexts. Jung saw alchemy as a Western proto-psychology dedicated to the achievement of individuation. In his interpretation, alchemy was the vessel by which Gnosticism survived its various purges into the Renaissance, a concept also followed by others such as Stephan A. Hoeller. In this sense, Jung viewed alchemy as comparable to a Yoga of the East, and more adequate to the Western mind than Eastern religions and philosophies. The practice of Alchemy seemed to change the mind and spirit of the Alchemist. Conversely, spontaneous changes on the mind of Western people undergoing any important stage in individuation seems to produce, on occasion, imagery known to Alchemy and relevant to the person's situation. His interpretation of Chinese alchemical texts in terms of his analytical psychology also served the function of comparing Eastern and Western alchemical imagery and core concepts and hence its possible inner sources (archetypes). Marie-Louise von Franz, a disciple of Jung, continued Jung's studies on Alchemy and its psychological meaning.
After the 15th century, many writers tended to compress "citrinitas" into "rubedo" and consider only three stages. However, it is in citrinitas that the Chemical Wedding takes place, generating the Philosophical Mercury without which the Philosopher's Stone, triumph of the Work, could never be accomplished. Within the Magnum Opus was the creation of the Sanctum Moleculae, that is the 'Sacred Masses' that were derived from the Sacrum Particulae, that is the 'Sacred Particles', needed to complete the process of achieving the Magnum Opus. Alchemy as a subject of historical research. The history of alchemy has become a vigorous academic field. As the obscure hermetic language of the alchemists is gradually being "deciphered", historians are becoming more aware of the intellectual connections between that discipline and other facets of Western cultural history, such as the sociology and psychology of the intellectual communities, kabbalism, spiritualism, Rosicrucianism, and other mystic movements, cryptography, witchcraft, and the evolution of science and philosophy.
In a historical sense, Alchemy is the pursuit of transforming common metals into valuable gold. According to Marie-Louise von Franz, the initial basis for alchemy were Egyptian metal technology and mummification, Mesopotamian technology and astrology, and Pre-Socratic Greek philosophers such as Empedocles, Thales of Miletus and Heraclitus. The origins of Western alchemy are traceable back to ancient Egypt. The Leyden papyrus X and the Stockholm papyrus along with the Greek magical papyri comprise the first "book" on alchemy still existent. Babylonian, Greek and Indian philosophers theorized that there were only four classical elements (rather than today's 117 chemical elements, a useful analogy is with the highly similar states of matter); Earth, Fire, Water, and Air. The Greek philosophers, in order to prove their point, burned a log: The log was the earth, the flames burning it was fire, the smoke being released was air, and the smoldering soot at the bottom was bubbling water. Because of this, the belief that these four "elements" were at the heart of everything soon spread, only later being replaced in the Middle Ages by Geber's theory of seven elements, which was then replaced by the modern theory of chemical elements during the early modern period. Alchemy encompasses several philosophical traditions spanning four millennia and three continents. These traditions' general penchant for cryptic and symbolic language makes it hard to trace their mutual influences and "genetic" relationships. Alchemy starts becoming much clearer in the 8th century with the works of the Islamic alchemist, Jabir ibn Hayyan (known as "Geber" in Europe), who introduced a methodical and experimental approach to scientific research based in the laboratory, in contrast to the ancient Greek and Egyptian alchemists whose works were mainly allegorical. Other famous alchemists include Rhazes, Avicenna and Imad ul-din in Persia; Wei Boyang in Chinese alchemy; and Nagarjuna in Indian alchemy; and Albertus Magnus and Pseudo-Geber in European alchemy; as well as the anonymous author of the "Mutus Liber", published in France in the late 17th century, which was a 'wordless book' that claimed to be a guide to making the philosopher's stone, using a series of 15 symbols and illustrations. The philosopher's stone was an object that was thought to be able to amplify one's power in alchemy and, if possible, grant the user ageless immortality, unless he fell victim to burnings or drowning; the common belief was that fire and water were the two greater elements that were implemented into the creation of the stone. In the case of the Chinese and European alchemists, there was a difference between the two. The European alchemists tried to transmute lead into gold, and, no matter how futile or toxic the element, would continue trying until it was royally outlawed later into the century. The Chinese, however, paid no heed to the philosopher's stone or transmutation of lead to gold; they focused more on medicine for the greater good. During Enlightenment, these "elixirs" were a strong cure for sicknesses, unless it was a test medicine. In general, most tests were fatal, but stabilized elixirs served great purposes. On the other hand, the Islamic alchemists were interested in alchemy for a variety of reasons, whether it was for the transmutation of metals or artificial creation of life, or for practical uses such as medicine.
Persian alchemy was a forerunner of modern scientific chemistry. Alchemists used many of the same laboratory tools that are used today. These tools were not usually sturdy or in good condition, especially during the medieval period of Europe. Many transmutation attempts failed when alchemists unwittingly made unstable chemicals. This was made worse by the unsafe conditions in which the alchemists worked. Up to the 16th century, alchemy was considered serious science in Europe; for instance, Isaac Newton devoted considerably more of his writing to the study of alchemy (see Isaac Newton's occult studies) than he did to either optics or physics, for which he is famous. Other eminent alchemists of the Western world are Roger Bacon, Saint Thomas Aquinas, Tycho Brahe, Thomas Browne, and Parmigianino. The decline of alchemy began in the 18th century with the birth of modern chemistry, which provided a more precise and reliable framework for matter transmutations and medicine, within a new grand design of the universe based on rational materialism.
Traditional medicines involve transmutation by alchemy, using pharmacological or a combination of pharmacological and spiritual techniques. In Chinese medicine the alchemical traditions of pao zhi will transform the nature of the temperature, taste, body part accessed or toxicity. In Ayurveda the samskaras are used to transform heavy metals and toxic herbs in a way that removes their toxicity. These processes are actively used to the present day.
A play by Ben Jonson, The Alchemist, is a satirical and skeptical take on the subject. Part 2 of Goethe's Faust, is full of alchemical symbolism. According to "Hermetic Fictions: Alchemy and Irony in the Novel" (Keele University Press, 1995), by David Meakin, alchemy is also featured in such novels and poems as those by William Godwin, Percy Bysshe Shelley, Emile Zola, Jules Verne, Marcel Proust, Thomas Mann, Hermann Hesse, James Joyce, Gustav Meyrink, Lindsay Clarke, Marguerite Yourcenar, Umberto Eco, Michel Butor, Paulo Coelho, Amanda Quick, Gabriel García Marquez and Maria Szepes. Hilary Mantel, in her novel Fludd (1989, Penguin), mentions the spagyric art. 'After separation, drying out, moistening, dissolving, coagulating, fermenting, comes purification, recombination: the creation of substances the world until now has never beheld. This is the opus contra naturem, this is the spagyric art, this is the Alchymical Wedding'. (page 79) In Dante's Inferno, it is placed within the Tenth ring of the 8th circle. In Angie Sage's Septimus Heap series, Marcellus Pye is an important Alchemist that first appears in Physik, the third book. In The Secrets of the Immortal Nicholas Flamel series, one of the main characters is a alchemist. The manga and anime series Fullmetal Alchemist bases itself of a more fantasised version of alchemy.
In the twentieth century alchemy was a profoundly important source of inspiration for the Surrealist artist Max Ernst, who used the symbolism of alchemy to inform and guide his work. M.E. Warlick wrote his "Max Ernst and Alchemy" describing this relationship in detail. Contemporary artists use alchemy as inspiring subject matter, like Odd Nerdrum, whose interest has been noted by Richard Vine, and the painter Michael Pearce, whose interest in alchemy dominates his work. His works "Fama" and "The Aviator's Dream" particularly express alchemical ideas in a painted allegory.
Austria (), officially the Republic of Austria (German:; Austro-Bavarian: Repubblik Östareich), is a landlocked country of roughly 8.3 million people in Central Europe. It borders Germany and the Czech Republic to the north, Slovakia and Hungary to the east, Slovenia and Italy to the south, and Switzerland and Liechtenstein to the west. The territory of Austria covers, and has a temperate and alpine climate. Austria's terrain is highly mountainous due to the presence of the Alps; only 32% of the country is below, and its highest point is. The majority of the population speaks German, which is also the country's official language. Other local official languages are Croatian, Hungarian and Slovene. The origins of Austria date back to the time of the Roman Empire when a Celtic kingdom was conquered by the Romans in approximately 15 BC, and later became Noricum, a Roman province, in the mid 1st century AD—an area which mostly encloses today's Austria. In 788 AD, the Frankish king Charlemagne conquered the area, and introduced Christianity. Under the native Habsburg dynasty, Austria became one of the great powers of Europe. In 1867, the Austrian Empire was reformed into Austria-Hungary. The Austro-Hungarian Empire collapsed in 1918 with the end of World War I. After establishing the First Austrian Republic in 1919 Austria was de facto annexed into Greater Germany by the Nazi regime in the so-called Anschluss in 1938. This lasted until the end of World War II in 1945, after which Austria was occupied by the Allies. In 1955, the Austrian State Treaty re-established Austria as a sovereign state, ending the occupation. In the same year, the Austrian Parliament created the Declaration of Neutrality which declared that the country would become permanently neutral. Today, Austria is a parliamentary representative democracy comprising nine federal states. The capital—and with a population exceeding 1.6 million, Austria's largest city—is Vienna. Austria is one of the richest countries in the world, with a nominal per capita GDP of $43,570. The country has developed a high standard of living, and in 2008 was ranked 14th in the world for its Human Development Index. Austria has been a member of the United Nations since 1955, joined the European Union in 1995, and is a founder of the OECD. Austria also signed the Schengen Agreement in 1995, and adopted the European currency, the euro, in 1999.
The German name of Austria, derives from the Old High German word Ostarrîchi "eastern realm", first attested in the famous "Ostarrîchi document" of AD 996, where the term refers to the Margraviate ruled by the Babenberg Count Henry I located mostly in what is today Lower Austria and part of Upper Austria. The name Austria is a latinisation of the same Germanic word for "east", *austrō also found in "Austrasia", the eastern part of Merovingian Francia. German "Österreich" is readily analysable as connected to "östlich" "eastern" and "Reich" "realm, dominion, empire". The term probably originates in a vernacular translation of the Medieval Latin name for the region:, which translates as "eastern marches" or "eastern borderland", as it was situated at the eastern edge of the Holy Roman Empire.== However, Friedrich Heer, one of the most important Austrian historians in the 20th century, stated in his book "Der Kampf um die österreichische Identität" ("The Struggle Over Austrian Identity"), that the Germanic form "Ostarrîchi" was not a translation of the Latin word, but both resulted from a much older term originating in the Celtic languages of ancient Austria: More than 2,500 years ago, the major part of the actual country was called "Norig" by the Celtic population (Hallstatt culture); "No-" or "Nor-" meant "east" or "eastern", whereas "-rig" is related to the modern German "Reich"; meaning "realm". Accordingly, "Norig" would essentially mean "Ostarrîchi" and "Österreich", thus "Austria". The Celtic name was eventually Latinised to "Noricum" after the Romans conquered the area that encloses most of modern day Austria, in approximately 15 BC. "Noricum" later became a Roman province in the mid 1st century AD.
Settled in ancient times, the Central European land that is now Austria was occupied in pre-Roman times by various Celtic tribes. The Celtic kingdom of Noricum was later claimed by the Roman Empire and made a province. Present day Petronell-Carnuntum in Eastern Austria was an important army camp turned capital city in what became known as the Upper Pannonia province. Fifty thousand people called Carnuntum home for nearly 400 years. After the fall of the Roman Empire the area was invaded by Bavarians, Slavs and Avars. The Slavic tribe of the Carantanians migrated into the Alps, and established the realm of Carantania, which covered much of eastern and central Austrian territory. Charlemagne conquered the area in 788 AD, encouraged colonisation and introduced Christianity. As part of Eastern Francia, the core areas that now encompass Austria were bequeathed to the house of Babenberg. The area was known as the "marchia Orientalis" and was given to Leopold of Babenberg in 976. The first record showing the name Austria is from 996 where it is written as "Ostarrîchi", referring to the territory of the Babenberg March. In 1156 the Privilegium Minus elevated Austria to the status of a duchy. In 1192, the Babenbergs also acquired the Duchy of Styria. With the death of Frederick II in 1246, the line of the Babenbergs went extinct. As a result Otakar II of Bohemia effectively assumed control of the duchies of Austria, Styria and Carinthia. His reign came to an end with his defeat at Dürnkrut at the hands of Rudolf I of Germany in 1278. Thereafter, until World War I, Austria's history was largely that of its ruling dynasty, the Habsburgs. In the 14th and 15th centuries, the Habsburgs began to accumulate other provinces in the vicinity of the Duchy of Austria. In 1438 Duke Albert V of Austria was chosen as the successor to his father-in-law, Emperor Sigismund. Although Albert himself only reigned for a year, every emperor of the Holy Roman Empire was a Habsburg, with only one exception. The Habsburgs began also to accumulate lands far from the hereditary lands. In 1477 Archduke Maximilian, only son of Emperor Frederick III, married the heiress Maria of Burgundy, thus acquiring most of the Netherlands for the family. His son Philip the Fair married the heiress of Castile and Aragon, and thus acquired Spain and its Italian, African and New World appendages for the Habsburgs. In 1526 following the Battle of Mohács, Bohemia and the part of Hungary not occupied by the Ottomans came under Austrian rule. Ottoman expansion into Hungary led to frequent conflicts between the two empires, particularly evident in the so-called Long War of 1593 to 1606. During the long reign of Leopold I (1657–1705) and following the successful defense of Vienna in 1683 (under the command of the King of Poland, John III Sobieski), a series of campaigns resulted in bringing all of Hungary to Austrian control by the Treaty of Carlowitz in 1699. Emperor Charles VI relinquished many of the fairly impressive gains the empire made in the previous years, largely due to his apprehensions at the imminent extinction of the House of Habsburg. Charles was willing to offer concrete advantages in territory and authority in exchange for other powers' worthless recognitions of the Pragmatic Sanction that made his daughter Maria Theresa his heir. With the rise of Prussia the Austrian–Prussian dualism began in Germany. Austria participated, together with Prussia and Russia, in the first and the third of the three Partitions of Poland (in 1772 and 1795). Austria later became engaged in a war with Revolutionary France, at the beginning highly unsuccessful, with successive defeats at the hands of Napoleon meaning the end of the old Holy Roman Empire in 1806. Two years earlier, in 1804, the Empire of Austria was founded. In 1814 Austria was part of the Allied forces that invaded France and brought to an end the Napoleonic wars. It thus emerged from the Congress of Vienna in 1815 as one of four of the continent's dominant powers and a recognised great power. The same year, the German Confederation, () was founded under the presidency of Austria. Because of unsolved social, political and national conflicts the German lands were shaken by the 1848 revolution aiming to create a unified Germany. A unified Germany would have been possible either as a Greater Germany, or a Greater Austria or just the German Confederation without Austria at all. As Austria was not willing to relinquish its German-speaking territories to what would become the German Empire of 1848, the crown of the newly-formed empire was offered to the Prussian King Friedrich Wilhelm IV. In 1864 Austria and Prussia fought together against Denmark, and successfully freed the independent duchies of Schleswig and Holstein. Nevertheless as they could not agree on a solution to the administration of the two duchies, they fought in 1866 the Austro-Prussian War. Defeated by Prussia in the Battle of Königgrätz, Austria had to leave the German Confederation and subsequently no longer took part in German politics. The Austro-Hungarian Compromise of 1867, the "Ausgleich", provided for a dual sovereignty, the Austrian Empire and the Kingdom of Hungary, under Franz Joseph I. The Austrian-Hungarian rule of this diverse empire included various Slavic groups including Croats, Czechs, Poles, Rusyns, Serbs, Slovaks, Slovenes and Ukrainians, as well as large Italian and Romanian communities. As a result, ruling Austria–Hungary became increasingly difficult in an age of emerging nationalist movements. Yet the government of Austria tried its best to be accommodating in some respects: The "Reichsgesetzblatt", publishing the laws and ordinances of Cisleithania, was issued in eight languages, all national groups were entitled to schools in their own language and to the use of their mothertongue at state offices, for example. The government of Hungary to the contrary tried to magyarise other ethnic entities. Thus the wishes of ethnic groups dwelling in both parts of the dual monarchy hardly could be solved. The assassination of Archduke Franz Ferdinand in Sarajevo in 1914 by Gavrilo Princip (a member of the Serbian nationalist group the Black Hand) was used by leading Austrian and Hungarian politicians and generals to persuade the emperor to declare war on Serbia, thereby risking and prompting the outbreak of World War I which led to the dissolution of the Austro-Hungarian Empire. Over one million Austro-Hungarian soldiers died in World War I. On October 21, 1918, the elected German members of the "Reichsrat" (parliament of Imperial Austria) met in Vienna as the Provisional National Assembly for German Austria ("Provisorische Nationalversammlung für Deutschösterreich"). On October 30 the assembly founded the State of German Austria by appointing a government, called "Staatsrat". This new government was invited by the emperor to take part in the decision on the planned armistice with Italy, but refrained from this business; this left the responsibility for the end of the war on November 3, 1918, solely to the emperor and his government. On November 11 the emperor, counseled by ministers of the old and the new government, declared he would not take part in state business any more; on November 12 German Austria, by law, declared itself to be a democratic republic and part of the new German republic. The constitution, renaming "Staatsrat" to "Bundesregierung" (federal government) and "Nationalversammlung" to "Nationalrat" (national council) was passed on November 10, 1920. The Treaty of Saint-Germain of 1919 (for Hungary the Treaty of Trianon of 1920) confirmed and consolidated the new order of Central Europe which to a great part had been established in November 1918, creating new states and resizing others. Over 3-million German Austrians found themselves living outside of the newborn Austrian Republic in the respective states of Czechoslovakia, Yugoslavia, Hungary and Italy. Between 1918 and 1919 Austria was officially known as the State of German Austria (). Not only did the Entente powers forbid German Austria to unite with Germany, they also ignored the name German Austria in the peace treaty to be signed; it was therefore changed to Republic of Austria in late 1919. After the war inflation began to devaluate the "Krone", still Austria's currency. In the autumn of 1922 Austria was granted an international loan supervised by the League of Nations. The purpose of the loan was to avert bankruptcy, stabilise the currency and improve its general economic condition. With the granting of the loan, Austria passed from an independent state to the control exercised by the League of Nations. In 1925 the "Schilling", replacing the "Krone" by 10,000:1, was introduced. Later it was called the Alpine dollar due to its stability. From 1925 to 1929 the economy enjoyed a short high before nearly crashing after Black Friday. The First Austrian Republic lasted until 1933 when Chancellor Engelbert Dollfuss, gladly using what he called "self-switch-off of Parliament" (), established an autocratic regime tending toward Italian fascism. The two big parties at this time, the Social Democrats and the Conservatives, had paramilitary armies; the Social Democrats' "Schutzbund" was now declared illegal but still operative as civil war broke out. In February 1934 several members of the "Schutzbund" were executed, the Social Democratic party was outlawed and many of its members were imprisoned or emigrated. On 1 May 1934, the Austrofascists imposed a new constitution ("Maiverfassung") which cemented Dollfuss's power but on 25 July he was assassinated in a Nazi coup attempt. His successor, Kurt Schuschnigg, struggled to keep Austria independent as "the better German state", but on 12 March 1938, German troops occupied the country while Austrian Nazis took over government. On 13 March 1938, the "Anschluss" of Austria was officially declared. Two days later Hitler, a native of Austria, proclaimed the re-unification of his home country with the rest of Germany on Vienna's Heldenplatz. He established a plebiscite confirming union with Germany in April 1938. Austria was incorporated into the Third Reich and ceased to exist as an independent state. The Aryanisation of the wealth of Jewish Austrians started immediately mid-March with a so called "wild" (i.e. extra-legal) phase but soon was structured legally and bureaucratically to strip Jewish citizens of any asset they may have possessed. The Nazis called Austria "Ostmark" until 1942 when it was again renamed and called "Alpen-Donau-Reichsgaue". Vienna fell on 13 April 1945, during the Soviet Vienna Offensive just before the total collapse of the Third Reich. Karl Renner and Adolf Schärf (Socialist Party of Austria [Social Democrats and Revolutionary Socialists]), Leopold Kunschak (Austria's People's Party [former Christian Social People's Party]) and Johann Koplenig (Communist Party of Austria) declared Austria's secession from the Third Reich by the Declaration of Independence on 27 April 1945, and set up a provisional government in Vienna under state Chancellor Renner the same day, with the approval of the victorious Red Army and backed by Stalin. (The date is officially named the birthday of the second republic.) At the end of April, most of Western and Southern Austria still was under Nazi rule. On May 1, 1945, the federal constitution of 1929 was put into validity again, which had been terminated by dictator Dollfuss on May 1, 1934. Total military deaths from 1939–1945 are estimated at 260,000. Jewish Holocaust victims totaled 65,000. About 140,000 Jewish Austrians had fled the country in 1938–39. Thousands of Austrians had taken part in serious Nazi crimes, a fact officially recognised by Chancellor Franz Vranitzky in 1992. Much like Germany, Austria was divided into a British, a French, a Soviet and a U.S. zone and governed by the Allied Commission for Austria. As forecast in the Moscow Declaration in 1943, there was a subtle difference in the treatment of Austria by the Allies. The Austrian Government, consisting of Social Democrats, Conservatives and Communists (until 1947) and residing in Vienna, which was surrounded by the Soviet zone, was recognised by the Western Allies in October 1945 after some doubts that Renner could be Stalin's puppet. Thereby the creation of a separate Western Austrian government and the division of the country could be avoided. Austria, in general, was treated as though it had been originally invaded by Germany and liberated by the Allies. On 15 May 1955, after talks which lasted for years and were influenced by the Cold War Austria regained full independence by concluding the Austrian State Treaty with the Four Occupying Powers. On 26 October 1955, after all occupation troops had left, Austria declared its "permanent neutrality" by an act of Parliament, which remains to this day but has been implicitly overlapped by constitutional amendments concerning Austria as member of the European Union from 1995 onward. The political system of the Second Republic is based on the constitution of 1920 and 1929, which was reintroduced in 1945. The system came to be characterised by "Proporz", meaning that most posts of political importance were split evenly between members of the Social Democrats and the People's Party. Interest group "chambers" with mandatory membership (e.g. for workers, business people, farmers) grew to considerable importance and were usually consulted in the legislative process, so that hardly any legislation was passed that did not reflect widespread consensus. Since 1945 a single-party government took place only 1966–1970 (Conservatives) and 1970–1983 (Social Democrats). During all other legislative periods, either a grand coalition of Conservatives and Social Democrats or a "small coalition" (one of these two and a smaller party) ruled the country. Following a referendum in 1994, at which consent reached a majority of two thirds, the country became a member of the European Union on 1 January 1995. According to its economic success, Austria is one of the "net contributors" of the union. The major parties SPÖ and ÖVP have contrary opinions about the future status of Austria's military non-alignment: While the SPÖ in public supports a neutral role, the ÖVP argues for stronger integration into the EU's security policy; even a future NATO membership is not ruled out by some ÖVP politicians. In reality, Austria is taking part in the EU's Common Foreign and Security Policy, participates in the so-called Petersburg Agenda (including peace keeping and peace creating tasks) and has become member of NATO's "Partnership for Peace"; the constitution has been amended accordingly. The term "neutrality" is only used to tranquilise voters afraid of change. Since 2008, due to the Schengen Agreement, the only neighbouring country performing border controls towards Austria is Liechtenstein.
The Parliament of Austria is located in Vienna, the country's largest city and capital. Austria became a federal, parliamentarian, democratic republic through the Federal Constitution of 1920. It was reintroduced in 1945 to the nine states of the Federal Republic. The head of state is the Federal President ("Bundespräsident"), who is directly elected by popular vote. The chairman of the Federal Government is the Federal Chancellor, who is appointed by the president. The government can be removed from office by either a presidential decree or by vote of no confidence in the lower chamber of parliament, the Nationalrat. Voting for the federal president and for the Parliament used to be compulsory in Austria, but this was abolished in steps from 1982 to 2004. The Parliament of Austria consists of two chambers. The composition of the Nationalrat (183 seats) is determined every five years (or whenever the Nationalrat has been dissolved by the federal president on a motion by the federal chancellor, or by Nationalrat itself) by a general election in which every citizen over 16 years (since 2007) has voting rights. While there is a general threshold of 4 percent for all parties at federal elections (Nationalratswahlen), there remains the possibility to gain a direct seat, or, in one of the 43 regional election districts. The Nationalrat is the dominant chamber in the formation of legislation in Austria. However, the upper house of parliament, the Bundesrat, has a limited right of veto (the Nationalrat can—in almost all cases—ultimately pass the respective bill by voting a second time. This is referred to as Beharrungsbeschluss", lit. "vote of persistence"). A convention, called the was convened in June 30, 2003 to decide upon suggestions to reform the constitution, but failed to produce a proposal that would receive the two-thirds of votes in the Nationalrat necessary for constitutional amendments and/or reform. With legislative and executive, the courts are the third column of Austrian state powers. Notably the Constitutional Court ("Verfassungsgerichtshof") may exert considerable influence on the political system by ruling out laws and ordinances not in compliance with the constitution. Since 1995, the European Court of Justice may overrule Austrian decisions in all matters defined in laws of the European Union. Concerning human rights, Austria also is implementing the decisions of the European Court of Human Rights, since the European Convention on Human Rights is part of the Austrian constitution.
After general elections held in October 2006, the Social Democrats emerged as the largest party, whereas the People's Party lost about 8% in votes. Political realities prohibited any of the two major parties from forming a coalition with smaller parties. In January 2007 the People's Party and Social Democrats formed a grand coalition with the social democrat Alfred Gusenbauer as Chancellor. This coalition broke up in June 2008. Elections in September 2008 further weakened both major parties (Social Democrats and People's Party) but together they still held more than 50% of the votes with the Social Democrats holding the majority. They formed a coalition with Werner Faymann from the Social Democrats as Chancellor. The positions of the Freedom Party and the deceased Jörg Haider's new party Alliance for the Future of Austria, both right-wing parties, were strengthened during the election.
The 1955 Austrian State Treaty ended the occupation of Austria following World War II and recognised Austria as an independent and sovereign state. On 26 October 1955, the Federal Assembly passed a constitutional article in which "Austria declares of her own free will her perpetual neutrality". The second section of this law stated that "in all future times Austria will not join any military alliances and will not permit the establishment of any foreign military bases on her territory". Since then, Austria has shaped its foreign policy on the basis of neutrality, but rather different from the neutrality of Switzerland. Austria began to reassess its definition of neutrality following the fall of the Soviet Union, granting overflight rights for the UN-sanctioned action against Iraq in 1991, and, since 1995, it has developed participation in the EU's Common Foreign and Security Policy (CFSP). Also in 1995, it joined the Partnership for Peace and subsequently participated in peacekeeping missions in Bosnia. Meanwhile, the only part of the Constitutional Law on Neutrality of 1955 still valid fully is not to allow foreign military bases in Austria. Austria attaches great importance to participation in the Organisation for Economic Co-operation and Development and other international economic organisations, and it has played an active role in the Organization for Security and Cooperation in Europe (OSCE).
In 1972, the country began construction of a nuclear-powered electricity-generation station at Zwentendorf on the River Danube, following a unanimous vote in parliament. However, in 1978, a referendum voted approximately 50.5% against nuclear power, 49.5% for, and parliament subsequently unanimously passed a law forbidding the use of nuclear power to generate electricity. Austria currently produces more than half of its electricity by hydropower. Together with other renewable energy sources such as wind, solar and biomass powerplants, the electricity supply from renewable energy amounts to 62.89% of total use in Austria, with the rest being produced by gas and oil powerplants.
The manpower of the Austrian Armed Forces () mainly relies on conscription. All males who have reached the age of eighteen and are found fit have to serve a six months military service, followed by an eight year reserve obligation. Both males and females at the age of sixteen are eligible for voluntary service. Conscientious objection is legally acceptable and those who claim this right are obliged to serve an institutionalised nine months civilian service instead. Since 1998, women volunteers have been allowed to become professional soldiers. The main sectors of the Bundesheer are Joint Forces (Streitkräfteführungskommando, SKFüKdo) which consist of Land Forces (Landstreitkräfte), Air Forces (Luftstreitkräfte), International Missions (Internationale Einsätze) and Special Forces (Spezialeinsatzkräfte), next to Mission Support (Kommando Einsatzunterstützung; KdoEU) and Command Support (Kommando Führungsunterstützung; KdoFüU). Being a landlocked country, Austria has no navy. In 2004, Austria's defence expenditures corresponded to approximately 0.9% of its GDP. The Army currently has about 45,000 soldiers, of whom about half are conscripts. As head of state, Austrian President (currently Heinz Fischer) is nominally the Commander-in-Chief of the Bundesheer. In practical reality, however, command of the Austrian Armed Forces is almost exclusively exercised by the Minister of Defense, currently Norbert Darabos. Since the end of the Cold War, and more importantly the removal of the former heavily guarded "Iron Curtain" separating Austria and Hungary, the Austrian military has been assisting Austrian border guards in trying to prevent border crossings by illegal immigrants. This assistance came to an end when Hungary joined the EU Schengen area in 2008, for all intents and purposes abolishing "internal" border controls between treaty states. Some politicians have called for a prolongation of this mission, but the legality of this is heavily disputed. In accordance with the Austrian constitution, armed forces may only be deployed in a limited number of cases, mainly to defend the country and aid in cases of national emergency, such as in the wake of natural disasters. They may generally not be used as auxiliary police forces. Within its self-declared status of permanent neutrality, Austria has a long and proud tradition of engaging in UN-led peacekeeping and other humanitarian missions. The Austrian Forces Disaster Relief Unit (AFDRU), in particular, an all-volunteer unit with close ties to civilian specialists (e.g. rescue dog handlers) enjoys a reputation as a quick (standard deployment time is 10 hours) and efficient SAR unit. Currently, larger contingents of Austrian forces are deployed in Bosnia, Kosovo and, since 1974, in the Golan Heights.
As a federal republic, Austria is divided into nine states (). These states are then divided into districts () and statutory cities (). Districts are subdivided into municipalities (). Statutory Cities have the competencies otherwise granted to both districts and municipalities. The states are not mere administrative divisions but have some legislative authority distinct from the federal government, e.g. in matters of culture, social care, youth and nature protection, hunting, building, and zoning ordinances. In recent years, it has been discussed whether today it is appropriate for a small country to maintain ten parliaments.
Austria is a largely mountainous country due to its location in the Alps. The Central Eastern Alps, Northern Limestone Alps and Southern Limestone Alps are all partly in Austria. Of the total area of Austria (), only about a quarter can be considered low lying, and only 32% of the country is below. The Alps of western Austria give way somewhat into low lands and plains in the eastern part of the country. Austria can be divided into five areas, the biggest being the Eastern Alps, which constitute 62% of nation's total area. The Austrian foothills at the base of the Alps and the Carpathians account for around 12% and the foothills in the east and areas surrounding the periphery of the Pannoni low country amount to about 12% of the total landmass. The second greater mountain area (much lower than the Alps) is situated in the north. Known as the Austrian granite plateau, it is located in the central area of the Bohemian Mass, and accounts for 10% of Austria. The Austrian portion of the Vienna basin comprises the remaining 4%. Phytogeographically, Austria belongs to the Central European province of the Circumboreal Region within the Boreal Kingdom. According to the WWF, the territory of Austria can be subdivided into four ecoregions: the Central European mixed forests, Pannonian mixed forests, Alps conifer and mixed forests and Western European broadleaf forests.
The greater part of Austria lies in the cool/temperate climate zone in which humid westerly winds predominate. With over half of the country dominated by the Alps, the alpine climate is the predominant one. In the east—in the Pannonian Plain and along the Danube valley—the climate shows continental features with less rain than the alpine areas. Although Austria is cold in the winter, summer temperatures can be relatively warm—reaching temperatures of around 20 – 40 °C.
Austria is one of the 12 richest countries in the world in terms of GDP (Gross domestic product) per capita, has a well-developed social market economy, and a high standard of living. Until the 1980s, many of Austria's largest industry firms were nationalised; in recent years, however, privatisation has reduced state holdings to a level comparable to other European economies. Labour movements are particularly strong in Austria and have large influence on labour politics. Next to a highly developed industry, international tourism is the most important part of the national economy. Germany has historically been the main trading partner of Austria, making it vulnerable to rapid changes in the German economy. However, since Austria became a member state of the European Union it has gained closer ties to other European Union economies, reducing its economic dependence on Germany. In addition, membership in the EU has drawn an influx of foreign investors attracted by Austria's access to the single European market and proximity to the aspiring economies of the European Union. Growth in GDP accelerated in recent years and reached 3.3% in 2006.
In Austria, the euro was introduced as an accounting currency on 1 January 1999, and euro coins and banknotes entered circulation on 1 January 2002. As a preparation for this date, the minting of the new euro coins started as early as 1999, however all Austrian euro coins introduced in 2002 have this year on it; unlike other countries of the Eurozone where mint year is minted in the coin. Eight different designs, one per face value, were selected for the Austrian coins. In 2007, to adopt the new common map like the rest of the Eurozone countries, Austria changed the common side of its coins. Before adopting the Euro in 2002 Austria had maintained use of the Austrian schilling which was first established in December 1924. The Schilling was abolished in the wake of the Anschluss in 1938 and has been reintroduced after the end of the World War II in November 1945. Austria has one of the richest collection of collectors' coins in the Eurozone, with face value ranging from 10 to 100 euro (although a 100,000 euro coin was exceptionally minted in 2004). These coins are a legacy of an old national practice of minting of silver and gold coins. Unlike normal issues, these coins are not legal tender in all the eurozone. For instance, a €5 Austrian commemorative coin cannot be used in any other country.
Responsibility for educational oversight in Austria is entrusted partly to the Austrian states (Bundesländer), and partly to the federal government. School attendance is compulsory for nine years, i.e. usually to the age of fifteen. Kindergarten education, free in most states, is provided for all children between the ages of three and six years and, whilst optional, is considered a normal part of a child's education, due to its high takeup rate. Maximum class size is around 30, each class normally being cared for by one qualified teacher and one assistant. Standard attendance times are 8am to 12am, with extra afternoon care also frequently provided for a fee. Primary education, or Volksschule, lasts for four years, starting at age six. Maximum class size is 30, but may be as low as 15. It is generally expected that a class will be taught by one teacher for the entire four years and the stable bond between teacher and pupil is considered important for a child's well-being. The "3Rs" dominate lesson time, with less time allotted to project work than in the UK. Children work individually and all members of a class follow the same plan of work. There is no streaming. Lessons begin at 8am and last until noon or 1pm with hourly five- or ten-minute breaks. Children are given homework daily from the first year. Historically there has been no lunch hour, children returning home to eat. However, due to a rise in the number of mothers in work, primary schools are increasingly offering pre-lesson and afternoon care. As in Germany, secondary education consists of two main types of schools, attendance at which is based on a pupil's ability as determined by grades from the primary school. The Gymnasium caters for the more able children, in the final year of which the Matura examination is taken, which is a requirement for access to university. The Hauptschule prepares pupils for vocational education but also for various types of further education (HTL = institution of higher technical education; HAK = commercial academy; HBLA = institution of higher education for economic business; etc.). Attendance at one of these further education institutes also leads to the Matura. Some schools aim to combine the education available at the Gymnasium and the Hauptschule, and are known as Gesamtschulen. In addition, a recognition of the importance of learning English has led some Gymnasiums to offer a bilingual stream, in which pupils deemed able in languages follow a modified curriculum, a portion of the lesson time being conducted in English. As at primary school, lessons at Gymnasium begin at 8am, and continue with short intervals until lunchtime or early afternoon, with children returning home to a late lunch. Older pupils often attend further lessons after a break for lunch, generally eaten at school. As at primary level, all pupils follow the same plan of work. Great emphasis is placed on homework and frequent testing. Satisfactory marks in the end-of-the-year report ("Zeugnis") are a prerequisite for moving up ("aufsteigen") to the next class. Pupils who do not meet the required standard re-sit their tests at the end of the summer holidays; those whose marks are still not satisfactory are required to re-sit the year ("sitzenbleiben"). It is not uncommon for a pupil to re-sit more than one year of school. After completing the first two years, pupils choose between one of two strands, known as "Gymnasium" (slightly more emphasis on arts) or "Realgymnasium" (slightly more emphasis on science). Whilst many schools offer both strands, some do not, and as a result, some children move schools for a second time at age 12. At age 14, pupils may choose to remain in one of these two strands, or to change to a vocational course, possibly with a further change of school. The Austrian university system had been open to any student who passed the Matura examination until recently. A 2006 bill allowed the introduction of entrance exams for studies such as Medicine. In 2001, an obligatory tuition fee ("Studienbeitrag") of €363.36 per term was introduced for all public universities. Since 2008, for all EU students the studies are free of charge, as long as a certain time-limit is not exceeded (the expected duration of the study plus usually two terms tolerance). When the time-limit is exceeded, the fee of around €363.36 per term is charged. Some further exceptions to the fee apply, e.g. for students with a year's salary of more than about €5000. In all cases, an obligatory fee of €15.50 for the student union and insurance is charged.
Austria's population estimate in January 2009 was 8,356,707. The population of the capital, Vienna, exceeds 1.6 million (2.2 million including the suburbs), representing about a quarter of the country's population. It is known for its vast cultural offerings and high standard of living. Vienna is by far the country's largest city. Graz is second in size, with 250,099 inhabitants, followed by Linz (188,968), Salzburg (150,000), and Innsbruck (117,346). All other cities have fewer than 100,000 inhabitants.
German, Austria's official language, is spoken natively by 88.6% of the population—followed by Turkish (2.3%), Serbian (2.2%), Croatian (1.6%), Hungarian (0.5%), and Bosnian (0.4%). The Austrian federal states of Carinthia and Styria are home to a significant indigenous Slovene-speaking minority with around 14,000 members (Austrian census; unofficial numbers of Slovene groups speak of up to 50,000). In the eastermost state, Burgenland (formerly part of the Hungarian portion of Austria–Hungary), about 20,000 Austrian citizens speak Hungarian and 30,000 speak Croatian. Of the remaining number of Austria's people that are of non-Austrian descent, many come from surrounding countries, especially from the former East Bloc nations. So-called guest workers "(Gastarbeiter)" and their descendants, as well as refugees from the Yugoslav wars and other conflicts, also form an important minority group in Austria. Since 1994 the Roma–Sinti (gypsies) are an officially recognised ethnic minority in Austria. According to census information published by Statistik Austria for 2001 there were a total of 710,926 foreign nationals living in Austria. Of these, 124,392 speak German as their mother tongue (mainly immigrants from Germany, some from Switzerland and South Tyrol, Italy) The next largest populations of linguistic and ethnic groups are 240,863 foreign nationals from the former Yugoslavia (Serbs being the largest number of these at 135,376, followed by Croatian at 105,487); 123,417 Turkish nationals; 25,155 whose native tongue is English; 24,446 Albanian; 17,899 Polish; 14,699 Hungarian; 12,216 Romanian; 7,982 Arabs; 6,902 Slovenes (not including the autochthonous minority); 6,891 Slovaks; 6,707 Czech; 5,916 Persian; 5,677 Italian; 5,466 Russian; 5,213 French; 4,938 Chinese; 4,264 Spanish; 3,503 Bulgarian. The populations of the rest fall off sharply below 3,000. Between 200,000 and 300,000 ethnic Turks (including minority of Turkish Kurds) currently live in Austria. They are the largest single immigrant group in Austria, closely followed by the Serbs. Austria's mountainous terrain led to the development of many distinct German dialects. All of the dialects in the country, however, belong to Austro-Bavarian groups of German dialects, with the exception of the dialect spoken in its western-most Bundesland, Vorarlberg, which belongs to the group of Alemannic dialects. There is also a distinct grammatical standard for Austrian German with a few differences to the German spoken in Germany. As of 2006, some of the Austrian states introduced standardised tests for new citizens, to assure their language ability, cultural knowledge and accordingly their ability to integrate into the Austrian society. For the national rules, see Austrian nationality law – Naturalisation.
An estimated 13,000 to 40,000 Slovenes in the Austrian state of Carinthia (the Carinthian Slovenes) as well as Croats (around 30,000) and Hungarians in Burgenland were recognised as a minority and have enjoyed special rights following the Austrian State Treaty () of 1955. The Slovenes in the Austrian state of Styria (estimated at a number between 1,600 and 5,000) are not recognised as a minority and do not enjoy special rights, although the State Treaty of July 27, 1955 states otherwise. The right for bilingual topographic signs for the regions where Slovene- and Croat-Austrians live alongside the German speaking population (as required by the 1955 State Treaty) is still to be fully implemented. Many Carinthians are afraid of Slovenian territorial claims, pointing to the fact that Yugoslav troops entered the state after each of the two World Wars and considering that some official Slovenian atlases show parts of Carinthia as Slovene cultural territory. The recently deceased governor, Jörg Haider, has made this fact a matter of public argument in autumn 2005 by refusing to increase the number of bilingual topographic signs in Carinthia. A poll by the Kärntner Humaninstitut conducted in January 2006 states that 65% of Carinthians are not in favour of an increase of bilingual topographic signs, since the original requirements set by the State Treaty of 1955 have already been fulfilled according to their point of view. Another interesting phenomenon is the so called "Windischen-Theorie" stating that the Slovenes can be split in two groups: actual Slovenes and "Windische" (a traditional German name for Slavs), based on differences in language between Austrian Slovenes, who were taught Slovene standard language in school and those Slovenes who spoke their local Slovene dialect but went to German schools. The term "Windische" was applied to the latter group as a means of distinction. This politically influenced theory, dividing Slovene Austrians into the "loyal Windische" and the "national Slovenes", was never generally accepted and fell out of use some decades ago.
At the end of the twentieth century, about 74% of Austria's population were registered as Roman Catholic, while about 5% considered themselves Protestants. Austrian Christians are obliged to pay a mandatory membership fee (calculated by income—about 1%) to their church; this payment is called "Kirchenbeitrag" ("Ecclesiastical/Church contribution"). Since the second half of the 20th century, the number of adherents and churchgoers has dropped. Data for the end of 2005 from the Austrian Roman Catholic church lists 5,662,782 members or 68.5% of the total Austrian population, and a Sunday church attendance of 753,701 or 9% of the total Austrian population. Data for the end of 2008 published by the Austrian Roman Catholic church shows a further reduction to 5,579,493 members or 66.8% of the total Austrian population, and a Sunday church attendance of 698,527 or 8% of the total Austrian population. The Lutheran church also recorded a large drop in adherents between 2001 and 2008. About 12% of the population declared that they have no religion. in 2001. Of the remaining people, around 340,000 are registered as members of various Muslim communities, mainly due to the influx from Turkey, Bosnia-Herzegovina and Albania. About 180,000 are members of Eastern Orthodox Churches, more than 20,000 are active Jehovah's Witnesses and about 8,100 are Jewish. The Austrian Jewish Community of 1938—Vienna alone counted more than 200,000—was reduced to around 4,500 during the Second World War, with approximately 65,000 Jewish Austrians killed in the Holocaust and 130,000 emigrating. The large majority of the current Jewish population are post-war immigrants, particularly from eastern Europe and central Asia (including Bukharan Jews). Buddhism was legally recognised as a religion in Austria in 1983. According to the most recent Eurobarometer Poll 2005, While northern and central Germany was the origin of the Reformation, Austria and Bavaria were the heart of the Counter-Reformation in the sixteenth and seventeenth centuries, when the absolute monarchy of Habsburg imposed a strict regime to restore Catholicism's power and influence among Austrians. The Habsburgs for a long time viewed themselves as the vanguard of Catholicism and all other confessions and religions were repressed. In 1781, in the era of Austrian enlightenment, Emperor Joseph II issued a Patent of Tolerance for Austria that allowed other confessions a limited freedom of worship. Religious freedom was declared a constitutional right in Cisleithania after the Austro-Hungarian "Ausgleich" in 1867 thus paying tribute to the fact that the monarchy was home of numerous religions beside Roman Catholicism such as Greek, Serbian, Romanian, Russian, and Bulgarian Orthodox Christians (Austria neighboured the Ottoman Empire for centuries), Calvinist, Lutheran Protestants and Jews. In 1912, after the annexation of Bosnia Hercegovina in 1908, Islam was officially recognised in Austria. Austria remained largely influenced by Catholicism. After 1918, First Republic Catholic leaders such as Theodor Innitzer and Ignaz Seipel took leading positions within or close to Austria's government and increased their influence during the time of the Austrofascism; Catholicism was treated much like a state religion by Engelbert Dollfuss and Kurt Schuschnigg. Although Catholic (and Protestant) leaders initially welcomed the Germans in 1938 during the Anschluss of Austria into Germany, Austrian Catholicism stopped its support of Nazism later on and many former religious public figures became involved with the resistance during the Third Reich. After the end of World War II in 1945, a stricter secularism was imposed in Austria, and religious influence on politics declined.
Austria's past as a European power and its cultural environment have generated a broad contribution to various forms of art, most notably among them music. Austria has been the birthplace of many famous composers such as Joseph Haydn, Franz Schubert, Anton Bruckner, Johann Strauss, Sr., Johann Strauss, Jr. and Gustav Mahler as well as members of the Second Viennese School such as Arnold Schoenberg, Anton Webern and Alban Berg. Wolfgang Amadeus Mozart was born in Salzburg, then an independent Church Principality, though one that was culturally closely connected to Austria, and much of Mozart's career was spent in Vienna. Vienna has long been especially an important centre of musical innovation. Eighteenth and nineteenth century composers were drawn to the city due to the patronage of the Habsburgs, and made Vienna the European capital of classical music. During the Baroque period, Slavic and Hungarian folk forms influenced Austrian music. Vienna's status began its rise as a cultural center in the early 1500s, and was focused around instruments including the lute. Ludwig van Beethoven spent the better part of his life in Vienna. Austria's current national anthem, attributed to Mozart, was chosen after World War II to replace the traditional Austrian anthem by Joseph Haydn. Austria has also produced one notable jazz musician, keyboardist Josef Zawinul, who helped pioneer electronic influences in jazz as well as being a notable composer in his own right. The pop and rock musician Falco was internationally acclaimed during the 1980s, especially for his song "Rock Me Amadeus" dedicated to Mozart. The drummer Thomas Lang was born in Vienna in 1967 and is now world renowned for his technical ability, having played with artists such as Geri Halliwell and Robbie Williams.
Austrian contributions to the worlds of film and theater have traditionally been strong. Sascha Kolowrat was an Austrian pioneer of filmmaking. Billy Wilder, Fritz Lang, Josef von Sternberg, and Fred Zinnemann originally came from Austria before establishing themselves as internationally relevant movie makers. Willi Forst, Ernst Marischka, or Franz Antel enriched the popular cinema in German language speaking countries. Michael Haneke became internationally known for his disturbing cinematic studies, before receiving a Golden Globe for his critically acclaimed film "The White Ribbon" in 2010. The first Austrian film director receiving an Academy Award was Stefan Ruzowitzky. Many Austrian actors were able to pursue a career, the impact of which was sensed beyond national borders. Among them were Peter Lorre, Curd Jürgens, Senta Berger, Oskar Werner, and Klaus Maria Brandauer. Hedy Lamarr and Arnold Schwarzenegger became American as well as international movie stars. Christoph Waltz rose to international fame with his performance in "Inglourious Basterds", earning an Golden Globe Award in 2010. Max Reinhardt was a master of spectacular and astute theater productions. Otto Schenk not only excelled as a stage actor, but also as an opera director.
Austria was the cradle of numerous scientists with international reputation. Among them are Ludwig Boltzmann, Ernst Mach, Victor Franz Hess and Christian Doppler, prominent scientists in the nineteenth century. In the twentieth century, contributions by Lise Meitner, Erwin Schrödinger and Wolfgang Pauli to nuclear research and quantum mechanics were key to these areas' development during the 1920s and 1930s. A present-day quantum physicist is Anton Zeilinger, noted as the first scientist to demonstrate quantum teleportation. In addition to physicists, Austria was the birthplace of two of the most noteworthy philosophers of the twentieth century, Ludwig Wittgenstein and Karl Popper. In addition to them biologists Gregor Mendel and Konrad Lorenz as well as mathematician Kurt Gödel and engineers such as Ferdinand Porsche and Siegfried Marcus were Austrians. A focus of Austrian science has always been medicine and psychology, starting in medieval times with Paracelsus. Eminent physicians like Theodore Billroth, Clemens von Pirquet, and Anton von Eiselsberg have built upon the achievements of the 19th century Vienna School of Medicine. Austria was home to psychologists Sigmund Freud, Alfred Adler, Paul Watzlawick and Hans Asperger and psychiatrist Viktor Frankl. The Austrian School of Economics, which is prominent as one of the main competitive directions for economic theory, is related to Austrian economists Joseph Schumpeter, Eugen von Böhm-Bawerk, Ludwig von Mises, and Friedrich Hayek. Other noteworthy Austrian-born émigrés include the management thinker Peter Drucker, scientist Sir Gustav Nossal, and the 38th Governor of California, Arnold Schwarzenegger.
Complementing its status as a land of artists and scientists, Austria has always been a country of poets, writers, and novelists. It was the home of novelists Arthur Schnitzler, Stefan Zweig, Thomas Bernhard, Franz Kafka, and Robert Musil, of poets Georg Trakl, Franz Werfel, Franz Grillparzer, Rainer Maria Rilke, Adalbert Stifter, Karl Kraus and children's author Eva Ibbotson. Famous contemporary playwrights and novelists are Nobel prize winner Elfriede Jelinek, Peter Handke and Daniel Kehlmann.
Austria's cuisine is derived from that of the Austro-Hungarian Empire. Austrian cuisine is mainly the tradition of Royal-Cuisine ("Hofküche") delivered over centuries. It is famous for its well-balanced variations of beef and pork and countless variations of vegetables. There is also the "Mehlspeisen" Bakery, which created particular delicacies such as Sachertorte, "Krapfen" which are doughnuts usually filled with apricot marmalade or custard, and "Strudel" such as "Apfelstrudel" filled with apple and "Topfenstrudel" filled with sweetened sour cream. In addition to native regional traditions, the cuisine has been influenced by Hungarian, Bohemia Czech, Jewish, Italian, Balkan and French cuisine, from which both dishes and methods of food preparation have often been borrowed. The Austrian cuisine is therefore one of the most multicultural and transcultural in Europe. Typical Austrian dishes include Wiener Schnitzel, Schweinsbraten, Kaiserschmarren, Knödel, Sachertorte and Tafelspitz. There are also Kärntner Kasnudeln, a cooked filled dough-bag with a type of cottage cheese and spearmint, and Eierschwammerl dishes. The "Eierschwammerl", also known as "Pfifferling", are native yellow, tan mushrooms. The candy Pez was invented in Austria, as well as Mannerschnitten. Austria is also famous for its Mozartkugeln, and its coffee tradition.
Due to the mountainous terrain, alpine skiing is a prominent sport in Austria. Similar sports such as snowboarding or ski-jumping are also widely popular. A popular team sport in Austria is football, which is governed by the Austrian Football Association. However, Austria rarely has international success in this discipline, going out in the first round of the 2008 UEFA European Football Championship which was co-hosted by Austria and Switzerland. Besides football, Austria also has professional national leagues for most major team sports including the Austrian Hockey League for ice hockey, and the Österreichische Basketball Bundesliga for basketball. Bobsleigh, luge, and skeleton are also popular events with a permanent track located in Igls, which hosted bobsleigh and luge competitions for the 1964 and 1976 Winter Olympics held in Innsbruck. The first Winter Youth Olympics in 2012 will be held in Innsbruck as well.
An astronomer is a scientist who studies celestial bodies such as planets, stars, and galaxies. Historically, astronomy was more concerned with the classification and description of phenomena in the sky, while astrophysics attempted to explain these phenomena and the differences between them using physical laws. Today, that distinction has mostly disappeared. Professional astronomers are highly educated individuals who typically have a PhD in physics or astronomy and are employed by research institutions or universities. They spend the majority of their time working on research, although they quite often have other duties such as teaching, building instruments, or aiding in the operation of an observatory. The number of professional astronomers in the United States is actually quite small. The American Astronomical Society, which is the major organization of professional astronomers in North America, has approximately 7,700 members. This number includes scientists from other fields such as physics, geology, and engineering, whose research interests are closely related to astronomy. The International Astronomical Union comprises almost 9,259 members from 89 different countries who are involved in astronomical research at the PhD level and beyond. While the number of professional astronomers worldwide is not much larger than the population of a small town, there is a huge community of amateur astronomers. Most cities have amateur astronomy clubs that meet on a regular basis and often host star parties in their communities. The Astronomical Society of the Pacific is the largest general astronomical society in the world, comprising both professional and amateur astronomers as well as educators from 70 different nations. Like any hobby, most people who think of themselves as amateur astronomers may devote a few hours a month to stargazing and reading the latest developments in research. However, amateurs span the range from so-called "armchair astronomers" to the very ambitious, who own science-grade telescopes and instruments with which they are able to make their own discoveries and assist professional astronomers in research.
(Arabic: زيج "astronomical tables of Sind and Hind") is a work consisting of approximately 37 chapters on calendrical and astronomical calculations and 116 tables with calendrical, astronomical and astrological data, as well as a table of sine values. This is the first of many Arabic "Zijes" based on the Indian astronomical methods known as the "sindhind". The work contains tables for the movements of the sun, the moon and the five planets known at the time. This work marked the turning point in Islamic astronomy. Hitherto, Muslim astronomers had adopted a primarily research approach to the field, translating works of others and learning already discovered knowledge. Al-Khwarizmi's work marked the beginning of non-traditional methods of study and calculations. The original Arabic version (written c. 820) is lost, but a version by the Spanish astronomer Maslamah Ibn Ahmad al-Majriti (c. 1000) has survived in a Latin translation, presumably by Adelard of Bath (January 26, 1126). The four surviving manuscripts of the Latin translation are kept at the Bibliothèque publique (Chartres), the Bibliothèque Mazarine (Paris), the Bibliotheca Nacional (Madrid) and the Bodleian Library (Oxford).
Contrary to the classical image of an old astronomer peering through a telescope through the dark hours of the night, it is very rare for a modern professional astronomer to use an eyepiece on a larger telescope. It is far more common to use a charge-coupled device camera to record a long, deep exposure, allowing a more sensitive image to be created because the light is added over time. Before CCDs, photographic plates were a common method of observation. Modern astronomers spend relatively little time at telescopes - most spend a few weeks per year observing, and the rest of their time reducing the data (changing it from raw data to processed images) and analyzing it. Many astronomers work entirely from astronomical survey or space observatory data. Others work with radio telescopes like the Very Large Array, which is entirely automated, although it is maintained by telescope operators. Astronomers who serve as faculty spend much of their time teaching undergraduate and graduate classes. Most universities also have outreach programs including public telescope time and sometime planetariums as a public service and to encourage interest in the field.
Amoeboids are single-celled life-forms characterized by an irregular shape. "Amoeboid" and "amoeba" are often used interchangeably even by biologists, and especially refers to a creature moving by using pseudopods. Most references to "amoebas" or "amoebae" are to amoeboids in general rather than to the specific genus "Amoeba". The genus "Amoeba" and amoeboids in general both derive their names from the ancient Greek word for change.
The superclass Rhizopoda has naked and shelled amoebas. They tend to ingest food and creep toward food. They move using pseudopodia, which are bulges of cytoplasm. Naked amoebas use all of their body for pseudopod movement. Usually half is used in shelled amoebas; the other half grips the shell. Amoebas breathe using their entire cell membrane that is constantly immersed in water. Because they live in water, they have no problem keeping it wet. But it does have one drawback. Excess water often crosses into the cytosol. All have one contractile vacuole that expels excess water. This contraction is powered by contracting cytosol. The nuclei of amoebas do not have chromosomes. They also persist during cell division. The nuclear membrane pinches in two during telophase. Amoebas also do not go through meiosis. Amoebas have one or more nucleoli. The food scources vary in rhizopoda. Some consume bacteria. Others are detritivores and eat dead organic material. Still others eat other protists. They extend a pair of pseudopodia around food. They fuse to make a food vacuole which then fuses with a lysosome to add digestive chemicals. Undigestied food is expelled at the cell membrane. Amoebas use pseudopodia to move and feed. They are powered by flexible microfilaments near the membrane. Microfilaments are at least 50% of the cytoskeleton. The other parts are more stiff and are composed of intermediate filaments and macrotubules. These are not used in amoeboid movement, but are stiff skeletons on which organelles are supported or can move on. The shells of amoebas are often composed of calcium ar other materials. The proteins or materials are synthesised in the cell and exported just outside the cell membrane. The shells are stiff and flexible, but not too much of either trait. Amoebas seem to have connections with two phyla of the lineage funguslike protists. The two phyla are myxomycota (plasmodial slime molds), and acrasiomycota (cellular slime molds). These two phyla use amoeboid movement in their feeding stage. One is basically a giant multinucleate amoeba, while the other lives solitary until food runs out; in which a colony of these functions as a unit. Myxomycotes use amoeboid gametes, as well. They definitely have a connection.
They have appeared in a number of different groups. Some cells in multicellular animals may be amoeboid, for instance human white blood cells, which consume pathogens. Many protists also exist as individual amoeboid cells, or take such a form at some point in their life-cycle. The most famous such organism is "Amoeba proteus"; the name amoeba is variously used to describe its close relatives, other organisms similar to it, or the amoeboids in general. As amoebas themselves are polyphyletic and subject to some imprecision in definition, the term "Amoeboid" does not provide identification of an organism, and is better understood as description of locomotion. When used in the broader sense, the term can include the following groups: Acanthamoeba, Acrasis, Adelphamoeba, Amoeba, Astramoeba, Balamuthia, Cashia, Chaos, Clydonella, Dactylamoeba, Dientamoeba, Dinamoeba, Discamoeba, Echinamoeba, Endamoeba, Entamoeba, Filamoeba, Flabelulla, Flagellipodium, Flamella, Gephyramoeba, Gibbodiscus, Glaeseria, Gocevia, Gruberella, Gyromitus, Hartmannella, Heteramoeba, Hollandella, Histomonas, Hyalodiscus, Hydramoeba, Hyperamoeba, Iodamoeba, Korotnevella, Labyrinthula, Learamoeba, Leptomyxa, Lingulamoeba, Macropharyngomonas, Malamoeba, Mastigamoeba, Mastigella, Mastigina, Mayorella, Metachaos, Micronuclearia, Monopylocystis, Naegleria, Neoparamoeba, Neovahlkampfia, Nollandia, Nuclearia, Oscillosignum, Paragocevia, Paramoeba, Paratetramitus, Paravahlkampfia, Parvamoeba, Pelomyxa, Pernina, Pfiesteria, Polychaos, Pontifex, Phreatamoeba, Platyamoeba, Protoacanthamoeba, Protonaegleria, Psalteriomonas, Pseudomastigamoeba, Plaesiobystra, Rhizamoeba, Rosculus, Rugipes, Saccamoeba, Sappinia, Sawyeria, Stachyamoeba, Stereomyxa, Striamoeba, Striolatus, Stygamoeba, Subulamoeba, Tetramitus, Thecamoeba, Theratromyxa, Trichamoeba, Trichosphaerium, Trienamoeba, Trimastigamoeba, Unda, Vahlkampfia, Vampyrella, Vampyrellium, Vannella, Vexillifera, and Willaertia.
Amoeboids may be divided into several morphological categories based on the form and structure of the pseudopods. Those where the pseudopods are supported by regular arrays of microtubules are called actinopods, and forms where they are not are called rhizopods, further divided into lobose, filose, and reticulose amoebae. There is also a strange group of giant marine amoeboids, the xenophyophores, that do not fall into any of these categories. Most amoeboid are now grouped in Amoebozoa or Rhizaria.
The American Standard Code for Information Interchange (acronym: ASCII;,) is a character-encoding scheme based on the ordering of the English alphabet. ASCII codes represent text in computers, communications equipment, and other devices that use text. Most modern character-encoding schemes are based on ASCII, though they support many more characters than did ASCII. US-ASCII is the IANA preferred charset name for ASCII. Historically, ASCII developed from telegraphic codes. Its first commercial use was as a seven-bit teleprinter code promoted by Bell data services. Work on ASCII formally began October 6, 1960, with the first meeting of the American Standards Association's (ASA) X3.2 subcommittee. The first edition of the standard was published during 1963, a major revision during 1967, and the most recent update during 1986. Compared to earlier telegraph codes, the proposed Bell code and ASCII were both ordered for more convenient sorting (i.e., alphabetization) of lists, and added features for devices other than teleprinters. ASCII includes definitions for 128 characters: 33 are non-printing control characters (now mostly obsolete) that affect how text and space is processed; 94 are printable characters, and the space is considered an invisible graphic. The most commonly used character encoding on the World Wide Web was US-ASCII until December 2007, when it was surpassed by UTF-8.
The American Standard Code for Information Interchange (ASCII) was developed under the auspices of a committee of the American Standards Association, called the X3 committee, by its X3.2 (later X3L2) subcommittee, and later by that subcommittee's X3.2.4 working group. The ASA became the United States of America Standards Institute or USASI and ultimately the American National Standards Institute. The X3.2 subcommittee designed ASCII based on earlier teleprinter encoding systems. Like other character encodings, ASCII specifies a correspondence between digital bit patterns and character symbols (i.e. graphemes and control characters). This allows digital devices to communicate with each other and to process, store, and communicate character-oriented information such as written language. Before ASCII was developed, the encodings in use included 26 alphabetic characters, 10 numerical digits, and from 11 to 25 special graphic symbols. To include all these, and control characters compatible with the Comité Consultatif International Téléphonique et Télégraphique standard, Fieldata, and early EBCDIC, more than 64 codes were required for ASCII. The committee debated the possibility of a shift key function (like the Baudot code), which would allow more than 64 codes to be represented by six bits. In a shifted code, some character codes determine choices between options for the following character codes. It allows compact encoding, but is less reliable for data transmission; an error in transmitting the shift code typically makes a long part of the transmission unreadable. The standards committee decided against shifting, and so ASCII required at least a seven-bit code. The committee considered an eight-bit code, since eight bits would allow two four-bit patterns to efficiently encode two digits with binary coded decimal. However, it would require all data transmission to send eight bits when seven could suffice. The committee voted to use a seven-bit code to minimize costs associated with data transmission. Since perforated tape at the time could record eight bits in one position, it also allowed for a parity bit for error checking if desired. Machines with octets as the native data type that did not use parity checking typically set the eighth bit to "0". The code itself was patterned so that most control codes were together, and all graphic codes were together. The first two columns (32 positions) were reserved for control characters. The "space" character had to come before graphics to make sorting algorithms easy, so it became position 0x20. The committee decided it was important to support upper case 64-character alphabets, and chose to pattern ASCII so it could be reduced easily to a usable 64-character set of graphic codes. Lower case letters were therefore not interleaved with upper case. To keep options available for lower case letters and other graphics, the special and numeric codes were arranged before the letters, and the letter 'A' was placed in position 0x41 to match the draft of the corresponding British standard. The digits 0–9 were arranged so they correspond to values in binary prefixed with 011, making conversion with binary-coded decimal straightforward. Many of the non-alphanumeric characters were positioned to correspond to their shifted position on typewriters. Thus #, $ and % were placed to correspond to 3, 4, and 5 in the adjacent column. The parentheses could not correspond to 9 and 0, however, because the place corresponding to 0 was taken by the space character. Since many European typewriters placed the parentheses with 8 and 9, those corresponding positions were chosen for the parentheses. The @ symbol was not used in continental Europe and the committee expected it would be replaced by an accented À in the French variation, so the @ was placed in position 0x40 next to the letter A. The control codes felt essential for data transmission were the start of message (SOM), end of address (EOA), end of message (EOM), end of transmission (EOT), "who are you?" (WRU), "are you?" (RU), a reserved device control (DC0), synchronous idle (SYNC), and acknowledge (ACK). These were positioned to maximize the Hamming distance between their bit patterns. With the other special characters and control codes filled in, ASCII was published as ASA X3.4-1963, leaving 28 code positions without any assigned meaning, reserved for future standardization, and one unassigned control code. It now seems obvious that these positions should have been assigned to the lower case alphabet, but there was some debate at the time whether there should be more control characters instead. The indecision did not last long: during May 1963 the CCITT Working Party on the New Telegraph Alphabet proposed to assign lower case characters to columns 6 and 7, and International Organization for Standardization TC 97 SC 2 voted during October to incorporate the change into its draft standard. The X3.2.4 task group voted its approval for the change to ASCII at its May 1963 meeting. Locating the lowercase letters in columns 6 and 7 caused the characters to differ in bit pattern from the upper case by a single bit, which simplified case-insensitive character matching and the construction of keyboards and printers. The X3 committee made other changes, including other new characters (the brace and vertical line characters), renaming some control characters (SOM became start of header (SOH)) and moving or removing others (RU was removed). ASCII was subsequently updated as USASI X3.4-1967, then USASI X3.4-1968, ANSI X3.4-1977, and finally, ANSI X3.4-1986 (the first two are occasionally retronamed ANSI X3.4-1967, and ANSI X3.4-1968). The X3 committee also addressed how ASCII should be transmitted (least significant bit first), and how it should be recorded on perforated tape. They proposed a 9-track standard for magnetic tape, and attempted to deal with some forms of punched card formats. ASCII itself was first used commercially during 1963 as a seven-bit teleprinter code for American Telephone & Telegraph's TWX (Teletype Wide-area eXchange) network. TWX originally used the earlier five-bit Baudot code, which was also used by the competing Telex teleprinter system. Bob Bemer introduced features such as the escape sequence. His British colleague Hugh McGregor Ross helped to popularize this work—according to Bemer, "so much so that the code that was to become ASCII was first called the Bemer-Ross Code in Europe". Because of his extensive work on ASCII, Bemer has been called "the father of ASCII." I have also approved recommendations of the Secretary of Commerce regarding standards for recording the Standard Code for Information Interchange on magnetic tapes and paper tapes when they are used in computer operations. All computers and related equipment configurations brought into the Federal Government inventory on and after July 1, 1969, must have the capability to use the Standard Code for Information Interchange and the formats prescribed by the magnetic tape and paper tape standards when these media are used. Other international standards bodies have ratified character encodings such as IEC 646 that are identical or nearly identical to ASCII, with extensions for characters outside the English alphabet and symbols used outside the United States, such as the symbol for the United Kingdom's pound sterling (£). Almost every country needed an adapted version of ASCII since ASCII only suited the needs of the USA and a few other countries. For example, Canada had its own version that supported French characters. Other adapted encodings include ISCII (India), VISCII (Vietnam), and YUSCII (Yugoslavia). Although these encodings are sometimes referred to as ASCII, true ASCII is defined strictly only by ANSI standard. ASCII was incorporated into the Unicode character set as the first 128 symbols, so the ASCII characters have the same numeric codes in both sets. This allows UTF-8 to be backward compatible with ASCII, a significant advantage.
ASCII reserves the first 32 codes (numbers 0–31 decimal) for control characters: codes originally intended not to represent printable information, but rather to control devices (such as printers) that make use of ASCII, or to provide meta-information about data streams such as those stored on magnetic tape. For example, character 10 represents the "line feed" function (which causes a printer to advance its paper), and character 8 represents "backspace". RFC 2822 refers to control characters that do not include carriage return, line feed or white space as non-whitespace control characters. Except for the control characters that prescribe elementary line-oriented formatting, ASCII does not define any mechanism for describing the structure or appearance of text within a document. Other schemes, such as markup languages, address page and document layout and formatting. The original ASCII standard used only short descriptive phrases for each control character. The ambiguity this caused was sometimes intentional (where a character would be used slightly differently on a terminal link than on a data stream) and sometimes accidental (such as what "delete" means). Probably the most influential single device on the interpretation of these characters was the ASR-33 Teletype series, which was a printing terminal with an available paper tape reader/punch option. Paper tape was a very popular medium for long-term program storage through the 1980s, less costly and in some ways less fragile than magnetic tape. In particular, the Teletype 33 machine assignments for codes 17 (Control-Q, DC1, also known as XON), 19 (Control-S, DC3, also known as XOFF), and 127 (DELete) became de facto standards. Because the keytop for the O key also showed a left-arrow symbol (from ASCII-1963, which had this character instead of underscore), a noncompliant use of code 15 (Control-O, Shift In) interpreted as "delete previous character" was also adopted by many early timesharing systems but eventually became neglected. The use of Control-S (XOFF, an abbreviation for transmit off) as a "handshaking" signal warning a sender to stop transmission because of impending overflow, and Control-Q (XON, "transmit on") to resume sending, persists to this day in many systems as a manual output control technique. On some systems Control-S retains its meaning but Control-Q is replaced by a second Control-S to resume output. Code 127 is officially named "delete" but the Teletype label was "rubout". Since the original standard did not give detailed interpretation for most control codes, interpretations of this code varied. The original Teletype meaning, and the intent of the standard, was to make it an ignored character, the same as NUL (all zeroes). This was useful specifically for paper tape, because punching the all-ones bit pattern on top of an existing mark would obliterate it. Tapes designed to be "hand edited" could even be produced with spaces of extra NULs (blank tape) so that a block of characters could be "rubbed out" and then replacements put into the empty space. As video terminals began to replace printing ones, the value of the "rubout" character was lost. DEC systems, for example, interpreted "Delete" to mean "remove the character before the cursor," and this interpretation also became common in Unix systems. Most other systems used "Backspace" for that meaning and used "Delete" to mean "remove the character at the cursor". That latter interpretation is the most common now. Many more of the control codes have been given meanings quite different from their original ones. The "escape" character (code 27), for example, was intended originally to allow sending other control characters as literals instead of invoking their meaning. This is the same meaning of "escape" encountered in URL encodings, C language strings, and other systems where certain characters have a reserved meaning. Over time this meaning has been co-opted and has eventually been changed. In modern use, an ESC sent to the terminal usually indicates the start of a command sequence, usually in the form of a so-called "ANSI escape code" (or, more properly, a "Control Sequence Introducer") beginning with ESC followed by a "[" (left-bracket) character. An ESC sent from the terminal is most often used as an out-of-band character used to terminate an operation, as in the TECO and vi text editors. The inherent ambiguity of many control characters, combined with their historical usage, created problems when transferring "plain text" files between systems. The best example of this is the newline problem on various operating systems. Teletypes required that a line of text be terminated with both "Carriage Return" and "Linefeed". The first returns the printing carriage to the beginning of the line and the second advances to the next line without moving the carriage. However, requiring two characters to mark the end of a line introduced unnecessary complexity and questions as to how to interpret each character when encountered alone. To simplify matters, plain text files on Unix and Amiga systems use line feeds alone to separate lines. Similarly, older Macintosh systems, among others, use only carriage returns in plain text files. Various DEC operating systems used both characters to mark the end of a line, perhaps for compatibility with teletypes. This de facto standard was copied into M and then into MS-DOS and eventually into Microsoft Windows. Transmission of text over the Internet, for protocols as E-mail and the World Wide Web, uses both characters. The pre-VMS DEC operating systems, along with CP/M, tracked file length only in units of disk blocks and used Control-Z (SUB) to mark the end of the actual text in the file (also done for CP/M compatibility in some cases in MS-DOS, though MS-DOS 2 added the ability to record exact file lengths and this is usually relied on today). Text strings ending with the null character are known as ASCIZ or C strings.
As computer technology spread throughout the world, different standards bodies and corporations developed many variations of ASCII to facilitate the expression of non-English languages that used Roman-based alphabets. One could class some of these variations as "ASCII extensions", although some misuse that term to represent all variants, including those that do not preserve ASCII's character-map in the 7-bit range. The PETSCII code Commodore International used for their 8-bit systems is probably unique among post-1970 codes in being based on ASCII-1963, instead of the more common ASCII-1967, such as found on the ZX Spectrum computer. Atari and Galaksija computers also used ASCII variants.
From early in its development, ASCII was intended to be just one of several national variants of an international character code standard, ultimately published as IEC 646 (1972), which would share most characters in common but assign other locally-useful characters to several code points reserved for "national use." However, the four years that elapsed between the publication of ASCII-1963 and ISO's first acceptance of an international recommendation during 1967 caused ASCII's choices for the national use characters to seem to be de facto standards for the world, causing confusion and incompatibility once other countries did begin to make their own assignments to these code points. ISO/IEC 646, like ASCII, was a 7-bit character set. It did not make any additional codes available, so the same code points encoded different characters in different countries. Escape codes were defined to indicate which national variant applied to a piece of text, but they were rarely used, so it was often impossible to know what variant to work with and therefore which character a code represented, and text-processing systems could generally cope with only one variant anyway. Because the bracket and brace characters of ASCII were assigned to "national use" code points that were used for accented letters in other national variants of ISO/IEC 646, a German, French, or Swedish, etc., programmer had to get used to reading and writing
C trigraphs were created to solve this problem for ANSI C, although their late introduction and inconsistent implementation in compilers limited their use. Eventually, as 8-, 16-, and 32-bit computers began to replace 18- and 36-bit computers as the norm, it became common to use an 8-bit byte to store each character in memory, providing an opportunity for extended, 8-bit, relatives of ASCII, with the 128 additional characters providing room to avoid most of the ambiguity that had been necessary in 7-bit codes. For example, IBM developed 8-bit code pages, such as code page 437, which replaced the control-characters with graphic symbols such as smiley faces, and mapped additional graphic characters to the upper 128 positions. Operating systems such as DOS supported these code-pages, and manufacturers of IBM PCs supported them in hardware. Digital Equipment Corporation developed the Multinational Character Set (DEC-MCS) for use in the popular VT220 terminal. Eight-bit standards such as IEC 8859 (derived from the DEC-MCS) and Mac OS Roman developed as true extensions of ASCII, leaving the original character-mapping intact, but adding additional character definitions after the first 128 (i.e., 7-bit) characters. This enabled representation of characters used in a broader range of languages. Because there were several competing 8-bit code standards, they continued to suffer from incompatibilities and limitations. Still, ISO-8859-1 (Latin 1), its variant Windows-1252 (often mislabeled as ISO-8859-1), and the original 7-bit ASCII remain the most common character encodings in use today.
Unicode and the ISO/IEC 10646 Universal Character Set (UCS) have a much wider array of characters, and their various encoding forms have begun to supplant ISO/IEC 8859 and ASCII rapidly in many environments. While ASCII is limited to 128 characters, Unicode and the UCS support more characters by separating the concepts of unique identification (using natural numbers called "code points") and encoding (to 8-, 16- or 32-bit binary formats, called UTF-8, UTF-16 and UTF-32). To allow backward compatibility, the 128 ASCII and 256 ISO-8859-1 (Latin 1) characters are assigned Unicode/UCS code points that are the same as their codes in the earlier standards. Therefore, ASCII can be considered a 7-bit encoding scheme for a very small subset of Unicode/UCS, and, conversely, the UTF-8 encoding forms are binary-compatible with ASCII for code points below 128, meaning all ASCII is valid UTF-8. The other encoding forms resemble ASCII in how they represent the first 128 characters of Unicode, but use 16 or 32 bits per character, so they require conversion for compatibility. (similarly UCS-2 is upwards compatible with UTF-16)
The bouncing ball animation (below) consists of these 6 frames. This animation moves at 10 frames per second. Animation is the rapid display of a sequence of images of 2-D or 3-D artwork or model positions in order to create an illusion of movement. It is an optical illusion of motion due to the phenomenon of persistence of vision, and can be created and demonstrated in a number of ways. The most common method of presenting animation is as a motion picture or video program, although several other forms of presenting animation also exist.
Early examples of attempts to capture the phenomenon of motion drawing can be found in paleolithic cave paintings, where animals are depicted with multiple legs in superimposed positions, clearly attempting to convey the perception of motion. A 5,200 year old earthen bowl found in Iran in Shahr-i Sokhta has five images of a goat painted along the sides. This has been claimed to be an example of early animation. However, since no equipment existed to show the images in motion, such a series of images cannot be called animation in a true sense of the word. The phenakistoscope, praxinoscope, as well as the common flip book were early popular animation devices invented during the 1800s, while a Chinese zoetrope-type device was invented already in 180 AD. These devices produced movement from sequential drawings using technological means, but animation did not really develop much further until the advent of cinematography. There is no single person who can be considered the "creator" of the art of film animation, as there were several people doing several projects which could be considered various types of animation all around the same time. Georges Méliès was a creator of special-effect films; he was generally one of the first people to use animation with his technique. He discovered a technique by accident which was to stop the camera rolling to change something in the scene, and then continue rolling the film. This idea was later known as stop-motion animation. Méliès discovered this technique accidentally when his camera broke down while shooting a bus driving by. When he had fixed the camera, a hearse happened to be passing by just as Méliès restarted rolling the film, his end result was that he had managed to make a bus transform into a hearse. This was just one of the great contributors to animation in the early years. The earliest surviving stop-motion advertising film was an English short by Arthur Melbourne-Cooper called "Matches: An Appeal" (1899). Developed for the Bryant and May Matchsticks company, it involved stop-motion animation of wired-together matches writing a patriotic call to action on a blackboard. J. Stuart Blackton was possibly the first American filmmaker to use the techniques of stop-motion and hand-drawn animation. Introduced to filmmaking by Edison, he pioneered these concepts at the turn of the 20th century, with his first copyrighted work dated 1900. Several of his films, among them "The Enchanted Drawing" (1900) and "Humorous Phases of Funny Faces" (1906) were film versions of Blackton's "lightning artist" routine, and utilized modified versions of Méliès' early stop-motion techniques to make a series of blackboard drawings appear to move and reshape themselves. 'Humorous Phases of Funny Faces' is regularly cited as the first true animated film, and Blackton is considered the first true animator. Another French artist, Émile Cohl, began drawing cartoon strips and created a film in 1908 called "Fantasmagorie". The film largely consisted of a stick figure moving about and encountering all manner of morphing objects, such as a wine bottle that transforms into a flower. There were also sections of live action where the animator’s hands would enter the scene. The film was created by drawing each frame on paper and then shooting each frame onto negative film, which gave the picture a blackboard look. This makes "Fantasmagorie" the first animated film created using what came to be known as traditional (hand-drawn) animation. Following the successes of Blackton and Cohl, many other artists began experimenting with animation. One such artist was Winsor McCay, a successful newspaper cartoonist, who created detailed animations that required a team of artists and painstaking attention for detail. Each frame was drawn on paper; which invariably required backgrounds and characters to be redrawn and animated. Among McCay's most noted films are "Little Nemo" (1911), "Gertie the Dinosaur" (1914) and "The Sinking of the Lusitania" (1918). The production of animated short films, typically referred to as "cartoons", became an industry of its own during the 1910s, and cartoon shorts were produced to be shown in movie theaters. The most successful early animation producer was John Randolph Bray, who, along with animator Earl Hurd, patented the cel animation process which dominated the animation industry for the rest of the decade.
Traditional animation (also called cel animation or hand-drawn animation) was the process used for most animated films of the 20th century. The individual frames of a traditionally animated film are photographs of drawings, which are first drawn on paper. To create the illusion of movement, each drawing differs slightly from the one before it. The animators' drawings are traced or photocopied onto transparent acetate sheets called cels, which are filled in with paints in assigned colors or tones on the side opposite the line drawings. The completed character cels are photographed one-by-one onto motion picture film against a painted background by a rostrum camera. The traditional cel animation process became obsolete by the beginning of the 21st century. Today, animators' drawings and the backgrounds are either scanned into or drawn directly into a computer system. Various software programs are used to color the drawings and simulate camera movement and effects. The final animated piece is output to one of several delivery media, including traditional 35 mm film and newer media such as digital video. The "look" of traditional cel animation is still preserved, and the character animators' work has remained essentially the same over the past 70 years. Some animation producers have used the term "tradigital" to describe cel animation which makes extensive use of computer technology. Examples of traditionally animated feature films include "Pinocchio" (United States, 1940), "Animal Farm" (United Kingdom, 1954), and "Akira" (Japan, 1988). Traditional animated films which were produced with the aid of computer technology include "The Lion King" (US, 1994) "Sen to Chihiro no Kamikakushi (Spirited Away)" (Japan, 2001), "Treasure Planet" (USA, 2002) and "Les Triplettes de Belleville" (2003).
3D animation are digitally modeled and manipulated by an animator. In order to manipulate a mesh, it is given a digital skeletal structure that can be used to control the mesh. This process is called rigging. Various other techniques can be applied, such as mathematical functions (ex. gravity, particle simulations), simulated fur or hair, effects such as fire and water and the use of Motion capture to name but a few, these techniques fall under the category of 3d dynamics. Many 3D animations are very believable and are commonly used as Visual effects for recent movies.
In Greek and Roman mythology, Apollo (in Greek, "Ἀπόλλων"—"Apóllōn" or "Ἀπέλλων"—"Apellōn"), is one of the most important and diverse of the Olympian deities. The ideal of the "kouros" (a beardless youth), Apollo has been variously recognized as a god of light and the sun; truth and prophecy; archery; medicin, healing and plague; music, poetry, and the arts; and more. Apollo is the son of Zeus and Leto, and has a twin sister, the chaste huntress Artemis. Apollo is known in Greek-influenced Etruscan mythology as "Apulu". Apollo was worshiped in both ancient Greek and Roman religion, as well as in the modern Greco–Roman Neopaganism. As the patron of Delphi ("Pythian Apollo"), Apollo was an oracular god — the prophetic deity of the Delphic Oracle. Medicine and healing were associated with Apollo, whether through the god himself or mediated through his son Asclepius, yet Apollo was also seen as a god who could bring ill-health and deadly plague as well as one who had the ability to cure. Amongst the god's custodial charges, Apollo became associated with dominion over colonists, and as the patron defender of herds and flocks. As the leader of the Muses ("Apollon Musagetes") and director of their choir, Apollo functioned as the patron god of music and poetry. Hermes created the lyre for him, and the instrument became a common attribute of Apollo. Hymns sung to Apollo were called paeans. In Hellenistic times, especially during the third century BCE, as "Apollo Helios" he became identified among Greeks with Helios, god of the sun, and his sister Artemis similarly equated with Selene, goddess of the moon. In Latin texts, on the other hand, Joseph Fontenrose declared himself unable to find any conflation of Apollo with Sol among the Augustan poets of the first century, not even in the conjurations of Aeneas and Latinus in "Aeneid" XII (161–215). Apollo and Helios/Sol remained separate beings in literary and mythological texts until the third century CE.
The etymology of "Apollo" is uncertain. Several instances of popular etymology are attested from ancient authors. Thus, Plato in "Cratylus" connects the name with "redeem", with "purification", and with "simple", in particular in reference to the Thessalian form of the name, and finally with "ever-shooting". Hesychius connects the name Apollo with the Doric απελλα, which means "assembly", so that Apollo would be the god of political life, and he also gives the explanation σηκος ("fold"), in which case Apollo would be the god of flocks and herds. It is also possible that "apellai" derives from an old form of Apollo which can be equated with Appaliunas, an Anatolian god whose name possibly means "father lion" or "father light". The Greeks later associated Apollo's name with the Greek verb απολλυμι (apollymi) meaning "to destroy". It has also been suggested that Apollo comes from the Hurrian and Hittite divinity, Aplu, who was widely evoked during the "plague years". Aplu, it is suggested, comes from the Akkadian "Aplu Enlil", meaning "the son of Enlil", a title that was given to the god Nergal, who was linked to Shamash, Babylonian god of the sun.
There are generally two broad opinions on the origins of Apollo: one derives him from the East, the other connects him to the Dorians and their apellai (cf. also the month Apellaios). In any case, Walter Burkert notes that components of various origins are discernible in his worship: a Dorian Greek, a Cretan-Minoan and a Syro-Hittite. According to the first opinion, both Greek and Etruscan Apollo came to the Aegean during the Iron Age (i.e. from c.1100 BCE to c. 800 BCE) from Anatolia. Homer pictures him on the side of the Trojans, against the Achaeans, during the Trojan War and he has close affiliations with a Luwian deity, Apaliunas, who in turn seems to have traveled west from further east. The Late Bronze Age (from 1700–1200 BCE) Hittite and Hurrian "Aplu", like the Homeric Apollo, was a god of plagues, and resembles the mouse god "Apollo Smintheus". Here we have an apotropaic situation, where a god originally bringing the plague was invoked to end it, merging over time through fusion with the Mycenaean healer-god Paeon (PA-JA-WO in Linear B); Paeon, in Homer's "Iliad", was the Greek healer of the wounded gods Ares and Hades. In other writers, the word becomes a mere epithet of Apollo in his capacity as a god of healing, but it is now known from Linear B that Paeon was originally a separate deity. Homer illustrated Paeon the god, as well as the song both of apotropaic thanksgiving or triumph, and Hesiod also separated the two; in later poetry Paeon was invoked independently as a god of healing. It is equally difficult to separate Paeon or Paean in the sense of "healer" from Paean in the sense of "song." Such songs were originally addressed to Apollo, and afterwards to other gods, Dionysus, Helios, Asclepius. About the fourth century BCE, the paean became merely a formula of adulation; its object was either to implore protection against disease and misfortune, or to offer thanks after such protection had been rendered. It was in this way that Apollo had become recognised as the god of music. Apollo's role as the slayer of the Python led to his association with battle and victory; hence it became the Roman custom for a paean to be sung by an army on the march and before entering into battle, when a fleet left the harbour, and also after a victory had been won. Apollo's links with oracles again seem to be associated with wishing to know the outcome of an illness. He is a god of music and the lyre. Healing belongs to his realm: he was the father of Asclepius, the god of medicine. The Muses are part of his retinue, so that music, history, poetry and dance all belong to him.
Unusually among the Olympic deities, Apollo had two cult sites that had widespread influence: Delos and Delphi. In cult practice, Delian Apollo and Pythian Apollo (the Apollo of Delphi) were so distinct that they might both have shrines in the same locality. Theophoric names such as "Apollodorus" or "Apollonios" and cities named Apollonia are met with throughout the Greek world. Apollo's cult was already fully established when written sources commenced, about 650 BCE.
Apollo's most common attributes were the bow and arrow. Other attributes of his included the kithara (an advanced version of the common lyre), the plectrum and the sword. Another common emblem was the sacrificial tripod, representing his prophetic powers. The Pythian Games were held in Apollo's honor every four years at Delphi. The bay laurel plant was used in expiatory sacrifices and in making the crown of victory at these games. The palm was also sacred to Apollo because he had been born under one in Delos. Animals sacred to Apollo included wolves, dolphins, roe deer, swans, cicadas (symbolizing music and song), hawks, ravens, crows, snakes (referencing Apollo's function as the god of prophecy), mice and griffins, mythical eagle–lion hybrids of Eastern origin. As god of colonization, Apollo gave oracular guidance on colonies, especially during the height of colonization, 750–550 BCE. According to Greek tradition, he helped Cretan or Arcadian colonists found the city of Troy. However, this story may reflect a cultural influence which had the reverse direction: Hittite cuneiform texts mention a Minor Asian god called "Appaliunas" or "Apalunas" in connection with the city of Wilusa attested in Hittite inscriptions, which is now generally regarded as being identical with the Greek Ilion by most scholars. In this interpretation, Apollo's title of "Lykegenes" can simply be read as "born in Lycia", which effectively severs the god's supposed link with wolves (possibly a folk etymology). In literary contexts, Apollo represents harmony, order, and reason—characteristics contrasted with those of Dionysus, god of wine, who represents ecstasy and disorder. The contrast between the roles of these gods is reflected in the adjectives Apollonian and Dionysian. However, the Greeks thought of the two qualities as complementary: the two gods are brothers, and when Apollo at winter left for Hyperborea, he would leave the Delphic oracle to Dionysus. This contrast appears to be shown on the two sides of the Borghese Vase. Apollo is often associated with the Golden Mean. This is the Greek ideal of moderation and a virtue that opposes gluttony.
The Roman worship of Apollo was adopted from the Greeks. As a quintessentially Greek god, Apollo had no direct Roman equivalent, although later Roman poets often referred to him as Phoebus. There was a tradition that the Delphic oracle was consulted as early as the period of the kings of Rome during the reign of Tarquinius Superbus. On the occasion of a pestilence in the 430s BC, Apollo's first temple at Rome was established in the Flaminian fields, replacing an older cult site there known as the "Apollinare". During the Second Punic War in 212 BC, the "Ludi Apollinares" ("Apollonian Games") were instituted in his honor, on the instructions of a prophecy attributed to one Marcius. In the time of Augustus, who considered himself under the special protection of Apollo and was even said to be his son, his worship developed and he became one of the chief gods of Rome. After the battle of Actium, which was fought near a sanctuary of Apollo, Augustus enlarged Apollo's temple, dedicated a portion of the spoils to him, and instituted quinquennial games in his honour. He also erected a new temple to the god on the Palatine hill. Sacrifices and prayers on the Palatine to Apollo and Diana formed the culmination of the Secular Games, held in 17 BCE to celebrate the dawn of a new era.
In art, Apollo is depicted as a handsome beardless young man, often with a kithara (as Apollo Citharoedus) or bow in his hand, or reclining on a tree (the Apollo Lykeios and Apollo Sauroctonos types). The Apollo Belvedere is a marble sculpture that was rediscovered in the late 15th century; for centuries it epitomized the ideals of Classical Antiquity for Europeans, from the Renaissance through the nineteenth century. The marble is a Hellenistic or Roman copy of a bronze original by the Greek sculptor Leochares, made between 350 and 325 BC. The lifesize so-called "Adonis" (shown at left) found in 1780 on the site of a "villa suburbana" near the Via Labicana in the Roman suburb of Centocelle and now in the Ashmolean Museum, Oxford, is identified as an Apollo by modern scholars. It was probably never intended as a cult object, but was a pastiche of several fourth-century and later Hellenistic model types, intended to please a Roman connoisseur of the second century AD, and to be displayed in his villa. In the late second century CE floor mosaic from El Djem, Roman "Thysdrus" (right), he is identifiable as Apollo Helios by his effulgent halo, though now even a god's divine nakedness is concealed by his cloak, a mark of increasing conventions of modesty in the later Empire. Another haloed Apollo in mosaic, from Hadrumentum, is in the museum at Sousse. The conventions of this representation, head tilted, lips slightly parted, large-eyed, curling hair cut in locks grazing the neck, were developed in the third century BCE to depict Alexander the Great (Bieber 1964, Yalouris 1980). Some time after this mosaic was executed, the earliest depictions of Christ will be beardless and haloed.
When Hera discovered that Leto was pregnant and that Zeus was the father, she banned Leto from giving birth on "terra firma", or the mainland, or any island. In her wanderings, Leto found the newly created floating island of Delos, which was neither mainland nor a real island, so she gave birth there. The island was surrounded by swans. Afterwards, Zeus secured Delos to the bottom of the ocean. This island later became sacred to Apollo. It is also stated that Hera kidnapped Ilithyia, the goddess of childbirth, to prevent Leto from going into labor. The other gods tricked Hera into letting her go by offering her a necklace, nine yards (8 m) long, of amber. Mythographers agree that Artemis was born first and then assisted with the birth of Apollo, or that Artemis was born one day before Apollo, on the island of Ortygia and that she helped Leto cross the sea to Delos the next day to give birth to Apollo. Apollo was born on the seventh day () of the month Thargelion —according to Delian tradition— or of the month Bysios— according to Delphian tradition. The seventh and twentieth, the days of the new and full moon, were ever afterwards held sacred to him.
Four days after his birth, Apollo killed the chthonic dragon Python, which lived in Delphi beside the Castalian Spring. This was the spring which emitted vapors that caused the oracle at Delphi to give her prophesies. Hera sent the serpent to hunt Leto to her death across the world. In order to protect his mother, Apollo begged Hephaestus for a bow and arrows. After receiving them, Apollo cornered Python in the sacred cave at Delphi. Apollo killed Python but had to be punished for it, since Python was a child of Gaia. Hera then sent the giant Tityos to kill Leto. This time Apollo was aided by his sister Artemis in protecting their mother. During the battle Zeus finally relented his aid and hurled Tityos down to Tartarus. There he was pegged to the rock floor, covering an area of, where a pair of vultures feasted daily on his liver.
When Zeus struck down Apollo's son Asclepius with a lightning bolt for resurrecting Hippolytus from the dead (transgressing Themis by stealing Hades's subjects), Apollo in revenge killed the Cyclopes, who had fashioned the bolt for Zeus. Apollo would have been banished to Tartarus forever, but was instead sentenced to one year of hard labor as punishment, thanks to the intercession of his mother, Leto. During this time he served as shepherd for King Admetus of Pherae in Thessaly. Admetus treated Apollo well, and, in return, the god conferred great benefits on Admetus. Apollo helped Admetus win Alcestis, the daughter of King Pelias and later convinced the Fates to let Admetus live past his time, if another took his place. But when it came time for Admetus to die, his parents, whom he had assumed would gladly die for him, refused to cooperate. Instead, Alcestis took his place, but Heracles managed to "persuade" Thanatos, the god of death, to return her to the world of the living.
Apollo shot arrows infected with the plague into the Greek encampment during the Trojan War in retribution for Agamemnon's insult to Chryses, a priest of Apollo whose daughter Chryseis had been captured. He demanded her return, and the Achaeans complied, indirectly causing the anger of Achilles, which is the theme of the "Iliad". When Diomedes injured Aeneas ("Iliad"), Apollo rescued him. First, Aphrodite tried to rescue Aeneas but Diomedes injured her as well. Aeneas was then enveloped in a cloud by Apollo, who took him to Pergamos, a sacred spot in Troy. Apollo aided Paris in the killing of Achilles by guiding the arrow of his bow into Achilles' heel. One interpretation of his motive is that it was in revenge for Achilles' sacrilege in murdering Troilus, the god's own son by Hecuba, on the very altar of the god's own temple.
The queen of Thebes and wife of Amphion, Niobe boasted of her superiority to Leto because she had fourteen children (Niobids), seven male and seven female, while Leto had only two. Apollo killed her sons as they practiced athletics, with the last begging for his life, and Artemis her daughters. Apollo and Artemis used poisoned arrows to kill them, though according to some versions of the myth, a number of the Niobids were spared (Chloris, usually). Amphion, at the sight of his dead sons, either killed himself or was killed by Apollo after swearing revenge. A devastated Niobe fled to Mount Sipylos in Asia Minor and turned into stone as she wept. Her tears formed the river Achelous. Zeus had turned all the people of Thebes to stone and so no one buried the Niobids until the ninth day after their death, when the gods themselves entombed them.
In explanation of the connection of Apollon with "daphne", the Laurel whose leaves his priestess employed at Delphi, it was told by Libanius, a fourth-century CE teacher of rhetoric, that Apollo chased a nymph, Daphne, daughter of Peneus, who had scorned him. In Ovid's telling for a Roman audience, Phoebus Apollo chaffs Cupid for toying with a man's weapon suited to a man, whereupon Cupid wounds him with an arrow with a golden dart; simultaneously, however, Eros had shot a leaden arrow into Daphne, causing her to be repulsed by Apollo. Following a spirited chase by Apollo, Daphne prayed to Mother Earth, or, alternatively, her father — a river god — to help her and he changed her into the Laurel tree, sacred to Apollo. Apollo had an affair with a human princess named Leucothea, daughter of Orchamus and sister of Clytia. Leucothea loved Apollo who disguised himself as Leucothea's mother to gain entrance to her chambers. Clytia, jealous of her sister because she wanted Apollo for herself, told Orchamus the truth, betraying her sister's trust and confidence in her. Enraged, Orchamus ordered Leucothea to be buried alive. Apollo refused to forgive Clytia for betraying his beloved, and a grieving Clytia wilted and slowly died. Apollo changed her into an incense plant, either heliotrope or sunflower, which follows the sun every day. Marpessa was kidnapped by Idas but was loved by Apollo as well. Zeus made her choose between them, and she chose Idas on the grounds that Apollo, being immortal, would tire of her when she grew old. Castalia was a nymph whom Apollo loved. She fled from him and dived into the spring at Delphi, at the base of Mt. Parnassos, which was then named after her. Water from this spring was sacred; it was used to clean the Delphian temples and inspire poets. By Cyrene, Apollo had a son named Aristaeus, who became the patron god of cattle, fruit trees, hunting, husbandry and bee-keeping. He was also a culture-hero and taught humanity dairy skills and the use of nets and traps in hunting, as well as how to cultivate olives. With Hecuba, wife of King Priam of Troy, Apollo had a son named Troilus. An oracle prophesied that Troy would not be defeated as long as Troilus reached the age of twenty alive. He was ambushed and killed by Achilles. Apollo also fell in love with Cassandra, daughter of Hecuba and Priam, and Troilus' half-sister. He promised Cassandra the gift of prophecy to seduce her, but she rejected him afterwards. Enraged, Apollo indeed gifted her with the ability to know the future, with a curse that she could only see the future tragedies and that no one would ever believe her. Coronis, daughter of Phlegyas, King of the Lapiths, was another of Apollo's liaisons. Pregnant with Asclepius, Coronis fell in love with Ischys, son of Elatus. A crow informed Apollo of the affair. When first informed he disbelieved the crow and turned all crows black (where they were previously white) as a punishment for spreading untruths. When he found out the truth he sent his sister, Artemis, to kill Coronis (in other stories, Apollo himself had killed Coronis). As a result he also made the crow sacred and gave them the task of announcing important deaths. Apollo rescued the baby and gave it to the centaur Chiron to raise. Phlegyas was irate after the death of his daughter and burned the Temple of Apollo at Delphi. Apollo then killed him for what he did. In Euripides' play "Ion", Apollo fathered Ion by Creusa, wife of Xuthus. Creusa left Ion to die in the wild, but Apollo asked Hermes to save the child and bring him to the oracle at Delphi, where he was raised by a priestess. One of his other liaisons was with Acantha, the spirit of the acanthus tree. Upon her death, Apollo transformed her into a sun-loving herb. According to the "Biblioteca", or "library" of mythology mis-attributed to Apollodorus, he fathered the Corybantes on the Muse Thalia.
Hyacinth (or Hyacinthus) was one of his male lovers. Hyacinthus was a Spartan prince, beautiful and athletic. The pair were practicing throwing the discus when a discus thrown by Apollo was blown off course by the jealous Zephyrus and struck Hyacinthus in the head, killing him instantly. Apollo is said to be filled with grief: out of Hyacinthus' blood, Apollo created a flower named after him as a memorial to his death, and his tears stained the flower petals with "άί" "άί", meaning alas. The Festival of Hyacinthus was a celebration of Sparta. Another male lover was Cyparissus, a descendant of Heracles. Apollo gave him a tame deer as a companion but Cyparissus accidentally killed it with a javelin as it lay asleep in the undergrowth. Cyparissus asked Apollo to let his tears fall forever. Apollo granted the request by turning him into the Cypress named after him, which was said to be a sad tree because the sap forms droplets like tears on the trunk.
Hermes was born on Mount Cyllene in Arcadia. The story is told in the Homeric Hymn to Hermes. His mother, Maia, had been secretly impregnated by Zeus. Maia wrapped the infant in blankets but Hermes escaped while she was asleep. Hermes ran to Thessaly, where Apollo was grazing his cattle. The infant Hermes stole a number of his cows and took them to a cave in the woods near Pylos, covering their tracks. In the cave, he found a tortoise and killed it, then removed the insides. He used one of the cow's intestines and the tortoise shell and made the first lyre. Apollo complained to Maia that her son had stolen his cattle, but Hermes had already replaced himself in the blankets she had wrapped him in, so Maia refused to believe Apollo's claim. Zeus intervened and, claiming to have seen the events, sided with Apollo. Hermes then began to play music on the lyre he had invented. Apollo, a god of music, fell in love with the instrument and offered to allow exchange of the cattle for the lyre. Hence, Apollo became a master of the lyre.
Apollo gave the order through the Oracle at Delphi, for Orestes to kill his mother, Clytemnestra, and her lover, Aegisthus. Orestes was punished fiercely by the Erinyes (the Furies, female personifications of vengeance) for this crime. Relentlessly pursued by the Furies, Orestes asked for the intercession of Athena, who decreed that he be tried by a jury of his peers, with Apollo acting as his advocate. In the Odyssey, Odysseus and his surviving crew landed on an island sacred to Helios the sun god, where he kept sacred cattle. Though Odysseus warned his men not to (as Tiresias and Circe had told him), they killed and ate some of the cattle and Helios had Zeus destroy the ship and all the men, except Odysseus. Apollo also had a lyre-playing contest with Cinyras, his son, who committed suicide when he lost. Apollo killed the Aloadae when they attempted to storm Mt. Olympus. Callimachus sang that Apollo rode on the back of a swan to the land of the Hyperboreans during the winter months. Apollo turned Cephissus into a sea monster.
Once Pan had the audacity to compare his music with that of Apollo, and to challenge Apollo, the god of the kithara, to a trial of skill. Tmolus, the mountain-god, was chosen to umpire. Pan blew on his pipes, and with his rustic melody gave great satisfaction to himself and his faithful follower, Midas, who happened to be present. Then Apollo struck the strings of his lyre. Tmolus at once awarded the victory to Apollo, and all but Midas agreed with the judgment. He dissented, and questioned the justice of the award. Apollo would not suffer such a depraved pair of ears any longer, and caused them to become the ears of a donkey.
Apollo has ominous aspects aside from his plague-bringing, death-dealing arrows: Marsyas was a satyr who challenged Apollo to a contest of music. He had found an aulos on the ground, tossed away after being invented by Athena because it made her cheeks puffy. The contest was judged by the Muses. After they each performed, both were deemed equal until Apollo decreed they play and sing at the same time. As Apollo played the lyre, this was easy to do. Marsyas could not do this as he only knew how to use the flute and could not sing at the same time. Apollo was declared the winner because of this. Apollo flayed Marsyas alive in a cave near Celaenae in Phrygia for his hubris to challenge a god. He then nailed Marsyas' shaggy skin to a nearby pine-tree. Marsyas' blood turned into the river Marsyas. Another variation is that Apollo played his instrument (the lyre) upside down. Marsyas could not do this with his instrument (the flute), and so Apollo hung him from a tree and flayed him alive. Graeco–Roman epithets and cult titles. Apollo, like other Greek deities, had a number of epithets applied to him, reflecting the variety of roles, duties, and aspects ascribed to the god. However, while Apollo has a great number of appellations in Greek myth, only a few occur in Latin literature, chief among them Phoebus ("shining one"), which was very commonly used by both the Greeks and Romans in Apollo's role as the god of light. In Apollo's role as healer, his appellations included Akesios, Iatros, and Acestor meaning "healer". He was also called Alexicacus ("restrainer of evil") and Apotropaeus ("he who averts evil"), and was referred to by the Romans as Averruncus ("averter of evils"). As a plague god and defender against rats and locusts, Apollo was known as Smintheus ("mouse-catcher") and Parnopius ("grasshopper"). The Romans also called Apollo Culicarius ("driving away midges"). In his healing aspect, the Romans referred to Apollo as Medicus ("the Physician"), and a temple was dedicated to "Apollo Medicus" at Rome, probably next to the temple of Bellona. As a sun-god he was worshiped as Aegletes, the radiant god. As a god of archery, Apollo was known as Aphetoros ("god of the bow") and Argurotoxos ("with the silver bow"). The Romans referred to Apollo as Articenens ("carrying the bow") as well. As a pastoral shepherd-god, Apollo was known as Nomios ("wandering"). As the protector of roads and homes he was Agyieus. Apollo was also known as Archegetes ("director of the foundation"), who oversaw colonies. He was known as Klarios, from the Doric "klaros" ("allotment of land"), for his supervision over cities and colonies. He was known as Delphinios ("Delphinian"), meaning "of the womb", in his association with "Delphoi" (Delphi). At Delphi, he was also known as Pythios ("Pythian"). An aitiology in the Homeric hymns connects the epitheton to dolphins. Kynthios, another common epithet, stemmed from his birth on Mt. Cynthus. He was also known as Lyceios or Lykegenes, which either meant "wolfish" or "of Lycia", Lycia being the place where some postulate that his cult originated. Specifically as god of prophecy, Apollo was known as Loxias ("the obscure"). He was also known as Coelispex ("he who watches the heavens") to the Romans. Apollo was attributed the epithet Musagetes as the leader of the muses, and Nymphegetes as "nymph-leader". Acesius was the epithet of Apollo worshipped in Elis, where he had a temple in the agora. This surname, which has the same meaning as "akestor" and "alexikakos", characterized the god as the averter of evil. Acraephius or Acraephiaeus was his epithet worshipped in the Boeotian town of Acraephia, reputedly founded by his son, Acraepheus. Actiacus was his epithet in Actium, one of the principal places of his worship. Celtic epithets and cult titles. Apollo was worshipped throughout the Roman Empire. In the traditionally Celtic lands he was most often seen as a healing and sun god. He was often equated with Celtic gods of similar character.
Apollo has often featured in postclassical art and literature. Percy Bysshe Shelley composed a "Hymn of Apollo" (1820), and the god's instruction of the Muses formed the subject of Igor Stravinsky's "Apollon musagète" (1927–1928). The name "Apollo" was given to NASA's Apollo Lunar program in the 1960s. The statue of Apollo from the west pediment of the Temple of Zeus at Olympia (currently in the Archaeological Museum of Olympia) was depicted on the obverse of the Greek 1000 drachmas banknote of 1987–2001.
Andre Kirk Agassi (; born April 29, 1970) is an American former World No. 1 professional tennis player who won eight Grand Slam singles tournaments and an Olympic gold medal in singles. Generally considered by critics and fellow players to be one of the greatest tennis players of all time, he has been called the best service returner in the history of tennis. Known for his unorthodox apparel and attitude, Agassi is often cited as one of the most charismatic players in the history of the game, and is credited for helping revive the popularity of tennis during the 1990s. He is married to fellow retired professional tennis player and multiple Grand Slam champion Steffi Graf. Agassi is, with Rod Laver, Don Budge, Fred Perry, Roy Emerson, and Roger Federer, one of only six men to have achieved a Career Grand Slam, one of only three (with Laver and Federer) since the beginning of the Open Era, and the only male player to have achieved a Career Golden Slam. In addition to his Grand Slam and Olympic singles titles, he won the Tennis Masters Cup and was part of a winning Davis Cup team. He won 17 ATP Masters Series tournaments, more than any other player. Agassi's Grand Slam composition is (4 Australian Open, 1 French Open, 1 Wimbledon, 2 US Open) for his career. After suffering from sciatica caused by two bulging discs in his back, a spondylolisthesis (vertebral displacement) and a bone spur that interferes with the nerve, Agassi retired from professional tennis on September 3, 2006, after losing in the third round of the US Open. He is the founder of the Andre Agassi Charitable Foundation, which has raised over $60 million for at-risk children in Southern Nevada. In 2001, the Foundation opened the Andre Agassi College Preparatory Academy in Las Vegas, a K-12 public charter school for at-risk children.
Agassi was born in Las Vegas, Nevada, to Emmanuel "Mike" Aghassian and Elizabeth "Betty" Agassi (née Dudley). His father is an Iranian of Armenian and Assyrian ethnicity who represented Iran in boxing at the 1948 and 1952 Olympic Games before emigrating to the United States. Andre Agassi's mother, Betty, is a breast cancer survivor. Mike Agassi was renowned for his domineering nature, reportedly taking a hammer to matches and banging on the fences in disgust when Andre lost a point. He sometimes screamed at officials and was ejected more than once. At age 13, Andre was sent to Nick Bollettieri's Tennis Academy in Florida. He was meant to stay for only 3 months because that was all his father could afford. However, after ten minutes of watching Agassi rally, Bollettieri called Mike and said: "Take your check back. He's here for free," claiming that Agassi had more natural talent than anyone else he had seen.
He turned professional at the age of 16 and his first tournament was in La Quinta, California. He won his first match against John Austin 6–4, 6–2 but then lost his second match to Mats Wilander 6–1, 6–1. By the end of the year, Agassi was ranked World No. 91. Agassi won his first top-level singles title in 1987 at the Sul American Open in Itaparica. He ended the year ranked World No. 25. He won six additional tournaments in 1988 (Memphis, U.S. Men's Clay Court Championships, Forest Hills WCT, Stuttgart Outdoor, Volvo International and Livingston Open), and, by December of that year, he had surpassed US$2 million in career prize money after playing in just 43 tournaments – the fastest anyone in history had reached that level. His year-end ranking was World No. 3, behind second-ranked Ivan Lendl and top-ranked Mats Wilander. Both the Association of Tennis Professionals and "Tennis" magazine named Agassi the Most Improved Player of the Year for 1988. In addition to not playing the Australian Open (which would later become his best Grand Slam event) for the first eight years of his career, Agassi chose not to play at Wimbledon from 1988 through 1990 and publicly stated that he did not wish to play there because of the event's traditionalism, particularly its "predominantly white" dress code to which players at the event are required to conform. Strong performances on the tour meant that Agassi was quickly tipped as a future Grand Slam champion. While still a teenager, he reached the semi-finals of both the French Open and the US Open in 1988, and made the US Open semifinals in 1989. He began the 1990s, however, with a series of near-misses. He reached his first Grand Slam final in 1990 at the French Open, where he was favored before losing in four sets to Andrés Gómez. He reached his second Grand Slam final of the year at the US Open, defeating defending champion Boris Becker in the semifinals. His opponent in the final was Pete Sampras; a year earlier, Agassi had beaten Sampras 6-2, 6-1 after which he told his coach that he felt bad for Sampras because he was never going to make it as a pro. Agassi lost the US Open final to Sampras 6–4, 6–3, 6–2. The rivalry between these two American players became the dominant rivalry in tennis over the rest of the decade. Also in 1990, Agassi helped the United States win its first Davis Cup in 8 years and won his only Tennis Masters Cup, beating reigning Wimbledon champion Stefan Edberg in the final. In 1991, Agassi reached his second consecutive French Open final, where he faced fellow Bollettieri Academy alumnus Jim Courier. Courier emerged the victor in a five set final. Agassi decided to play at Wimbledon in 1991, leading to weeks of speculation in the media about the clothes he would wear. He eventually emerged for the first round in a completely white outfit. He went on to reach the quarter-finals on that occasion, losing in five sets to David Wheaton. Agassi's Grand Slam tournament breakthrough came at Wimbledon, not at the French Open or the US Open where he had previously enjoyed success. In 1992, he defeated Goran Ivanišević in a five set final. Along the way, Agassi overcame two former Wimbledon champions in Boris Becker and John McEnroe. No other baseliner would triumph at Wimbledon until Lleyton Hewitt ten years later. Agassi was named the BBC Overseas Sports Personality of the Year in 1992. Agassi once again played on the United States' Davis Cup winning team in 1992. It was their second Davis cup title in three years. 1993 saw Agassi win the only doubles title of his career, at the Cincinnati Masters, partnered with Petr Korda. Agassi missed much of the early part of that year with injuries. Although he made the quarterfinals in his Wimbledon title defense, he lost to eventual champion and World number one Pete Sampras in five-sets. Agassi lost in the first-round at the US Open to Thomas Enqvist and required wrist surgery late in the year.
With new coach Brad Gilbert on board, Agassi began to employ more of a tactical, consistent approach, which fueled his resurgence. Agassi started slowly in 1994, losing in the first week at the French Open and Wimbledon. Nevertheless, Agassi emerged during the hard court season, winning the Canadian Open. His comeback culminated at the 1994 US Open with a 5-set fourth-round victory against compatriot Michael Chang and then becoming the first man to capture the US Open as an unseeded player, beating Michael Stich in the final. In 1995, Agassi shaved his balding head, breaking with his old "image is everything" style. He competed in the 1995 Australian Open (his first appearance at the event) and won, beating Sampras in a four set final. Agassi and Sampras met in five tournament finals in 1995, all on hardcourt, with Agassi winning three. Agassi won three Masters Series events in 1995 (Cincinnati, Key Biscayne, and the Canadian Open) and seven titles total. He compiled a career-best 26-match winning streak during the summer hardcourt circuit, which ended when he lost the US Open final to Sampras. Agassi reached the World No. 1 ranking for the first time in April 1995. He held that ranking until November, for a total of 30 weeks. In terms of win/loss record, 1995 was Agassi's best year. He won 73 matches and lost only 9. Agassi was also once again a key player on the United States' Davis Cup winning team - the third and final Davis Cup title of Agassi's career. 1996 was a less successful year for Agassi, as he failed to reach any Grand Slam final. He suffered two early round losses at the hands of compatriots Chris Woodruff and Doug Flach at the French Open and Wimbledon, respectively, and lost to Chang in straight sets in the Australian and US Open semifinals. At the time, Agassi blamed the loss on the windy conditions but later admitted in his biography that he had tanked (lost on purpose) this match as he bore a grudge against Boris Becker whom he would have faced in the final. The high point for Agassi was winning the men's singles gold medal at the Olympic Games in Atlanta, beating Sergi Bruguera of Spain in the final 6–2, 6–3, 6–1. Agassi also successfully defended his singles titles in Cincinnati and Key Biscayne. 1997 was the low point of Agassi's career. His wrist injury resurfaced, and he played only 24 matches during the year. He would later confess that he started using crystal methamphetamine at that time, allegedly on the urging of a friend. He failed an ATP drug test, but wrote a letter claiming the same friend spiked a drink. The ATP dropped the failed drug test as a warning. He stated upon admitting to his drug use that the letter was a lie. He quit the drug soon after. He won no top-level titles and his ranking sank to World No. 141 on November 10, 1997.
In 1998, Agassi began a rigorous conditioning program and worked his way back up the rankings by playing in Challenger Series tournaments (a circuit for professional players ranked outside the world's top 50). He played some classic matches in this period, most notably against his rival Pete Sampras and popular Australian Patrick Rafter. In 1998, Agassi won five titles and leapt from World No. 122 at the start of the year to World No. 6 at the end of it, making it the highest jump into the top 10 made by any player during a single calendar year. At Wimbledon that year, he had an early loss in the second round to ATP player Tommy Haas. He won five titles in ten finals and was runner-up at the Masters Series tournament in Key Biscayne, losing to Marcelo Ríos, who became World No. 1 as a result of winning that tournament. Agassi entered the history books in 1999 when he came back from two sets to love down to beat Andrei Medvedev in a five-set French Open final, thereby becoming, at the time, only the fifth male player (joining Rod Laver, Fred Perry, Roy Emerson and Don Budge-these have since been joined by a sixth, Roger Federer) to have won all four Grand Slam singles titles during his career. This win also made him the first (of only two, the second being Roger Federer) male players in history to have won all four Grand Slam titles on three different surfaces (clay, grass, and hard courts), a tribute to his adaptability, as the other four men had won their Grand Slam titles on clay and grass courts. Agassi also became the first male player to win the Career Golden Slam, consisting of all four Grand Slam tournaments plus an Olympic gold medal. Agassi followed his 1999 French Open victory by reaching the Wimbledon final, where he lost to Sampras in straight sets. He rebounded from his Wimbledon defeat by winning the US Open, beating Todd Martin in five sets (rallying from a 2 sets to 1 deficit) in the final. Agassi ended 1999 as the World No. 1, ending Sampras's record of six consecutive year-ending top rankings (1993–1998). This was the only time Agassi ended the year at number one. Agassi began the next year by capturing his second Australian Open title, beating Sampras in a five-set semifinal and Yevgeny Kafelnikov in a four-set final. He was the first male player to have reached four consecutive Grand Slam finals since Rod Laver achieved the Grand Slam in 1969. At the time, Agassi was also only the fourth player since Laver to be the reigning champion of three of four Grand Slam events, missing only the Wimbledon title. 2000 also saw Agassi reach the semifinals at Wimbledon, where he lost in five sets to Rafter in a match considered by many to be one of the best ever played at Wimbledon. At the inaugural Tennis Masters Cup in Lisbon, Agassi reached the final after defeating Marat Safin 6–3, 6–3 in the semifinals to end the Russian's hopes to become the youngest World No. 1 in the history of tennis. Agassi then lost to Gustavo Kuerten in the final, allowing Kuerten to be crowned year-end World No. 1. Agassi opened 2001 by successfully defending his Australian Open title with a straight-sets final win over Arnaud Clément. Enroute, he beat a cramping Rafter (7–5, 2–6, 6–7, 6–2, 6–3) in front of a sell-out crowd in what turned out to be the Aussie's last Australian Open. At Wimbledon, they met again in the semifinals, where Agassi lost another close match to Rafter, 8–6 in the fifth set. In the quarterfinals at the US Open, Agassi lost a 3 hour, 33 minute epic match with Sampras 6–7(7), 7–6(7), 7–6(2), 7–6(5), with no breaks of serve during the 48-game match. Despite the setback, Agassi finished 2001 ranked World No. 3, becoming the only male tennis player to finish a year ranked in the top 10 in three different decades (1980s - finishing World No. 3 in 1988 and No. 7 in 1989; 1990s - finishing World No. 4 in 1990, No. 10 in 1991, No. 9 in 1992, No. 2 in 1994 and 1995, No. 8 in 1996, No. 6 in 1998 and No. 1 in 1999; 2000s - finishing World No. 6 in 2000, No. 3 in 2001, No. 2 in 2002, No. 4 in 2003, No. 8 in 2004 and No. 7 in 2005). He also was the oldest player (age 31) to finish in the top three since 32-year old Connors finished at World No. 2 in 1984. 2002 opened with disappointment for Agassi, as injury forced him to skip the Australian Open, where he was a two-time defending champion. The last duel between Agassi and Sampras came in the final of the US Open, which Sampras won in four sets and left Sampras with a 20–14 edge in their 34 career meetings. The match proved to be the last of Sampras's career. Agassi's US Open finish, along with his Masters Series victories in Key Biscayne, Rome, and Madrid, helped him finish 2002 as the oldest year-end World No. 2 at 32 years and 8 months. In 2003, Agassi won the eighth (and final) Grand Slam title of his career at the Australian Open, where he beat Rainer Schüttler in straight sets in the final. In March, he won his sixth career and third consecutive Key Biscayne title, in the process surpassing his wife, Steffi Graf, who was a 5-time winner of the event. The final was his 18th straight win in that tournament, which broke the previous record of 17 set by Sampras from 1993–1995. (Agassi's winning streak continued to 20 after winning his first two matches at the 2004 edition of that tournament before bowing to Agustín Calleri.) With the victory, Agassi became the youngest (19 years old) and oldest (32) winner of the Key Biscayne tournament. On April 28, 2003, he recaptured the World No. 1 ranking after a quarterfinal victory over Xavier Malisse at the Queen's Club Championships to become the oldest top ranked male player since the ATP rankings began at 33 years and 13 days. He held the World No. 1 ranking for two weeks when Lleyton Hewitt took it back on May 12, 2003. Agassi then recaptured the World No. 1 ranking once again on June 16, 2003, which he held for 12 weeks until September 7, 2003. During his career, Agassi held the World No. 1 ranking for a total of 101 weeks. Agassi's ranking slipped when injuries forced him to withdraw from many events. He did manage to reach the US Open semifinals, where he lost to Juan Carlos Ferrero and surrendered his World No. 1 ranking to Ferrero. At the year-ending Tennis Masters Cup, Agassi lost in the final to Federer and finished the year ranked World No. 4. At age 33, he was the oldest player to rank in the top five since Connors, at age 35, was World No. 4 in 1987.
In 2004, Agassi won the Masters series event in Cincinnati to bring his career total to 59 top-level singles titles and a record 17 ATP Masters Series titles, having already won seven of the nine ATP Masters tournament—all except the tournaments in Monte Carlo and Hamburg. At 34, he became the second-oldest singles champion in Cincinnati tournament history (the tournament began in 1899), surpassed only by Ken Rosewall who won the title in 1970 at age 35. He finished the year ranked World No. 8, the oldest player to finish in the top 10 since the 36-year-old Connors was World No. 7 in 1988. Agassi also became only the sixth male player during the open era to reach 800 career wins with his first round victory over Alex Bogomolov in Countrywide Classic in Los Angeles. Agassi's 2005 began with a quarterfinal loss to Federer at the Australian Open. Agassi had several other deep runs at tournaments but had to withdraw from several events due to injury. He lost to Jarkko Nieminen in the first round of the French Open. He won his fourth title in Los Angeles and reached the final of the Rogers Cup before falling to World No. 2 Rafael Nadal. Agassi's 2005 was defined by an improbable run to the US Open final. After beating Răzvan Sabău and Ivo Karlović in straight sets and Tomáš Berdych in four sets, Agassi won three consecutive five-set matches to advance to the final. The most notable of these matches was his quarterfinal victory over James Blake, where he rallied from two sets down to win 3–6, 3–6, 6–3, 6–3, 7–6(6). His other five-set victims were Xavier Malisse in the fourth round and Robby Ginepri in the semifinals. In the final, Agassi faced Federer, who was seeking his second consecutive US Open title and his sixth Grand Slam title in two years. Federer defeated Agassi in four sets, although Agassi gave him a scare when Agassi was up a break in the third set after splitting the first two sets. Before the 2005 Tennis Masters Cup in Shanghai, Agassi rolled his ankle in a racquetball accident and tore several ligaments. He was unable to walk for weeks. He nevertheless committed to the tournament, in which he was seeded third, and played Nikolay Davydenko in his first round robin match. Agassi's movement was noticeably hindered, particularly on his backhand return of serve, and he lost in straight sets. He then withdrew from the tournament. Agassi finished 2005 ranked World No. 7, his 16th time in the year-end top 10 rankings, which tied Connors for the most times ranked in the top 10 at year's end. In 2005, Agassi left Nike after 17 years and signed an endorsement deal with Adidas. A major reason for Agassi leaving Nike was because Nike refused to donate to Agassi's charities and Adidas was more than happy to do so. Agassi had a poor start to 2006. He was still recovering from an ankle injury and also suffering from back and leg pain and lack of match play. Agassi withdrew from the Australian Open because of the ankle injury, and his back injury and other pains forced him to withdraw from several other events, eventually skipping the entire clay court season, including the French Open. This caused his ranking to drop out of the top 10 for the last time. Agassi returned for the grass court season, playing a tune-up and then Wimbledon. He was defeated in the third round by World No. 2 (and eventual runner-up) Rafael Nadal 7–6(5), 6–2, 6–4. Against conventions, Agassi, the losing player, was interviewed on court after the match. At Wimbledon, Agassi announced his plans to retire following the US Open. Agassi played only two events during the summer hardcourt season, with his best result being a quarterfinal loss at the Countrywide Classic in Los Angeles to Fernando González of Chile 6–4, 3–6, 7–5. As a result, he was unseeded at the US Open. Agassi had a short but dramatic run in his final US Open. Because of extreme back pain, Agassi was forced to receive anti-inflammatory injections after every match. After a tough four-set win against Andrei Pavel, Agassi faced eighth-seeded Marcos Baghdatis in the second round, who had earlier advanced to the 2006 Australian Open final and Wimbledon semifinals. Agassi won 6–4, 6–4, 3–6, 5–7, 7–5 as the younger Baghdatis succumbed to muscle cramping in the final set. In his last match, Agassi fell to 112th ranked big-serving Benjamin Becker of Germany in four sets. Agassi received an eight minute standing ovation from the crowd after the match and delivered a memorable retirement speech.
Since retiring after the 2006 US Open, Agassi has participated in a series of charity tournaments and continues his work with his own charity. On September 5, 2007, Agassi was a surprise guest commentator for the Andy Roddick/Roger Federer US Open quarterfinal. He played an exhibition match at Wimbledon, teamed with his wife, Steffi Graf, to play with Tim Henman and Kim Clijsters. He will play World Team Tennis for the Philadelphia Freedoms in the summer of 2009 and played at the Outback Champions Series event for the first time. He played the Cancer Treatment Centers of America Tennis Championships at Surprise, Arizona where he reached the final before bowing to eventual champion Todd Martin who captured his fourth career Outback Champions Series win. On the way to the finals, Agassi beat Mikael Pernfors in the quarterfinals and Wayne Ferreira in the semifinals. However, he clarified that he will not be playing the tour on a full-time basis as he only played the tournament as a favor to long-time friend Jim Courier.
Early on in his career, Agassi would look to end points quickly, typically by inducing a weak return with a deep, hard shot, and then playing a winner at an extreme angle. His return of serve, baseline game, and keen sense of anticipation were among the best in the game, and helped him win the Wimbledon title in 1992. On the rare occasion that he charged the net, Agassi liked to take the ball in the air and hit a swinging volley for the winner. Agassi continually put pressure on opponents with a preference to taking the ball early and was famously known for swinging deep angles like a smoking backhand up the line. His strength was dictating play from the back of the court. While growing up his father and Nick Bollettieri trained him in this way. He was never known for a strong serve, net work or volleying. When in control of a point, Agassi would often pass up an opportunity to attempt a winner and hit a slightly more conservative shot, both to minimize his errors and to make his opponent run more. His penchant for running players around point after point has earned him the nickname "The Punisher". Agassi's serve was never the strength of his game, but it improved steadily over the course of his career, and went from being a liability to being above average. He often used his hard slice serve in the deuce service box to seek to send his opponent off the court, followed by a shot to the opponent's opposite corner. Agassi's service speed when hitting a flat first serve would often range between to. His second serve however was usually only in the mid 80's. He relied on a heavy kick serve for his second serve.
Agassi married actress Brooke Shields on April 19, 1997. In February 1998, they filed suit against "The National Enquirer" claiming it printed "false and fabricated" statements about the couple, but the case was dismissed. The couple later filed for divorce, which was granted on April 9, 1999. At the 1999 French Open, Agassi and Steffi Graf were the surprise champions, since he had not won a Grand Slam title since 1995 and she since 1996. At the winners' ball, they met each other for the second time. Shortly after, they started dating. Graf retired after they both reached the Wimbledon final in July. They were married on October 22, 2001. Their son, Jaden Gil, was born four days later, October 26. Their daughter, Jaz Elle, was born on October 3, 2003. The couple live in the Las Vegas area and own several vacation homes. Agassi's older sister, Rita, was married to tennis player Pancho Gonzales. In 1995, when Gonzales died in Las Vegas, Agassi paid for the funeral. Long-time trainer Gil Reyes has been called one of Agassi's closest friends; some have described him as being a "father figure". Andre Agassi's other sister, like their mother, Betty, is a breast cancer survivor. In December 2008, Agassi's childhood friend and former business manager Perry Rogers sued Graf for $50,000 in management fees he claimed that she owed him. Agassi's autobiography, "Open" (written with assistance from J. R. Moehringer) was published in November 2009. In it, Agassi admitted to using and testing positive for methamphetamine in 1997, and that his then-distinctive long hair was actually a wig; in Agassi's opinion, his defeat in the 1990 French Open final was partly due to issues with the wearing of the wig. He also revealed that he had always hated tennis during his career, because of the constant pressure it exerted on him. He also revealed he thought Pete Sampras was "robotic". The book reached #1 on the New York Times Best Seller list and received favorable reviews.
Agassi has participated in many charity organizations and founded the Andre Agassi Charitable Association in 1994, which assists Las Vegas' young people. Agassi was awarded the ATP Arthur Ashe Humanitarian award in 1995 for his efforts to help disadvantaged youth. He is regularly cited as the most charitable and socially involved player in professional tennis. It has also been surmised that he may be the most charitable athlete of his generation, which includes Lance Armstrong. Andre Agassi's charities help in assisting children reach their athletic potential. His Boys & Girls Club sees 2,000 children throughout the year and boasts a world class junior tennis team. It also has a basketball program (the Agassi Stars) and a rigorous system that encourages a mix of academics and athletics. In 2001, Agassi opened up the Andre Agassi College Preparatory Academy in Las Vegas, a tuition-free charter school for at-risk children in the area. In 2009, the graduating class had 100 percent graduation rate and a 100 percent college acceptance rate. Among other child-related programs that Agassi supports through his Andre Agassi Charitable Foundation is Clark County's only residential facility for abused and neglected children called Child Haven. In 1997, Agassi donated funding to Child Haven for a six-room classroom building now named the Agassi Center for Education. His foundation also provided $720,000 to assist in the building of the Andre Agassi Cottage for Medically Fragile Children. This facility opened in December 2001 and accommodates developmentally delayed or handicapped children and children quarantined for infectious diseases. It houses approximately 20 beds and gives children with special needs the special attention needed to make them feel comfortable in their new surroundings." In 2007, Agassi, Muhammad Ali, Lance Armstrong, Warrick Dunn, Jeff Gordon, Mia Hamm, Tony Hawk, Andrea Jaeger, Jackie Joyner-Kersee, Mario Lemieux, Alonzo Mourning and Cal Ripken, Jr. founded the charity Athletes for Hope, which helps professional athletes get involved in charitable causes and inspires millions of non-athletes to volunteer and support the community.
The Austro-Asiatic languages are a large language family of Southeast Asia, and also scattered throughout India and Bangladesh. The name comes from the Latin word for "south" and the Greek name of Asia, hence "South Asia." Among these languages, only Khmer, Vietnamese, and Mon have a long established recorded history, and only Vietnamese and Khmer have official status (in Vietnam and Cambodia, respectively). The rest of the languages are spoken by minority groups. Ethnologue identifies 168 Austro-Asiatic languages. These are traditionally divided into two families, Mon-Khmer and Munda, but two recent classifications have abandoned Mon-Khmer as a valid node, although this is tentative and not generally accepted. Austro-Asiatic languages have a disjunct distribution across India, Bangladesh and Southeast Asia, separated by regions where other languages are spoken. It is widely believed that the Austro-Asiatic languages are the autochthonous languages of Southeast Asia and the eastern Indian subcontinent, and that the other languages of the region, including the Indo-European, Kradai, Dravidian and Sino-Tibetan languages, are the result of later migrations of people. The Austro-Asiatic languages are well known for having a "sesqui-syllabic" pattern, with basic nouns and verbs consisting of a reduced minor syllable plus a full syllable. Many of them also have infixes.
Linguists traditionally recognize two primary divisions of Austro-Asiatic: the Mon-Khmer languages of Southeast Asia, Northeast India and the Nicobar Islands, and the Munda languages of East and Central India and parts of Bangladesh. However, no evidence for this classification has ever been published, and it is possible that the linguistic classification has been influenced by researchers' subjective perception of a racial dichotomy between the speakers of languages that have traditionally been classified as Mon-Khmer and those that have traditionally been classified as Munda. Each of the families that is written in boldface type below is accepted as a valid clade. However, the relationships between these families within Austro-Asiatic is debated; in addition to the traditional classification, two recent proposals are given, neither of which accept traditional Mon-Khmer as a valid unit. It should be noted that little of the data used for competing classifications has ever been published, and therefore cannot be evaluated by peer review.
The Afroasiatic languages constitute a language family with about 375 living languages and more than 350 million speakers spread throughout North Africa, the Horn of Africa, and Southwest Asia, as well as parts of the Sahel, West Africa and East Africa. The most widely spoken Afroasiatic language is Arabic, with 230 million speakers (all the colloquial varieties). In addition to languages now spoken, Afroasiatic includes several ancient languages, such as Ancient Egyptian, Biblical Hebrew, and Akkadian. The term "Afroasiatic" (often now spelled as Afro-Asiatic) was coined by Maurice Delafosse (1914). It did not come into general use until it was adopted by Joseph Greenberg (1950) to replace the earlier term "Hamito-Semitic", following his demonstration that Hamitic is not a valid language family. The term "Hamito-Semitic" remains in use in the academic traditions of some European countries. Some authors now replace "Afro-Asiatic" with "Afrasian", or, reflecting an opinion that it is more African than Asian, "Afrasan". Individual scholars have called the family "Erythraean" (Tucker 1966) and "Lisramic" (Hodge 1972).
In the 9th century, the Hebrew grammarian Judah ibn Quraysh of Tiaret in Algeria was the first to link two branches of Afroasiatic together; he perceived a relationship between Berber and Semitic. He knew of Semitic through Arabic, Hebrew, and Aramaic. In the course of the 19th century, Europeans also began suggesting such relationships. In 1844, Theodor Benfey suggested a language family consisting of Semitic, Berber, and Cushitic (calling the latter "Ethiopic"). In the same year, T.N. Newman suggested a relationship between Semitic and Hausa, but this would long remain a topic of dispute and uncertainty. Friedrich Müller named the traditional "Hamito-Semitic" family in 1876 in his "Grundriss der Sprachwissenschaft". He defined it as consisting of a Semitic group plus a "Hamitic" group containing Egyptian, Berber, and Cushitic; he excluded the Chadic group. These classifications relied in part on non-linguistic anthropological and racial arguments (see Hamitic hypothesis). Leo Reinisch (1909) proposed linking Cushitic and Chadic, while urging a more distant affinity to Egyptian and Semitic, thus foreshadowing Greenberg, but his suggestion found little resonance. Marcel Cohen (1924) rejected the idea of a distinct Hamitic subgroup and included Hausa (a Chadic language) in his comparative Hamito-Semitic vocabulary. Joseph Greenberg (1950) strongly confirmed Cohen's rejection of "Hamitic", added (and sub-classified) the Chadic branch, and proposed the new name "Afroasiatic" for the family. Nearly all scholars have accepted Greenberg's classification. In 1969, Harold Fleming proposed that what had previously been known as Western Cushitic is an independent branch of Afroasiatic, suggesting for it the new name Omotic. This proposal and name have met with widespread acceptance. Several scholars, including Harold Fleming and Robert Hetzron, have since questioned the traditional inclusion of Beja in Cushitic.
Little agreement exists on the subgrouping of the five or six branches of Afroasiatic: Semitic, Egyptian, Berber, Chadic, Cushitic, and Omotic (if Omotic is not included in Cushitic). However, Christopher Ehret (1979), Harold Fleming (1981), and Joseph Greenberg (1981) all agree that the Omotic branch split from the rest first. Position among the world's languages. Afroasiatic is one of the four language families of Africa identified by Joseph Greenberg in his book "The Languages of Africa" (1963). It is the only one that extends outside of Africa, via the Semitic branch.
All Afroasiatic subfamilies show evidence of a causative affix "s", but a similar suffix also appears in other groups, such as the Niger-Congo languages. Semitic, Berber, Cushitic (including Beja), and Chadic support possessive suffixes. Tonal languages appear in the Omotic, Chadic, and Cushitic branches of Afroasiatic, according to Ehret (1996). The Semitic, Berber, and Egyptian branches do not use tones phonemically.
Andorra, officially the Principality of Andorra (), also called the Principality of the Valleys of Andorra, is a small country in southwestern Europe, located in the eastern Pyrenees mountains and bordered by Spain and France. It is the sixth smallest nation in Europe having an area of and an estimated population of 83,888 in 2009. Its capital, Andorra la Vella, is the highest capital city in Europe, being at an elevation of 1023 metres. The official language is Catalan, although Spanish, French, and Portuguese are also commonly spoken. The Principality was formed in 1278. The role of monarch is shared between the President of the French Republic and the Bishop of Urgell, Catalonia, Spain. It is a prosperous country mainly because of its tourism industry, which services an estimated 10.2 million visitors annually, and also because of its status as a tax haven. It is not a member of the European Union, but the euro is the "de facto" currency. The people of Andorra have the 2nd highest human life expectancy in the world — 82 years at birth.
Tradition holds that Charles the Great (Charlemagne) granted a charter to the Andorran people in return for fighting against the Moors. Overlordship of the territory was by the Count of Urgell and eventually by the bishop of the Diocese of Urgell. In 988, Borrell II, Count of Urgell, gave the Andorran valleys to the Diocese of Urgell in exchange for land in Cerdanya. Since then the Bishop of Urgell, based in Seu d'Urgell, has owned Andorra. Before 1095, Andorra did not have any type of military protection and the Bishop of Urgell, who knew that the Count of Urgell wanted to reclaim the Andorran valleys, asked for help and protection from the Lord of Caboet. In 1095, the Lord of Caboet and the Bishop of Urgell signed under oath a declaration of their co-sovereignty over Andorra. Arnalda, daughter of Arnau of Caboet, married the Viscount of Castellbò and both became Viscounts of Castellbò and Cerdanya. Years later their daughter, Ermessenda, married Roger Bernat II, the French Count of Foix. They became Roger Bernat II and Ermessenda I, Counts of Foix, Viscounts of Castellbò and Cerdanya, and also co-sovereigns of Andorra (shared with the Bishop of Urgell). In the eleventh century, a dispute arose between the Bishop of Urgell and the Count of Foix. The conflict was resolved in 1278 with the mediation of Aragon by the signing of the first paréage which provided that Andorra's sovereignty be shared between the count of Foix (whose title would ultimately transfer to the French head of state) and the Bishop of Urgell, in Catalonia. This gave the principality its territory and political form. Over the years, the French co-title to Andorra passed to the kings of Navarre. After Henry of Navarre became King Henry IV of France, he issued an edict in 1607 that established the head of the French state and the Bishop of Urgell as co-princes of Andorra. In 1812–13, the First French Empire annexed Catalonia and divided it in four départements, with Andorra being made part of the district of Puigcerdà (département of Sègre).
Andorra declared war on Imperial Germany during World War I, but did not actually take part in the fighting. It remained in an official state of belligerency until 1957 as it was not included in the Treaty of Versailles. In 1933, France occupied Andorra as a result of social unrest before elections. On July 12, 1934, adventurer Boris Skossyreff issued a proclamation in Urgell, declaring himself Boris I, sovereign prince of Andorra, simultaneously declaring war on the Bishop of Urgell. He was arrested by Spanish authorities on July 20 and ultimately expelled from Spain. From 1936 to 1940, a French detachment was garrisoned in Andorra to prevent influences of the Spanish Civil War and Franco's Spain. Francoist troops reached the Andorran border in the later stages of the war. During World War II, Andorra remained neutral and was an important smuggling route between Vichy France and Spain. Given its relative isolation, Andorra has existed outside the mainstream of European history, with few ties to countries other than France and Spain. In recent times, however, its thriving tourist industry along with developments in transport and communications have removed the country from its isolation. Its political system was thoroughly modernised in 1993, the year in which it became a member of the United Nations and the Council of Europe.
Andorra is a parliamentary co-principality with the President of France and the Bishop of Urgell (Catalonia, Spain), as co-princes, in a duumvirate. The politics of Andorra take place in a framework of a parliamentary representative democracy, whereby the Prime Minister of Andorra is the head of government, and of a pluriform multi-party system. The current Prime Minister is Jaume Bartumeu of the Social Democratic Party (PS). Executive power is exercised by the government. Legislative power is vested in both the government and parliament. The Parliament of Andorra is known as the General Council. The General Council consists of between 28 and 42 Councilors, as the members of the legislative branch are called. The Councilors serve for four-year terms and elections are held between the thirtieth and fortieth days following the dissolution of the previous Council. The Councilors can be elected on two equal constituencies. Half are elected in equal number from each of the seven administrative parishes and the other half of the Councilors are elected from a single national constituency. Fifteen days after the election, the Councilors hold their inauguration. During this session, the Syndic General, who is the head of the General Council, and the Subsyndic General, his assistant, are elected. Eight days later, the Council convenes once more. During this session the Head of Government, the Prime Minister of Andorra, is chosen from among the Councilors. Candidates for the prime-ministerial nomination can be proposed by a minimum of one-fifth of the Councilors. The Council then elects the candidate with the absolute majority of votes to be Head of Government. The Syndic General then notifies the Co-princes who in turn appoint the elected candidate as the Prime Minister of Andorra. The General Council is also responsible for proposing and passing laws. Bills may be presented to the Council as Private Members' Bills by three of the Local Parish Councils jointly or by at least one tenth of the citizens of Andorra. The Council also approves the annual budget of the principality. The government must submit the proposed budget for parliamentary approval at least two months before the previous budget expires. If the budget is not approved by the first day of the next year, the previous budget is extended until a new one is approved. Once any bill is approved, the Syndic General is responsible for presenting it to the Co-princes so that they may sign and enact it. If the Head of Government is not satisfied with the Council, he may request that the Co-princes dissolve the Council and order new elections. In turn, the Councilors have the power to remove the Head of Government from office. After a motion of censure is approved by at least one-fifth of the Councilors, the Council will vote and if it receives the absolute majority of votes, the Prime Minister is removed.
The judiciary is composed of the Magistrates Court, the Criminal Law Court, the High Court of Andorra, and the Constitutional Court. The High Court of Justice is composed of five judges: one appointed by the Head of Government, one each by the Coprinces, one by the Syndic General, and one by the Judges and Magistrates. It is presided over by the member appointed by the Syndic General and the judges hold office for six-year terms. The Magistrates and Judges are appointed by the High Court, and so is the President of the Criminal Law Court. The High Court also appoints members of the Office of the Attorney General. The Constitutional Court is responsible for interpreting the Constitution and reviewing all appeals of unconstitutionality against laws and treaties. It is composed of four judges, one appointed by each of the Coprinces and two by the General Council. They serve eight-year terms. The Court is presided over by one of the Judges on a two-year rotation so that each judge at one point will be the leader of the Court.
Due to its location in the eastern Pyrenees mountain range, Andorra consists predominantly of rugged mountains, the highest being the Coma Pedrosa at, and the average elevation of Andorra is. These are dissected by three narrow valleys in a Y shape that combine into one as the main stream, the Gran Valira river, leaves the country for Spain (at Andorra's lowest point of). Andorra's surface area is. Phytogeographically, Andorra belongs to the Atlantic European province of the Circumboreal Region within the Boreal Kingdom. According to the WWF, the territory of Andorra belongs to the ecoregion of Pyrenees conifer and mixed forests.
Tourism, the mainstay of Andorra's tiny, well-to-do economy, accounts for roughly 80% of GDP. An estimated 10.2 million tourists visit annually, attracted by Andorra's duty-free status and by its summer and winter resorts. Andorra's comparative advantage has recently eroded as the economies of adjoining France and Spain have been opened up, providing broader availability of goods and lower tariffs. The banking sector, with its tax haven status, also contributes substantially to the economy. Agricultural production is limited—only 2% of the land is arable—and most food has to be imported. Some tobacco is grown locally. The principal livestock activity is domestic sheep raising. Manufacturing output consists mainly of cigarettes, cigars, and furniture. Andorra's natural resources include hydroelectric power, mineral water, timber, iron ore, and lead. Andorra is not a member of the European Union, but enjoys a special relationship with it, such as being treated as an EU member for trade in manufactured goods (no tariffs) and as a non-EU member for agricultural products. Andorra lacks a currency of its own and uses that of its two surrounding nations. Andorra used the French franc and the Spanish peseta until 1999 when both currencies were replaced by the EU's single currency, the euro. Coins and notes of both the franc and the peseta, however, remained legal tender in Andorra until 2002. Andorra is negotiating to issue its own euro coins.
The historic and official language is Catalan, a Romance language. Because of immigration, historical links, and close geographic proximity, other languages such as Spanish, French and Portuguese are also commonly spoken. Most Andorrans also speak Spanish (Castilian), French or both. Andorra is one of only four European countries (together with France, Monaco, and Turkey) that have never signed the Council of Europe Framework Convention on National Minorities.
The population of Andorra is predominantly (90%) Roman Catholic. Their patron saint is Our Lady of Meritxell. Though it is not an official state religion, the constitution acknowledges a special relationship with the Roman Catholic Church, offering some special privileges to that group. The Muslim community is primarily made up of North African immigrants. Other Christian denominations include the Anglican Church, Jehovah’s Witnesses, the Reunification Church, the New Apostolic Church, and The Church of Jesus Christ of Latter-day Saints. There is a small community of Hindus.
Children between the ages of 6 and 16 are required by law to have full-time education. Education up to secondary level is provided free of charge by the government. There are three systems of schools – Andorran, French and Spanish – which use Catalan, French and Spanish, respectively, as the main language of instruction. Parents may choose which system their children attend. All schools are built and maintained by Andorran authorities, but teachers in the French and Spanish schools are paid for the most part by France and Spain. About 50% of Andorran children attend the French primary schools, and the rest attend Spanish or Andorran schools.
The University of Andorra (UdA) is the state public university and is the only university in Andorra. It was established in 1997. The University provides first-level degrees in nursing, computer science, business administration, and educational sciences, in addition to higher professional education courses. The only two graduate schools in Andorra are the Nursing School and the School of Computer Science, the latter having a PhD programme.
The geographical complexity of the country as well as the small number of students prevents the University of Andorra from developing a full academic programme, and it serves principally as a centre for virtual studies, connected to Spanish and French universities. The Virtual Studies Centre ("Centre d’Estudis Virtuals") at the University runs in the region of twenty degrees at both undergraduate and postgraduate levels in fields including tourism, law, Catalan philology, humanities, psychology, political sciences, audiovisual communication, telecommunications engineering, and East Asia studies. The Centre also runs various postgraduate programmes and continuing-education courses for professionals.
Healthcare in Andorra is provided to all employed persons and their families by the government-run social security system, CASS (Caixa Andorrana de Seguretat Social), which is funded by employer and employee contributions in respect of salaries. The cost of healthcare is covered by CASS at rates of 75% for out-patient expenses such as medicines and hospital visits, 90% for hospitalisation, and 100% for work-related accidents. The remainder of the costs may be covered by private health insurance. Other residents and tourists require full private health insurance. The main hospital, Meritxell, is in Escaldes-Engordany. Its services include 24-hour accident and emergency, anatomy, angiology, pathology, anesthesiology, clinical cardiology, clinical biochemistry, clinical neurology, dermatology, endocrinology, gastroenterology, genetics, geriatrics, hematology, immunology, intensive care, internal medicine, medical oncology, microbiology, nephrology and dialysis, neurophysiology, obstetrics and gynecology, oncology, ophthalmology, otorhinolaringology, orthopedics, pediatrics, physiotherapy, neonatology, plastic surgery, general surgery, oral and maxillofacial surgery, neurosurgery, pediatric surgery, thoracic surgery, trauma surgery, cardiovascular surgery, parasitology, psychiatry, radiodiagnostics, radiotherapy, urology, and venerealogy. There are also 12 primary health care centres in various locations around the Principality.
Andorra has a road network of, of which is unpaved. The two main roads out of Andorra la Vella are the CG-1 to the Spanish border, and the CG-2 to the French border via the Envalira Tunnel near Pas de la Casa. In winter, the main roads in Andorra are usually quickly cleared of snow and remain accessible, but the main road out of Andorra on the French side (RN-20/22) is less frequently cleared and is sometimes closed by avalanches. Other main roads out of Andorra la Vella are the CG-3 and CG-4 to Arcalis and Pal, respectively. Bus services cover all metropolitan areas and many rural communities, with services on most major routes running half-hourly or more frequently during peak travel times. There are frequent long-distance bus services from Andorra to Barcelona and Barcelona Airport, and also to Toulouse and Toulouse Airport, in each case taking approximately 3 hours. Bus routes also serve Girona Airport and Portugal via Lleida. Bus services are mostly run by private companies, but some local ones are operated by the Government. The private bus companies are Autocars Nadal, Camino Bus, Cooperativa Interurbana Andorrana, Eurolines, Hispano Andorrana, and Novatel. There are no railways, ports, or airports for fixed-wing aircraft in Andorra. There are, however, heliports in La Massana, Arinsal and Escaldes-Engordany with commercial helicopter services. Nearby airports are located in Barcelona, Toulouse, Perpignan, Reus, and Girona. The closest public airport is Perpignan - Rivesaltes Airport, which is away and has short-haul services to several destinations in the United Kingdom and France. La Seu d'Urgell Airport, a small airfield south of Andorra currently used only by private aeroplanes, is being studied by the Catalan government as a possible future airport for public aviation services. The nearest railway station is L'Hospitalet-près-l'Andorre east of Andorra which is on the -gauge line from Latour-de-Carol, () southeast of Andorra, to Toulouse and on to Paris by the French high-speed trains. This line is operated by the SNCF. Latour-de-Carol has a scenic metre-gauge trainline to Villefranche-de-Conflent, as well as the SNCF's -gauge line connecting to Perpignan, and the RENFE's -gauge line to Barcelona.
In Andorra, mobile and fixed telephony and internet services are operated exclusively by the Andorran national telecommunications company, SOM, also known as "Servei de Telecomunicacions d'Andorra" (STA). The same company also manages the technical infrastructure for national broadcasting of digital television and radio. By the end of 2010, it is planned that every home in the country will have Fibre-Optic to the Home for internet access at a minimum speed of 100 Mbps. There is only one Andorran television station, "Ràdio i Televisió d'Andorra" (RTVA). "Radio Nacional d’Andorra" operates two radio stations, "Radio Andorra" and "Andorra Música". There are three national newspapers, "Diari D'Andorra", "El Periòdic", and "Bon Dia" as well as several local newspapers.
The official and historic language is Catalan. Thus, its culture is Catalan with some own specificity. Andorra is home to folk dances like the contrapàs and marratxa, which survive in Sant Julià de Lòria especially. Andorran folk music has similarities to the music of its neighbours, but is especially Catalan in character, especially in the presence of dances such as the sardana. Other Andorran folk dances include contrapàs in Andorra la Vella and Saint Anne's dance in Escaldes-Engordany. Andorra's national holiday is Our Lady of Meritxell Day, September 8.
In mathematics and statistics, the arithmetic mean (or simply the mean) of a list of numbers is the sum of all of the list divided by the number of items in the list. If the list is a statistical population, then the mean of that population is called a population mean. If the list is a statistical sample, we call the resulting statistic a sample mean. The mean is the most commonly-used type of average and is often referred to simply as the "average". The term "mean" or "arithmetic mean" is preferred in mathematics and statistics to distinguish it from other averages such as the median and the mode.
If we denote a set of data by "X" = ("x"1, "x"2..., "x'n"), then the sample mean is typically denoted with a horizontal bar over the variable (formula_1, enunciated "x" bar"). The Greek letter μ is used to denote the arithmetic mean of an entire population. Or, for a random variable that has a defined mean, μ is the "probabilistic mean" or expected value of the random number. If the set "X" is a collection of random numbers with probabilistic mean of μ, then for any individual sample, "x'i", from that collection, μ = E is the expected value of that sample. In practice, the difference between μ and formula_1 is that μ is typically unobservable because one observes only a sample rather than the whole population, and if the sample is drawn randomly, then one may treat formula_1, but not μ, as a random variable, attributing a probability distribution to it (the sampling distribution of the mean). It is a U-statistic for the function formula_5 meaning that it is obtained by averaging a 1-sample statistic over the population. If "X" is a random variable, then the expected value of "X" can be seen as the long-term arithmetic mean that occurs on repeated measurements of "X". This is the content of the law of large numbers. As a result, the sample mean is used to estimate unknown expected values. Simple algebra will prove that a mean of "n" + 1 numbers is larger than the mean of "n" numbers if and only if the new number is larger than the old mean, smaller if and only if it is smaller, and remains stable if and only if it is equal to the old mean. The larger "n" is, the smaller is the magnitude of the change in the mean relative to the distance between the old mean and the new number. Note that several other "means" have been defined, including the generalized mean, the generalized f-mean, the harmonic mean, the arithmetic-geometric mean, and various weighted means.
While the mean is often used to report central tendency, it is not a robust statistic, meaning that it is greatly influenced by outliers. Notably, for skewed distributions, the arithmetic mean may not accord with one's notion of "middle", and robust statistics such as the median may be a better description of central tendency. A classic example is average income. The arithmetic mean may be misinterpreted as the median to imply that most people's incomes are higher than is in fact the case. When presented with an "average" one may be led to believe that "most" people's incomes are near this number. This "average" (arithmetic mean) income "is" higher than most people's incomes, because high income outliers skew the result higher (in contrast, the median income "resists" such skew). However, this "average" says nothing about the number of people near the median income (nor does it say anything about the modal income that most people are near). Nevertheless, because one might carelessly relate "average" and "most people" one might incorrectly assume that most people's incomes would be higher (nearer this inflated "average") than they are. For instance, reporting the "average" net worth in Medina, Washington as the arithmetic mean of all annual net worths would yield a surprisingly high number because of Bill Gates. Consider the scores (1, 2, 2, 2, 3, 9). The arithmetic mean is 3.17, but five out of six scores are below this.
If numbers "multiply" instead of "add," one should average using the geometric mean, not the arithmetic mean. This most often happens when computing the rate of return, as in finance. For example, if a stock fell 10 % in the first year, and rose 30 % in the second year, then it would be incorrect to report its "average" increase per year over this two year period as the arithmetic mean (−10 % + 30 %)/2 = 10 %; the correct average in this case is the compound annual growth rate, which yields an annualized increase per year of only 8.2 %. The reason for this is that each of those percents have different starting points: the 30% is 30% "of a smaller number". If the stock starts at $30 and falls 10 %, it is now at $27. If the stock then rises 30 %, it is now $35.1. The arithmetic mean of those rises is 10 %, but since the stock rose by $5.1 in 2 years, an average of 8.2 % would result in the final $35.1 figure [$30(1-10 %)(1+30 %) = $30(1+8.2 %)(1+8.2 %) = $35.1]. If one used the arithmetic mean 10 % in the same way, one would not get the actual increase [$30(1+10 %)(1+10 %) = $36.3]. Stated generally, compounding yields 90% * 130% = 117% overall growth, and annualizing yields formula_8, so 8.2% per year.
Particular care must be taken when using cyclic data such as phases or angles. Naïvely taking the arithmetic mean of 1° and 359° yields a result of 180°. In general application such an oversight will lead to the average value artificially moving towards the middle of the numerical range. A solution to this problem is to use the optimization formulation (viz, define the mean as the central point: the point about which one has the lowest dispersion), and redefine the difference as a modular distance (i.e., the distance on the circle: so the modular distance between 1° and 359° is 2°, not 358°).
Each AFC team plays the other teams in their division twice (home and away) during the regular season, in addition to 10 other games assigned to their schedule by the NFL the previous May. Two of these games are assigned on the basis of the team's final division standing in the previous season. The remaining 8 games are split between the roster of two other NFL divisions. This assignment shifts each year. For instance, in the 2007 regular season, each team in the AFC West played one game against each team in both the AFC South and the NFC North. In this way division competition consists of common opponents, with the exception of the 2 games assigned on the strength of each team's prior division standing. (i.e. the division winner will face the other two division winners in the AFC divisions that they are not scheduled to play) The NFC operates according to the same system. At the end of each football season, there are playoff games involving the top six teams in the AFC (the four division champions by place standing and the top two remaining non-division-champion teams ("wild cards") by record). The last two teams remaining play in the AFC Championship game with the winner receiving the Lamar Hunt Trophy. The AFC champion plays the NFC champion in the Super Bowl. After Super Bowl XLIII the AFC has won 19 Super Bowls to the 21 won by the NFC. Since losing 13 consecutive Super Bowls in the 1980s and 1990s (XIX–XXXI), the AFC has won nine of the last twelve. The losing coach of the AFC Championship game is the coach of the Pro Bowl the week after the Super Bowl.
The AFC was created after the NFL merged with the American Football League (AFL) in 1970. All of the 10 former AFL teams along with the NFL's Cleveland Browns, Pittsburgh Steelers, and the then-Baltimore Colts joined the AFC. Since the merger, five expansion teams have joined the AFC and two have left, thus making the current total 16. When the Seattle Seahawks and the Tampa Bay Buccaneers joined the league in 1976, they were temporarily placed in the NFC and AFC respectively. This arrangement lasted for one season only before the two teams switched conferences. The Seahawks eventually returned to the NFC as a result of the 2002 realignment. The expansion Jacksonville Jaguars joined the AFC in 1995. Due to the relocation controversy of the Cleveland Browns, a new AFC franchise called the Baltimore Ravens was officially established in 1996 while the Browns were reactivated in 1999. The Houston Texans were then added to the league in 2002, joining the AFC.
The merged league created a new logo for the AFC that took elements of the old AFL logo, specifically the "A" and the six stars surrounding it. The AFC logo basically remained unchanged from 1970 to 2009. The 2010 NFL season introduced an updated AFC logo, with the most notable revision being the addition of a fourth star (representing the four divisions of the AFC), and moving the stars inside the letter, similar to the NFC logo.
"Animal Farm" is a dystopian allegorical novella by George Orwell. Published in England on 17 August 1945, the book reflects events leading up to and during the Stalin era before World War II. Orwell, a democratic socialist and a member of the Independent Labour Party for many years, was a critic of Joseph Stalin and was suspicious of Moscow-directed Stalinism after his experiences with the NKVD during the Spanish Civil War. In a letter to Yvonne Davet, Orwell described "Animal Farm" as his novel "contre Stalin". The original title was "Animal Farm: A Fairy Story", but "A Fairy Story" was dropped by the US publishers for its 1946 publication. Of all the translations during Orwell's lifetime, only Telugu kept the original title. Other variations in the title include: "A Satire" and "A Contemporary Satire". Orwell suggested for the French translation the title "Union des républiques socialistes animales", recalling the French name of the Soviet Union, "Union des républiques socialistes soviétiques", and which abbreviates URSA, which means "bear", a symbol of Russia, in Latin. "Time" Magazine chose the book as one of the 100 best English-language novels (1923 to 2005); it also places at number 31 on the Modern Library List of Best 20th-Century Novels. It won a Retrospective Hugo Award in 1996 and is also included in the Great Books of the Western World.
The novel addresses not only the corruption of the revolution by its leaders but also how wickedness, indifference, ignorance, greed and myopia destroy any possibility of a Utopia. While this novel portrays corrupt leadership as the flaw in revolution (and not the act of revolution itself), it also shows how potential ignorance and indifference to problems within a revolution could allow horrors to happen if smooth transition to a people's government isn't satisfied.
Old Major, the old boar on the Manor Farm, calls the animals on the farm for a meeting, where he compares the humans to parasites and teaches the animals a revolutionary song, "Beasts of England." When Major dies three days later, two young pigs, Snowball and Napoleon, assume command and turn his dream into a philosophy. The animals revolt and drive the drunken and irresponsible Mr. Jones from the farm, renaming it "Animal Farm." The Seven Commandments of Animalism are written on the wall of a barn. The most important is the seventh, "All animals are equal." All the animals work, but the workhorse, Boxer, does more than others and adopts the maxim — "I will work harder." Snowball attempts to teach the animals reading and writing; food is plentiful; and the farm runs smoothly. The pigs elevate themselves to positions of leadership and set aside special food items ostensibly for their personal health. Napoleon takes the pups from the farm dogs and trains them privately. When Mr. Jones tries retaking the farm, the animals defeat him at what they call the "Battle of the Cowshed." Napoleon and Snowball struggle for leadership. When Snowball announces his idea for a windmill, Napoleon opposes it. Snowball makes a speech in favour of the windmill, whereupon Napoleon has his dogs chase Snowball away. In Snowball's absence, Napoleon declares himself leader and makes changes. Meetings will no longer be held and instead a committee of pigs will run the farm. Using a young pig named Squealer as a mouthpiece, Napoleon announces that Snowball stole the idea for the windmill from him. The animals work harder with the promise of easier lives with the windmill. After a violent storm, the animals find the windmill annihilated. Napoleon and Squealer convince the animals that Snowball destroyed the windmill, although the scorn of the neighbouring farmers suggests the windmill's walls were too thin. Once Snowball becomes a scapegoat, Napoleon begins purging the farm, killing animals he accuses of consorting with Snowball. Meanwhile, Boxer takes up a second maxim: "Napoleon is always right." Napoleon abuses his powers, making life harder for the animals; the pigs impose more control while reserving privileges for themselves. The pigs rewrite history, villainizing Snowball and glorifying Napoleon. Squealer justifies every statement Napoleon makes, even the pigs' alteration of the Seven Commandments of Animalism. "No animal shall drink alcohol" is changed to "No animal shall drink alcohol "to excess" when the pigs discover the farmer's whisky. "Beasts of England" is banned as inappropriate, as according to Napoleon the dream of Animal Farm has been realized. It is replaced by an anthem glorifying Napoleon, who appears to be adopting the lifestyle of a man. The animals, though cold, starving, and overworked, remain convinced through psychological conditioning that they are better off than they were when ruled by Mr. Jones. Squealer abuses the animals' poor memories and invents numbers to show their improvement. Mr. Frederick, one of the neighbouring farmers, swindles Napoleon by buying old wood with forged money, and then attacks the farm, using blasting powder to blow up the restored windmill. Though the animals win the battle, they do so at great cost, as many, including Boxer, are wounded. Boxer continues working harder and harder, until he collapses while working on the windmill. Napoleon sends for a van to take Boxer to the veterinarian, explaining that better care can be given there. Benjamin the donkey, who "could read as well as any pig", notices that the van belongs to "Alfred Simmonds, Horse Slaughterer and Glue Boiler", and attempts to mount a rescue; but the animals' attempts are futile. Squealer reports that the van was purchased by the hospital and the writing from the previous owner had not been repainted. He recounts a tale of Boxer's death in the hands of the best medical care. In reality, the pigs sent Boxer to his death in exchange for money to buy more whisky. Years pass, and the pigs learn to walk upright, carry whips, and wear clothes. The Seven Commandments are reduced to a single phrase: "All animals are equal, but some animals are more equal than others." Napoleon holds a dinner party for the pigs and the humans of the area, who congratulate Napoleon on having the hardest-working animals in the country on the least feed. Napoleon announces an alliance with the humans, against the labouring classes of both "worlds". He abolishes practices and traditions related to the Revolution, and reverts the name of the farm to "Manor Farm". The animals, overhearing the conversation, notice that the faces of the pigs have begun changing. During a poker match, an argument breaks out between Napoleon and Mr. Pilkington when they both play the Ace of Spades, and the animals realize that the faces of the pigs look like the faces of humans and no one can tell the difference between them.
Animalism is an allegorical mirror of the Soviet Union, particularly between the 1910s and the 1940s, as well as the evolution of the view of the Russian revolutionaries and government of how to practice it. It is invented by the highly respected pig Old Major. The pigs Snowball, Napoleon, and Squealer adapt Old Major's ideas into an actual philosophy, which they formally name Animalism. Soon after, Napoleon and Squealer indulge in the vices of humans (drinking alcohol, sleeping in beds, trading). Squealer is employed to alter the Seven Commandments to account for his humanization, which represents the Soviet government's tweaking of communist theory to make it more a reformation of capitalism than a replacement. Later, Napoleon and his pigs are corrupted by the absolute power they hold over the farm. To maintain their popularity with the other animals, Squealer secretly paints additions to some commandments to benefit the pigs while keeping them free of accusations of breaking the laws (such as "No animal shall drink alcohol" having "to excess" appended to it and "No animal shall sleep in a bed" with "with sheets" added to it). Eventually the laws are replaced with "All animals are equal, "but some animals are more equal than others", and "Four legs good, two legs "better!" as the pigs become more human.
There are four main equine characters: Boxer, Clover, and Mollie, who are horses, and Benjamin, who is a donkey. Boxer is a loyal, kind, dedicated, and respectful worker. He is physically the strongest animal on the farm, but naive and slow, which leaves him constantly stating "I will work harder" and "Napoleon is always right" despite the corruption. Clover is Boxer's companion, who constantly cares for him, and she also acts as the matriarch for the other horses, and other animals in general (such as the ducklings she shelters with her fore-legs and hooves during Old Major's speech). Mollie is a self-centred, self-indulgent and vain young white mare who likes wearing ribbons in her mane, eating sugar cubes, and being pampered and groomed by humans. She quickly leaves for another farm and is only once mentioned again. Benjamin is one of the longest-lived animals, has the worst temper and one of the few who can read. Benjamin is a very dedicated friend to Boxer, and does nothing to warn the other animals of the pigs' corruption, which he secretly realizes is steadily unfolding. When asked if he was happier post-Revolution than before the Revolution, Benjamin remarks, "Donkeys live a long time. None of you has ever seen a dead donkey." He is cynical and pessimistic, his most often made statement being "Life will go on as it has always gone on — that is, badly". But he is also one of the wisest animals on the farm, and is able to "read as well as any pig".
George Orwell wrote the manuscript in 1943 and 1944 following his experiences during the Spanish Civil War, which he described in his 1938 "Homage to Catalonia". In the preface of a 1947 Ukrainian edition of Animal Farm he explained how escaping the communist purges in Spain taught him "how easily totalitarian propaganda can control the opinion of enlightened people in democratic countries." This motivated Orwell to expose and strongly condemn what he saw as the Stalinist corruption of the original socialist ideals. Orwell encountered great difficulty getting the manuscript published. Four publishers refused; one had initially accepted the work but declined after consulting with the Ministry of Information. Eventually Secker and Warburg published the first edition in 1945.
In the Eastern Bloc both "Animal Farm" and later, also "Nineteen Eighty-Four" were on the list of forbidden books up until "die Wende" in 1989, and were only available via clandestine Samizdat networks. The novel's "Battle of the Windmill" is referred to by Sant Singh Bal as one "of the important episodes which constitute the essence of the plot of the novel." Harold Bloom writes that the "Battle of the Windmill rings a special bell: the repulse of the Duke of Brunswick in 1792, following the Prussian bombardment that made the windmill of Valmy famous." By contrast, Peter Edgerly Firchow and Peter Hobley Davison consider that in real life, with events in "Animal Farm" mirroring those in the Soviet Union, this fictional battle represents the Great Patriotic War (World War II), especially the Battle of Stalingrad and the Battle of Moscow. Prestwick House's "Activity Pack" for "Animal Farm" also identifies the Battle of the Windmill as an allegory for World War II, while noting that the "catalyst for the Battle of the Windmill, though, is less clear." During the battle, Fredrick drills a hole and places explosives inside, and it is followed by "All the animals, except Napoleon" took cover; Orwell had the publisher alter this from "All the animals, including Napoleon" in recognition of Joseph Stalin's decision to remain in Moscow during the German advance. The "Battle of the Cowshed" represents the allied invasion of the Soviet Russia in 1918, and the defeat of the White Russians in the Russian Civil War. Efforts to find a publisher. During World War II it became apparent to Orwell that anti-Soviet literature was not something which most major publishing houses would touch — including his regular publisher Gollancz. He also submitted the manuscript to Faber and Faber, where the poet T. S. Eliot (who was a director of the firm) also rejected it; Eliot wrote back to Orwell praising its "good writing" and "fundamental integrity" but declaring that they would only accept it for publication if they had some sympathy for the viewpoint "which I take to be generally Trotskyite". Eliot said he found the view "not convincing", and contended that the pigs were made out to be the best to run the farm; he posited that someone might argue "what was needed.. was not more communism but more public-spirited pigs". Although it was written in 1943, "Animal Farm" was not published until 1945 due to paper rationing and fear of damaging the Anglo-Soviet alliance. "The Freedom of the Press". Orwell originally wrote a preface which complains about self-imposed British self-censorship and how the British people were suppressing criticism of the USSR, their World War II ally. "The sinister fact about literary censorship in England is that it is largely voluntary. ... Things are kept right out of the British press, not because the Government intervenes but because of a general tacit agreement that 'it wouldn't do' to mention that particular fact." The preface itself was censored and as of June 2009 has not been published with most editions of the book. His wife Eileen Blair had worked during the war at the Ministry of Information censoring newspapers. Secker and Warburg published the first edition of Animal Farm in 1945 without any introduction. However, the publisher had provided space for a preface in the author's proof composited from the manuscript. For reasons unknown, no preface was supplied and all the page numbers needed to be redone at the last minute. Years later, in 1972, Ian Angus found the original typescript titled "The Freedom of the Press", and Bernard Crick published it, together with his own introduction in The Times Literary Supplement on 15 September 1972 as "How the essay came to be written". Orwell's essay criticized British self-censorship by the press, specifically the suppression of unflattering descriptions of Stalin and the Soviet government. The same essay also appeared in the Italian 1976 Animal Farm edition, with another introduction by Crick, claiming to be the first edition with the preface. Other publishers were still declining to publish it.
On July 17, 2009, Amazon.com withdrew certain Amazon Kindle titles, including "Animal Farm" and "Nineteen Eighty-Four" by George Orwell, from sale, refunded buyers, and remotely deleted items from purchasers' devices after discovering that the publisher lacked rights to publish the titles in question. Notes and annotations for the books made by users on their devices were also deleted. After the move prompted outcry and comparisons to "Nineteen Eighty-Four" itself, Amazon spokesman Drew Herdener stated that the company is "… changing our systems so that in the future we will not remove books from customers' devices in these circumstances."
Amphibians (class Amphibia), such as frogs, toads, salamanders, newts, and caecilians, are ectothermic (or cold-blooded) animals that either metamorphose from a juvenile water-breathing form, to an adult air-breathing form, or paedomorph and retain some juvenile characteristics. Mudpuppies and waterdogs are good examples of paedomorphic species. Though amphibians typically have four limbs, the caecilians are notable for being limbless. Unlike other land vertebrates (amniotes), amphibians lay eggs in water. Amphibians are superficially similar to reptiles. Amphibians are ecological indicators, and in recent decades there has been a dramatic decline in amphibian populations around the globe. Many species are now threatened or extinct. Amphibians evolved in the Devonian Period and were top predators in the Carboniferous and Permian Periods, but many lineages were wiped out during the Permian–Triassic extinction. One group, the metoposaurs, remained important predators during the Triassic, but as the world became drier during the Early Jurassic they died out, leaving a handful of relict temnospondyls like "Koolasuchus" and the modern orders of Lissamphibia.
The first major groups of amphibians developed in the Devonian Period from fish similar to the modern coelacanth and lungfish which had evolved multi-jointed leg-like fins that enabled them to crawl along the sea bottom. These amphibians were as much as one to five meters in length. However, amphibians never developed the ability to live their entire lives on land, having to return to water to lay their shell-less eggs. In the Carboniferous Period, the amphibians moved up in the food chain and began to occupy the ecological position currently occupied by crocodiles. These amphibians were notable for eating the mega insects on land and many types of fishes in the water. During the Triassic Period, the better land-adapted proto-crocodiles began to compete with amphibians, leading to their reduction in size and importance in the biosphere.
Of these only the last subclass includes recent species. With the phylogenetic revolution, this classification has been modified, or changed, and the Labyrinthodontia discarded as being a paraphyletic group without unique defining features apart from shared primitive characteristics. Classification varies according to the preferred phylogeny of the author, whether they use a stem-based or node-based classification. Generally amphibians are defined as the group that includes the common ancestors of all living amphibians (frogs, salamanders, etc.) and all their descendants. This may also include extinct groups like the temnospondyls (traditionally placed in the disbanded subclass “labyrinthodontia”), and the Lepospondyls. This means that there are a now large number of basal Devonian and Carboniferous tetrapod groups, described as “amphibians” in earlier books, that are no longer placed in the formal Amphibia. All recent amphibians are included in the subclass Lissamphibia, superorder Salientia, which is usually considered a clade (which means that it is thought that they evolved from a common ancestor apart from other extinct groups), although it has also been suggested that salamanders arose separately from a temnospondyl-like ancestor. Authorities also disagree on whether Salientia is a Superorder that includes the order Anura, or whether Anura is a sub-order of the order Salientia. Practical considerations seem to favor using the former arrangement now. The Lissamphibia, superorder Salientia, are traditionally divided into three orders, but an extinct salamander-like family, the Albanerpetontidae, is now considered part of the Lissamphibia, besides the superorder Salientia. Furthermore, Salientia includes all three recent orders plus a single Triassic proto-frog, "Triadobatrachus". The actual number of species partly also depends on the taxonomic classification followed, the two most common classifications being the classification of the website AmphibiaWeb, University of California (Berkeley) and the classification by herpetologist Darrel Frost and The American Museum of Natural History, available as the online reference database Amphibian Species of the World. The numbers of species cited above follow Frost.
For the purpose of reproduction most amphibians require fresh water. A few (e.g. "Fejervarya raja") can inhabit brackish water and even survive (though not thrive) in seawater, but there are no true marine amphibians. Several hundred frog species in adaptive radiations (e.g., "Eleutherodactylus", the Pacific Platymantines, the Australo-Papuan microhylids, and many other tropical frogs), however, do not need any water for breeding in the wild. They reproduce via direct development, an ecological and evolutionary adaptation that has allowed them to be completely independent from free-standing water. Almost all of these frogs live in wet tropical rainforests and their eggs hatch directly into miniature versions of the adult, passing through the tadpole stage within the egg. Several species have also adapted to arid and semi-arid environments, but most of them still need water to lay their eggs. Symbiosis with single celled algae that lives in the jelly-like layer of the eggs has evolved several times. The larvae (tadpoles or polliwogs) breathe with exterior gills. After hatching, they start to transform gradually into the adult's appearance. This process is called metamorphosis. Typically, the animals then leave the water and become terrestrial adults, but there are many interesting exceptions to this general way of reproduction.
Dramatic declines in amphibian populations, including population crashes and mass localized extinction, have been noted in the past two decades from locations all over the world, and amphibian declines are thus perceived as one of the most critical threats to global biodiversity. A number of causes are believed to be involved, including habitat destruction and modification, over-exploitation, pollution, introduced species, climate change, endocrine-disrupting pollutants, destruction of the ozone layer (ultraviolet radiation has shown to be especially damaging to the skin, eyes, and eggs of amphibians), and diseases like chytridiomycosis. However, many of the causes of amphibian declines are still poorly understood, and are a topic of ongoing discussion. A global strategy to stem the crisis has been released in the form of the Amphibian Conservation Action Plan (available at http://www.amphibians.org). Developed by over 80 leading experts in the field, this call to action details what would be required to curtail amphibian declines and extinctions over the next 5 years - and how much this would cost. The Amphibian Specialist Group of the World Conservation Union (IUCN) is spearheading efforts to implement a comprehensive global strategy for amphibian conservation. On January 21, 2008, Evolutionarily Distinct and Globally Endangered (EDGE), as given by chief Helen Meredith, identified nature's most endangered species: "The EDGE amphibians are amongst the most remarkable and unusual species on the planet and yet an alarming 85% of the top 100 are receiving little or no conservation attention." The top 10 endangered species (in the List of endangered animal species) include: the Chinese giant salamander, a distant relative of the newt, the tiny Gardiner's Seychelles, the limbless Sagalla caecilian, South African ghost frogs, lungless Mexican salamanders, the Malagasy rainbow frog, Chile's Darwin frog (Rhinoderma rufum) and the Betic Midwife Toad.
Alaska () is the largest state of the United States by area; it is situated in the northwest extremity of the North American continent, with Canada to the east, the Arctic Ocean to the north, and the Pacific Ocean to the west and south, with Russia further west across the Bering Strait. Approximately half of Alaska's 698,473 residents live within the Anchorage metropolitan area. As of 2009, Alaska remains the least densely populated state of the U.S. The U.S. Senate approved the purchase of Alaska from the Russian Empire on March 30, 1867, for $7.2 million at about two cents per acre ($4.74/km2). The land went through several administrative changes before becoming an organized territory on May 11, 1912, and the 49th state of the U.S. on January 3, 1959. The name "Alaska" (Аляска) was already introduced in the Russian colonial time, when it was used only for the peninsula and is derived from the Aleut "alaxsxaq", meaning "the mainland" or more literally, "the object towards which the action of the sea is directed". It is also known as Alyeska, the "great land", an Aleut word derived from the same root.
Alaska has a longer coastline than all the other U.S. states combined. It is the only non-contiguous U.S. state on continental North America; about of British Columbia (Canada) separate Alaska from Washington state. Alaska is thus an exclave of the United States. It is technically part of the continental U.S., but is often not included in colloquial use; Alaska is not part of the contiguous U.S., often called "the Lower 48." The capital city, Juneau, is situated on the mainland of the North American continent, but is not connected by road to the rest of the North American highway system. The state is bordered by the Yukon Territory and British Columbia in Canada, to the east, the Gulf of Alaska and the Pacific Ocean to the south, the Bering Sea, Bering Strait, and Chukchi Sea to the west and the Arctic Ocean to the north. Alaska's territorial waters touch Russia's territorial waters in the Bering Strait, as the Russian and Alaskan islands are only apart. As it extends into the eastern hemisphere, it is technically both the westernmost and easternmost state in the United States, as well as also being the northernmost. Alaska is the largest state in the United States in land area at, over twice the size of Texas, the next largest state. Alaska is larger than all but 18 sovereign countries. Counting territorial waters, Alaska is larger than the combined area of the next three largest states: Texas, California, and Montana. It is also larger than the combined area of the 22 smallest U.S. states. The International Date Line was drawn west of 180° to keep the whole state, and thus the entire North American continent, within the same legal day.
With its myriad islands, Alaska has nearly of tidal shoreline. The Aleutian Islands chain extends west from the southern tip of the Alaska Peninsula. Many active volcanoes are found in the Aleutians. Unimak Island, for example, is home to Mount Shishaldin, which is an occasionally smoldering volcano that rises to above the North Pacific. It is the most perfect volcanic cone on Earth, even more symmetrical than Japan's Mount Fuji. The chain of volcanoes extends to Mount Spurr, west of Anchorage on the mainland. Alaska has more volcanoes than any other state. Geologists have identified Alaska as part of Wrangellia, a large region consisting of multiple states and Canadian provinces in the Pacific Northwest which is actively undergoing continent building. One of the world's largest tides occurs in Turnagain Arm, just south of Anchorage – tidal differences can be more than. (Many sources say Turnagain has the second-greatest tides in North America, but several areas in Canada have larger tides.) Alaska has more than three million lakes. Marshlands and wetland permafrost cover (mostly in northern, western and southwest flatlands). Glacier ice covers some of land and of tidal zone. The Bering Glacier complex near the southeastern border with Yukon covers alone. With over 100,000 of them, Alaska has half of the world's glaciers.
According to an October 1998 report by the United States Bureau of Land Management, approximately 65% of Alaska is owned and managed by the U.S. federal government as public lands, including a multitude of national forests, national parks, and national wildlife refuges. Of these, the Bureau of Land Management manages 87 million acres (350,000 km²), or 23.8% of the state. The Arctic National Wildlife Refuge is managed by the United States Fish and Wildlife Service. It is the world's largest wildlife refuge, comprising. Of the remaining land area, the State of Alaska owns; another are owned by 12 regional and dozens of local Native corporations created under the Alaska Native Claims Settlement Act. Thus, indirectly, the 84,000 Eskimo, Aleut and American Indian inhabitants of Alaska own one-ninth of the state. Various private interests own the remaining land, totaling about one percent of the state.
The climate in Juneau and the southeast panhandle is a mid-latitude oceanic climate (Köppen climate classification "Cfb") in the southern sections and a subarctic oceanic climate (Köppen "Cfc") in the northern parts. On an annual basis, the panhandle is both the wettest and warmest part of Alaska with milder temperatures in the winter and high precipitation throughout the year. Juneau averages over of precipitation a year, while other areas receive over. This is also the only region in Alaska in which the average daytime high temperature is above freezing during the winter months. The climate of Anchorage and south central Alaska is mild by Alaskan standards due to the region's proximity to the seacoast. While the area gets less rain than southeast Alaska, it gets more snow, and days tend to be clearer. On average, Anchorage receives of precipitation a year, with around of snow, although there are areas in the south central which receive far more snow. It is a subarctic climate (Köppen "Dfc") due to its brief, cool summers. The climate of Western Alaska is determined in large part by the Bering Sea and the Gulf of Alaska. It is a subarctic oceanic climate in the southwest and a continental subarctic climate farther north. The temperature is somewhat moderate considering how far north the area is. This area has a tremendous amount of variety in precipitation. The northern side of the Seward Peninsula is technically a desert with less than of precipitation annually, while some locations between Dillingham and Bethel average around of precipitation. The climate of the interior of Alaska is subarctic. Some of the highest and lowest temperatures in Alaska occur around the area near Fairbanks. The summers may have temperatures reaching into the 90s°F (the low to mid 30s °C), while in the winter, the temperature can fall below −60 °F (-52 °C). Precipitation is sparse in the Interior, often less than a year, but what precipitation falls in the winter tends to stay the entire winter. The highest and lowest recorded temperatures in Alaska are both in the Interior. The highest is 100 °F (38 °C) in Fort Yukon (which is just inside the arctic circle) on June 27, 1915, tied with Pahala, Hawaii as the lowest high temperature in the United States. The lowest official Alaska temperature is −80 °F (-62 °C) in Prospect Creek on January 23, 1971, one degree above the lowest temperature recorded in continental North America (in Snag, Yukon, Canada). The climate in the extreme north of Alaska is Arctic (Köppen "ET") with long, very cold winters and short, cool summers. Even in July, the average low temperature in Barrow is 34 °F (1 °C). Precipitation is light in this part of Alaska, with many places averaging less than per year, mostly as snow which stays on the ground almost the entire year.
The first European contact with Alaska occurred in 1741, when Vitus Bering led an expedition for the Russian Navy aboard the "St. Peter". After his crew returned to Russia bearing sea otter pelts judged to be the finest fur in the world, small associations of fur traders began to sail from the shores of Siberia towards the Aleutian islands. The first permanent European settlement was founded in 1784, and the Russian-American Company carried out an expanded colonization program during the early to mid-1800s. New Archangel on Kodiak Island was Alaska's first capital, but for a century under both Russia and the U.S. Sitka was the capital. The Russians never fully colonized Alaska, and the colony was never very profitable. William H. Seward, the U.S. Secretary of State, negotiated the Alaskan purchase with the Russians in 1867 for $7.2 million. Alaska was loosely governed by the military initially, and was unofficially a territory of the United States from 1884 on. In the 1890s, gold rushes in Alaska and the nearby Yukon Territory brought thousands of miners and settlers to Alaska. Alaska was granted official territorial status in 1912. At this time the capital was moved to Juneau. During World War II, the Aleutian Islands Campaign focused on the three outer Aleutian Islands – Attu, Agattu and Kiska – that were invaded by Japanese troops and occupied between June 1942 and August 1943. Unalaska/Dutch Harbor became a significant base for the U.S. Army Air Corps and Navy submariners. The U.S. Lend-Lease program involved the flying of American warplanes through Canada to Fairbanks and thence Nome; Soviet pilots took possession of these aircraft, ferrying them to fight the German invasion of the Soviet Union. The construction of military bases contributed to the population growth of some Alaskan cities. Statehood was approved on July 7, 1958. Alaska was officially proclaimed a state on January 3, 1959. In 1964, the massive "Good Friday Earthquake" killed 131 people and destroyed several villages, mainly by the resultant tsunamis. It was the third most powerful earthquake in the recorded history of the world, with a moment magnitude of 9.2. It was over one thousand times more powerful than the 1989 San Francisco earthquake. Luckily, the epicenter was in an unpopulated area or thousands more would have been killed. The 1968 discovery of oil at Prudhoe Bay and the 1977 completion of the Trans-Alaska Pipeline led to an oil boom. In 1989, the "Exxon Valdez" hit a reef in the Prince William Sound, spilling over 11 million gallons of crude oil over 1,100 miles (1,600 km) of coastline. Today, the battle between philosophies of development and conservation is seen in the contentious debate over oil drilling in the Arctic National Wildlife Refuge.
The United States Census Bureau, as of July 1, 2008, estimated Alaska's population at 686,293, which represents an increase of 59,361, or 9.5%, since the last census in 2000. This includes a natural increase since the last census of 60,994 people (that is 86,062 births minus 25,068 deaths) and a decrease due to net migration of 5,469 people out of the state. Immigration from outside the U.S. resulted in a net increase of 4,418 people, and migration within the country produced a net loss of 9,887 people. In 2000 Alaska ranked the 48th state by population, ahead of Vermont and Wyoming (and Washington D.C.). Alaska is the least densely populated state, and one of the most sparsely populated areas in the world, at 1.0 person per square mile (0.42/km²), with the next state, Wyoming, at 5.1 per square mile (1.97/km²). Alaska is the largest U.S. state by area, and the sixth wealthiest (per capita income).
According to the 2000 U.S. Census, White Americans made up 69.3% of Alaska's population. African Americans made up 3.5% of Alaska's population. In addition, American Indians and Alaska Natives were the largest minority group; they made up 15.6% of Alaska's population. Asian Americans made up 4.0% of Alaska's population. Pacific Islander Americans made up 0.5% of Alaska's population. Individuals from some other race made up 1.6% of Alaska's population while individuals from two or more races made up 5.4% of the state's population. In addition, Hispanics and Latinos made up 4.1% of Alaska's population. In terms of ancestry, German Americans were the largest single ethnic group in Alaska; they made up 16.6% of Alaska's population and they were the only ethnic group in the state to number over 100,000 members. Irish Americans made up 10.8% of Alaska's population while English Americans made up 9.6% of the state's population. Norwegian Americans made up 4.2% of Alaska's population and French Americans made up 3.2% of the state's population. As of the 2005–2007 American Community Survey conducted by the U.S. Census Bureau, White Americans made up 68.5% of Alaska's population. Blacks or African Americans made up 3.8% of Alaska's population. American Indians and Alaska Natives made up 13.4% of Alaska's population; still remaining the largest minority group. Asian Americans made up 4.6% of Alaska's population. Pacific Islander Americans remained at 0.5% of the state's population. Individuals from some other race made up 1.9% of Alaska's population while individuals from two or more races made up 7.2% of the state's population. Hispanics or Latinos made up 5.5% of Alaska's population. In terms of ancestry, German Americans remained the largest single ethnic group in Alaska; they made up 19.3% of Alaska's population and were still the only ethnic group in the state with over 100,000 members. Irish Americans made up 12.5% of Alaska's population while English Americans made up 10.8% of the state's population. Norwegian Americans remained at 4.2% of Alaska's population and French Americans made up 3.6% of the state's population.
According to the 2005–2007 American Community Survey, 84.7% of people over the age of five speak only English at home. About 3.5% speak Spanish at home. About 2.2% speak another Indo-European language at home and about 4.3% speak an Asian language at home. And about 5.3% speak other languages at home. A total of 5.2% of Alaskans speak one of the state's 22 indigenous languages, known locally as "native languages". These languages belong to two major language families: Eskimo-Aleut and Na-Dene. As the homeland of these two major language families of North America, Alaska has been described as the crossroads of the continent, providing evidence for the recent settlement of North America by way of the Bering land bridge.
Alaska has been identified, along with Pacific Northwest states Washington and Oregon, as being the least religious in the U.S. According to statistics collected by the Association of Religion Data Archives, about 39% of Alaska residents were members of religious congregations. Evangelical Protestants had 78,070 members, Roman Catholics had 54,359, and mainline Protestants had 37,156. After Catholicism, the largest single denominations are The Church of Jesus Christ of Latter Day Saints (Mormons/LDS) with 29,460, Southern Baptists with 22,959, and Orthodox with 20,000. The large Eastern Orthodox (with 49 parishes and up to 50,000 followers) population is a result of early Russian colonization and missionary work among Alaska Natives. In 1795, the First Russian Orthodox Church was established in Kodiak. Intermarriage with Alaskan Natives helped the Russian immigrants integrate into society. As a result, an increasing number of Russian Orthodox churches gradually became established within Alaska. Alaska also has the largest Quaker population (by percentage) of any state. In 2003 there were 3,000 Jews in Alaska (for whom observance of the mitzvah may pose special problems). Estimates for the number of Alaskan Muslims range from 2,000 to 5,000. Alaskan Hindus often share venues and celebrations with members of other religious communities including Sikhs and Jains.
The 2007 gross state product was $44.9 billion, 45th in the nation. Its per capita personal income for 2007 was $40,042, ranking 15th in the nation. The oil and gas industry dominates the Alaskan economy, with more than 80% of the state's revenues derived from petroleum extraction. Alaska's main export product (excluding oil and natural gas) is seafood, primarily salmon, cod, Pollock and crab. Agriculture represents only a fraction of the Alaskan economy. Agricultural production is primarily for consumption within the state and includes nursery stock, dairy products, vegetables, and livestock. Manufacturing is limited, with most foodstuffs and general goods imported from elsewhere. Employment is primarily in government and industries such as natural resource extraction, shipping, and transportation. Military bases are a significant component of the economy in both Fairbanks and Anchorage. Federal subsidies are also an important part of the economy, allowing the state to keep taxes low. Its industrial outputs are crude petroleum, natural gas, coal, gold, precious metals, zinc and other mining, seafood processing, timber and wood products. There is also a growing service and tourism sector. Tourists have contributed to the economy by supporting local lodging.
Alaska has vast energy resources. Major oil and gas reserves are found in the Alaska North Slope (ANS) and Cook Inlet basins. According to the Energy Information Administration, Alaska ranks second in the nation in crude oil production. Prudhoe Bay on Alaska's North Slope is the highest yielding oil field in the United States and on North America, typically producing about. The Trans-Alaska Pipeline can pump up to of crude oil per day, more than any other crude oil pipeline in the United States. Additionally, substantial coal deposits are found in Alaska's bituminous, sub-bituminous, and lignite coal basins. The United States Geological Survey estimates that there are of undiscovered, technically recoverable gas from natural gas hydrates on the Alaskan North Slope. Alaska also offers some of the highest hydroelectric power potential in the country from its numerous rivers. Large swaths of the Alaskan coastline offer wind and geothermal energy potential as well. Alaska's economy depends heavily on increasingly expensive diesel fuel for heating, transportation, electric power and light. Though wind and hydroelectric power are abundant and underdeveloped, proposals for state-wide energy systems (e.g. with special low-cost electric interties) were judged uneconomical (at the time of the report, 2001) due to low (<$0.50/Gal) fuel prices, long distances and low population. The cost of a gallon of gas in urban Alaska today is usually $0.30-$0.60 higher than the national average; prices in rural areas are generally significantly higher but vary widely depending on transportation costs, seasonal usage peaks, nearby petroleum development infrastructure and many other factors. Alaska accounts for one-fifth (20 percent) of domestically produced United States oil production. Prudhoe Bay (North America's largest oil field) alone accounts for 8% of the U.S. domestic oil production.
The Alaska Permanent Fund is a legislatively controlled appropriation established in 1976 to manage a surplus in state petroleum revenues from the recently constructed Trans-Alaska Pipeline System. From its initial principal of $734,000, the fund has grown to $40 billion as a result of oil royalties and capital investment programs. Starting in 1982, dividends from the fund's annual growth have been paid out each year to eligible Alaskans, ranging from $331.29 in 1984 to $3,269.00 in 2008 (which included a one-time $1200 "Resource Rebate"). Every year, the state legislature takes out 8 percent from the earnings, puts 3 percent back into the principal for inflation proofing, and the remaining 5 percent is distributed to all qualifying Alaskans. To qualify for the Alaska State Permanent Fund one must have lived in the state for a minimum of 12 months, and maintain constant residency.
The cost of goods in Alaska has long been higher than in the contiguous 48 states. This has changed for the most part in Anchorage and to a lesser extent in Fairbanks, where the cost of living has dropped somewhat in the past five years. Federal government employees, particularly United States Postal Service (USPS) workers and active-duty military members, receive a Cost of Living Allowance usually set at 25% of base pay because, while the cost of living has gone down, it is still one of the highest in the country. The introduction of big-box stores in Anchorage, Fairbanks (Wal-Mart in March 2004), and Juneau also did much to lower prices. However, rural Alaska suffers from extremely high prices for food and consumer goods, compared to the rest of the country due to the relatively limited transportation infrastructure. Many rural residents come into these cities and purchase food and goods in bulk from warehouse clubs like Costco and Sam's Club. Some have embraced the free shipping offers of some online retailers to purchase items much more cheaply than they could in their own communities, if they are available at all.
Due to the northern climate and steep terrain, relatively little farming occurs in Alaska. Most farms are in either the Matanuska Valley, about northeast of Anchorage, or on the Kenai Peninsula, about southwest of Anchorage. The short 100-day growing season limits the crops that can be grown, but the long sunny summer days make for productive growing seasons. The primary crops are potatoes, carrots, lettuce, and cabbage. Farmers exhibit produce at the Alaska State Fair. "Alaska Grown" is used as an agricultural slogan. Alaska has an abundance of seafood, with the primary fisheries in the Bering Sea and the North Pacific, and seafood is one of the few food items that is often cheaper within the state than outside it. Many Alaskans fish the rivers during salmon season to gather significant quantities of their household diet while fishing for subsistence, sport, or both. Hunting for subsistence, primarily caribou, moose, and Dall sheep is still common in the state, particularly in remote Bush communities. An example of a traditional native food is Akutaq, the Eskimo ice cream, which can consist of reindeer fat, seal oil, dried fish meat and local berries. Most food in Alaska is transported into the state from "outside", and shipping costs make food in the cities relatively expensive. In rural areas, subsistence hunting and gathering is an essential activity because imported food is prohibitively expensive. The cost of importing food to villages begins at 7¢ per pound (15¢/kg) and rises rapidly to 50¢ per pound ($1.10/kg) or more. The cost of delivering a seven-pound gallon of milk is about $3.50 in many villages where per capita income can be $20,000 or less. Fuel for snow machines and boats that consume a couple of gallons per hour can exceed $8.00 per gallon.
Alaska has few road connections compared to the rest of the U.S. The state's road system covers a relatively small area of the state, linking the central population centers and the Alaska Highway, the principal route out of the state through Canada. The state capital, Juneau, is not accessible by road, only a car ferry, which has spurred several debates over the decades about moving the capital to a city on the road system, or building a road connection from Haines. The western part of Alaska has no road system connecting the communities with the rest of Alaska. One unique feature of the Alaska Highway system is the Anton Anderson Memorial Tunnel, an active Alaska Railroad tunnel recently upgraded to provide a paved roadway link with the isolated community of Whittier on Prince William Sound to the Seward Highway about southeast of Anchorage. At the tunnel was the longest road tunnel in North America until 2007. The tunnel is the longest combination road and rail tunnel in North America.
Built around 1915, the Alaska Railroad (ARR) played a key role in the development of Alaska through the 20th century. It links north Pacific shipping through providing critical infrastructure with tracks that run from Seward to Interior Alaska by way of South Central Alaska, passing through Anchorage, Eklutna, Wasilla, Talkeetna, Denali, and Fairbanks, with spurs to Whittier, Palmer and North Pole. The cities, towns, villages, and region served by ARR tracks are known statewide as "The Railbelt". In recent years, the ever-improving paved highway system began to eclipse the railroad's importance in Alaska's economy. The railroad, though famed for its summertime tour passenger service, played a vital role in Alaska's development, moving freight into Alaska while transporting natural resources southward (i.e., coal from the Usibelli coal mine near Healy to Seward and gravel from the Matanuska Valley to Anchorage). The Alaska Railroad was one of the last railroads in North America to use cabooses in regular service and still uses them on some gravel trains. It continues to offer one of the last flag stop routes in the country. A stretch of about of track along an area north of Talkeetna remains inaccessible by road; the railroad provides the only transportation to rural homes and cabins in the area; until construction of the Parks Highway in the 1970s, the railroad provided the only land access to most of the region along its entire route. In northern Southeast Alaska, the White Pass and Yukon Route also partly runs through the State from Skagway northwards into Canada (British Columbia and Yukon Territory), crossing the border at White Pass Summit. This line is now mainly used by tourists, often arriving by cruise liner at Skagway. It featured in the 1983 BBC television series Great Little Railways.
Most cities, towns and villages in the state do not have road or highway access; the only modes of access involve travel by air, river, or the sea. Alaska's well-developed state-owned ferry system (known as the Alaska Marine Highway) serves the cities of Alaska Panhandle, the Gulf Coast and the Alaska Peninsula. The system also operates a ferry service from Bellingham, Washington and Prince Rupert, British Columbia in Canada through the Inside Passage to Skagway. The Inter-Island Ferry Authority also serves as an important marine link for many communities in the Prince of Wales Island region of Southeast and works in concert with the Alaska Marine Highway. In recent years, large cruise ships began creating a summertime tourism market, mainly connecting the Pacific Northwest to Southeast Alaska and, to a lesser degree, towns along the north gulf coast. Several times each summer, the population of Ketchikan sharply rises for a few hours when two ships dock to debark more than a thousand passengers each while four other ships lie at anchor nearby, waiting their turn at the dock.
Cities not served by road, sea, or river can be reached only by air, foot, dogsled, or snowmachine accounting for Alaska's extremely well-developed bush air services—an Alaskan novelty. Anchorage itself, and to a lesser extent Fairbanks, are served by many major airlines. Because of limited highway access, air travel remains the most efficient form of transportation in and out of the state. Anchorage recently completed extensive remodeling and construction at Ted Stevens Anchorage International Airport to help accommodate the upsurge in tourism (in 2000–2001, the latest year for which data is available, 2.4 million total arrivals to Alaska were counted, 1.7 million by air travel; 1.4 million were visitors). Regular flights to most villages and towns within the state that are commercially viable are challenging to provide, so they are heavily subsidized by the federal government through the Essential Air Service program. Alaska Airlines is the only major airline offering in-state travel with jet service (sometimes in combination cargo and passenger Boeing 737-400s) from Anchorage and Fairbanks to regional hubs like Bethel, Nome, Kotzebue, Dillingham, Kodiak, and other larger communities as well as to major Southeast and Alaska Peninsula communities. The bulk of remaining commercial flight offerings come from small regional commuter airlines such as Era Aviation, PenAir, and Frontier Flying Service. The smallest towns and villages must rely on scheduled or chartered bush flying services using general aviation aircraft such as the Cessna Caravan, the most popular aircraft in use in the state. Much of this service can be attributed to the Alaska bypass mail program which subsidizes bulk mail delivery to Alaskan rural communities. The program requires 70% of that subsidy to go to carriers who offer passenger service to the communities. Many communities have small air taxi services, such as Hudson's Air Service, Kantishna Air Taxi, and Talkeetna Air Taxi. These operations, though now catering primarily to tourists, originated from the demand for customized transport to remote areas. Perhaps the most quintessentially Alaskan plane is the bush seaplane. The world's busiest seaplane base is Lake Hood, located next to Ted Stevens Anchorage International Airport, where flights bound for remote villages without an airstrip carry passengers, cargo, and many items from stores and warehouse clubs. Alaska has the highest number of pilots per capita of any U.S. state: out of the estimated 663,661 residents, 8,550 are pilots, or about one in 78.
Another Alaskan transportation method is the dogsled. In modern times (that is, any time after the mid-late 1920s), dog mushing is more of a sport than a true means of transportation. Various races are held around the state, but the best known is the Iditarod Trail Sled Dog Race, a 1150-mile (1850 km) trail from Anchorage to Nome (although the mileage varies from year to year, the official distance is set at 1049 miles). The race commemorates the famous 1925 serum run to Nome in which mushers and dogs like Togo and Balto took much-needed medicine to the diphtheria-stricken community of Nome when all other means of transportation had failed. Mushers from all over the world come to Anchorage each March to compete for cash, prizes, and prestige. The "Serum Run" is another sled dog race that more accurately follows the route of the famous 1925 relay, leaving from the community of Nenana (southwest of Fairbanks) to Nome. In areas not served by road or rail, primary transportation in summer is by all-terrain vehicle and in winter by snowmobile or "snow machine," as it is commonly referred to in Alaska.
Like all other U.S. states, Alaska is governed as a republic, with three branches of government: an executive branch consisting of the Governor of Alaska and the other independently elected constitutional officers; a legislative branch consisting of the Alaska House of Representatives and Alaska Senate; and a judicial branch consisting of the Alaska Supreme Court and lower courts. The State of Alaska employs approximately 15,000 employees statewide. The Alaska Legislature consists of a 40-member House of Representatives and a 20-member Senate. Senators serve four year terms and House members two. The Governor of Alaska serves four-year terms. The lieutenant governor runs separately from the governor in the primaries, but during the general election, the nominee for governor and nominee for lieutenant governor run together on the same ticket. Alaska's court system has four levels: the Alaska Supreme Court, the court of appeals, the superior courts and the district courts. The superior and district courts are trial courts. Superior courts are courts of general jurisdiction, while district courts only hear certain types of cases, including misdemeanor criminal cases and civil cases valued up to $100,000. The Supreme Court and the Court Of Appeals are appellate courts. The Court Of Appeals is required to hear appeals from certain lower-court decisions, including those regarding criminal prosecutions, juvenile delinquency, and habeas corpus. The Supreme Court hears civil appeals and may in its discretion hear criminal appeals.
Although Alaska entered the union as a Democratic state, since the early 1970s Alaska has been characterized as a Republican-leaning state. Local political communities have often worked on issues related to land use development, fishing, tourism, and individual rights. Alaska Natives, while organized in and around their communities, have been active within the Native corporations. These have been given ownership over large tracts of land, which require stewardship. Alaska is the only state in which possession of one ounce or less of marijuana in one's home is completely legal under state law, though the federal law remains in force. The state has an independence movement favoring a vote on secession from the United States, with the Alaska Independence Party labeled as one of "the most significant state-level third parties operating in the 20th century". Six Republicans and four Democrats have served as governor of Alaska. In addition, Republican Governor Wally Hickel was elected to the office for a second term in 1990 after leaving the Republican party and briefly joining the Alaskan Independence Party ticket just long enough to be reelected. He subsequently officially rejoined the Republican party in 1994.
To finance state government operations, Alaska depends primarily on petroleum revenues and federal subsidies. This allows it to have the lowest individual tax burden in the United States, and be one of only five states with no state sales tax, one of seven states that do not levy an individual income tax, and one of two states that has neither. The Department of Revenue Tax Division reports regularly on the state's revenue sources. The Department also issues an annual summary of its operations, including new state laws that directly affect the tax division. While Alaska has no state sales tax, 89 municipalities collect a local sales tax, from 1–7.5%, typically 3–5%. Other local taxes levied include raw fish taxes, hotel, motel, and bed-and-breakfast 'bed' taxes, severance taxes, liquor and tobacco taxes, gaming (pull tabs) taxes, tire taxes and fuel transfer taxes. A part of the revenue collected from certain state taxes and license fees (such as petroleum, aviation motor fuel, telephone cooperative) is shared with municipalities in Alaska. Fairbanks has one of the highest property taxes in the state as no sales or income taxes are assessed in the Fairbanks North Star Borough (FNSB). A sales tax for the FNSB has been voted on many times, but has yet to be approved, leading law makers to increase taxes dramatically on other goods such as liquor and tobacco. In 2008 the Tax Foundation ranked Alaska as having the 4th most "business friendly" tax policy. More "friendly" states were Wyoming, Nevada, and South Dakota.
In presidential elections, the state's electoral college votes have been won by the Republican nominee in every election since statehood, except for 1964. No state has voted for a Democratic presidential candidate fewer times. Alaska supported Democratic nominee Lyndon B. Johnson in the landslide year of 1964, although the 1960 and 1968 elections were close. Republican John McCain defeated Democrat Barack Obama in Alaska, 59.49% to 37.83%. McCain's running mate was Sarah Palin, the state's governor and the first Alaskan on a major party ticket. The Alaska Bush, the city of Juneau and midtown and downtown Anchorage have been strongholds of the Democratic party. Matanuska-Susitna Borough and South Anchorage typically have the strongest Republican showing. As of 2004, well over half of all registered voters have chosen "Non-Partisan" or "Undeclared" as their affiliation, despite recent attempts to close primaries. Because of its population relative to other U.S. states, Alaska has only one member in the U.S. House of Representatives. This seat is currently being held by Republican Don Young, who was re-elected to his 19th consecutive term in 2008. On November 19, 2008, Democrat Mark Begich, mayor of Anchorage, defeated long-time Republican senator Ted Stevens. Stevens had been convicted on seven felony counts of failing to report gifts on Senate financial discloser forms one week before the election. The conviction was set aside in April 2009 after evidence of prosecutorial misconduct emerged. Republican Frank Murkowski held the state's other senatorial position. After being elected governor in 2002, he resigned from the Senate and appointed his daughter, State Representative Lisa Murkowski as his successor. In response to a subsequent ballot initiative, the state legislature attempted to amend the law to limit the length of gubernatorial appointments. She won a full six-year term in 2004. In 2006 Frank Murkowski was defeated in the Republican primary by Sarah Palin, who in 2008 became the Republican nominee for Vice President of the United States.
Alaska is not divided into counties, as most of the other U.S. states, but it is divided into "boroughs". Many of the more densely populated parts of the state are part of Alaska's sixteen boroughs, which function somewhat similarly to counties in other states. However, unlike county-equivalents in the other 49 states, the boroughs do not cover the entire land area of the state. The area not part of any borough is referred to as the Unorganized Borough. The Unorganized Borough has no government of its own, but the U.S. Census Bureau in cooperation with the state divided the Unorganized Borough into 11 census areas solely for the purposes of statistical analysis and presentation. A recording district is a mechanism for administration of the public record in Alaska. The state is divided into 34 recording districts which are centrally administered under a State Recorder. All recording districts use the same acceptance criteria, fee schedule, etc., for accepting documents into the public record. Whereas many U.S. states use a three-tiered system of decentralization—state/county/township—most of Alaska uses only two tiers—state/borough. Owing to the low population density, most of the land is located in the Unorganized Borough which, as the name implies, has no intermediate borough government of its own, but is administered directly by the state government. Currently (2000 census) 57.71% of Alaska's area has this status, with 13.05% of the population. For statistical purposes the United States Census Bureau divides this territory into census areas. Anchorage merged the city government with the Greater Anchorage Area Borough in 1975 to form the Municipality of Anchorage, containing the city proper and the communities of Eagle River, Chugiak, Peters Creek, Girdwood, Bird, and Indian. Fairbanks has a separate borough (the Fairbanks North Star Borough) and municipality (the City of Fairbanks). The state's most populous city is Anchorage, home to 278,700 people in 2006, 225,744 of whom live in the urbanized area. The richest location in Alaska by per capita income is Halibut Cove ($89,895). Yakutat City, Sitka, Juneau, and Anchorage are the four largest cities in the U.S. by area.
The Alaska Department of Education and Early Development administers many school districts in Alaska. In addition, the state operates a boarding school, Mt. Edgecumbe High School in Sitka; and provides partial funding for other boarding schools including, Nenana Student Living Center in Nenana, and The Galena Interior Learning Academy in Galena. There are more than a dozen colleges and universities in Alaska. Accredited universities in Alaska include the University of Alaska Anchorage, University of Alaska Fairbanks, University of Alaska Southeast, and Alaska Pacific University. 43% of the population attends or attended college. Alaska has had a problem with a "brain drain". Many of its young people, including most of the highest academic achievers, leave the state after high school graduation and do not return. The University of Alaska has attempted to combat this by offering partial four-year scholarships to the top 10% of Alaska high school graduates, via the Alaska Scholars Program. Public health and public safety. Alaska residents have long had a problem with alcohol use and abuse. Many rural communities in Alaska have outlawed its import. This problem directly relates to Alaska's high rate of Fetal alcohol syndrome (FAS) as well as contributing to the high rate of suicides and teenage pregnancies. Suicide rates for rural residents are higher than urban. Domestic abuse and other violent crimes are also at high levels in the state; this is in part linked to alcohol abuse.
Some of Alaska's popular annual events are the Iditarod Trail Sled Dog Race that starts in Anchorage and ends in Nome, World Ice Art Championships in Fairbanks, the Alaska Hummingbird Festival in Ketchikan, the Sitka Whale Fest, and the Stikine River Garnet Fest in Wrangell. The Stikine River features the largest springtime concentration of American Bald Eagles in the world. The Alaska Native Heritage Center celebrates the rich heritage of Alaska's 11 cultural groups. Their purpose is to enhance self-esteem among Native people and to encourage cross-cultural exchanges among all people. The Alaska Native Arts Foundation promotes and markets Native art from all regions and cultures in the State, both on the internet; at its gallery in Anchorage, 500 West Sixth Avenue, and at the Alaska House New York, 109 Mercer Street in SoHo. Alaska Natives – Inuit, Inupiaq or Yupik drummers and dancers – give informal performances in the lobby of the Alaska Native Medical Center in Anchorage on weekday evenings.
Influences on music in Alaska include the traditional music of Alaska Natives as well as folk music brought by later immigrants from Russia and Europe. Prominent musicians from Alaska include singer Jewel, traditional Aleut flautist Mary Youngblood, folk singer-songwriter Libby Roderick, Christian music singer/songwriter Lincoln Brewster, metal/post hardcore band 36 Crazyfists and the groups Pamyua and Portugal. The Man. There are many established music festivals in Alaska, including the Alaska Folk Festival, the Fairbanks Summer Arts Festival the Anchorage Folk Festival, the Athabascan Old-Time Fiddling Festival, the Sitka Jazz Festival, and the Sitka Summer Music Festival. The most prominent symphony in Alaska is the Anchorage Symphony Orchestra, though the Fairbanks Symphony Orchestra and Juneau Symphony are also notable. The Anchorage Opera is currently the state's only professional opera company, though there are several volunteer and semi-professional organizations in the state as well. The official state song of Alaska is "Alaska's Flag", which was adopted in 1955; it celebrates the flag of Alaska.
Alaska's first independent picture all made on place was in the silent years. The Chechahcos, was released in 1924 by the Alaska Moving Picture Corp. It was the only film the company made. One of the most prominent movies filmed in Alaska is MGM's Academy Award winning classic "Mala The Magnificent" starring Alaska's own Ray Mala. In 1932 an expedition set out from MGM's studios in Hollywood to Alaska to film what was then billed as "The Biggest Picture Ever Made." Upon arriving in Alaska, they set up "Camp Hollywood" in Northwest Alaska, where they lived during the duration of the filming. Louis B. Mayer spared no expense in making sure they had everything they needed during their stay—he even sent the famous chef from the Hotel Roosevelt on Hollywood Blvd (the site of the first Oscars) with them to Alaska to cook for them. When "Eskimo" premiered at the famed Astor Theatre in Times Square, New York, the studio received the largest amount of feedback in the history of the studio up to that time. "Eskimo" was critically acclaimed and released worldwide; as a result Inupiat Eskimo actor Ray Mala became an international movie star. "Eskimo" is significant for the following: winning the very first Oscar for Best Film Editing at the Academy Awards, for forever preserving Inupiat culture on film, and for being the first motion picture to be filmed in an all native language (Inupiat). The psychological thriller "Insomnia", starring Al Pacino and Robin Williams was shot in Canada, but was set in Alaska. The 2007 horror feature "30 Days of Night" is set in Barrow, Alaska but was filmed in New Zealand. Most films and television shows set in Alaska are not filmed there; for example, "Northern Exposure", set in the fictional town of Cicely, Alaska, was actually filmed in Roslyn, Washington. The 1983 Disney movie "Never Cry Wolf" was at least partially shot in Alaska. The 1991 film "White Fang", starring Ethan Hawke, was filmed in and around Haines, Alaska. The 1999 John Sayles film "Limbo", starring David Strathairn, Mary Elizabeth Mastrantonio and Kris Kristofferson, was filmed in Juneau. The 2007 film directed by Sean Penn, "Into The Wild" was partially filmed and set in Alaska. The film, which is based on the novel of the same name, follows the adventures of Christopher McCandless, who died in a remote abandoned bus in Alaska in 1992.
Agriculture is the production of food and goods through farming. Agriculture was the key development that led to the rise of human civilization, with the husbandry of domesticated animals and plants (i.e. crops) creating food surpluses that enabled the development of more densely populated and stratified societies. The study of agriculture is known as agricultural science. Central to human society, agriculture is also observed in certain species of ant and termite. Agriculture encompasses a wide variety of specialties and techniques, including ways to expand the lands suitable for plant raising, by digging water-channels and other forms of irrigation. Cultivation of crops on arable land and the pastoral herding of livestock on rangeland remain at the foundation of agriculture. In the past century there has been increasing concern to identify and quantify various forms of agriculture. In the developed world the range usually extends between sustainable agriculture (e.g. permaculture or organic agriculture) and intensive farming (e.g. industrial agriculture). Modern agronomy, plant breeding, pesticides and fertilizers, and technological improvements have sharply increased yields from cultivation, and at the same time have caused widespread ecological damage and negative human health effects. Selective breeding and modern practices in animal husbandry such as intensive pig farming (and similar practices applied to the chicken) have similarly increased the output of meat, but have raised concerns about animal cruelty and the health effects of the antibiotics, growth hormones, and other chemicals commonly used in industrial meat production. The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials. In the 2000s, plants have been used to grow biofuels, biopharmaceuticals, bioplastics, and pharmaceuticals. Specific foods include cereals, vegetables, fruits, and meat. Fibers include cotton, wool, hemp, silk and flax. Raw materials include lumber and bamboo. Other useful materials are produced by plants, such as resins. Biofuels include methane from biomass, ethanol, and biodiesel. Cut flowers, nursery plants, tropical fish and birds for the pet trade are some of the ornamental products. In 2007, about one third of the world's workers were employed in agriculture. The services sector has overtaken agriculture as the economic sector employing the most people worldwide. Despite the size of its workforce, agricultural production accounts for less than five percent of the gross world product (an aggregate of all gross domestic products).
Agriculture has played a key role in the development of human civilization. Until the Industrial Revolution, the vast majority of the human population labored in agriculture. Development of agricultural techniques has steadily increased agricultural productivity, and the widespread diffusion of these techniques during a time period is often called an agricultural revolution. A remarkable shift in agricultural practices has occurred over the past century in response to new technologies. In particular, the Haber-Bosch method for synthesizing ammonium nitrate made the traditional practice of recycling nutrients with crop rotation and animal manure less necessary. Synthetic nitrogen, along with mined rock phosphate, pesticides and mechanization, have greatly increased crop yields in the early 20th century. Increased supply of grains has led to cheaper livestock as well. Further, global yield increases were experienced later in the 20th century when high-yield varieties of common staple grains such as rice, wheat, and corn (maize) were introduced as a part of the Green Revolution. The Green Revolution exported the technologies (including pesticides and synthetic nitrogen) of the developed world to the developing world. Thomas Malthus famously predicted that the Earth would not be able to support its growing population, but technologies such as the Green Revolution have allowed the world to produce a surplus of food. Many governments have subsidized agriculture to ensure an adequate food supply. These agricultural subsidies are often linked to the production of certain commodities such as wheat, corn (maize), rice, soybeans, and milk. These subsidies, especially when instituted by developed countries have been noted as protectionist, inefficient, and environmentally damaging. In the past century agriculture has been characterized by enhanced productivity, the use of synthetic fertilizers and pesticides, selective breeding, mechanization, water contamination, and farm subsidies. Proponents of organic farming such as Sir Albert Howard argued in the early 1900s that the overuse of pesticides and synthetic fertilizers damages the long-term fertility of the soil. While this feeling lay dormant for decades, as environmental awareness has increased in the 2000s there has been a movement towards sustainable agriculture by some farmers, consumers, and policymakers. In recent years there has been a backlash against perceived external environmental effects of mainstream agriculture, particularly regarding water pollution, resulting in the organic movement. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding. Recent mainstream technological developments include genetically modified food. In late 2007, several factors pushed up the price of grains consumed by humans as well as used to feed poultry and dairy cows and other cattle, causing higher prices of wheat (up 58%), soybean (up 32%), and maize (up 11%) over the year. Food riots took place in several countries across the world. Contributing factors included drought in Australia and elsewhere, increasing demand for grain-fed animal products from the growing middle classes of countries such as China and India, diversion of foodgrain to biofuel production and trade restrictions imposed by several countries. An epidemic of stem rust on wheat caused by race Ug99 is currently spreading across Africa and into Asia and is causing major concern. Approximately 40% of the world's agricultural land is seriously degraded. In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa.
Since its development roughly 10,000 years ago, agriculture has expanded vastly in geographical coverage and yields. Throughout this expansion, new technologies and new crops were integrated. Even then crops were modified through cross-breeding for better yields. Agricultural practices such as irrigation, crop rotation, fertilizers, and pesticides were developed long ago, but have made great strides in the past century. The history of agriculture has played a major role in human history, as agricultural progress has been a crucial factor in worldwide socio-economic change. Wealth-concentration and militaristic specializations rarely seen in hunter-gatherer cultures are commonplace in societies which practice agriculture. So, too, are arts such as epic literature and monumental architecture, as well as codified legal systems. When farmers became capable of producing food beyond the needs of their own families, others in their society were freed to devote themselves to projects other than food acquisition. Historians and anthropologists have long argued that the development of agriculture made civilization possible.
The Fertile Crescent of Western Asia, Egypt, and India were sites of the earliest planned sowing and harvesting of plants that had previously been gathered in the wild. Independent development of agriculture occurred in northern and southern China, Africa's Sahel, New Guinea and several regions of the Americas. The eight so-called Neolithic founder crops of agriculture appear: first emmer wheat and einkorn wheat, then hulled barley, peas, lentils, bitter vetch, chick peas and flax. By 7000 BC, small-scale agriculture reached Egypt. From at least 7000 BC the Indian subcontinent saw farming of wheat and barley, as attested by archaeological excavation at Mehrgarh in Balochistan. By 6000 BC, mid-scale farming was entrenched on the banks of the Nile. About this time, agriculture was developed independently in the Far East, with rice, rather than wheat, as the primary crop. Chinese and Indonesian farmers went on to domesticate taro and beans including mung, soy and azuki. To complement these new sources of carbohydrates, highly organized net fishing of rivers, lakes and ocean shores in these areas brought in great volumes of essential protein. Collectively, these new methods of farming and fishing inaugurated a human population boom that dwarfed all previous expansions and continues today. By 5000 BC, the Sumerians had developed core agricultural techniques including large-scale intensive cultivation of land, mono-cropping, organized irrigation, and the use of a specialized labor force, particularly along the waterway now known as the Shatt al-Arab, from its Persian Gulf delta to the confluence of the Tigris and Euphrates. Domestication of wild aurochs and mouflon into cattle and sheep, respectively, ushered in the large-scale use of animals for food/fiber and as beasts of burden. The shepherd joined the farmer as an essential provider for sedentary and semi-nomadic societies. Maize, manioc, and arrowroot were first domesticated in the Americas as far back as 5200 BC. The potato, tomato, pepper, squash, several varieties of bean, tobacco, and several other plants were also developed in the New World, as was extensive terracing of steep hillsides in much of Andean South America. The Greeks and Romans built on techniques pioneered by the Sumerians but made few fundamentally new advances. Southern Greeks struggled with very poor soils, yet managed to become a dominant society for years. The Romans were noted for an emphasis on the cultivation of crops for trade.
During the Middle Ages, farmers in North Africa, the Near East, and Europe began making use of agricultural technologies including irrigation systems based on hydraulic and hydrostatic principles, machines such as norias, water-raising machines, dams, and reservoirs. This combined with the invention of a three-field system of crop rotation and the moldboard plow greatly improved agricultural efficiency.
After 1492, a global exchange of previously local crops and livestock breeds occurred. Key crops involved in this exchange included the tomato, maize, potato, manioc, cocoa bean and tobacco going from the New World to the Old, and several varieties of wheat, spices, coffee, and sugar cane going from the Old World to the New. The most important animal exportation from the Old World to the New were those of the horse and dog (dogs were already present in the pre-Columbian Americas but not in the numbers and breeds suited to farm work). Although not usually food animals, the horse (including donkeys and ponies) and dog quickly filled essential production roles on western-hemisphere farms. The potato became an important staple crop in northern Europe. Since being introduced by Portuguese in the 16th century, maize and manioc have replaced traditional African crops as the continent's most important staple food crops. By the early 1800s, agricultural techniques, implements, seed stocks and cultivated plants selected and given a unique name because of its decorative or useful characteristics had so improved that yield per land unit was many times that seen in the Middle Ages. With the rapid rise of mechanization in the late 19th and 20th centuries, particularly in the form of the tractor, farming tasks could be done with a speed and on a scale previously impossible. These advances have led to efficiencies enabling certain modern farms in the United States, Argentina, Israel, Germany, and a few other nations to output volumes of high-quality produce per land unit at what may be the practical limit. The Haber-Bosch method for synthesizing ammonium nitrate represented a major breakthrough and allowed crop yields to overcome previous constraints. In the past century agriculture has been characterized by enhanced productivity, the substitution of labor for synthetic fertilizers and pesticides, water pollution, and farm subsidies. In recent years there has been a backlash against the external environmental effects of conventional agriculture, resulting in the organic movement. The cereals rice, corn, and wheat provide 60% of human food supply. Between 1700 and 1980, "the total area of cultivated land worldwide increased 466%" and yields increased dramatically, particularly because of selectively-bred high-yielding varieties, fertilizers, pesticides, irrigation, and machinery. For example, irrigation increased corn yields in eastern Colorado by 400 to 500% from 1940 to 1997. However, concerns have been raised over the sustainability of intensive agriculture. Intensive agriculture has become associated with decreased soil quality in India and Asia, and there has been increased concern over the effects of fertilizers and pesticides on the environment, particularly as population increases and food demand expands. The monocultures typically used in intensive agriculture increase the number of pests, which are controlled through pesticides. Integrated pest management (IPM), which "has been promoted for decades and has had some notable successes" has not significantly affected the use of pesticides because policies encourage the use of pesticides and IPM is knowledge-intensive. Although the "Green Revolution" significantly increased rice yields in Asia, yield increases have not occurred in the past 15–20 years. The genetic "yield potential" has increased for wheat, but the yield potential for rice has not increased since 1966, and the yield potential for maize has "barely increased in 35 years". It takes a decade or two for herbicide-resistant weeds to emerge, and insects become resistant to insecticides within about a decade. Crop rotation helps to prevent resistances. Agricultural exploration expeditions, since the late nineteenth century, have been mounted to find new species and new agricultural practices in different areas of the world. Two early examples of expeditions include Frank N. Meyer's fruit- and nut-collecting trip to China and Japan from 1916-1918 and the Dorsett-Morse Oriental Agricultural Exploration Expedition to China, Japan, and Korea from 1929-1931 to collect soybean germplasm to support the rise in soybean agriculture in the United States. In 2005, the agricultural output of China was the largest in the world, accounting for almost one-sixth of world share, followed by the EU, India and the USA, according to the International Monetary Fund. More than 40 million Chinese farmers have been displaced from their land in recent years, usually for economic development, contributing to the 87,000 demonstrations and riots across China in 2005. Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 2.6 times more productive than it was in 1948. Six countries - the US, Canada, France, Australia, Argentina and Thailand - supply 90% of grain exports. The United States controls almost half of world grain exports. Water deficits, which are already spurring heavy grain imports in numerous middle-sized countries, including Algeria, Iran, Egypt, and Mexico, may soon do the same in larger countries, such as China or India.
Cropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer. Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10-20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs. Further industrialization lead to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time are other kinds of annual cropping systems known as polycultures. In tropical environments, all of these cropping systems are practiced. In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual cropping is the dominant farming system. The last century has seen the intensification, concentration and specialization of agriculture, relying upon new technologies of agricultural chemicals (fertilizers and pesticides), mechanization, and plant breeding (hybrids and GMO's). In the past few decades, a move towards sustainability in agriculture has also developed, integrating ideas of socio-economic justice and conservation of resources and the environment within a farming system. This has led to the development of many responses to the conventional agriculture approach, including organic agriculture, urban agriculture, community supported agriculture, ecological or biological agriculture, integrated farming and holistic management, as well as an increased trend towards agricultural diversification.
Animals, including horses, mules, oxen, camels, llamas, alpacas, and dogs, are often used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers. Animal husbandry not only refers to the breeding and raising of animals for meat or to harvest animal products (like milk, eggs, or wool) on a continual basis, but also to the breeding and care of species for work and companionship. Livestock production systems can be defined based on feed source, as grassland - based, mixed, and landless. Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30-40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastic (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops. Approximately 68% of all agricultural land is permanent pastures used in the production of livestock. Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in OECD member countries. In the U.S., 70% of the grain grown is fed to animals on feedlots. Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution.
Tillage is the practice of plowing soil to prepare for planting or for nutrient incorporation or for pest control. Tillage varies in intensity from conventional to no-till. It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO2, and reduces the abundance and diversity of soil organisms. Pest control includes the management of weeds, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort. Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and mined minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures. Water management is where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year. Agriculture represents 70% of freshwater use worldwide.
In the United States, food costs attributed to processing, distribution, and marketing have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. From 1960 to 1980 the farm share was around 40%, but by 1990 it had declined to 30% and by 1998, 22.2%. Market concentration has increased in the sector as well, with the top 20 food manufacturers accounting for half the food-processing value in 1995, over double that produced in 1954. As of 2000 the top six US supermarket groups had 50% of sales compared to 32% in 1992. Although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.
Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles gave plant breeders a better understanding of genetics and brought great insights to the techniques utilized by plant breeders. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism. Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray an ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley. The green revolution popularized the use of conventional hybridization to increase yield many folds by creating "high-yielding varieties". For example, average yields of corn (maize) in the USA have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging)..
Genetically Modified Organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops. After mechanical tomato-harvesters were developed in the early 1960s, agricultural scientists genetically modified tomatoes to be more resistant to mechanical handling. More recently, genetic engineering is being employed in various parts of the world, to create crops with other beneficial traits.
Roundup-Ready seed has a herbicide resistant gene implanted into its genome that allows the plants to tolerate exposure to glyphosate. Roundup is a trade name for a glyphosate based product, which is a systemic, non-selective herbicide used to kill weeds. Roundup-Ready seeds allow the farmer to grow a crop that can be sprayed with glyphosate to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. Today, 92% of soybean acreage in the US is planted with genetically-modified herbicide-tolerant plants. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.
Other GMO crops utilized by growers include insect-resistant crops, which have a gene from the soil bacterium "Bacillus thuringiensis" (Bt), which produces a toxin specific to insects. These crops protect plants from damage by insects; one such crop is Starlink. Another is cotton, which accounts for 63% of US cotton acreage. Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least nineteen diseases did so through crossing with wild populations of tomatoes. Costs and Benefits of GMOs. Genetic engineers may someday develop transgenic plants which would allow for irrigation, drainage, conservation, sanitary engineering, and maintaining or increasing yields while requiring fewer fossil fuel derived inputs than conventional crops. Such developments would be particularly important in areas which are normally arid and rely upon constant irrigation, and on large scale farms. However, genetic engineering of plants has proven to be controversial. Many issues surrounding food security and environmental impacts have risen regarding GMO practices. For example, GMOs are questioned by some ecologists and economists concerned with GMO practices such as terminator seeds, which is a genetic modification that creates sterile seeds. Terminator seeds are currently under strong international opposition and face continual efforts of global bans. Another controversial issue is the patent protection given to companies that develop new types of seed using genetic engineering. Since companies have intellectual ownership of their seeds, they have the power to dictate terms and conditions of their patented product. Currently, ten seed companies control over two-thirds of the global seed sales. Vandana Shiva argues that these companies are guilty of biopiracy by patenting life and exploiting organisms for profit Farmers using patented seed are restricted from saving seed for subsequent plantings, which forces farmers to buy new seed every year. Since seed saving is a traditional practice for many farmers in both developing and developed countries, GMO seeds legally bind farmers to change their seed saving practices to buying new seed every year. Locally adapted seeds are an essential hertitage that has the potential to be lost with current hybridized crops and GMOs. Locally adapted seeds, also called land races or crop eco-types, are important because they have adapted over time to the specific microclimates, soils, other environmental conditions, field designs, and ethnic preference indigenous to the exact area of cultivation. Introducing GMOs and hybridized commercial seed to an area brings the risk of cross-pollination with local land races Therefore, GMOs pose a threat to the sustainability of land races and the ethnic heritage of cultures. Once seed contains transgenic material, it becomes subject to the conditions of the seed company that owns the patent of the transgenic material. There is also concern that GMOs will cross-pollinate with wild species and permanently alter native populations’ genetic integrity; there are already identified populations of wild plants with transgenic genes. GMO gene flow to related weed species is a concern, as well as cross-pollination with non-transgenic crops. Since many GMO crops are harvested for their seed, such as rapeseed, seed spillage in is problematic for volunteer plants in rotated fields, as well as seed-spillage during transportation.
Food security issues also coincide with food safety and food labeling concerns. Currently a global treaty, the BioSafety Protocol, regulates the trade of GMOs. The EU currently requires all GMO foods to be labeled, whereas the US does not require transparent labeling of GMO foods. Since there are still questions regarding the safety and risks associated with GMO foods, some believe the public should have the freedom to choose and know what they are eating and require all GMO products to be labeled.
Agriculture imposes external costs upon society through pesticides, nutrient runoff, excessive water usage, and assorted other problems. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare. A 2005 analysis of these costs in the USA concluded that cropland imposes approximately $5 to 16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies concluded that more should be done to internalize external costs, and neither included subsidies in their analysis, but noted that subsidies also influence the cost of agriculture to society. Both focused on purely fiscal impacts. The 2000 review included reported pesticide poisonings but did not include speculative chronic effects of pesticides, and the 2004 review relied on a 1992 estimate of the total impact of pesticides.
A senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said "Livestock are one of the most significant contributors to today's most serious environmental problems". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO2 equivalents. By comparison, all transportation emits 13.5% of the CO2. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO2,) and 37% of all human-induced methane (which is 23 times as warming as CO2. It also generates 64% of the ammonia, which contributes to acid rain and acidification of ecosystems. Livestock expansion is cited as a key factor driving deforestation, in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity.
Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity. Estimates of the amount of land transformed by humans vary from 39–50%. Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented. The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land. Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).
Eutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems.
Pesticide use has increased since 1950 to 2.5 million tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that 3 million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the 'pesticide treadmill' in which pest resistance warrants the development of a new pesticide. An alternative argument is that the way to 'save the environment' and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides simply replace good agronomic practices such as crop rotation.
Climate change has the potential to affect agriculture through changes in temperature, rainfall (timing and quantity), CO2, solar radiation and the interaction of these elements. Agriculture can both mitigate or worsen global warming. Some of the increase in CO2 in the atmosphere comes from the decomposition of organic matter in the soil, and much of the methane emitted into the atmosphere is caused by the decomposition of organic matter in wet soils such as rice paddies. Further, wet or anaerobic soils also lose nitrogen through denitrification, releasing the greenhouse gas nitric oxide. Changes in management can reduce the release of these greenhouse gases, and soil can further be used to sequester some of the CO2 in the atmosphere. Distortions in modern global agriculture. Differences in economic development, population density and culture mean that the farmers of the world operate under very different conditions. A US cotton farmer may receive US$230 in government subsidies per acre planted (in 2003), while farmers in Mali and other third-world countries do without. When prices decline, the heavily subsidised US farmer is not forced to reduce his output, making it difficult for cotton prices to rebound, but his Mali counterpart may go broke in the meantime. A livestock farmer in South Korea can calculate with a (highly subsidized) sales price of US$1300 for a calf produced. A South American Mercosur country rancher calculates with a calf's sales price of US$120–200 (both 2008 figures). With the former, scarcity and high cost of land is compensated with public subsidies, the latter compensates absence of subsidies with economics of scale and low cost of land. In the Peoples Republic of China, a rural household's productive asset may be one hectare of farmland. In Brazil, Paraguay and other countries where local legislature allows such purchases, international investors buy thousands of hectares of farmland or raw land at prices of a few hundred US$ per hectare.
Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between 1950 and 1984, the Green Revolution transformed agriculture around the globe, with world grain production increasing by 250% as world population doubled. Modern agriculture's heavy reliance on petrochemicals and mechanization has raised concerns that oil shortages could increase costs and reduce agricultural output, causing food shortages. Modern or industrialized agriculture is dependent on fossil fuels in two fundamental ways: 1) direct consumption on the farm and 2) indirect consumption to manufacture inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery; and use of gas, liquid propane, and electricity to power dryers, pumps, lights, heaters, and coolers. American farms directly consumed about 1.2 exajoules (1.1 quadrillion BTU) in 2002, or just over 1 percent of the nation's total energy. Indirect consumption is mainly oil and natural gas used to manufacture fertilizers and pesticides, which accounted for 0.6 exajoules (0.6 quadrillion BTU) in 2002. The energy used to manufacture farm machinery is also a form of indirect agricultural energy consumption, but it is not included in USDA estimates of U.S. agricultural energy use. Together, direct and indirect consumption by U.S. farms accounts for about 2 percent of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has gradually declined over the past 30 years. Food systems encompass not just agricultural production, but also off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for approximately one-fifth of food system energy use in the United States. Oil shortages could impact this food supply. Some farmers using modern organic-farming methods have reported yields as high as those available from conventional farming without the use of synthetic fertilizers and pesticides. However, the reconditioning of soil to restore nutrients lost during the use of monoculture agriculture techniques made possible by petroleum-based technology takes time. In 2007, higher incentives for farmers to grow non-food biofuel crops combined with other factors (such as over-development of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth) to cause food shortages in Asia, the Middle East, Africa, and Mexico, as well as rising food prices around the globe. As of December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. Some of these shortages resulted in food riots and even deadly stampedes. The biggest fossil fuel input to agriculture is the use of natural gas as a hydrogen source for the Haber-Bosch fertilizer-creation process. Natural gas is used because it is the cheapest currently available source of hydrogen. When oil production becomes so scarce that natural gas is used as a partial stopgap replacement, and hydrogen use in transportation increases, natural gas will become much more expensive. If the Haber Process is unable to be commercialized using renewable energy (such as by electrolysis) or if other sources of hydrogen are not available to replace the Haber Process, in amounts sufficient to supply transportation and agricultural needs, this major source of fertilizer would either become extremely expensive or unavailable. This would either cause food shortages or dramatic rises in food prices. Mitigation of effects of petroleum shortages. One effect oil shortages could have on agriculture is a full return to organic agriculture. In light of peak-oil concerns, organic methods are more sustainable than contemporary practices because they use no petroleum-based pesticides, herbicides, or fertilizers. Some farmers using modern organic-farming methods have reported yields as high as those available from conventional farming. Organic farming may however be more labor-intensive and would require a shift of the workforce from urban to rural areas. It has been suggested that rural communities might obtain fuel from the biochar and synfuel process, which uses agricultural "waste" to provide charcoal fertilizer, some fuel "and" food, instead of the normal food vs fuel debate. As the synfuel would be used on-site, the process would be more efficient and might just provide enough fuel for a new organic-agriculture fusion. It has been suggested that some transgenic plants may some day be developed which would allow for maintaining or increasing yields while requiring fewer fossil-fuel-derived inputs than conventional crops. The possibility of success of these programs is questioned by ecologists and economists concerned with unsustainable GMO practices such as terminator seeds, and a January 2008 report shows that GMO practices "fail to deliver environmental, social and economic benefits." While there has been some research on sustainability using GMO crops, at least one hyped and prominent multi-year attempt by Monsanto Company has been unsuccessful, though during the same period traditional breeding techniques yielded a more sustainable variety of the same crop. Additionally, a survey by the bio-tech industry of subsistence farmers in Africa to discover what GMO research would most benefit sustainable agriculture only identified non-transgenic issues as areas needing to be addressed. Nevertheless, some governments in Africa continue to view investments in new transgenic technologies as an essential component of efforts to improve sustainability.
Aldous Leonard Huxley (26 July 1894 – 22 November 1963) was an English writer and one of the most prominent members of the famous Huxley family. He spent the later part of his life in the United States, living in Los Angeles from 1937 until his death in 1963. Best known for his novels including "Brave New World" and wide-ranging output of essays, Huxley also edited the magazine "Oxford Poetry", and published short stories, poetry, travel writing, and film stories and scripts. Aldous Huxley was a humanist and pacifist, and he was latterly interested in spiritual subjects such as parapsychology and philosophical mysticism. He is also well known for advocating and taking psychedelics. By the end of his life Huxley was considered, in some academic circles, a leader of modern thought and an intellectual of the highest rank, and highly regarded as one of the most prominent explorers of visual communication and sight-related theories as well.
Aldous Huxley was born in Godalming, Surrey, UK in 1894. He was the third son of the writer and school-master Leonard Huxley and first wife, Julia Arnold who founded Prior's Field School. Julia was the niece of Matthew Arnold and the sister of Mrs. Humphrey Ward. Aldous was the grandson of Thomas Henry Huxley, the zoologist, agnostic and controversialist ("Darwin's Bulldog"). His brother Julian Huxley and half-brother Andrew Huxley also became outstanding biologists. Huxley had another brother Noel Trevenen (1891–1914) who committed suicide after a period of clinical depression. Huxley began his learning in his father's well-equipped botanical laboratory, then continued in a school named Hillside. His teacher was his mother who supervised him for several years until she became terminally ill. After Hillside, he was educated at Eton College. Huxley's mother died in 1908, when he was fourteen. In 1911, he suffered an illness (keratitis punctata) which "left [him] practically blind for two to three years". Aldous's near-blindness disqualified him from service in the First World War. Once his eyesight recovered sufficiently, he was able to study English literature at Balliol College, Oxford. In 1916 he edited "Oxford Poetry" and later graduated with first class honours. Following his education at Balliol, Huxley was financially indebted to his father and had to earn a living. He taught French for a year at Eton, where Eric Blair (later known by the pen name George Orwell) and Stephen Runciman were among his pupils, but was remembered as an incompetent and hopeless teacher who couldn’t keep discipline. Nevertheless, Blair and others were impressed by his use of words. For a short while in 1918, he was employed acquiring provisions at the Air Ministry. Significantly, Huxley also worked for a time in the 1920s at the technologically-advanced Brunner and Mond chemical plant in Billingham, Teesside, and the most recent introduction to his famous science fiction novel "Brave New World" (1932) states that this experience of "an ordered universe in a world of planless incoherence" was one source for the novel. Huxley completed his first (unpublished) novel at the age of seventeen and began writing seriously in his early twenties. His earlier work includes important novels on the dehumanizing aspects of scientific progress, most famously "Brave New World", and on pacifist themes (for example, "Eyeless in Gaza"). In "Brave New World" Huxley portrays a society operating on the principles of mass production and Pavlovian conditioning. Huxley was strongly influenced by F. Matthias Alexander and included him as a character in "Eyeless in Gaza".
During the First World War, Huxley spent much of his time at Garsington Manor, home of Lady Ottoline Morrell, working as a farm labourer. Here he met several Bloomsbury figures including Bertrand Russell and Clive Bell. Later, in "Crome Yellow" (1921) he caricatured the Garsington lifestyle. In 1919 he married Maria Nys (10 September 1899 - 12 February 1955), a Belgian woman he met at Garsington. They had one child, Matthew Huxley (19 April 1920 - 10 February 2005), who had a career as an epidemiologist. The family lived in Italy part of the time in the 1920s, where Huxley would visit his friend D. H. Lawrence. Following Lawrence's death in 1930, Huxley edited Lawrence's letters (1933). In 1937, Huxley moved to Hollywood, California with his wife Maria, son Matthew, and friend Gerald Heard. He lived in the U.S., mainly in southern California, until his death, but also for a time in Taos, New Mexico, where he wrote "Ends and Means" (published in 1937). In this work he examines the fact that although most people in modern civilization agree that they want a world of "liberty, peace, justice, and brotherly love", they have not been able to agree on how to achieve it. Heard introduced Huxley to Vedanta (Veda-Centric Hinduism), meditation, and vegetarianism through the principle of ahimsa. In 1938 Huxley befriended J. Krishnamurti, whose teachings he greatly admired. He also became a Vedantist in the circle of Hindu Swami Prabhavananda, and introduced Christopher Isherwood to this circle. Not long after, Huxley wrote his book on widely held spiritual values and ideas, "The Perennial Philosophy", which discussed the teachings of renowned mystics of the world. Huxley became a close friend of Remsen Bird, president of Occidental College. He spent much time at the college, which is in the Eagle Rock neighborhood of Los Angeles. The college appears as "Tarzana College" in his satirical novel "After Many a Summer" (1939). The novel won Huxley that year's James Tait Black Memorial Prize for fiction. Huxley also incorporated Bird into the novel. During this period Huxley earned some Hollywood income as a writer. In March 1938, his friend Anita Loos, a novelist and screenwriter, put him in touch with Metro-Goldwyn-Mayer who hired Huxley for "Madame Curie" which was originally to star Greta Garbo and be directed by George Cukor. (The film was eventually filmed by MGM in 1943 with a different director and stars.) Huxley received screen credit for "Pride and Prejudice" (1940) and was paid for his work on a number of other films, including "Jane Eyre" (1944). However, his experience in Hollywood was not a success. When he wrote a synopsis of "Alice in Wonderland", Walt Disney rejected it on the grounds that "he could only understand every third word". Huxley's leisurely development of ideas, it seemed, was not suitable for the movie moguls, who demanded fast, dynamic dialogue above all else. On 21 October 1949, Huxley wrote to George Orwell, author of "Nineteen Eighty-Four", congratulating Orwell on "how fine and how profoundly important the book is". In his letter to Orwell, he predicted: "Within the next generation I believe that the world's leaders will discover that infant conditioning and narco-hypnosis are more efficient, as instruments of government, than clubs and prisons, and that the lust for power can be just as completely satisfied by suggesting people into loving their servitude as by flogging them and kicking them into obedience."
After the Second World War Huxley applied for United States citizenship, but his application was continuously deferred on the grounds that he would not say he would take up arms to defend the U.S., so he withdrew it. Nevertheless, he remained in the country, and in 1959 he turned down an offer of a Knight Bachelor by the Macmillan government. During the 1950s Huxley's interest in the field of psychical research grew keener, and his later works are strongly influenced by both mysticism and his experiences with psychedelic drugs. In October 1930, the occultist Aleister Crowley dined with Huxley in Berlin, and to this day rumours persist that Crowley introduced Huxley to peyote on that occasion. He was introduced to mescaline (considered to be the key active ingredient of peyote) by the psychiatrist Humphry Osmond in 1953. Through Dr. Osmond, Huxley met millionaire Alfred Matthew Hubbard who would deal with LSD on a wholesale basis. On 24 December 1955, Huxley took his first dose of LSD. Indeed, Huxley was a pioneer of self-directed psychedelic drug use "in a search for enlightenment", famously taking 100 micrograms of LSD as he lay dying. His psychedelic drug experiences are described in the essays "The Doors of Perception" (the title deriving from some lines in the book "The Marriage of Heaven and Hell" by William Blake), and "Heaven and Hell". Some of his writings on psychedelics became frequent reading among early hippies. While living in Los Angeles, Huxley was a friend of Ray Bradbury. According to Sam Weller's biography of Bradbury, the latter was dissatisfied with Huxley, especially after Huxley encouraged Bradbury to take psychedelic drugs. In 1955, Huxley's wife, Maria, died of breast cancer. In 1956 he married Laura Archera (1911–2007), also an author. She wrote "This Timeless Moment", a biography of Huxley. In 1960 Huxley himself was diagnosed with cancer, and in the years that followed, with his health deteriorating, he wrote the Utopian novel "Island", and gave lectures on "Human Potentialities" at the Esalen institute, which were fundamental to the forming of the Human Potential Movement. On his deathbed, unable to speak, Huxley made a written request to his wife for "LSD, 100 µg, intramuscular". According to her account of his death, in "This Timeless Moment", she obliged with an injection at 11:45 am and another a couple of hours later. He died at 5:21 pm on 22 November 1963, aged 69. Huxley's ashes were interred in the family grave at the Watts Cemetery, home of the Watts Mortuary Chapel in Compton, a village near Guildford, Surrey, England. Media coverage of his death was overshadowed by the assassination of President John F. Kennedy, on the same day, as was the death of the Irish author C. S. Lewis. This coincidence was the inspiration for Peter Kreeft's book '. Huxley's only child, Matthew Huxley, was also an author, as well as an educator, anthropologist, and prominent epidemiologist. Aldous Huxley is also survived by two grandchildren.
Beginning in 1939 and continuing until his death in 1963, Huxley had an extensive association with the Vedanta Society of Southern California, founded and headed by Swami Prabhavananda. Together with Gerald Heard, Christopher Isherwood, and other followers he was initiated by the Swami and was taught meditation and spiritual practices. In 1944 Huxley wrote the introduction to the "Bhagavad Gita: The Song of God", translated by Swami Prabhavanada and Christopher Isherwood, which was published by The Vedanta Society of Southern California. From 1941 through 1960 Huxley contributed 48 articles to "Vedanta and the West", published by the Society. He also served on the editorial board with Isherwood, Heard, and playwright John van Druten from 1951 through 1962. Huxley also occasionally lectured at the Hollywood and Santa Barbara Vedanta temples. Two of those lectures have been released on CD: "Knowledge and Understanding" and "Who Are We" from 1955. After the publication of "The Doors of Perception", Huxley and the Swami disagreed about the meaning and importance of the LSD drug experience, which may have caused the relationship to cool, but Huxley continued to write articles for the Society's journal, lecture at the temple, and attend social functions.
"Crome Yellow" (1921) attacks Victorian and Edwardian social principles which led to World War I and its terrible aftermath. Together with Huxley's second novel, "Antic Hay" (1923), the book expresses much of the mood of disenchantment of the early 1920s. It was intended to reflect, as Huxley stated in a letter to his father, "the life and opinions of an age which has seen the violent disruption of almost all the standards, conventions and values current in the present epoch." Huxley's reputation for iconoclasm and emancipation grew. He was condemned for his explicit discussion of sex and free thought in his fiction. "Antic Hay", for example, was burned in Cairo and in the years that followed many of Huxley's books were received with disapproval or banned at one time or another. The exclusion of "Brave New World", "Point Counter Point" and "Island" from "Time" magazine's Best 100 novels list in 2006 created an uproar. Huxley, however, said that a novel should be full of interesting opinions and arresting ideas, describing his aim as a novelist as being 'to arrive, technically, at a perfect fusion of the novel and the essay'; and with "Point Counter Point" (1928), Huxley wrote his first true 'novel of ideas', the type of thought-provoking fiction with which he is now associated. One of his main ideas was pessimism about the cultural future of society, a pessimism which sprang largely from his visit to the United States between September 1925 and June 1926. He recounted his experiences in "Jesting Pilate" (1926): "The thing which is happening in America is a reevaluation of values, a radical alteration (for the worse) of established standards", and it was soon after this visit that he conceived the idea of writing a satire of what he had encountered. "Brave New World" (1932) as well as "Island" (1962) form the cornerstone of Huxley's damning indictment of commercialism based upon goods generally manufactured from other countries. Indeed also, "Brave New World" (along with Orwell's "Nineteen Eighty-Four" and Yevgeni Zamyatin's "We") helped form the anti-utopian or dystopian tradition in literature and has become synonymous with a future world in which the human spirit is subject to conditioning and control. "Island" acts as an antonym to "Brave New World"; it is described as "one of the truly great philosophical novels". He devoted his time at his small house at Llano in the Mojave Desert to a life of contemplation, mysticism, and experimentation with hallucinogenic drugs. His suggestions in The Doors of Perception (1954) that mescaline and lysergic acid were 'drugs of unique distinction' which should be exploited for the 'supernaturally brilliant' visionary experience they offered provoked even more outrage than his passionate defense of the Bates method in "The Art of Seeing" (1942). However, the book went on to become a cult text in the psychedelic 1960s, and inspire the name of the rock band The Doors (although it was originally derived from William Blake's "Marriage of Heaven and Hell"). Huxley also appears on the sleeve of The Beatles' landmark 1967 album "Sgt. Pepper's Lonely Hearts Club Band".
With respect to details about the true quality of Huxley’s eyesight at specific points in his life, there are differing accounts. Around 1939, Huxley encountered the Bates Method for better eyesight, and a teacher, Margaret Corbett, who was able to teach him in the method. In 1940, Huxley relocated from Hollywood to a "ranchito" in the high desert hamlet of Llano, California in northernmost Los Angeles County. Huxley then said that his sight improved dramatically with the Bates Method and the extreme and pure natural lighting of the southwestern American desert. He reported that for the first time in over 25 years, he was able to read without glasses and without strain. He even tried driving a car along the dirt road beside the ranch. He wrote a book about his successes with the Bates Method, "The Art of Seeing" which was published in 1942 (US), 1943 (UK). It was from this period, with the publication of the generally disputed theories contained in the latter book, that a growing degree of popular controversy arose over the subject of Huxley’s eyesight. "Then suddenly he faltered—and the disturbing truth became obvious. He wasn't reading his address at all. He had learned it by heart. To refresh his memory he brought the paper closer and closer to his eyes. When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him. It was an agonizing moment." "...Although I feel it was an injustice to treat Aldous as though he were blind, it is true there were many indications of his impaired vision. For instance, although Aldous did not wear glasses, he would quite often use a magnifying lens..." "The most characteristic fact about the functioning of the total organism, or any part of the organism, is that it is not constant, but highly variable." Nevertheless, the topic of Huxley’s eyesight continues to endure similar, significant controversy, regardless of how trivial a subject matter it might initially appear.
Notable works include the original screenplay for Disney's animated "Alice in Wonderland" (which was rejected because it was too literary), two productions of "Brave New World", one of "Point Counter Point", one of "Eyeless in Gaza", and one of "Ape and Essence". He was a credited screenwriter for "Pride and Prejudice" (1940), co-authored the screenplay for "Jane Eyre" (1944) with John Houseman, "A Woman's Vengeance" (1947), and contributed to the screenplays of "Madame Curie" (1943) and "Alice in Wonderland" (1951) without credit. Director Ken Russell's 1971 film "The Devils", starring Vanessa Redgrave and Oliver Reed, was adapted from Huxley's "The Devils of Loudun". A made-for-television adaptation of "Brave New World" was made in 1990.
Algae (or; singular "alga", Latin for "seaweed") are a large and diverse group of simple, typically autotrophic organisms, ranging from unicellular to multicellular forms. The largest and most complex marine forms are called seaweeds. They are photosynthetic, like plants, and "simple" because they lack the many distinct organs found in land plants. Though the prokaryotic "Cyanobacteria" (commonly referred to as blue-green algae) were traditionally included as "algae" in older textbooks, many modern sources regard this as outdated as they are now considered to be closely related to bacteria. The term "algae" is now restricted to eukaryotic organisms. All true algae therefore have a nucleus enclosed within a membrane and chloroplasts bound in one or more membranes. Algae constitute a paraphyletic and polyphyletic group, as they do not include all the descendants of the last universal ancestor nor do they all descend from a common algal ancestor, although their chloroplasts seem to have a single origin. Diatoms are also examples of algae. Algae lack the various structures that characterize land plants, such as phyllids (leaves) and rhizoids in nonvascular plants, or leaves, roots, and other organs that are found in tracheophytes (vascular plants). Many are photoautotrophic, although some groups contain members that are mixotrophic, deriving energy both from photosynthesis and uptake of organic carbon either by osmotrophy, myzotrophy, or phagotrophy. Some unicellular species rely entirely on external energy sources and have limited or no photosynthetic apparatus. Nearly all algae have photosynthetic machinery ultimately derived from the Cyanobacteria, and so produce oxygen as a by-product of photosynthesis, unlike other photosynthetic bacteria such as purple and green sulfur bacteria. Fossilized filamentous algae from the Vindhya basin have been dated back to 1.6 to 1.7 billion years ago. The first alga to have its genome sequenced was "Cyanidioschyzon merolae".
The singular "alga" is the Latin word for a particular seaweed and retains that meaning in English. The etymology is obscure. Although some speculate that it is related to Latin "algēre", "be cold", there is no known reason to associate seaweed with temperature. A more likely source is "alliga", "binding, entwining." Since Algae has become a biological classification, alga can also mean one classification under Algae, parallel to a fungus being a species of fungi, a plant being a species of plant, and so on. The ancient Greek word for seaweed was "φῦκος" (fūkos or phykos), which could mean either the seaweed, probably Red Algae, or a red dye derived from it. The Latinization, "fūcus", meant primarily the cosmetic rouge. The etymology is uncertain, but a strong candidate has long been some word related to the Biblical "פוך" (pūk), "paint" (if not that word itself), a cosmetic eye-shadow used by the ancient Egyptians and other inhabitants of the eastern Mediterranean. It could be any color: black, red, green, blue. Accordingly the modern study of marine and freshwater algae is called either phycology or algology. The name Fucus appears in a number of taxa.
While "Cyanobacteria" have been traditionally included among the Algae, recent works usually exclude them due to large differences such as the lack of membrane-bound organelles, the presence of a single circular chromosome, the presence of peptidoglycan in the cell walls, and ribosomes different in size and content from those of the Eukaryotes. Rather than in chloroplasts, they conduct photosynthesis on specialized infolded cytoplasmic membranes called thylakoid membranes. Therefore, they differ significantly from the Algae despite occupying similar ecological niches. By modern definitions Algae are Eukaryotes and conduct photosynthesis within membrane-bound organelles called chloroplasts. Chloroplasts contain circular DNA and are similar in structure to Cyanobacteria, presumably representing reduced cyanobacterial endosymbionts. The exact nature of the chloroplasts is different among the different lines of Algae, reflecting different endosymbiotic events. The table below describes the composition of the three major groups of Algae. Their lineage relationships are shown in the figure in the upper right. Many of these groups contain some members that are no longer photosynthetic. Some retain plastids, but not chloroplasts, while others have lost plastids entirely. W.H.Harvey (1811—1866) was the first to divide the Algae into four divisions based on their pigmentation. This is the first use of a biochemical criterion in plant systematics. Harvey's four divisions are: Red Algae (Rhodophyta), Brown Algae (Heteromontophyta), Green Algae (Chlorophyta) and Diatomaceae.
A range of algal morphologies are exhibited, and convergence of features in unrelated groups is common. The only groups to exhibit three dimensional multicellular thalli are the reds and browns, and some chlorophytes. Apical growth is constrained to subsets of these groups: the florideophyte reds, various browns, and the charophytes. The form of charophytes is quite different to those of reds and browns, because have distinct nodes, separated by internode 'stems'; whorls of branches reminiscent of the horsetails occur at the nodes. Conceptacles are another polyphyletic trait; they appear in the coralline algae and the Hildenbrandiales, as well as the browns. Most of the simpler algae are unicellular flagellates or amoeboids, but colonial and non-motile forms have developed independently among several of the groups. Some of the more common organizational levels, more than one of which may occur in the life cycle of a species, are In three lines even higher levels of organization have been reached, with full tissue differentiation. These are the brown algae,—some of which may reach 50 m in length (kelps)—the red algae, and the green algae. The most complex forms are found among the green algae (see Charales and Charophyta), in a lineage that eventually led to the higher land plants. The point where these non-algal plants begin and algae stop is usually taken to be the presence of reproductive organs with protective cell layers, a characteristic not found in the other alga groups.
"Lichens" are defined by the International Association for Lichenology to be "an association of a fungus and a photosynthetic symbiont resulting in a stable vegetative body having a specific structure." The fungi, or mycobionts, are from the Ascomycota with a few from the Basidiomycota. They are not found alone in nature but when they began to associate is not known. One mycobiont associates with the same phycobiont species, rarely two, from the Green Algae, except that alternatively the mycobiont may associate with the same species of Cyanobacteria (hence "photobiont" is the more accurate term). A photobiont may be associated with many specific mycobionts or live independently; accordingly, lichens are named and classified as fungal species. The association is termed a morphogenesis because the lichen has a form and capabilities not possessed by the symbiont species alone (they can be experimentally isolated). It is possible that the photobiont triggers otherwise latent genes in the mycobiont.
Coral reefs are accumulated from the calcareous exoskeletons of marine invertebrates of the Scleractinia order; i.e., the Stony Corals. As animals they metabolize sugar and oxygen to obtain energy for their cell-building processes, including secretion of the exoskeleton, with water and carbon dioxide as byproducts. As the reef is the result of a favorable equilibrium between construction by the corals and destruction by marine erosion, the rate at which metabolism can proceed determines the growth or deterioration of the reef. Algae of the Dinoflagellate phylum are often endosymbionts in the cells of marine invertebrates, where they accelerate host-cell metabolism by generating immediately available sugar and oxygen through photosynthesis using incident light and the carbon dioxide produced in the host. Endosymbiont algae in the Stony Corals are described by the term zooxanthellae, with the host Stony Corals called on that account hermatypic corals, which although not a taxon are not in healthy condition without their endosymbionts. Zooxanthellae belong almost entirely to the genus "Symbiodinium". The loss of "Symbiodinium" from the host is known as coral bleaching, a condition which unless corrected leads to the deterioration and loss of the reef.
Rhodophyta, Chlorophyta and Heterokontophyta, the three main algal Phyla, have life-cycles which show tremendous variation with considerable complexity. In general there is an asexual phase where the seaweed's cells are diploid, a sexual phase where the cells are haploid followed by fusion of the male and female gametes. Asexual reproduction is advantageous in that it permits efficient population increases, but less variation is possible. Sexual reproduction allows more variation, but is more costly. Often there is no strict alternation between the sporophyte and also because there is often an asexual phase, which could include the fragmentation of the thallus.
The "Algal Collection of the U.S. National Herbarium" (located in the National Museum of Natural History) consists of approximately 320500 dried specimens, which, although not exhaustive (no exhaustive collection exists), gives an idea of the order of magnitude of the number of algal species (that number remains unknown). Estimates vary widely. For example, according to one standard textbook, in the British Isles the "UK Biodiversity Steering Group Report" estimated there to be 20000 algal species in the UK. Another checklist reports only about 5000 species. Regarding the difference of about 15000 species, the text concludes: "It will require many detailed field surveys before it is possible to provide a reliable estimate of the total number of species..." Regional and group estimates have been made as well: 5000—5500 species of Red Algae worldwide, "some 1300 in Australian Seas," 400 seaweed species for the western coastline of South Africa, 669 marine species from California (U.S.A.), 642 in the check-list of Britain and Ireland, and so on, but lacking any scientific basis or reliable sources, these numbers have no more credibility than the British ones mentioned above. Most estimates also omit the microscopic Algae, such as the phytoplankta, entirely.
The topic of distribution of algal species has been fairly well studied since the founding of phytogeography in the mid-19th century AD. Algae spread mainly by the dispersal of spores analogously to the dispersal of Plantae by seeds and spores. Spores are everywhere in all parts of the Earth: the waters fresh and marine, the atmosphere, free-floating and in precipitation or mixed with dust, the humus and in other organisms, such as humans. Whether a spore is to grow into an organism depends on the combination of the species and the environmental conditions. The spores of fresh-water Algae are dispersed mainly by running water and wind, as well as by living carriers. The bodies of water into which they are transported are chemically selective. Marine spores are spread by currents. Ocean water is temperature selective, resulting in phytogeographic zones, regions and provinces. To some degree the distribution of Algae is subject to floristic discontinuities caused by geographical features, such as Antarctica, long distances of ocean or general land masses. It is therefore possible to identify species occurring by locality, such as "Pacific Algae" or "North Sea Algae". When they occur out of their localities, it is usually possible to hypothesize a transport mechanism, such as the hulls of ships. For example, "Ulva reticulata" and "Ulva fasciata" travelled from the mainland to Hawaii in this manner. Mapping is possible for select species only: "there are many valid examples of confined distribution patterns." For example, "Clathromorphum" is an arctic genus and is not mapped far south of there. On the other hand, scientists regard the overall data as insufficient due to the "difficulties of undertaking such studies."
Algae are prominent in bodies of water, common in terrestrial environments and are found in unusual environments, such as on snow and on ice. Seaweeds grow mostly in shallow marine waters, under; however some have been recorded to a depth of The various sorts of algae play significant roles in aquatic ecology. Microscopic forms that live suspended in the water column (phytoplankton) provide the food base for most marine food chains. In very high densities (algal blooms) these algae may discolor the water and outcompete, poison, or asphyxiate other life forms. Algae are variously sensitive to different factors, which has made them useful as biological indicators in the Ballantine Scale and its modification.
To be competitive and independent from fluctuating support from (local) policy on the long run, biofuels should equal or beat the cost level of fossil fuels. Here, algae based fuels hold great promise, directly related to the potential to produce more biomass per unit area in a year than any other form of biomass. The break-even point for algae-based biofuels should be within reach in about ten years.
For centuries seaweed has been used as a fertilizer; George Owen of Henllys writing in the 16th century referring to drift weed in South Wales:This kind of ore they often gather and lay on great heapes, where it heteth and rotteth, and will have a strong and loathsome smell; when being so rotten they cast on the land, as they do their muck, and thereof springeth good corn, especially barley... After spring-tydes or great rigs of the sea, they fetch it in sacks on horse backes, and carie the same three, four, or five miles, and cast it on the lande, which doth very much better the ground for corn and grass. Today Algae are used by humans in many ways; for example, as fertilizers, soil conditioners and livestock feed. Aquatic and microscopic species are cultured in clear tanks or ponds and are either harvested or used to treat effluents pumped through the ponds. Algaculture on a large scale is an important type of aquaculture in some places. Maerl is commonly used as a soil conditioner.
Naturally growing seaweeds are an important source of food, especially in Asia. They provide many vitamins including: A, B1, B2, B6, niacin and C, and are rich in iodine, potassium, iron, magnesium and calcium. In addition commercially cultivated microalgae, including both Algae and Cyanobacteria, are marketed as nutritional supplements, such as Spirulina, Chlorella and the Vitamin-C supplement, Dunaliella, high in beta-carotene. Algae are national foods of many nations: China consumes more than 70 species, including "fat choy", a cyanobacterium considered a vegetable; Japan, over 20 species; Ireland, dulse; Chile, cochayuyo. Laver is used to make "laver bread" in Wales where it is known as "bara lawr"; in Korea, gim; in Japan, nori and aonori. It is also used along the west coast of North America from California to British Columbia, in Hawaii and by the Māori of New Zealand. Sea lettuce and badderlocks are a salad ingredient in Scotland, Ireland, Greenland and Iceland. The oils from some Algae have high levels of unsaturated fatty acids. For example, "Parietochloris incisa" is very high in arachidonic acid, where it reaches up to 47% of the triglyceride pool. Some varieties of Algae favored by vegetarianism and veganism contain the long-chain, essential omega-3 fatty acids, Docosahexaenoic acid (DHA) and Eicosapentaenoic acid (EPA), in addition to vitamin B12. The vitamin B12 in algae is not biologically active. Fish oil contains the omega-3 fatty acids, but the original source is algae (microalgae in particular), which are eaten by marine life such as copepods and are passed up the food chain. Algae has emerged in recent years as a popular source of omega-3 fatty acids for vegetarians who cannot get long-chain EPA and DHA from other vegetarian sources such as flaxseed oil, which only contains the short-chain Alpha-Linolenic acid (ALA).
In statistics, analysis of variance (ANOVA) is a collection of statistical models, and their associated procedures, in which the observed variance is partitioned into components due to different explanatory variables. In its simplest form ANOVA gives a statistical test of whether the means of several groups are all equal, and therefore generalizes Student's two-sample "t"-test to more than two groups. ANOVAs are helpful because they possess a certain advantage over a two-sample t-test. Doing multiple two-sample t-tests would result in a largely increased chance of committing a type I error. For this reason, ANOVAs are useful in comparing three or more means.
Random effects models are used when the treatments are not fixed. This occurs when the various treatments (also known as factor levels) are sampled from a larger population. Because the treatments themselves are random variables, some assumptions and the method of contrasting the treatments differ from ANOVA model 1. Most random-effects or mixed-effects models are not concerned with making inferences concerning the particular sampled factors. For example, consider a large manufacturing plant in which many machines produce the same product. The statistician studying this plant would have very little interest in comparing the three particular machines to each other. Rather, inferences that can be made for "all" machines are of interest, such as their variability and the mean.
There are several approaches to the analysis of variance. A model often presented in textbooks. Levene's test for homogeneity of variances is typically used to examine the plausibility of homoscedasticity. The Kolmogorov–Smirnov or the Shapiro–Wilk test may be used to examine normality. When used in the analysis of variance to test the hypothesis that all treatments have exactly the same effect, the F-test is robust (Ferguson & Takane, 2005, pp. 261–2). The Kruskal–Wallis test is a nonparametric alternative which does not rely on an assumption of normality. And the Friedman test is the nonparametric alternative for a one way repeated measures ANOVA. The separate assumptions of the textbook model imply that the errors are independently, identically, and normally distributed for fixed effects models, that is, that the errors are independent and
In a randomized controlled experiment, the treatments are randomly assigned to experimental units, following the experimental protocol. This randomization is objective and declared before the experiment is carried out. The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of C. S. Peirce and Ronald A. Fisher. This design-based analysis was advocated and developed by Oscar Kempthorne at Iowa State University. Kempthorne and his students make an assumption of "unit treatment additivity", which is discussed in the books of Kempthorne and David R. Cox.
In its simplest form, the assumption of treatment unit additivity states that the observed response formula_2 from experimental unit formula_3 when receiving treatment formula_4 can be written as the sum formula_5. The assumption of unit treatment addivity implies that every treatment have exactly the same effect on every experiment unit. The assumption of unit treatment additivity is a hypothesis which is not directly falsifiable, according to Cox and Kempthorne. However, many consequences of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of treatment additivity implies that the variance is constant for all treatments. Therefore, by contraposition, a necessary condition for unit treatment additivity is that the variance is constant. The property of unit treatment additivity is not invariant under a change of scale, so statisticians often use transformations to achieve unit treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be tranformed to stabilize the variance. Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model. The assumption of unit treatment additivity was enunciated in experimental design by Kempthorne and Cox. Kempthorne's use of unit treatment additivity and randomization is similar to the design-based analysis of finite population survey sampling.
Kempthorne uses the randomization-distribution and the assumption of "unit treatment additivity" to produce a "derived linear model", very similar to the textbook model discussed previously. The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies by Kempthorne and his students (Hinkelmann and Kempthorne). However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations (Hinkelmann and Kempthorne, volume one, chapter 7; Bailey chapter 1.14). In the randomization-based analysis, there is "no assumption" of a "normal" distribution and certainly "no assumption" of "independence". On the contrary, "the observations are dependent"! The randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time. Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments. Statistical models for observational data. However, when applied to data from non-randomized experiments or observational studies, model-based analysis lacks the warrant of randomization. For observational data, the derivation of confidence intervals must use "subjective" models, as emphasized by Ronald A. Fisher and his followers. In practice, the estimates of treatment-effects from observational studies generally are often inconsistent (Freedman). In practice, "statistical models" and observational data are useful for suggesting hypothesis that should be treated very cautiously by the public (Freedman). Partitioning of the sum of squares. The fundamental technique is a partitioning of the total sum of squares (abbreviated SS) into components related to the effects used in the model. For example, we show the model for a simplified ANOVA with one type of treatment at different levels. So, the number of degrees of freedom (abbreviated df) can be partitioned in a similar way and specifies the chi-square distribution which describes the associated sums of squares. See also Lack-of-fit sum of squares.
The F-test is used for comparisons of the components of the total deviation. For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic to the F-distribution with "I" − 1,"n"T − "I" degrees of freedom. Using the F-distribution is a natural candidate because the test statistic is the quotient of two mean sums of squares which have a chi-square distribution.
When the data do not meet the assumptions of normality, the suggestion has arisen to replace each original data value by its rank (from 1 for the smallest to N for the largest), then run a standard ANOVA calculation on the rank-transformed data. Conover and Iman (1981) provided a review of the four main types of rank transformations. Commercial statistical software packages (e.g., SAS, 1985, 1987, 2008) followed with recommendations to data analysts to run their data sets through a ranking procedure (e.g., PROC RANK) prior to conducting standard analyses using parametric procedures. This rank-based procedure has been recommended as being robust to non-normal errors, resistant to outliers, and highly efficient for many distributions. It may result in a known statistic (e.g., Wilcoxon Rank-Sum / Mann-Whitney U), and indeed provide the desired robustness and increased statistical power that is sought. For example, Monte Carlo studies have shown that the rank transformation in the two independent samples t test layout can be successfully extended to the one-way independent samples ANOVA, as well as the two independent samples multivariate Hotelling's T2 layouts (Nanna, 2002). Conducting factorial ANOVA on the ranks of original scores has also been suggested (Conover & Iman, 1976, Iman, 1974, and Iman & Conover, 1976). However, Monte Carlo studies by Sawilowsky (1985a; 1989 et al.; 1990) and Blair, Sawilowsky, and Higgins (1987), and subsequent asymptotic studies (e.g. Thompson & Ammann, 1989; "there exist values for the main effects such that, under the null hypothesis of no interaction, the expected value of the rank transform test statistic goes to infinity as the sample size increases," Thompson, 1991, p. 697), found that the rank transformation is inappropriate for testing interaction effects in a 4x3 and a 2x2x2 factorial design. As the number of effects (i.e., main, interaction) become non-null, and as the magnitude of the non-null effects increase, there is an increase in Type I error, resulting in a complete failure of the statistic with as high as a 100% probability of making a false positive decision. Similarly, Blair and Higgins (1985) found that the rank transformation increasingly fails in the two dependent samples layout as the correlation between pretest and posttest scores increase. Headrick (1997) discovered the Type I error rate problem was exacerbated in the context of Analysis of Covariance, particularly as the correlation between the covariate and the dependent variable increased. For a review of the properties of the rank transformation in designed experiments see Sawilowsky (2000). A variant of rank-transformation is 'quantile normalization' in which a further transformation is applied to the ranks such that the resulting values have some defined distribution (often a normal distribution with a specified mean and variance). Further analyses of quantile-normalized data may then assume that distribution to compute significance values. However, two specific types of secondary transformations, the random normal scores and expected normal scores transformation, have been shown to greatly inflate Type I errors and severely reduce statistical power (Sawilowsky, 1985a, 1985b).
Several standardized measures of effect are used within the context of ANOVA to describe the degree of relationship between a predictor or set of predictors and the dependent variable. Effect size estimates are reported to allow researchers to compare findings in studies and across disciplines. Common effect size estimates reported in bivariate (e.g. ANOVA) and multivariate (MANOVA, ANCOVA, Multiple Discriminant Analysis) statistical analysis includes eta-squared, partial eta-squared, omega, and intercorrelation (Strang, 2009). Eta-squared describes the ratio of variance explained in the dependent variable by a predictor while controlling for other predictors. Eta-squared is a biased estimator of the variance explained by the model in the population (it only estimates effect size in the sample). On average it overestimates the variance explained in the population. As the sample size gets larger the amount of bias gets smaller. It is, however, an easily calculated estimator of the proportion of the variance in a population explained by the treatment. Note that earlier versions of statistical software (such as SPSS) incorrectly reports Partial eta squared under the misleading title "Eta squared". Partial eta-squared describes the "proportion of total variation attributable to the factor, partialling out (excluding) other factors from the total nonerror variation" (Pierce, Block & Aguinis, 2004, p. 918). Partial eta squared is normally higher than eta squared (except in simple one-factor models). The generally accepted regression benchmark for effect size comes from (Cohen, 1992; 1988): 0.20 is a minimal solution (but significant in social science research); 0.50 is a medium effect; anything equal to or greater than 0.80 is a large effect size (Keppel & Wickens, 2004; Cohen, 1992). Because this common interpretation of effect size has been repeated from Cohen (1988) over the years with no change or comment to validity for contemporary experimental research, it is questionable outside of psychological/behavioural studies, and more so questionable even then without a full understanding of the limitations ascribed by Cohen. Note: The use of specific partial eta-square values for large medium or small as a "rule of thumb" should be avoided. Nevertheless, alternative rules of thumb have emerged in certain disciplines: Small = 0.01; medium = 0.06; large = 0.14 (Kittler, Menard & Phillips, 2007). This measure of effect size is frequently encountered when performing power analysis calculations. Conceptually it represents the square root of variance explained over variance not explained.
A statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses. Follow up tests are often distinguished in terms of whether they are planned (a priori) or post hoc. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data. Post hoc tests such as Tukey's range test most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors. Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of groups means where one set has at two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.
In a first experiment, Group A is given vodka, Group B is given gin, and Group C is given a placebo. All groups are then tested with a memory task. A one-way ANOVA can be used to assess the effect of the various treatments (that is, the vodka, gin, and placebo). In a second experiment, Group A is given vodka and tested on a memory task. The same group is allowed a rest period of five days and then the experiment is repeated with gin. The procedure is repeated using a placebo. A one-way ANOVA with repeated measures can be used to assess the effect of the vodka versus the impact of the placebo. Each group is then tested on a memory task. The advantage of this design is that multiple variables can be tested at the same time instead of running two different experiments. Also, the experiment can determine whether one variable affects the other variable (known as interaction effects). A factorial ANOVA (2×2) can be used to assess the effect of expecting vodka or the placebo and the actual reception of either.
The analysis of variance was used informally by researchers in the 1800s using least squares. In physics and psychology, researchers included a term for the operator-effect, the influence of a particular person on measurements, according to Stephen Stigler's histories. In its modern form, the analysis of variance was one of the many important statistical innovations of Ronald A. Fisher. Fisher proposed a formal analysis of variance in his 1918 paper The Correlation Between Relatives on the Supposition of Mendelian Inheritance. His first application of the analysis of variance was published in 1921. Analysis of variance became widely known after being included in Fisher's 1925 book "Statistical Methods for Research Workers".
Alkanes, also known as paraffins, are chemical compounds that consist only of the elements carbon (C) and hydrogen (H) (i.e., hydrocarbons), wherein these atoms are linked together exclusively by single bonds (i.e., they are saturated compounds) without any cyclic structure (i.e. loops). Alkanes belong to a homologous series of organic compounds in which the members differ by a constant relative molecular mass of 14. Each carbon atom must have 4 bonds (either C-H or C-C bonds), and each hydrogen atom must be joined to a carbon atom (H-C bonds). A series of linked carbon atoms is known as the carbon skeleton or carbon backbone. In general, the number of carbon atoms is often used to define the size of the alkane (e.g., C2-alkane). An alkyl group is a functional group or side-chain that, like an alkane, consists solely of singly-bonded carbon and hydrogen atoms, for example a methyl or ethyl group. Saturated hydrocarbons can be linear (general formula) wherein the carbon atoms are joined in a snake-like structure, branched (general formula, "n" > 3) wherein the carbon backbone splits off in one or more directions, or cyclic (general formula, "n" > 2) wherein the carbon backbone is linked so as to form a loop. According to the definition by IUPAC, the former two are alkanes, whereas the third group is called cycloalkanes. Saturated hydrocarbons can also combine any of the linear, cyclic (e.g., polycyclic) and branching structures, and they are still alkanes (no general formula) as long as they are acyclic (i.e., having no loops). The simplest possible alkane (the parent molecule) is methane, CH4. There is no limit to the number of carbon atoms that can be linked together, the only limitation being that the molecule is acyclic, is saturated, and is a hydrocarbon. Saturated oils and waxes are examples of larger alkanes where the number of carbons in the carbon backbone tends to be greater than 10. Alkanes are not very reactive and have little biological activity. Alkanes can be viewed as a molecular tree upon which can be hung the interesting biologically active/reactive portions (functional groups) of the molecule.
butane is the only C4H6 compound and has no isomer; tetrahedrane (not shown) is the only C4H4 compound and has also no isomer. Branched alkanes can be chiral: 3-methylhexane and its higher homologues are chiral due to their stereogenic center at carbon atom number 3. Chiral alkanes are of certain importance in biochemistry, as they occur as sidechains in chlorophyll and tocopherol (vitamin E). Chiral alkanes can be resolved into their enantiomers by enantioselective chromatography. In addition to these isomers, the chain of carbon atoms may form one or more loops. Such compounds are called cycloalkanes.
The IUPAC nomenclature (systematic way of naming compounds) for alkanes is based on identifying hydrocarbon chains. Unbranched, saturated hydrocarbon chains are named systematically with a Greek numerical prefix denoting the number of carbons and the suffix "-ane". August Wilhelm von Hofmann suggested systematizing nomenclature by using the whole sequence of vowels a, e, i, o and u to create suffixes -ane, -ene, -ine (or -yne), -one, -une, for the hydrocarbons. The first three name hydrocarbons with single, double and triple bonds; "-one" represents a ketone; "-ol" represents an alcohol or OH group; "-oxy-" means an ether and refers to oxygen between two carbons, so that methoxy-methane is the IUPAC name for dimethyl ether. It is difficult or impossible to find compounds with more than one IUPAC name. This is because shorter chains attached to longer chains are prefixes and the convention includes brackets. Numbers in the name, referring to which carbon a group is attached to, should be as low as possible, so that 1- is implied and usually omitted from names of organic compounds with only one side-group; "1-" is implied in Nitro-octane. Symmetric compounds will have two ways of arriving at the same name.
Straight-chain alkanes are sometimes indicated by the prefix "n-" (for "normal") where a non-linear isomer exists. Although this is not strictly necessary, the usage is still common in cases where there is an important difference in properties between the straight-chain and branched-chain isomers, e.g., "n"-hexane or 2- or 3-methylpentane. These names were derived from methanol, ether, propionic acid and butyric acid, respectively. Alkanes with five or more carbon atoms are named by adding the suffix -ane to the appropriate numerical multiplier prefix with elision of any terminal vowel ("-a" or "-o") from the basic numerical term. Hence, pentane, C5H12; hexane, C6H14; heptane, C7H16; octane, C8H18; etc. The prefix is generally Greek, with the exceptions of nonane which has a Latin prefix, and undecane and tridecane which have mixed-language prefixes. For a more complete list, see List of alkanes.
So-called cyclic alkanes are, in the technical sense, "not" alkanes, but cycloalkanes. They are hydrocarbons just like alkanes, but contain one or more rings. Simple cycloalkanes have a prefix "cyclo-" to distinguish them from alkanes. Cycloalkanes are named as per their acyclic counterparts with respect to the number of carbon atoms, e.g., cyclopentane (C5H10) is a cycloalkane with 5 carbon atoms just like pentane (C5H12), but they are joined up in a five-membered ring. In a similar manner, propane and cyclopropane, butane and cyclobutane, etc. Substituted cycloalkanes are named similar to substituted alkanes — the cycloalkane ring is stated, and the substituents are according to their position on the ring, with the numbering decided by Cahn-Ingold-Prelog rules.
The trivial (non-systematic) name for alkanes is "paraffins." Together, alkanes are known as the "paraffin series". Trivial names for compounds are usually historical artifacts. They were coined before the development of systematic names, and have been retained due to familiar usage in industry. Cycloalkanes are also called naphthenes. It is almost certain that the term paraffin stems from the petrochemical industry. Branched-chain alkanes are called "isoparaffins". The use of the term "paraffin" is a general term and often does not distinguish between a pure compounds and mixtures of isomers with the same chemical formula (i.e., like a chemical anagram), e.g., pentane and isopentane.
Alkanes experience inter-molecular van der Waals forces. Stronger inter-molecular van der Waals forces give rise to greater boiling points of alkanes. Under standard conditions, from CH4 to C4H10 alkanes are gaseous; from C5H12 to C17H36 they are liquids; and after C18H38 they are solids. As the boiling point of alkanes is primarily determined by weight, it should not be a surprise that the boiling point has almost a linear relationship with the size (molecular weight) of the molecule. As a rule of thumb, the boiling point rises 20 - 30 °C for each carbon added to the chain; this rule applies to other homologous series. A straight-chain alkane will have a boiling point higher than a branched-chain alkane due to the greater surface area in contact, thus the greater van der Waals forces, between adjacent molecules. For example, compare isobutane (2-methylpropane) and n-butane (butane), which boil at -12 and 0 °C, and 2,2-dimethylbutane and 2,3-dimethylbutane which boil at 50 and 58 °C, respectively. For the latter case, two molecules 2,3-dimethylbutane can "lock" into each other better than the cross-shaped 2,2-dimethylbutane, hence the greater van der Waals forces. On the other hand, cycloalkanes tend to have higher boiling points than their linear counterparts due to the locked conformations of the molecules, which give a plane of intermolecular contact.
The melting points of the alkanes follow a similar trend to boiling points for the same reason as outlined above. That is, (all other things being equal) the larger the molecule the higher the melting point. There is one significant difference between boiling points and melting points. Solids have more rigid and fixed structure than liquids. This rigid structure requires energy to break down. Thus the stronger better put together solid structures will require more energy to break apart. For alkanes, this can be seen from the graph above (i.e., the blue line). The odd-numbered alkanes have a lower trend in melting points than even numbered alkanes. This is because even numbered alkanes pack well in the solid phase, forming a well-organised structure, which requires more energy to break apart. The odd-number alkanes pack less well and so the "looser" organised solid packing structure requires less energy to break apart. The melting points of branched-chain alkanes can be either higher or lower than those of the corresponding straight-chain alkanes, again depending on the ability of the alkane in question to packing well in the solid phase: This is particularly true for isoalkanes (2-methyl isomers), which often have melting points higher than those of the linear analogues.
Alkanes do not conduct electricity, nor are they substantially polarized by an electric field. For this reason they do not form hydrogen bonds and are insoluble in polar solvents such as water. Since the hydrogen bonds between individual water molecules are aligned away from an alkane molecule, the coexistence of an alkane and water leads to an increase in molecular order (a reduction in entropy). As there is no significant bonding between water molecules and alkane molecules, the second law of thermodynamics suggests that this reduction in entropy should be minimised by minimising the contact between alkane and water: Alkanes are said to be hydrophobic in that they repel water. Their solubility in nonpolar solvents is relatively good, a property that is called lipophilicity. Different alkanes are, for example, miscible in all proportions among themselves. The density of the alkanes usually increases with increasing number of carbon atoms, but remains less than that of water. Hence, alkanes form the upper layer in an alkane-water mixture.
The molecular structure of the alkanes directly affects their physical and chemical characteristics. It is derived from the electron configuration of carbon, which has four valence electrons. The carbon atoms in alkanes are always sp3 hybridised, that is to say that the valence electrons are said to be in four equivalent orbitals derived from the combination of the 2s orbital and the three 2p orbitals. These orbitals, which have identical energies, are arranged spatially in the form of a tetrahedron, the angle of cos−1(−⅓) ≈ 109.47° between them. Bond lengths and bond angles. An alkane molecule has only C – H and C – C single bonds. The former result from the overlap of a sp³-orbital of carbon with the 1s-orbital of a hydrogen; the latter by the overlap of two sp³-orbitals on different carbon atoms. The bond lengths amount to 1.09×10−10 m for a C – H bond and 1.54×10−10 m for a C – C bond. The spatial arrangement of the bonds is similar to that of the four sp³-orbitals—they are tetrahedrally arranged, with an angle of 109.47° between them. Structural formulae that represent the bonds as being at right angles to one another, while both common and useful, do not correspond with the reality.
The structural formula and the bond angles are not usually sufficient to completely describe the geometry of a molecule. There is a further degree of freedom for each carbon – carbon bond: the torsion angle between the atoms or groups bound to the atoms at each end of the bond. The spatial arrangement described by the torsion angles of the molecule is known as its conformation. Ethane forms the simplest case for studying the conformation of alkanes, as there is only one C – C bond. If one looks down the axis of the C – C bond, one will see the so-called Newman projection. The hydrogen atoms on both the front and rear carbon atoms have an angle of 120° between them, resulting from the projection of the base of the tetrahedron onto a flat plane. However, the torsion angle between a given hydrogen atom attached to the front carbon and a given hydrogen atom attached to the rear carbon can vary freely between 0° and 360°. This is a consequence of the free rotation about a carbon – carbon single bond. Despite this apparent freedom, only two limiting conformations are important: eclipsed conformation and staggered conformation. The two conformations, also known as rotamers, differ in energy: The staggered conformation is 12.6 kJ/mol lower in energy (more stable) than the eclipsed conformation (the least stable). This difference in energy between the two conformations, known as the torsion energy, is low compared to the thermal energy of an ethane molecule at ambient temperature. There is constant rotation about the C-C bond. The time taken for an ethane molecule to pass from one staggered conformation to the next, equivalent to the rotation of one CH3-group by 120° relative to the other, is of the order of 10−11 seconds. The case of higher alkanes is more complex but based on similar principles, with the antiperiplanar conformation always being the most favoured around each carbon-carbon bond. For this reason, alkanes are usually shown in a zigzag arrangement in diagrams or in models. The actual structure will always differ somewhat from these idealised forms, as the differences in energy between the conformations are small compared to the thermal energy of the molecules: Alkane molecules have no fixed structural form, whatever the models may suggest.
The carbon–hydrogen stretching mode gives a strong absorption between 2850 and 2960 cm−1, while the carbon–carbon stretching mode absorbs between 800 and 1300 cm−1. The carbon–hydrogen bending modes depend on the nature of the group: methyl groups show bands at 1450 cm−1 and 1375 cm−1, while methylene groups show bands at 1465 cm−1 and 1450 cm−1. Carbon chains with more than four carbon atoms show a weak absorption at around 725 cm−1.
The proton resonances of alkanes are usually found at δH = 0.5 – 1.5. The carbon-13 resonances depend on the number of hydrogen atoms attached to the carbon: δC = 8 – 30 (primary, methyl, -CH3), 15 – 55 (secondary, methylene, -CH2-), 20 – 60 (tertiary, methyne, C-H) and quaternary. The carbon-13 resonance of quaternary carbon atoms is characteristically weak, due to the lack of Nuclear Overhauser effect and the long relaxation time, and can be missed in weak samples, or sample that have not been run for a sufficiently long time.
Alkanes have a high ionisation energy, and the molecular ion is usually weak. The fragmentation pattern can be difficult to interpret, but, in the case of branched chain alkanes, the carbon chain is preferentially cleaved at tertiary or quaternary carbons due to the relative stability of the resulting free radicals. The fragment resulting from the loss of a single methyl group (M−15) is often absent, and other fragment are often spaced by intervals of fourteen mass units, corresponding to sequential loss of CH2-groups.
In general, alkanes show a relatively low reactivity, because their C bonds are relatively stable and cannot be easily broken. Unlike most other organic compounds, they possess no functional groups. They react only very poorly with ionic or other polar substances. The acid dissociation constant (pKa) values of all alkanes are above 60, hence they are practically inert to acids and bases (see: carbon acids). This inertness is the source of the term "paraffins" (with the meaning here of "lacking affinity"). In crude oil the alkane molecules have remained chemically unchanged for millions of years. However redox reactions of alkanes, in particular with oxygen and the halogens, are possible as the carbon atoms are in a strongly reduced condition; in the case of methane, the lowest possible oxidation state for carbon (−4) is reached. Reaction with oxygen leads to combustion without any smoke; with halogens, substitution. In addition, alkanes have been shown to interact with, and bind to, certain transition metal complexes in (See: carbon-hydrogen bond activation). Free radicals, molecules with unpaired electrons, play a large role in most reactions of alkanes, such as cracking and reformation where long-chain alkanes are converted into shorter-chain alkanes and straight-chain alkanes into branched-chain isomers. In highly branched alkanes, the bond angle may differ significantly from the optimal value (109.5°) in order to allow the different groups sufficient space. This causes a tension in the molecule, known as steric hindrance, and can substantially increase the reactivity. Reactions with oxygen (combustion reaction). See the alkane heat of formation table for detailed data. The standard enthalpy change of combustion, Δc"H"o, for alkanes increases by about 650 kJ/mol per CH2 group. Branched-chain alkanes have lower values of Δc"H"o than straight-chain alkanes of the same number of carbon atoms, and so can be seen to be somewhat more stable.
Cracking breaks larger molecules into smaller ones. This can be done with a thermal or catalytic method. The thermal cracking process follows a homolytic mechanism with formation of free-radicals. The catalytic cracking process involves the presence of acid catalysts (usually solid acids such as silica-alumina and zeolites), which promote a heterolytic (asymmetric) breakage of bonds yielding pairs of ions of opposite charges, usually a carbocation and the very unstable hydride anion. Carbon-localized free-radicals and cations are both highly unstable and undergo processes of chain rearrangement, C-C scission in position beta (i.e., cracking) and intra- and intermolecular hydrogen transfer or hydride transfer. In both types of processes, the corresponding reactive intermediates (radicals, ions) are permanently regenerated, and thus they proceed by a self-propagating chain mechanism. The chain of reactions is eventually terminated by radical or ion recombination.
Alkanes will react with steam in the presence of a nickel catalyst to give hydrogen. Alkanes can be chlorosulfonated and nitrated, although both reactions require special conditions. The fermentation of alkanes to carboxylic acids is of some technical importance. In the Reed reaction, sulfur dioxide, chlorine and light convert hydrocarbons to sulfonyl chlorides. Occurrence of alkanes in the Universe. Alkanes form a small portion of the atmospheres of the outer gas planets such as Jupiter (0.1% methane, 0.0002% ethane), Saturn (0.2% methane, 0.0005% ethane), Uranus (1.99% methane, 0.00025% ethane) and Neptune (1.5% methane, 1.5 ppm ethane). Titan (1.6% methane), a satellite of Saturn, was examined by the "Huygens" probe, which indicate that Titan's atmosphere periodically rains liquid methane onto the moon's surface. Also on Titan, a methane-spewing volcano was spotted and this volcanism is believed to be a significant source of the methane in the atmosphere. There also appear to be Methane/Ethane lakes near the north polar regions of Titan, as discovered by Cassini's radar imaging. Methane and ethane have also been detected in the tail of the comet Hyakutake. Chemical analysis showed that the abundances of ethane and methane were roughly equal, which is thought to imply that its ices formed in interstellar space, away from the Sun, which would have evaporated these volatile molecules. Alkanes have also been detected in meteorites such as carbonaceous chondrites. Occurrence of alkanes on Earth. Traces of methane gas (about 0.0001% or 1 ppm) occur in the Earth's atmosphere, produced primarily by organisms such as Archaea, found for example in the gut of cows. These hydrocarbons collected in porous rocks, located beneath an impermeable cap rock and so are trapped. Unlike methane, which is constantly reformed in large quantities, higher alkanes (alkanes with 9 or more carbon atoms) rarely develop to a considerable extent in nature. These deposits, e.g., oil fields, have formed over millions of years and once exhausted cannot be readily replaced. The depletion of these hydrocarbons is the basis for what is known as the energy crisis. Solid alkanes are known as tars and are formed when more volatile alkanes such as gases and oil evaporate from hydrocarbon deposits. One of the largest natural deposits of solid alkanes is in the asphalt lake known as the Pitch Lake in Trinidad and Tobago. Methane is also present in what is called biogas, produced by animals and decaying matter, which is a possible renewable energy source. Alkanes have a low solubility in water, so the content in the oceans is negligible; however, at high pressures and low temperatures (such as at the bottom of the oceans), methane can co-crystallize with water to form a solid methane hydrate. Although this cannot be commercially exploited at the present time, the amount of combustible energy of the known methane hydrate fields exceeds the energy content of all the natural gas and oil deposits put together;methane extracted from methane hydrate is considered therefore a candidate for future fuels.
Although alkanes occur in nature in various way, they do not rank biologically among the essential materials. Cycloalkanes with 14 to 18 carbon atoms occur in musk, extracted from deer of the family Moschidae. All further information refers to (acyclic) alkanes. Certain types of bacteria can metabolise alkanes: they prefer even-numbered carbon chains as they are easier to degrade than odd-numbered chains. Methanogens are also the producers of marsh gas in wetlands, and release about two billion tonnes of methane per year—the atmospheric content of this gas is produced nearly exclusively by them. The methane output of cattle and other herbivores, which can release up to 150 litres per day, and of termites, is also due to methanogens. They also produce this simplest of all alkanes in the intestines of humans. Methanogenic archaea are, hence, at the end of the carbon cycle, with carbon being released back into the atmosphere after having been fixed by photosynthesis. It is probable that our current deposits of natural gas were formed in a similar way. Alkanes also play a role, if a minor role, in the biology of the three eukaryotic groups of organisms: fungi, plants and animals. Some specialised yeasts, e.g., "Candida tropicale", "Pichia" sp., "Rhodotorula" sp., can use alkanes as a source of carbon and/or energy. The fungus "Amorphotheca resinae" prefers the longer-chain alkanes in aviation fuel, and can cause serious problems for aircraft in tropical regions. In plants, the solid long-chain alkanes are found in the plant cuticle and epicuticular wax of many species, but are only rarely major constituents. They protect the plant against water loss, prevent the leaching of important minerals by the rain, and protect against bacteria, fungi, and harmful insects. The carbon chains in plant alkanes are usually odd-numbered, between twenty-seven and thirty-three carbon atoms in length and are made by the plants by decarboxylation of even-numbered fatty acids. The exact composition of the layer of wax is not only species-dependent, but changes also with the season and such environmental factors as lighting conditions, temperature or humidity. Alkanes are found in animal products, although they are less important than unsaturated hydrocarbons. One example is the shark liver oil, which is approximately 14% pristane (2,6,10,14-tetramethylpentadecane, C19H40). Their occurrence is more important in pheromones, chemical messenger materials, on which above all insects are dependent for communication. With some kinds, as the support beetle "Xylotrechus colonus", primarily pentacosane (C25H52), 3-methylpentaicosane (C26H54) and 9-methylpentaicosane (C26H54), they are transferred by body contact. With others like the tsetse fly "Glossina morsitans morsitans", the pheromone contains the four alkanes 2-methylheptadecane (C18H38), 17,21-dimethylheptatriacontane (C39H80), 15,19-dimethylheptatriacontane (C39H80) and 15,19,23-trimethylheptatriacontane (C40H82), and acts by smell over longer distances, a useful characteristic for pest control. Waggle-dancing honeybees produce and release two alkanes, tricosane and pentacosane.
One example, in which both plant and animal alkanes play a role, is the ecological relationship between the sand bee ("Andrena nigroaenea") and the early spider orchid ("Ophrys sphegodes"); the latter is dependent for pollination on the former. Sand bees use pheromones in order to identify a mate; in the case of "A. nigroaenea", the females emit a mixture of tricosane (C23H48), pentacosane (C25H52) and heptacosane (C27H56) in the ratio 3:3:1, and males are attracted by specifically this odour. The orchid takes advantage of this mating arrangement to get the male bee to collect and disseminate its pollen; parts of its flower not only resemble the appearance of sand bees, but also produce large quantities of the three alkanes in the same ratio as female sand bees. As a result numerous males are lured to the blooms and attempt to copulate with their imaginary partner: although this endeavour is not crowned with success for the bee, it allows the orchid to transfer its pollen, which will be dispersed after the departure of the frustrated male to different blooms.
The applications of a certain alkane can be determined quite well according to the number of carbon atoms. The first four alkanes are used mainly for heating and cooking purposes, and in some countries for electricity generation. Methane and ethane are the main components of natural gas; they are normally stored as gases under pressure. It is, however, easier to transport them as liquids: This requires both compression and cooling of the gas. Propane and butane can be liquefied at fairly low pressures, and are well known as liquified petroleum gas (LPG). Propane, for example, is used in the propane gas burner, butane in disposable cigarette lighters. The two alkanes are used as propellants in aerosol sprays. From pentane to octane the alkanes are reasonably volatile liquids. They are used as fuels in internal combustion engines, as they vaporise easily on entry into the combustion chamber without forming droplets, which would impair the uniformity of the combustion. Branched-chain alkanes are preferred as they are much less prone to premature ignition, which causes knocking, than their straight-chain homologues. This propensity to premature ignition is measured by the octane rating of the fuel, where 2,2,4-trimethylpentane ("isooctane") has an arbitrary value of 100, and heptane has a value of zero. Apart from their use as fuels, the middle alkanes are also good solvents for nonpolar substances. Alkanes from nonane to, for instance, hexadecane (an alkane with sixteen carbon atoms) are liquids of higher viscosity, less and less suitable for use in gasoline. They form instead the major part of diesel and aviation fuel. Diesel fuels are characterised by their cetane number, cetane being an old name for hexadecane. However, the higher melting points of these alkanes can cause problems at low temperatures and in polar regions, where the fuel becomes too thick to flow correctly. Alkanes from hexadecane upwards form the most important components of fuel oil and lubricating oil. In latter function, they work at the same time as anti-corrosive agents, as their hydrophobic nature means that water cannot reach the metal surface. Many solid alkanes find use as paraffin wax, for example, in candles. This should not be confused however with true wax, which consists primarily of esters. Alkanes with a chain length of approximately 35 or more carbon atoms are found in bitumen, used, for example, in road surfacing. However, the higher alkanes have little value and are usually split into lower alkanes by cracking. Some synthetic polymers such as polyethylene and polypropylene are alkanes with chains containing hundreds of thousands of carbon atoms. These materials are used in innumerable applications, and billions of kilograms of these materials are made and used each year.
When released in the environment, alkanes don't undergo rapid biodegradation, because they haven't functional groups (like hydroxyl or carbonyl) that are needed by most organismsm in order to metabolize the compound. However, some bacteria can metabolize some alkanes (expecially those linear and short), by oxidizing the terminal carbon atom. The product is an alcohol, that could be next oxidized to an aldehyde, and finally to a carboxylic acid. The resulting fatty acid could be metabolized through the fatty acid degradation pathway.
Methane is explosive when mixed with air (1 – 8% CH4) and is a strong greenhouse gas: Other lower alkanes can also form explosive mixtures with air. The lighter liquid alkanes are highly flammable, although this risk decreases with the length of the carbon chain. Pentane, hexane, heptane, and octane are classed as "dangerous for the environment" and "harmful". The straight-chain isomer of hexane is a neurotoxin. Halogen-rich alkanes, like chloroform, can be carcinogenic as well.
In law, an appeal is a process for requesting a formal change to an official decision. The specific procedures for appealing, including even whether there is a right of appeal from a particular type of decision, can vary greatly from country to country. Even within a jurisdiction, the nature of an appeal can vary greatly depending on the type of case. An appellate court is a court that hears cases on appeal from another court. Depending on the particular legal rules that apply to each circumstance, a party to a court case who is unhappy with the result might be able to challenge that result in an appellate court on specific grounds. These grounds typically could include errors of law, fact, or procedure (in the United States, due process). In different jurisdictions, appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts.
A party who files an appeal is called an "appellant" or "petitioner", and a party on the other side is called a "respondent" (in most common-law countries) or an "appellee" (in the United States). A "cross-appeal" is an appeal brought by the respondent. For example, suppose at trial the judge found for the plaintiff and ordered the defendant to pay $50,000. If the defendant files an appeal arguing that he should not have to pay any money, then the plaintiff might file a cross-appeal arguing that the defendant should have to pay $200,000 instead of $50,000. The appellant is the party who, having lost part or all their claim in a lower court decision, is appealing to a higher court to have their case reconsidered. This is usually done on the basis that the lower court judge erred in the application of law, but it may also be possible to appeal on the basis of court misconduct, or that a finding of fact was entirely unreasonable to make on the evidence. The appellant in the new case can be either the plaintiff (or "claimant"), defendant, or respondent (appellee) from the lower case, depending on who was the losing party. The winning party from the lower court, however, is now the respondent. In unusual cases the appellant can be the victor in the court below, but still appeal. For example, in "Doyle v Olby (Ironmongers) Ltd" [1969] 2 QB 158, the claimant appealed (successfully) on the basis that, although he won in the court below, the lower court had applied the wrong measure of damages and he had not been fully recompensed. An appellee is the party to an appeal in which the lower court judgment was in its favor. The appellee is required to respond to the petition, oral arguments, and legal briefs of the appellant. In general, the appellee takes the procedural posture that the lower court's decision should be affirmed.
An appeal "as of right" is one that is guaranteed by statute or some underlying constitutional or legal principle. The appellate court cannot refuse to listen to the appeal. An appeal "by leave" or "permission" requires the appellant to move for leave to appeal; in such a situation either or both of the lower court and the appellate court may have the discretion to grant or refuse the appellant's demand to appeal the lower court's decision. A good example of this is the U.S. Supreme Court in which at least three justices must agree to hear the case if there is a constitutional issue. In tort, equity, or other civil matters either party to a previous case may file an appeal. In criminal matters, however, the state or prosecution generally has no appeal "as of right". And due to the double jeopardy principle, in the United States the state or prosecution may never appeal a jury or bench verdict of acquittal. But in some jurisdictions, the state or prosecution may appeal "as of right" from a trial court's dismissal of an indictment in whole or in part or from a trial court's granting of a defendant's suppression motion. Likewise, in some jurisdictions, the state or prosecution may appeal an issue of law "by leave" from the trial court and/or the appellate court. The ability of the prosecution to appeal a decision in favor of a defendant varies significantly internationally. All parties must present grounds to appeal, or it will not be heard. By convention in some law reports, the appellant is named first. This can mean that where it is the defendant who appeals, the name of the case in the law reports reverses (in some cases twice) as the appeals work their way up the court hierarchy. This is not always true, however. In the United States federal courts, the parties' names always stay in the same order as the lower court when an appeal is taken to the circuit courts of appeals, and are re-ordered only if the appeal reaches the United States Supreme Court.
Many jurisdictions recognize two types of appeals, particularly in the criminal context. The first is the traditional "direct" appeal in which the appellant files an appeal with the next higher court of review. The second is the collateral appeal or post-conviction petition, in which the petitioner-appellant files the appeal in a court of first instance—usually the court that tried the case. The key distinguishing factor between direct and collateral appeals is that the former only reviews evidence that was presented in the trial court, but the latter allows review of evidence dehors the record: depositions, affidavits, and witness statements that did not come in at trial. The standard for post-conviction relief is high, typically requiring the petitioner to demonstrate that the evidence presented was not available in the usual course of trial discovery. Relief in post-conviction is rare and is most often found in capital or violent felony cases. The typical scenario involves an incarcerated defendant locating DNA evidence demonstrating the defendant's actual innocence.
There are a number of appeal actions, their differences being potentially confusing, thus bearing some explanation. Three of the most common are an appeal to which the defendant has as a right, a writ of certiorari and a writ of habeas corpus. An appeal to which the defendant has a right cannot be abridged by the court which is, by designation of its jurisdiction, obligated to hear the appeal. In such an appeal, the appellant feels that some error has been made in his trial, necessitating an appeal. A matter of importance is the basis on which such an appeal might be filed: generally appeals as a matter of right may only address issues which were originally raised in trial (as evidenced by documentation in the official record). Any issue not raised in the original trial may not be considered on appeal and will be considered estoppel. A convenient test for whether a petition is likely to succeed on the grounds of error is confirming that (1) a mistake was indeed made (2) an objection to that mistake was presented by counsel and (3) that mistake negatively affected the defendant’s trial. A writ of certiorari, otherwise know as simply as cert, is an order by a higher court directing a lower court to send record of a case for review, and is the next logical step in post-trial procedure. While states may have similar processes, a writ of cert is usually only issued, in the United States, by the Supreme Court, although some states retain this procedure. Unlike the aforementioned appeal, a writ of cert is not a matter of right. A writ of cert will have to be petitioned for, the higher court issuing such writs on limited bases according to constraints such as time. In another sense, a writ of cert is like an appeal in its constraints; it too may only seek relief on grounds raised in the original trial. A writ of habeas corpus is the last opportunity for the defendant to find relief against his guilty conviction. Habeas corpus may be pursued if a defendant is unsatisfied with the outcome of his appeal and has been refused (or did not pursue) a writ of cert, at which point he may petition one of several courts for a writ of habeas corpus. Again, these are granted at the discretion of the court and require a petition. Like appeals or writs of cert, a writ of habeas corpus may overturn a defendant's guilty conviction by finding some error in the original trial. The major difference is that writs of habeas corpus may, and often, focus on issues that lay outside the original premises of the trial, i.e., issues that could not be raised by appeal or writs of cert. These often fall in two logical categories: (1) that the trial lawyer was ineffectual or incompetent or (2) that some constitutional right has been violated.
A notice of appeal is a form or document that in many cases is required to begin an appeal. The form is completed by the appellant or by the appellant's legal representative. The nature of this form can vary greatly from country to country and from court to court within a country. The specific rules of the legal system will dictate exactly how the appeal is officially begun. For example, the appellant might have to file the notice of appeal with the appellate court, or with the court from which the appeal is taken, or both. Some courts have samples of a notice of appeal on the court's own web site. The deadline for beginning an appeal can often be very short: traditionally, it is measured in days, not years. This can vary from country to country, as well as within a country, depending on the specific rules in force. How an appeal is processed. Generally speaking the appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether that decision was legally sound or not. The appellate court will typically be deferential to the lower court's findings of fact (such as whether a defendant committed a particular act), unless clearly erroneous, and so will focus on the court's application of the law to those facts (such as whether the act found by the court to have occurred fits a legal definition at issue). If the appellate court finds no defect, it "affirms" the judgment. If the appellate court does find a legal defect in the decision "below" (i.e., in the lower court), it may "modify" the ruling to correct the defect, or it may nullify ("reverse" or "vacate") the whole decision or any part of it. It may, in addition, send the case back ("remand" or "remit") to the lower court for further proceedings to remedy the defect. In some cases, an appellate court may review a lower court decision "de novo" (or completely), challenging even the lower court's findings of fact. This might be the proper standard of review, for example, if the lower court resolved the case by granting a pre-trial motion to dismiss or motion for summary judgment which is usually based only upon written submissions to the trial court and not on any trial testimony. Another situation is where appeal is by way of "re-hearing". Certain jurisdictions permit certain appeals to cause the trial to be heard afresh in the appellate court. An example would be an appeal from a magistrates' court to the Crown Court in England and Wales. Sometimes, the appellate court finds a defect in the procedure the parties used in filing the appeal and dismisses the appeal without considering its merits, which has the same effect as affirming the judgment below. (This would happen, for example, if the appellant waited too long, under the appellate court's rules, to file the appeal.) In England and many other jurisdictions, however, the phrase "appeal dismissed" is equivalent to the U.S. term "affirmed"; and the phrase "appeal allowed" is equivalent to the U.S. term "reversed". Generally, there is no trial in an appellate court, only consideration of the record of the evidence presented to the trial court and all the pre-trial and trial court proceedings are reviewed—unless the appeal is by way of re-hearing, new evidence will usually only be considered on appeal in "very" rare instances, for example if that material evidence was unavailable to a party for some very significant reason such as prosecutorial misconduct. In some systems, an appellate court will only consider the written decision of the lower court, together with any written evidence that was before that court and is relevant to the appeal. In other systems, the appellate court will normally consider the record of the lower court. In those cases the record will first be certified by the lower court. The appellant has the opportunity to present arguments for the granting of the appeal and the appellee (or respondent) can present arguments against it. Arguments of the parties to the appeal are presented through their appellate lawyers, if represented, or "pro se" if the party has not engaged legal representation. Those arguments are presented in written briefs and sometimes in oral argument to the court at a hearing. At such hearings each party is allowed a brief presentation at which the appellate judges ask questions based on their review of the record below and the submitted briefs. It is important to note that in an adversarial system appellate courts do not have the power to review lower court decisions unless a party appeals it. Therefore if a lower court has ruled in an improper manner or against legal precedent that judgment will stand even if it might have been overturned on appeal.
The United States legal system generally recognizes two types of appeals: a trial "de novo" or an appeal on the record. A trial de novo is usually available for review of informal proceedings conducted by some minor judicial tribunals in proceedings that do not provide all the procedural attributes of a formal judicial trial. If unchallenged, these decisions have the power to settle more minor legal disputes once and for all. If a party is dissatisfied with the finding of such a tribunal, one generally has the power to request a trial "de novo" by a court of record. In such a proceeding, all issues and evidence may be developed newly, as though never heard before, and one is not restricted to the evidence heard in the lower proceeding. Sometimes, however, the decision of the lower proceeding is itself admissible as evidence, thus helping to curb frivolous appeals. In an appeal on the record from a decision in a judicial proceeding, both appellant and respondent are bound to base their arguments wholly on the proceedings and body of evidence as they were presented in the lower tribunal. Each seeks to prove to the higher court that the result they desired was the just result. Precedent and case law figure prominently in the arguments. In order for the appeal to succeed, the appellant must prove that the lower court committed reversible error, that is, an impermissible action by the court acted to cause a result that was unjust, and which would not have resulted had the court acted properly. Some examples of reversible error would be erroneously instructing the jury on the law applicable to the case, permitting seriously improper argument by an attorney, admitting or excluding evidence improperly, acting outside the court's jurisdiction, injecting bias into the proceeding or appearing to do so, juror misconduct, etc. The failure to formally object at the time, to what one views as improper action in the lower court, may result in the affirmance of the lower court's judgment on the grounds that one did not "preserve the issue for appeal" by objecting. In cases where a judge rather than a jury decided issues of fact, an appellate court will apply an "abuse of discretion" standard of review. Under this standard, the appellate court gives deference to the lower court's view of the evidence, and reverses its decision only if it were a clear abuse of discretion. This is usually defined as a decision outside the bounds of reasonableness. On the other hand, the appellate court normally gives less deference to a lower court's decision on issues of law, and may reverse if it finds that the lower court applied the wrong legal standard. In some rare cases, an appellant may successfully argue that the law under which the lower decision was rendered was unconstitutional or otherwise invalid, or may convince the higher court to order a new trial on the basis that evidence earlier sought was concealed or only recently discovered. In the case of new evidence, there must be a high probability that its presence or absence would have made a material difference in the trial. Another issue suitable for appeal in criminal cases is effective assistance of counsel. If a defendant has been convicted and can prove that his lawyer did not adequately handle his case "and" that there is a reasonable probability that the result of the trial would have been different had the lawyer given competent representation, he is entitled to a new trial. In the United States, a lawyer traditionally starts an oral argument to any appellate court with the words "May it please the court." After an appeal is heard, the "mandate" is a formal notice of a decision by a court of appeal; this notice is transmitted to the trial court and, when filed by the clerk of the trial court, constitutes the final judgment on the case, unless the appeal court has directed further proceedings in the trial court. The mandate is distinguished from the appeal court's opinion, which sets out the legal reasoning for its decision. In some U.S. jurisdictions the mandate is known as the "remittitur".
Appellate review is the general term for the process by which courts with appellate jurisdiction take jurisdiction of matters decided by lower courts. It is distinguished from judicial review, which refers to the court's overriding constitutional or statutory right to determine if a legislative act or administrative decision is defective for jurisdictional or other reasons (which may vary by jurisdiction). In most jurisdictions the normal and preferred way of seeking appellate review is by filing an appeal of the final judgment. Generally, an appeal of the judgment will also allow appeal of all other orders or rulings made by the trial court in the course of the case. This is because such orders cannot be appealed "as of right". However, certain critical interlocutory court orders, such as the denial of a request for an interim injunction, or an order holding a person in contempt of court, can be appealed immediately although the case may otherwise not have been fully disposed of. In American law, there are two distinct forms of appellate review, "direct" and "collateral". For example, a criminal defendant may be convicted in state court, and lose on "direct appeal" to higher state appellate courts, and if unsuccessful, mount a "collateral" action such as filing for a writ of habeas corpus in the federal courts. Generally speaking, "[d]irect appeal statutes afford defendants the opportunity to challenge the merits of a judgment and allege errors of law or fact.... [Collateral review], on the other hand, provide[s] an independent and civil inquiry into the validity of a conviction and sentence, and as such are generally limited to challenges to constitutional, jurisdictional, or other fundamental violations that occurred at trial." "Graham v. Borgen", __ F 3d. __ (7th Cir. 2007) (no. 04-4103) (slip op. at 7) (citation omitted). In Anglo-American common law courts, appellate review of lower court decisions may also be obtained by filing a petition for review by prerogative writ in certain cases. There is no corresponding right to a writ in any pure or continental civil law legal systems, though some mixed systems such as Quebec recognize these prerogative writs.
Generally, an answer is a reply to a question or is a solution, a retaliation, or a response that is relevant to the said question. In law, an answer was originally a solemn assertion in opposition to some one or something, and thus generally any counter-statement or defense, a reply to a question or objection, or a correct solution of a problem. In the common law, an answer is the first pleading by a defendant, usually filed and served upon the plaintiff within a certain strict time limit after a civil complaint or criminal information or indictment has been served upon the defendant. It may have been preceded by an "optional" "pre-answer" motion to dismiss or demurrer; if such a motion is unsuccessful, the defendant "must" file an answer to the complaint or risk an adverse default judgment. The "answer" establishes which allegations (cause of action in civil matters) set forth by the complaining party will be contested by the defendant, and states all the defendant's defenses, thus establishing the nature and parameters of the controversy to be decided by the court. In a criminal case, there is usually an arraignment or some other kind of appearance before defendant comes to court. The pleading in the criminal case, which is entered on the record in open court, is usually either guilty or not guilty. Generally speaking in private, civil cases there is no plea entered of guilt or innocence. There is only a judgment that grants money damages or some other kind of equitable remedy such as restitution or a permanent injunction. Criminal cases may lead to fines or other punishment, such as imprisonment. The famous Latin "Responsa Prudentium" ("answers of the learned ones") were the accumulated views of many successive generations of Roman lawyers, a body of legal opinion which gradually became authoritative. In music an "answer" (also known as countersubject) is the technical name in counterpoint for the repetition or modification by one part or instrument of a theme proposed by another.
An appellate court is any court of law that is empowered to hear an appeal of a trial court or other lower tribunal. In most jurisdictions, the court system is divided into at least three levels: the trial court, which initially hears cases and reviews evidence and testimony to determine the facts of the case; at least one intermediate appellate court; and a supreme court (or court of last resort) which primarily reviews the decisions of the intermediate courts. A supreme court is therefore itself a kind of appellate court. Appellate courts worldwide can operate by varying rules. For example, the Isle of Man's traditional local appellate court is the Staff of Government Division which has only two Justices, titled "Deemsters," whose decisions are joined to the original trial decision. They almost always have a majority, if either Deemster agrees with the trial Judge.
Many US jurisdictions title their appellate court a Court of Appeal or Court of Appeals. Historically, others have titled their appellate court a Court of Errors (or Court of Errors and Appeals), on the premise that it was intended to correct errors made by lower courts. Examples of such courts include the New Jersey Court of Errors and Appeals (which existed from 1844 to 1947), the Connecticut Supreme Court of Errors (which has been renamed the Connecticut Supreme Court), the Kentucky Court of Errors (since renamed the Kentucky Supreme Court), and the Mississippi High Court of Errors and Appeals (since renamed the Supreme Court of Mississippi). In some jurisdictions, courts able to hear appeals are known as an Appellate Division. Depending on the system, certain courts may serve as both trial courts and appellate courts, hearing appeals of decisions made by courts with more limited jurisdiction. Some jurisdictions have specialized appellate courts, such as the Texas Court of Criminal Appeals, which only hears appeals raised in criminal cases, and the United States Court of Appeals for the Federal Circuit, which has general jurisdiction but derives most of its caseload from patent cases, on the one hand, and appeals from the Court of Federal Claims on the other.
The authority of appellate courts to review a decisions of lower courts varies widely from one jurisdiction to another. In some places, the appellate court has limited powers of review. For example, in the United States, both state and federal appellate courts are usually restricted to examining whether the court below made the correct legal determinations, rather than hearing direct evidence and determining what the facts of the case were. Furthermore, U.S. appellate courts are usually restricted to hearing appeals based on matters that were originally brought up before the trial court. Hence, such an appellate court will not consider an appellant's argument if it is based on a theory that is raised for the first time in the appeal. In most U.S. states, and in U.S. federal courts, parties before the court are allowed one appeal as of right. This means that a party who is unsatisfied with the outcome of a trial may bring an appeal to contest that outcome. However, appeals may be costly, and the appellate court must find an error on the part of the court below that justifies upsetting the verdict. Therefore, only a small proportion of trial court decisions result in appeals. Some appellate courts, particularly supreme courts, have the power of discretionary review, meaning that they can decide whether they will hear an appeal brought in a particular case.
Arraignment is a formal reading of a criminal complaint in the presence of the defendant to inform the defendant of the charges against him or her. In response to arraignment, the accused is expected to enter a plea. Acceptable pleas vary among jurisdictions, but they generally include "guilty", "not guilty", and the peremptory pleas (or pleas in bar) setting out reasons why a trial cannot proceed. Pleas of "nolo contendere" (no contest) and the "Alford plea" are allowed in some circumstances. In England, Wales, and Northern Ireland, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment. The defendant is asked whether he or she pleads guilty or not guilty to each individual charge. This process is the same in Australian jurisdictions. In the U.S. District Court, Central District of California, arraignment takes place in two stages. The first is called the initial arraignment and must take place within 48 hours of an individual's arrest. During this arraignment the defendant is informed of any pending legal charges and is informed of his or her right to retain counsel. The presiding judge will also decide whether or not to set bail, and, if so, for how much money. The second arraignment is called a post-indictment arraignment or PIA. It is during this second arraignment that a defendant will be allowed to enter a plea. Guilty and not guilty pleas. If the defendant pleads guilty, an evidentiary hearing usually follows. The court is not required to accept a guilty plea. During the hearing, the judge will assess the offense, mitigating factors, and the defendant's character, and pass sentence. If the defendant pleads not guilty, a date will be set for a preliminary hearing or a trial. In the past, a defendant who refused to plead (or "stood mute") would be subject to peine forte et dure (Law French for "strong and hard punishment"). Today in common law jurisdictions, defendants who refuse to enter a plea will have a plea of not guilty entered for them by the court. The rationale for this is the defendant's right to silence.
This is also often the stage at which arguments in favor or against pre-trial release and bail are made, depending on the alleged crime and jurisdiction. United States Federal Rules of Criminal Procedure. Under the Federal Rules of Criminal Procedure, "arraignment shall...[consist of an] open...reading [of] the indictment...to the defendant...and calling on him to plead thereto. He shall be given a copy of the indictment...before he is called upon to plead."
"America the Beautiful" is an American patriotic song. The lyrics were written by Katharine Lee Bates and the music composed by church organist and choirmaster Samuel A. Ward. Bates originally wrote the words as a poem, "Pikes Peak", first published in the July 4th edition of the church periodical "The Congregationalist" in 1895. The poem was titled "America" for publication. Ward had originally written the music, "Materna", for the 1600s hymn "O Mother dear, Jerusalem" in 1882. Ward's music combined with the Bates poem was first published in 1910 and titled "America the Beautiful". The song is one of the most beloved and popular of the many American patriotic songs. From time to time it has been proposed as a replacement for "The Star-Spangled Banner" as the National Anthem.
In 1893, at the age of thirty-three Katharine Lee Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach a short summer school session at Colorado College. Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the "White City" with its promise of the future contained within its alabaster buildings; the wheat fields of America's heartland Kansas, through which her train was riding on July 4; and the majestic view of the Great Plains from high atop Zebulon's Pikes Peak. On the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel. The poem was initially published two years later in "The Congregationalist," to commemorate the Fourth of July. It quickly caught the public's fancy. Amended versions were published in 1904 and 1913. Several existing pieces of music were adapted to the poem. A hymn tune composed by Samuel A. Ward was generally considered the best music as early as 1910 and is still the popular tune today. Just as Bates had been inspired to write her poem, Ward too was inspired to compose his tune. The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City, after a leisurely summer day in 1882, and he immediately wrote it down. He was so anxious to capture the tune in his head, he asked fellow passenger friend Harry Martin for his shirt cuff to write the tune on, thus perhaps the "off the cuff" analogy. He composed the tune for the old hymn "O Mother Dear, Jerusalem", retitling the work "Materna". Ward's music combined with Bates' poem were first published together in 1910 and titled, "America the Beautiful". Ward died in 1903, not knowing the national stature his music would attain, as the music was only first applied to the song in 1904. Miss Bates was more fortunate, as the song's popularity was well-established by her death in 1929. At various times in the more than 100 years that have elapsed since the song as we know it was born, particularly during the John F. Kennedy administration, there have been efforts to give "America the Beautiful" legal status either as a national hymn, or as a national anthem equal to, or in place of, "The Star-Spangled Banner", but so far this has not succeeded. Proponents prefer "America the Beautiful" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as "The Star-Spangled Banner." Some prefer "America the Beautiful" over "The Star-Spangled Banner" due to the latter's war-oriented imagery. (Others prefer "The Star-Spangled Banner" for the same reason.) While that national dichotomy has stymied any effort at changing the tradition of the national anthem, "America the Beautiful" continues to be held in high esteem by a large number of Americans. Popularity of the song increased greatly following the September 11, 2001 attacks; at some sporting events it was sung in addition to the traditional singing of the national anthem. During the first taping of the "Late Show with David Letterman" following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse. Ray Charles is credited with the song's most well known rendition in current times (although Elvis Presley had good success with it in the 1970s). His recording is very commonly played at major sporting events, such as the Super Bowl; Charles gave a live performance of the song prior to Super Bowl XXXV, the last Super Bowl played before the September 11 terrorist attacks. His unique take on it places the third verse first, after which he sings the usual first verse. In the third verse (see below), the author scolds the materialistic and self-serving robber barons of her day, and urges America to live up to its noble ideals and to honor, with both word and deed, the memory of those who died for their country. Symbolically, Marian Anderson (a noted opera singer of her day) sang a rendition of America on the steps of the Lincoln Memorial in 1939 after being refused use of Constitution Hall by the Daughters of the American Revolution because of her skin color. An all-star version of "America the Beautiful" performed by country music singers Trace Adkins, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Martina McBride, Jamie O'Neal, Kenny Rogers and Keith Urban reached #58 on the "Billboard" Hot Country Singles & Tracks chart in July 2001. The song re-entered the chart following the September 11 terrorist attacks. When Richard Nixon visited the People's Republic of China in 1972, this song was played by Chinese as the welcome music. Interestingly, the Chinese characters for United States literally mean "Beautiful Country." The song is often included in songbooks in a wide variety of religious congregations in the United States.
"From sea to shining sea" is an American idiom meaning from the Pacific Ocean to the Atlantic Ocean (or vice versa). Many songs have used this term, including the American patriotic songs "America, The Beautiful" and "God Bless the USA". In addition to these, it is also featured in Schoolhouse Rock's "Elbow Room". A term similar to this is the Canadian motto "A Mari Usque Ad Mare" ("From sea to sea.")
Assistive technology (AT) is a generic term that includes assistive, adaptive, and rehabilitative devices for people with disabilities and includes the process used in selecting, locating, and using them. The Technology-Related Assistance for Individuals with Disabilities Act of 1988 (US Public Law 100-407) states that it is "technology designed to be utilized in an assistive technology device or assistive technology service." AT promotes greater independence by enabling people to perform tasks that they were formerly unable to accomplish, or had great difficulty accomplishing, by providing enhancements to or changed methods of interacting with the technology needed to accomplish such tasks. Likewise, disability advocates point out that technology is often created without regard to people with disabilities, creating unnecessary barriers to hundreds of millions of people. Assistive technology and universal accessibility. Universal (or broadened) accessibility, or universal design means greater usability, particularly for people with disabilities. Universally accessible technology yields great rewards to the typical user as well; good accessible design "is" universal design. One example is the "curb cuts" (or dropped curbs) in the sidewalk at street crossings. While these curb cuts enable pedestrians with mobility impairments to cross the street, they also aid parents with carriages and strollers, shoppers with carts, and travellers and workers with pull-type bags. As an example, the modern telephone is inaccessible to people who are deaf or hard of hearing. Combined with a text telephone (also known as a TDD Telecommunications device for the deaf and in the USA generally called a TTY[TeleTYpewriter]), which converts typed characters into tones that may be sent over the telephone line, a deaf person is able to communicate immediately at a distance. Together with "relay" services, in which an operator reads what the deaf person types and types what a hearing person says, the deaf person is then given access to everyone's telephone, not just those of people who possess text telephones. Many telephones now have volume controls, which are primarily intended for the benefit of people who are hard of hearing, but can be useful for all users at times and places where there is significant background noise. Some have larger keys well-spaced to facilitate accurate dialing. Also, a person with a mobility impairment can have difficulty using calculators. Speech recognition software recognizes short commands and makes use of calculators easier. People with learning disabilities like dyslexia or dysgraphia are using text-to-speech (TTS) software for reading and spelling programs for assistance in writing texts. Computers with their peripheral devices, editing, spellchecking and speech synthesis software are becoming the core-stones of the assistive technologies coming for relief to the people with learning disabilities and to the people with visual impairments. The assisting spelling programs and voice facilities are bringing better and more convenient text reading and writing experience to the general public. Toys which have been adapted to be used by children with disabilities may have advantages for non-disabled children as well. The Lekotek movement assists parents by lending assistive technology toys and expertise to families. The following professionals may be certified by RESNA (RESNA.org) to serve the assistive technology needs of individuals: occupational therapists, physical therapists, speech language pathologists/audiologists, orthotists and prosthetists, educators, and a variety of other rehabilitation and health professionals.
Personal Emergency Response Systems (PERS), or Telecare (UK term), are a particular sort of assistive technology that use electronic sensors connected to an alarm system to help caregivers manage risk and help vulnerable people stay independent at home longer. An example would be the systems being put in place for senior people such as fall detectors, thermometers (for hypothermia risk), flooding and unlit gas sensors (for people with mild dementia). Notably, these alerts can be customized to the particular person's risks. When the alert is triggered, a message is sent to a carer or contact centre who can respond appropriately. Technology similar to PERS can also be used to act within a person's home rather than just to respond to a detected crisis. Using one of the examples above, gas sensors for people with dementia can be used to trigger a device that turns off the gas and tells someone what has happened. Designing for people with dementia is a good example of how the design of the interface of a piece of AT is critical to its usefulness. People with dementia or any other identified user group must be involved in the design process to make sure that the design is accessible and usable. In the example above, a voice message could be used to remind the person with dementia to turn off the gas himself, but whose voice should be used, and what should the message say? Questions like these must be answered through user consultation, involvement and evaluation.
Sitting at a desk with a QWERTY keyboard and a mouse remains the dominant way of interacting with a personal computer. Some Assistive Technology reduces the strain of this way of work through ergonomic accessories with height-adjustable furniture, footrests, wrist rests, and arm supports to ensure correct posture. Keyguards fit over the keyboard to help prevent unintentional keypresses. More ambitiously, and quite crucially when keyboard or mouse prove unusable, AT can also replace the keyboard and mouse with alternative devices such as the LOMAK keyboard, trackballs, joysticks, graphics tablets, touchpads, touch screens, foot mice, a microphone with speech recognition software, sip-and-puff input, switch access, and vision-based input devices, such as eye trackers which allow the user to control the mouse with their eyes.
Choice of appropriate hardware and software will depend on the user's level of functional vision. Augmentative and Alternative Communication (AAC). Augmentative and alternative communication is a well defined specialty within AT. It involves ways of communication that either enhance or replace verbal language. When combined with Applied Behavior Analysis (ABA) teaching methods, AAC has improved communication skills in children with Autism.
The abacus, also called a counting frame, is a calculating tool used primarily in parts of Asia for performing arithmetic processes. Today, abacuses are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves in sand or on tablets of wood, stone, or metal. The abacus was in use centuries before the adoption of the written modern numeral system and is still widely used by merchants, traders and clerks in Asia, Africa, and elsewhere. The user of an abacus is called an abacist.
The period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system. Some scholars point to a character from the Babylonian cuneiform which may have been derived from a representation of the abacus. It is the belief of Carruccio (and other Old Babylonian scholars) that Babylonians "may have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations".
The use of the abacus in Ancient Egypt is mentioned by the Greek historian Herodotus, who writes that the manner of this disk's usage by the Egyptians was opposite in direction when compared with the Greek method. Archaeologists have found ancient disks of various sizes that are thought to have been used as counters. However, wall depictions of this instrument have not been discovered, casting some doubt over the extent to which this instrument was used.
The earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC. The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations. This Greek abacus saw use in Achaemenid Persia, the Etruscan civilization, Ancient Rome and, until the French Revolution, the Western Christian world. A tablet found on the Greek island Salamis in 1846 AD dates back to 300 BC, making it the oldest counting board discovered so far. It is a slab of white marble long, wide, and thick, on which are 5 groups of markings. In the center of the tablet is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line. Below these lines is a wide space with a horizontal crack dividing it. Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line.
The normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table. Originally pebbles, calculi, were used. Later, and in medieval Europe, jetons were manufactured. Marked lines indicated units, fives, tens etc. as in the Roman numeral system. This system of 'counter casting' continued into the late Roman empire and in medieval Europe, and persisted in limited use into the nineteenth century. Writing in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus. One example of archaeological evidence of the Roman abacus, shown here in reconstruction, dates to the 1st century AD. It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each. The groove marked I indicates units, X tens, and so on up to millions. The beads in the shorter grooves denote fives –five units, five tens etc., essentially in a bi-quinary coded decimal system, obviously related to the Roman numerals. The short grooves on the right may have been used for marking Roman ounces.
The earliest known written documentation of the Chinese abacus dates to the 2nd century BC. The Chinese abacus, known as the "suànpán"(算盤, lit. "Counting tray"), is typically tall and comes in various widths depending on the operator. It usually has more than seven rods. There are two beads on each rod in the upper deck and five beads each in the bottom for both decimal and hexadecimal computation. The beads are usually rounded and made of a hardwood. The beads are counted by moving them up or down towards the beam. If you move them toward the beam, you count their value. If you move away, you don't count their value. The suanpan can be reset to the starting position instantly by a quick jerk along the horizontal axis to spin all the beads away from the horizontal beam at the center. Suanpans can be used for functions other than counting. Unlike the simple counting board used in elementary schools, very efficient suanpan techniques have been developed to do multiplication, division, addition, subtraction, square root and cube root operations at high speed. There are currently schools teaching students how to use it. In the famous long scroll "Along the River During the Qingming Festival" painted by Zhang Zeduan (1085–1145 AD) during the Song Dynasty (960–1297 AD), a suanpan is clearly seen lying beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao). The similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, as there is some evidence of a trade relationship between the Roman Empire and China. However, no direct connection can be demonstrated, and the similarity of the abaci may be coincidental, both ultimately arising from counting with five fingers per hand. Where the Roman model (like most modern Japanese) has 4 plus 1 bead per decimal place, the standard suanpan has 5 plus 2, allowing use with a hexadecimal numeral system. Instead of running on wires as in the Chinese and Japanese models, the beads of Roman model run in grooves, presumably making arithmetic calculations much slower. Another possible source of the suanpan is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a place holder. The zero was probably introduced to the Chinese in the Tang Dynasty (618-907 AD) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.
Some sources mention the use of an abacus called a "nepohualtzintzin" in ancient Mayan culture. This Mesoamerican abacus used a 5-digit base-20 system. The word Nepohualtzintzin comes from the Nahuatl and it is formed by the roots; Ne - personal -; pohual or pohualli - the account -; and tzintzin - small similar elements. And its complete meaning is taken as: counting with small similar elements by somebody. Its use was taught in the "Kalmekak" to the "temalpouhkeh", who were students dedicated to take the accounts of skies, from childhood. Unfortunately the Nepohualtzintzin and its teaching were among the victims of the conquering destruction, when a diabolic origin was attributed to them after observing the tremendous properties of representation, precision and speed of calculations.. This arithmetic tool is based on the vigesimal system (base 20). For the aztec the count by 20s was completely natural, since the use of "huaraches" (native sandals) allowed them to also use the toes for their calculations. In this way, the amount of 20 meant to them a complete human being. The Nepohualtzintzin is divided in two main parts separated by a bar or intermediate cord. In the left part there are four beads, which in the first row have unitary values (1, 2, 3, and 4), and in the right side there are three beads with values of 5, 10, and 15 respectively. In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding account in the first row. Altogether, there are 13 rows with 7 beads in each one, which makes up 91 beads in each Nepohualtzintzin. This is a basic number to understand the close relation conceived between the exact accounts and the natural phenomena. This is so that one Nepohualtzintzin (91) represents the number of days that a season of the year lasts, two Nepohualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepohualtzintzin (273) is the number of days of a baby's gestation, and four Nepohualtzintzin (364) complete a cycle and approximate a year (1 1/4 days short). It is worth to mention that in the Nepohualtzintzin, amounts in the rank from 10 to the 18 can be calculated, with floating point, which allows calculating stellar as well as infinitesimal amounts with absolute precision. The rediscovering of the Nepohualtzintzin is due to the teacher David Esparza Hidalgo, who in his wandering by all Mexico has found diverse engravings and paintings of this instrument and has reconstructed several of them made in gold, jade, incrustations of shell, etc. There have also been found very old Nepohualtzintzin attributed to the Olmeca culture, and even some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures. The quipu of the Incas was a system of knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations. Calculations were carried out using a yupana (quechua for "counting tool"; see figure) which was still in use after the conquest of Peru. The working principle of a yupana is unknown, but in 2001 an explanation of the mathematical basis of these instruments was proposed. By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20 and 40 as place values for the different fields in the instrument. Using the Fibonacci sequence would keep the number of grains within any one field at minimum.
The Russian abacus, the "schety" (счёты), usually has a single slanted deck, with ten beads on each wire (except one wire which has four beads, for quarter-ruble fractions. This wire is usually near the user). (Older models have another 4-bead wire for quarter-kopeks, which were minted until 1916.) The Russian abacus is often used vertically, with wires from left to right in the manner of a book. The wires are usually bowed to bulge upward in the center, to keep the beads pinned to either of the two sides. It is cleared when all the beads are moved to the right. During manipulation, beads are moved to the left. For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different colour from the other eight beads. Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color. As a simple, cheap and reliable device, the Russian abacus was in use in all shops and markets throughout the former Soviet Union, and the usage of it was taught in most schools until the 1990s. Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia and likewise the mass production of Felix arithmometers since 1924 did not significantly reduce their use in the Soviet Union. Russian abacus began to lose popularity only after the mass production of microcalculators had started in the Soviet Union in 1974. On Today it is regarded as an archaism and replaced by microcalculator. The Russian abacus was brought to France around 1820 by the mathematician Jean-Victor Poncelet, who served in Napoleon's army and had been a prisoner of war in Russia. The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods. To Poncelet's French contemporaries, it was something new. Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid.
Around the world, abaci have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic. In Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame has been common (see image). It is still often seen as a plastic or wooden toy. The type of abacus shown here is often used to represent numbers without the use of place value. Each bead and each wire has the same value and used in this way it can represent numbers up to 100.
An adapted abacus, invented by Tim Cranmer, called a Cranmer abacus is still commonly used by individuals who are blind. A piece of soft fabric or rubber is placed behind the beads so that they do not move inadvertently. This keeps the beads in place while the users feel or manipulate them. They use an abacus to perform the mathematical functions multiplication, division, addition, subtraction, square root and cubic root. Although blind students have benefited from talking calculators, the abacus is still very often taught to these students in early grades, both in public schools and state schools for the blind. The abacus teaches mathematical skills that can never be replaced with talking calculators and is an important learning tool for blind students. Blind students also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems can be long and difficult. The abacus gives blind and visually impaired students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper. Many blind people find this number machine a very useful tool throughout life.
An abacus that explains how computers manipulate numbers. The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII. The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an 'on' or 'off' position. In 1985, Dr. Robert C. Good, Jr. of the Widener University School of Engineering published on the binary abacus.
An acid (...from the Latin "acidus" meaning "sour") is traditionally considered any chemical compound that, when dissolved in water, gives a solution with a hydrogen ion activity greater than in pure water, i.e. a pH less than 7.0 in its standard state. That approximates the modern definition of Johannes Nicolaus Brønsted and Martin Lowry, who independently defined an acid as a compound which donates a hydrogen ion (H+) to another compound (called a base). Common examples include acetic acid (in vinegar) and sulfuric acid (used in car batteries). Acid/base systems are different from redox reactions in that there is no change in oxidation state. Acids can occur in solid, liquid or gaseous form, depending on the temperature. They can exist as pure substances or in solution. Chemicals or substances having the property of an acid are said to be acidic.
In pure water the majority of molecules exist as H2O, but a small number of molecules are constantly dissociating and re-associating. Pure water is neutral with respect to acidity or basicity because the concentration of hydroxide ions is always equal to the concentration of hydronium ions. An Arrhenius base is a molecule which increases the concentration of the hydroxide ion when dissolved in water. Note that chemists often write H+("aq") and refer to the hydrogen ion when describing acid-base reactions but the free hydrogen nucleus, a proton, does not exist alone in water, it exists as the hydronium ion, H3O+.
As with the acetic acid reactions, both definitions work for the first example, where water is the solvent and hydronium ion is formed. The next two reactions do not involve the formation of ions but can still be viewed as proton transfer reactions. In the second reaction hydrogen chloride and ammonia (dissolved in benzene) react to form solid ammonium chloride in a benzene solvent and in the third gaseous HCl and NH3 combine to form the solid.
A third concept was proposed by Gilbert N. Lewis which includes reactions with acid-base characteristics that do not involve a proton transfer. A Lewis acid is a species that accepts a pair of electrons from another species; in other words, it is an electron pair acceptor. Brønsted acid-base reactions are proton transfer reactions while Lewis acid-base reactions are electron pair transfers. All Brønsted acids are also Lewis acids, but not all Lewis acids are Brønsted acids. Contrast the following reactions which could be described in terms of acid-base chemistry. In the first reaction a fluoride ion, F-, gives up an electron pair to boron trifluoride to form the product tetrafluoroborate. Fluoride "loses" a pair of valence electrons because the electrons shared in the B—F bond are located in the region of space between the two atomic nuclei and are therefore more distant from the fluoride nucleus than they are in the lone fluoride ion. BF3 is a Lewis acid because it accepts the electron pair from fluoride. This reaction cannot be described in terms of Brønsted theory because there is no proton transfer. The second reaction can be described using either theory. A proton is transferred from an unspecified Brønsted acid to ammonia, a Brønsted base; alternatively, ammonia acts as a Lewis base and transfers a lone pair of electrons to form a bond with a hydrogen ion. The species that gains the electron pair is the Lewis acid; for example, the oxygen atom in H3O+ gains a pair of electrons when one of the H—O bonds is broken and the electrons shared in the bond become localized on oxygen. Depending on the context, a Lewis acid may also be described as an oxidizer or an electrophile. The Brønsted-Lowry definition is the most widely used definition; unless otherwise specified acid-base reactions are assumed to involve the transfer of a proton (H+) from an acid to a base.
Reactions of acids are often generalized in the form HA H+ + A-, where HA represents the acid and A- is the conjugate base. Acid-base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively). Note that the acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as HA+ H+ + A. In solution there exists an equilibrium between the acid and its conjugate base. The equilibrium constant "K" is an expression of the equilibrium concentrations of the molecules or the ions in solution. Brackets indicate concentration, such that [H2O] means "the concentration of H2O". The acid dissociation constant "K"a is generally used in the context of acid-base reactions. The numerical value of "K"a is equal to the concentration of the products divided by the concentration of the reactants, where the reactant is the acid (HA) and the products are the conjugate base and H+. The stronger of two acids will have a higher "K"a than the weaker acid; the ratio of hydrogen ions to acid will be higher for the stronger acid as the stronger acid has a greater tendency to lose its proton. Because the range of possible values for "K"a spans many orders of magnitude, a more manageable constant, p"K"a is more frequently used, where p"K"a = -log10 "K"a. Stronger acids have a smaller p"K"a than weaker acids. Experimentally determined p"K"a at 25°C in aqueous solution are often quoted in textbooks and reference material.
In the classical naming system, acids are named according to their anions. That ionic suffix is dropped and replaced with a new suffix (and sometimes prefix), according to the table below. For example, HCl has chloride as its anion, so the -ide suffix makes it take the form hydrochloric acid. In the IUPAC naming system, "aqueous" is simply added to the name of the ionic compound. Thus, for hydrogen chloride, the IUPAC name would be aqueous hydrogen chloride. The prefix "hydro-" is added only if the acid is made up of just hydrogen and one other element.
The strength of an acid refers to its ability or tendency to lose a proton. A strong acid is one that completely dissociates in water; in other words, one mole of a strong acid HA dissolves in water yielding one mole of H+ and one mole of the conjugate base, A-, and none of the protonated acid HA. In contrast a weak acid only partially dissociates and at equilibrium both the acid and the conjugate base are in solution. Examples of strong acids are hydrochloric acid (HCl), hydroiodic acid (HI), hydrobromic acid (HBr), perchloric acid (HClO4), nitric acid (HNO3) and sulfuric acid (H2SO4). In water each of these essentially ionizes 100%. The stronger an acid is, the more easily it loses a proton, H+. Two key factors that contribute to the ease of deprotonation are the polarity of the H—A bond and the size of atom A, which determines the strength of the H—A bond. Acid strengths are also often discussed in terms of the stability of the conjugate base. Stronger acids have a higher "K"a and a lower p"K"a than weaker acids. Sulfonic acids, which are organic oxyacids, are a class of strong acids. A common example is toluenesulfonic acid (tosylic acid). Unlike sulfuric acid itself, sulfonic acids can be solids. In fact, polystyrene functionalized into polystyrene sulfonate is a solid strongly acidic plastic that is filterable. Superacids are acids stronger than 100% sulfuric acid. Examples of superacids are fluoroantimonic acid, magic acid and perchloric acid. Superacids can permanently protonate water to give ionic, crystalline hydronium "salts". They can also quantitatively stabilize carbocations. Polarity and the inductive effect. The electronegative element need not be directly bonded to the acidic hydrogen to increase its acidity. An electronegative atom can pull electron density out of an acidic bond through the inductive effect. The electron-withdrawing ability diminishes quickly as the electronegative atom moves away from the acidic bond. The effect is illustrated by the following series of halogenated butanoic acids. Chlorine is more electronegative than bromine and therefore has a stronger effect. The hydrogen atom bonded to the oxygen is the acidic hydrogen. Butanoic acid is a carboxylic acid. As the chlorine atom moves further away from the acidic O—H bond, its effect diminishes. When the chlorine atom is just one carbon removed from the carboxylic acid group the acidity of the compound increases significantly, compared to butanoic acid (a.k.a. butyric acid). However, when the chlorine atom is separated by several bonds the effect is much smaller. Bromine is much more electronegative than either carbon or hydrogen, but not as electronegative as chlorine, so the p"K"a of 2-bromobutanoic acid is slightly greater than the p"K"a of 2-chlorobutanoic acid. The number of electronegative atoms adjacent an acidic bond also affects acid strength. Oxoacids have the general formula HOX where X can be any atom and may or may not share bonds to other atoms. Increasing the number of electronegative atoms or groups on atom X decreases the electron density in the acidic bond, making the loss of the proton easier. Perchloric acid is a very strong acid (p"K"a ≈ -8) and completely dissociates in water. Its chemical formula is HClO4 and it comprises a central chlorine atom with three chlorine-oxygen double bonds (Cl=O) and one chlorine-oxygen single bond (Cl—O). The singly bonded oxygen bears an extremely acidic hydrogen atom which is easily abstracted. In contrast, chloric acid (HClO3) is a weaker acid, though still quite strong (p"K"a = -1.0), while chlorous acid (HClO2, p"K"a = +2.0) and hypochlorous acid (HClO, p"K"a = +7.53) acids are weak acids. Carboxylic acids are organic acids that contain an acidic hydroxyl group and a carbonyl (C=O bond). Carboxylic acids can be reduced to the corresponding alcohol; the replacement of an electronegative oxygen atom with two electropositive hydrogens yields a product which is essentially non-acidic. The reduction of acetic acid to ethanol using LiAlH4 (lithium aluminium hydride or LAH) and ether is an example of such a reaction. The p"K"a for ethanol is 16, compared to 4.76 for acetic acid. Atomic radius and bond strength. Another factor that contributes to the ability of an acid to lose a proton is the strength of the bond between the acidic hydrogen and the atom that bears it. This, in turn, is dependent on the size of the atoms sharing the bond. For an acid HA, as the size of atom A increases, the strength of the bond decreases, meaning that it is more easily broken, and the strength of the acid increases. Bond strength is a measure of how much energy it takes to break a bond. In other words, it takes less energy to break the bond as atom A grows larger, and the proton is more easily removed by a base. This partially explains why hydrofluoric acid is considered a weak acid while the other hydrohalic acids (HCl, HBr, HI) are strong acids. Although fluorine is more electronegative than the other halogens, its atomic radius is also much smaller, so it shares a stronger bond with hydrogen. Moving down a column on the periodic table atoms become less electronegative but also significantly larger, and the size of the atom tends to dominate its acidity when sharing a bond to hydrogen. Hydrogen sulfide, H2S, is a stronger acid than water, even though oxygen is more electronegative than sulfur. Just as with the halogens, this is because sulfur is larger than oxygen and the H—S bond is more easily broken than the H—O bond.
Polyprotic acids are able to donate more than one proton per acid molecule, in contrast to monoprotic acids that only donate one proton per molecule. Specific types of polyprotic acids have more specific names, such as diprotic acid (two potential protons to donate) and triprotic acid (three potential protons to donate). A diprotic acid (here symbolized by H2A) can undergo one or two dissociations depending on the pH. Each dissociation has its own dissociation constant, Ka1 and Ka2. The first dissociation constant is typically greater than the second; i.e., "K"a1 > "K"a2. For example, sulfuric acid (H2SO4) can donate one proton to form the bisulfate anion (HSO4-), for which "K"a1 is very large; then it can donate a second proton to form the sulfate anion (SO42-), wherein the "K"a2 is intermediate strength. The large "K"a1 for the first dissociation makes sulfuric a strong acid. In a similar manner, the weak unstable carbonic acid (H2CO3) can lose one proton to form bicarbonate anion (HCO3-) and lose a second to form carbonate anion (CO32-). Both "K"a values are small, but "K"a1 > "K"a2. A triprotic acid (H3A) can undergo one, two, or three dissociations and has three dissociation constants, where "K"a1 > "K"a2 > "K"a3. An inorganic example of a triprotic acid is orthophosphoric acid (H3PO4), usually just called phosphoric acid. All three protons can be successively lost to yield H2PO4-, then HPO42-, and finally PO43-, the orthophosphate ion, usually just called phosphate. An organic example of a triprotic acid is citric acid, which can successively lose three protons to finally form the citrate ion. Even though the positions of the protons on the original molecule may be equivalent, the successive "K"a values will differ since it is energetically less favorable to lose a proton if the conjugate base is more negatively charged.
Neutralization is the basis of titration, where a pH indicator shows equivalence point when the equivalent number of moles of a base have been added to an acid. It is often wrongly assumed that neutralization should result in a solution with pH 7.0, which is only the case with similar acid and base strengths during a reaction. Neutralization with a base weaker than the acid results in a weakly acidic salt. An example is the weakly acidic ammonium chloride, which is produced from the strong acid hydrogen chloride and the weak base ammonia. Conversely, neutralizing a weak acid with a strong base gives a weakly basic salt, e.g. sodium fluoride from hydrogen fluoride and sodium hydroxide.
In order to lose a proton, it is necessary that the pH of the system rise above the p"K"a of the protonated acid. The decreased concentration of H+ in that basic solution shifts the equilibrium towards the conjugate base form (the deprotonated form of the acid). In lower-pH (more acidic) solutions, there is a high enough H+ concentration in the solution to cause the acid to remain in its protonated form, or to protonate its conjugate base (the deprotonated form). Solutions of weak acids and salts of their conjugate bases form buffer solutions.
There are numerous uses for acids. Acids are often used to remove rust and other corrosion from metals in a process known as pickling. They may be used as an electrolyte in a wet cell battery, such as sulfuric acid in a car battery. Strong acids, sulfuric acid in particular, are widely used in mineral processing. For example, phosphate minerals react with sulfuric acid to produce phosphoric acid for the production of phosphate fertilizers, and zinc is produced by dissolving zinc oxide into sulfuric acid, purifying the solution and electrowinning. In the chemical industry, acids react in neutralization reactions to produce salts. For example, nitric acid reacts with ammonia to produce ammonium nitrate, a fertilizer. Additionally, carboxylic acids can be esterified with alcohols, to produce esters. Acids are used as catalysts; for example, sulfuric acid is used in very large quantities in the alkylation process to produce gasoline. Strong acids, such as sulfuric, phosphoric and hydrochloric acids also effect dehydration and condensation reactions. Acids are used as additives to drinks and foods, as they alter their taste and serve as preservatives. Phosphoric acid, for example, is a component of cola drinks.
Many biologically important molecules are acids. Nucleic acids, including DNA and RNA contain the genetic code that determines much of an organism's characteristics, and is passed from parents to offspring. DNA contains the chemical blueprint for the synthesis of proteins which are made up of amino acid subunits. Cell membranes contain fatty acid esters such as phospholipids. An α-amino acid has a central carbon (the α or "alpha" carbon) which is covalently bonded to a carboxyl group (thus they are carboxylic acids), an amino group, a hydrogen atom and a variable group. The variable group, also called the R group or side chain, determines the identity and many of the properties of the a specific amino acid. In glycine, the simplest amino acid, the R group is a hydrogen atom, but in all other amino acids it is contains one or more carbon atoms bonded to hydrogens, and may contain other elements such as sulfur, oxygen or nitrogen. With the exception of glycine, naturally occurring amino acids are chiral and almost invariably occur in the L-configuration. Peptidoglycan, found in some bacterial cell walls contains some D-amino acids. At physiologic pH, typically around 7, free fatty acids exist in a charged form, where the acidic carboxyl group (-COOH) loses a proton (-COO-) and the basic amine group (-NH2) gains a proton (-NH3+). The entire molecule has a net neutral charge and is a zwitterion. Fatty acids and fatty acid derivatives are another group of carboxylic acids that play a significant role in biology. These contain long hydrocarbon chains and a carboxylic acid group on one end. The cell membrane of nearly all organisms is primarily made up of a phospholipid bilayer, a micelle of hydrophobic fatty acid esters with polar, hydrophilic phosphate "head" groups. Membranes contain additional components, some of which can participate in acid-base reactions. In humans and many other animals, hydrochloric acid is a part of the gastric acid secreted within the stomach to help hydrolyze proteins and polysaccharides, as well as converting the inactive pro-enzyme, pepsinogen into the enzyme, pepsin. Some organisms produce acids for defense; for example, ants produce formic acid. Acid-base equilibrium plays a critical role in regulating mammalian breathing. Oxygen gas (O2) drives cellular respiration, the process by which animals release the chemical potential energy stored in food, producing carbon dioxide (CO2) as a byproduct. Oxygen and carbon dioxide are exchanged in the lungs, and the body responds to changing energy demands by adjusting the rate of ventilation. For example, during periods of exertion the body rapidly breaks down stored carbohydrates and fat, releasing CO2 into the blood stream. In aqueous solutions such as blood CO2 exists in equilibrium with carbonic acid and bicarbonate ion. It is the decrease in pH that signals the brain to breath faster and deeper, expelling the excess CO2 and resupplying the cells with O2. Cell membranes are generally impermeable to charged or large, polar molecules because of the lipophilic fatty acyl chains comprising their interior. Many biologically important molecules, including a number of pharmaceutical agents, are organic weak acids which can cross the membrane in their protonated, uncharged form but not in their charged form (i.e. as the conjugate base). For this reason the activity of many drugs can be enhanced or inhibited by the use of antacids or acidic foods. The charged form, however, is often more soluble in blood and cytosol, both aqueous environments. When the extracellular environment is more acidic than the neutral pH within the cell, certain acids will exist in their neutral form and will be membrane soluble, allowing them to cross the phospholipid bilayer. Acids that lose a proton at the intracellular pH will exist in their soluble, charged form and are thus able to diffuse through the cytosol to their target. Ibuprofen, aspirin and penicillin are examples of drugs that are weak acids.
Asphalt () is a sticky, black and highly viscous liquid or semi-solid that is present in most crude petroleums and in some natural deposits sometimes termed asphaltum. It is most commonly modelled as a colloid, with "asphaltenes" as the dispersed phase and ' as the continuous phase (though there is some disagreement amongst chemists regarding its structure). One writer states that although a "considerable amount of work has been done on the composition of asphalt, it is exceedingly difficult to separate individual hydrocarbon in pure form", and "it is almost impossible to separate and identify all the different molecules of asphalt, because the number of molecules with different chemical structure is extremely large". In U.S. and Polish terminology, asphalt (or asphalt cement) is the carefully refined residue from the distillation process of selected crude oils. Outside these countries, the product is often called bitumen. The primary use of asphalt is in road construction, where it is used as the glue or binder for the aggregate particles. The road surfacing material is usually called 'asphaltic concrete', AC in North America, or 'asphalt' elsewhere. Within North America the apparent interchangeability of the words asphalt and 'bitumen' causes confusion outside the road construction industry despite quite clear definitions within industry circles.
The word asphalt is derived from the late Middle English: from French asphalte, based on Late Latin asphalton, asphaltum, from the Greek ásphalton, ásphaltos ("άσφαλτος"), a word of uncertain origin meaning "asphalt/bitumen/pitch" which some derive from α- "without" and σφάλλω "to make fall". Note that in French, the term asphalte is used for naturally-occurring bitumen-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the "asphaltic concrete" used to pave roads. Another description has it that the term derives from the Accadian term "asphaltu" or "sphallo," meaning "to split." It was later adopted from the Homeric Greeks as a verb meaning "to make firm or stable," "to secure". It is a significant fact that the first use of asphalt by the ancients was in the nature of a cement for securing or joining together various objects, and it thus seems likely that the name itself was expressive of this application. From the Greek, the word passed into late Latin, and thence into French ("asphalte") and English ("asphalt"). The expression "bitumen" originated in the Sanskrit, where we find the words "jatu," meaning "pitch," and "jatu-krit," meaning "pitch creating," "pitch producing" (referring to coniferous or resinous trees). The Latin equivalent is claimed by some to be originally 'gwitu-men' (pertaining to pitch), and by others, "pixtumens" (exuding or bubbling pitch), which was subsequently shortened to "bitumen," thence passing via French into English. From the same root is derived the Anglo Saxon word "cwidu" (Mastix), the German word "Kitt" (cement or mastic) and the old Norse word "kvada".
Asphalt or bitumen can sometimes be confused with tar, which is a similar black thermo-plastic material produced by the destructive distillation of coal. During the early- and mid-twentieth century when town gas was produced, tar was a readily available product and extensively used as the binder for road aggregates. The addition of tar to macadam roads led to the word tarmac, which is now used in common parlance to refer to road making materials. However, since the 1970s, when natural gas succeeded town gas, asphalt (bitumen) has completely overtaken the use of tar in these applications. Asphalt can be separated from the other components in crude oil (such as naphtha, gasoline and diesel) by the process of fractional distillation, usually under vacuum conditions. A better separation can be achieved by further processing of the heavier fractions of the crude oil in a de-asphalting unit, which uses either propane or butane in a supercritical phase to dissolve the lighter molecules which are then separated. Further processing is possible by "blowing" the product: namely reacting it with oxygen. This makes the product harder and more viscous. Natural deposits of asphalt include lake asphalts (primarily from the Pitch Lake in Trinidad and Tobago and Bermudez Lake in Venezuela), Gilsonite, the Dead Sea, and Tar Sands. Asphalt was mined at Ritchie Mines in Macfarlan in Ritchie County, West Virginia in the United States from 1852 to 1873. Asphalt is typically stored and transported at temperatures around 150 degrees Celsius (300 °F). Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture. This mixture is often called bitumen feedstock, or BFS. Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm. The backs of tippers carrying asphalt, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release. Diesel oil is sometimes used as a release agent, although it can mix with and thereby reduce the quality of the asphalt.
In the ancient Middle East, natural asphalt deposits were used for mortar between bricks and stones, to cement parts of carvings such as eyes into place, for ship caulking, and for waterproofing. The Persian word for asphalt is "mumiya", which is related to the English word mummy. Asphalt was also used by ancient Egyptians to embalm mummies. In the ancient Far East, natural asphalt was slowly boiled to get rid of the higher fractions, leaving a material of higher molecular weight which is thermoplastic and when layered on objects, became quite hard upon cooling. This was used to cover objects that needed waterproofing, such as scabbards and other items. Statuettes of household deities were also cast with this type of material in Japan, and probably also in China. In North America, archaeological recovery has indicated that asphaltum was sometimes used to apply stone projectile points to a wooden shaft.
The use of asphalt in the United Kingdom and United States was preceded by its use in Europe. An 1838 edition of "Mechanics Magazine" cites an early use of asphalt in France. A pamphlet dated 1621, by "a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel", and that he proposed to use it in a variety of ways - "principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusin of dirt and filth", which at that time made the water unusable. "He expatiates also on the excellence of this material for forming level and durable terraces" in palaces, "the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation". But it was generally neglected in France until the revolution of 1830. Then, in the 1830s, there was a surge of interest, and asphalt became widely used "for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes". Its rise in Europe was "a sudden phenomenon", after natural deposits were found "in France at Osbann (BasRhin), the Parc (l'Ain) and the Puy-de-la-Poix (Puy-de-Dome)", although it could also be made artificially. Early use in the United Kingdom. William Salmon's "Polygraphice" (1673) provides a recipe for varnish used in etching, consisting of three ounces of virgin wax, two ounces of mastic, and one ounce of asphaltum. In Britain, the first patent was 'Cassell's patent asphalte or bitumen' in 1834. Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain. Dr T. Lamb Phipson claims that his father, Samuel Ryland Phipson, a friend of Claridge, was also "instrumental in introducing the asphalte pavement (in 1836)". In 1838, Claridge obtained patents in Scotland on 27 March, and Ireland on 23 April, and in 1851 he sought to extend the duration of all three patents. He formed "Claridge's Patent Asphalte Company" for the purpose of introducing to Britain "Asphalte in its natural state from the mine at Pyrimont Seysell in France", and "laid one of the first asphalt pavements in Whitehall". Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, "and subsequently on the space at the bottom of the steps leading from Waterloo Place to St. James Park". "The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry". "By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving "continue(d) in good order". Indeed in 1838, there was a flurry of entrepreneurial activity over asphalt. On the London stockmarket, there were various claims as to the priority of asphalt quality from France, Germany and England. And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other. In England, "Claridge's was the type most used in the 1840s and 50s" Claridge's own company ceased operating in 1917. Early use in the United States. The first use of asphaltum in the New World was by indigenous Indian tribes. On the west coast, as early as the 1200s, the Tongva and Chumash Nations collected the naturally occurring asphaltum that seeped to the surface above underlying petroleum deposits. Both tribes used the substance as an adhesive. It is found on many different artifacts of tools and ceremonial items. For example, it was used on rattles to adhere gourds or turtle shells to rattle handles. It was also used in decorations. Small round shell beads were often set in asphatum to provide decorations. It was used as a sealant on baskets to make them water tight for carrying water. Asphaltum was used also to seal the planks on ocean-going canoes. Roads in the US have been paved with asphalt since at least 1870, when a street in front of Newark, NJ's City Hall was paved. In 1876, asphalt was used to pave Pennsylvania Avenue in Washington, DC, in time for the celebration of the national centennial. Asphalt was also used for flooring, paving and waterproofing of baths and swimming pools during the early 1900s, following similar trends in Europe.
The largest use of asphalt is for making asphalt concrete for road surfaces and accounts for approximately 85% of the asphalt consumed in the United States. Asphalt pavement material is commonly composed of 5 percent asphalt cement and 95 percent aggregates (stone, sand, and gravel). Due to its highly viscous nature, asphalt cement must be heated so that it can be mixed with the aggregates at the asphalt mixing plant. There are about 4,000 asphalt mixing plants in the U.S. Asphalt road surface is the most widely recycled material in the US, both by gross tonnage and by percentage. According to a report issued by the Federal Highway Administration and the United States Environmental Protection Agency, 80% of the asphalt from road surfaces' that is removed each year during widening and resurfacing projects is reused as part of new roads, roadbeds, shoulders and embankments. Roofing shingles account for most of the remaining asphalt consumption. Other uses include cattle sprays, fence post treatments, and waterproofing for fabrics. Asphalt is widely used in airports around the world. Due to the sturdiness, it is widely used for runways dedicated to aircraft landing and taking off.
Mastic asphalt is a type of asphalt which differs from dense graded asphalt (asphalt concrete) in that it has a higher bitumen (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt, which has only around 5% added bitumen. This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground. Mastic asphalt is heated to a temperature of and is spread in layers to form a impervious barrier about thick. There is a proper apprenticeship and trainees go to college to learn this trade.
A number of technologies allow asphalt to be mixed at much lower temperatures. These involve mixing the asphalt with petroleum solvents to form "cutbacks" with reduced melting point or mixtures with water to turn the asphalt into an emulsion. Asphalt emulsions contain up to 70% asphalt and typically less than 1.5% chemical additives. There are two main types of emulsions with different affinity for aggregates, cationic and anionic. Asphalt emulsions are used in a wide variety of applications. Chipseal involves spraying the road surface with asphalt emulsion followed by a layer of crushed rock or gravel. Slurry Seal involves the creation of a mixture of asphalt emulsion and fine crushed aggregate that is spread on the surface of a road. Cold mixed asphalt can also be made from asphalt emulsion to create pavements similar to hot-mixed asphalt, several inches in depth and asphalt emulsions are also blended into recycled hot-mix asphalt to create low cost pavements.
The American National Standards Institute or ANSI () is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States. The organization also coordinates U.S. standards with international standards so that American products can be used worldwide. For example, standards make sure that people who own cameras can find the film they need for that camera anywhere around the globe. ANSI accredits standards that are developed by representatives of standards developing organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards. The organization's headquarters are in Washington, DC. ANSI's operations office is located in New York City.
ANSI was originally formed in 1918, when five engineering societies and three government agencies founded the American Engineering Standards Committee (AESC). In 1928, the AESC became the American Standards Association (ASA). In 1966, the ASA was reorganized and became the United States of America Standards Institute (USASI). The present name was adopted in 1969. Prior to 1918, these five engineering societies, the American Institute of Electrical Engineers (AIEE, now IEEE), American Society of Mechanical Engineers (ASME), American Society of Civil Engineers (ASCE), the American Institute of Mining and Metallurgical Engineers (now AIME), and the American Society for Testing Materials (now ASTM International), had been members of the United Engineering Society (UES). At the behest of the AIEE, they invited the U.S. government Departments of War, Navy and Commerce to join in founding a national standards organization. According to Paul G. Agnew, the first permanent secretary and head of staff in 1919, AESC started as an ambitious program and little else. Staff for the first year consisted of one executive, Clifford B. LePage, who was on loan from a founding member, ASME. An annual budget of $7,500 was provided by the founding bodies. In 1931, the organization (renamed ASA in 1928) became affiliated with the U.S. National Committee of the International Electrotechnical Commission (IEC), which had been formed in 1904 to develop electrical and electronics standards.http://www.iec.ch/
Though ANSI itself does not develop standards, the Institute oversees the development and use of standards by accrediting the procedures of standards developing organizations. ANSI accreditation signifies that the procedures used by standards developing organizations meet the Institute's requirements for openness, balance, consensus, and due process. ANSI also designates specific standards as American National Standards, or ANS, when the Institute determines that the standards were developed in an environment that is equitable, accessible and responsive to the requirements of various stakeholders. Voluntary consensus standards quicken the market acceptance of products while making clear how to improve the safety of those products for the protection of consumers. There are approximately 9,500 American National Standards that carry the ANSI designation.
In addition to facilitating the formation of standards in the U.S., ANSI promotes the use of U.S. standards internationally, advocates U.S. policy and technical positions in international and regional standards organizations, and encourages the adoption of international standards as national standards where appropriate. The Institute is the official U.S. representative to the two major international standards organizations, the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC), via the U.S. National Committee (USNC). ANSI participates in almost the entire technical program of both the ISO and the IEC, and administers many key committees and subgroups. In many instances, U.S. standards are taken forward to ISO and IEC, through ANSI or the USNC, where they are adopted in whole or in part as international standards.
The Apollo 11 mission landed the first humans on the Moon. Launched on July 16, 1969, the third lunar mission of NASA's Apollo Program was crewed by Commander Neil Alden Armstrong, Command Module Pilot Michael Collins, and Lunar Module Pilot Edwin Eugene 'Buzz' Aldrin, Jr. On July 20, Armstrong and Aldrin became the first humans to walk on the Moon, while Collins orbited in the Command Module. The mission fulfilled President John F. Kennedy's goal of reaching the moon by the end of the 1960s, which he had expressed during a speech given before a joint session of Congress on May 25, 1961: "I believe that this nation should commit itself to achieving the goal, before this decade is out, of landing a man on the Moon and returning him safely to the Earth."
Each crewmember of Apollo 11 had made a spaceflight before this mission, making it the third all-veteran crew in manned spaceflight history. Collins was originally slated to be the Command Module Pilot (CMP) on Apollo 8 but was removed when he required surgery on his back and was replaced by Jim Lovell, his backup for that flight. After Collins was medically cleared, he took what would have been Lovell's spot on Apollo 11; as a veteran of Apollo 8, Lovell was transferred to Apollo 11's backup crew, but promoted to backup commander.
In early 1969 Bill Anders accepted a job with the National Space Council effective in August 1969 and announced his retirement as an astronaut. At that point Ken Mattingly was moved from the support crew into parallel training with Anders as backup Command Module Pilot in case Apollo 11 was delayed past its intended July launch (at which point Anders would be unavailable if needed) and would later join Lovell's crew and ultimately be assigned as the original Apollo 13 CMP.
The lunar module was named "Eagle" for the national bird of the United States, the bald eagle, and featured prominently on the mission insignia. The command module was named "Columbia" for the feminine personification of the United States used traditionally in song and poetry. During early mission planning, the names "Snowcone" and "Haystack" were used but changed before announcement to the press. Launch and lunar orbit injection. In addition to throngs of people crowding highways and beaches near the launch site, millions watched the event on television, with NASA Chief of Public Information Jack King providing commentary. President Richard Nixon viewed the proceedings from the Oval Office of the White House. A Saturn V launched "Apollo 11" from Launch Pad 39A, part of the Launch Complex 39 site at the Kennedy Space Center on July 16, 1969 at 13:32:00 UTC (9:32:00 a.m. local time). It entered orbit 12 minutes later. After one and a half orbits, the S-IVB third-stage engine pushed the spacecraft onto its trajectory toward the Moon with the Trans Lunar Injection burn at 16:22:13 UTC. About 30 minutes later the service module pair separated from this last remaining Saturn V stage and docked with the lunar module still nestled in the Lunar Module Adaptor. After the lunar module was extracted, the combined spacecraft headed for the Moon, while the third stage booster flew on a trajectory past the moon and into solar orbit. On July 19 at 17:21:50 UTC, "Apollo 11" passed behind the Moon and fired its service propulsion engine to enter lunar orbit. In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquility (Mare Tranquillitatis) about 20 kilometers (12 mi) southwest of the crater Sabine D (0.67408N, 23.47297E). The landing site was selected in part because it had been characterized as relatively flat and smooth by the automated "Ranger 8" and "Surveyor 5" landers along with the "Lunar Orbiter" mapping spacecraft and unlikely to present major landing or extra-vehicular activity (EVA) challenges.
On July 20, 1969 the lunar module (LM) "Eagle" separated from the command module "Columbia". Collins, alone aboard "Columbia", inspected "Eagle" as it pirouetted before him to ensure the craft was not damaged. As the descent began, Armstrong and Aldrin found that they were passing landmarks on the surface 4 seconds early and reported that they were "long": they would land miles west of their target point. Five minutes into the descent burn, and 6000 feet above the surface of the moon, the LM navigation and guidance computer distracted the crew with the first of several unexpected "1202" and "1201" program alarms. Inside Mission Control Center in Houston, Texas, computer engineer Jack Garman told guidance officer Steve Bales it was safe to continue the descent and this was relayed to the crew. The program alarms indicated "executive overflows", where the guidance computer could not complete all of its tasks in real time and had to postpone some of them. This was neither a computer error nor an astronaut error, but stemmed from a mistake in how the astronauts had been trained. Although unneeded for the landing, the rendezvous radar was intentionally turned on to make ready for a fast abort. Ground simulation setups had not foreseen that a fast stream of spurious interrupts from this radar could happen, depending upon how the hardware randomly powered up before the LM then began nearing the lunar surface: hence the computer had to deal with data from two radars, not the landing radar alone, which led to the overload. When Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a 300 meter diameter crater (later determined to be "West crater", named for its location in the western part of the originally planned landing ellipse). Armstrong took semi-automatic control and, with Aldrin calling out altitude and velocity data, landed at 20:17 UTC on July 20 with about 25 seconds of fuel left. "Apollo 11" landed with less fuel than other missions, and the astronauts also encountered a premature low fuel warning. This was later found to have been due to greater propellant 'slosh' than expected uncovering a fuel sensor. On subsequent missions, extra baffles were added to the tanks to prevent this. Throughout the descent Aldrin had called out navigation data to Armstrong, who was busy piloting the LM. A few moments before the landing, a light informed Aldrin that at least one of the 67-inch probes hanging from "Eagles footpads had touched the surface, and he said "Contact light!". Three seconds later, "Eagle" landed and Armstrong said "Shutdown". Aldrin immediately said "Okay, engine stop. ACA - out of detent." Armstrong acknowledged "Out of detent. Auto" and Aldrin continued "Mode control - both auto. Descent engine command override off. Engine arm - off. 413 is in." Charles Duke, acting as CAPCOM during the landing phase, acknowledged their landing by saying "We copy you down, Eagle". Armstrong continued with the remainder of the post landing checklist, "Engine arm is off." before responding to Duke with the famous words, "Houston, Tranquility Base here. The "Eagle" has landed." Armstrong's abrupt change of call sign from "Eagle" to "Tranquility Base" caused momentary confusion at Mission Control and Duke remained silent for a couple of seconds before replying: "Roger, Twank...Tranquility, we copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot!" expressing the relief of Mission Control after the unexpectedly drawn-out descent. He then took Communion privately. At this time NASA was still fighting a lawsuit brought by atheist Madalyn Murray O'Hair (who had objected to the "Apollo 8" crew reading from the Book of Genesis) which demanded that their astronauts refrain from religious activities while in space. As such, Aldrin chose to refrain from directly mentioning this. He had kept the plan quiet (not even mentioning it to his wife) and did not reveal it publicly for several years. Buzz Aldrin was an elder at Webster Presbyterian Church in Webster, TX. His communion kit was prepared by the pastor of the church, the Rev. Dean Woodruff. Aldrin described communion on the moon and the involvement of his church and pastor in the October, 1970 edition of Guideposts magazine and in his book "Return to Earth." Webster Presbyterian possesses the chalice used on the moon, and commemorates the Lunar Communion each year on the Sunday closest to July 20. The schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period, since they had been awake since early morning. However, they elected to forgo the sleep period and begin the preparations for the EVA early, thinking that they would be unable to sleep.
The astronauts planned placement of the Early Apollo Scientific Experiment Package (EASEP) and the U.S. flag by studying their landing site through "Eagles twin triangular windows, which gave them a 60° field of view. Preparation required longer than the two hours scheduled. Armstrong initially had some difficulties squeezing through the hatch with his Portable Life Support System (PLSS). According to veteran moonwalker John Young, a redesign of the LM to incorporate a smaller hatch had not been followed by a redesign of the PLSS backpack, so some of the highest heart rates recorded from "Apollo" astronauts occurred during LM egress and ingress. At 02:39 UTC on Monday July 21 (10:39pm EDT, Sunday July 20), 1969, Armstrong opened the hatch, and at 02:51 UTC began his descent to the Moon's surface. The Remote Control Unit controls on his chest kept him from seeing his feet. Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the Modular Equipment Stowage Assembly (MESA) folded against "Eagles side and activate the TV camera, and at 02:56 UTC (10:56pm EDT) he set his left foot on the surface. The first landing used slow-scan television incompatible with commercial TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor, significantly reducing the quality of the picture. The signal was received at Goldstone in the USA but with better fidelity by Honeysuckle Creek Tracking Station in Australia. Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia. Despite some technical and weather difficulties, ghostly black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth. Although copies of this video in broadcast format were saved and are widely available, recordings of the original slow scan source transmission from the moon were accidentally destroyed during routine magnetic tape re-use at NASA. Archived copies of the footage were eventually located in Perth, Australia, which was one of the sites that originally received the Moon broadcast. After describing the surface dust ("fine and almost like a powder"), Armstrong stepped off "Eagles footpad and into history as the first human to set foot on another world. It was then that he uttered his famous line "That's one small step for [a] man, one giant leap for mankind" six and a half hours after landing. Aldrin joined him, describing the view as "Magnificent desolation." Armstrong said that moving in the Moon's gravity, one-sixth of Earth's, was "even perhaps easier than the simulations... It's absolutely no trouble to walk around". In addition to fulfilling President John F. Kennedy's mandate to land a man on the Moon before the end of the 1960s, "Apollo 11" was an engineering test of the Apollo system; therefore, Armstrong snapped photos of the LM so engineers would be able to judge its post-landing condition. He then collected a contingency soil sample using a sample bag on a stick. He folded the bag and tucked it into a pocket on his right thigh. He removed the TV camera from the MESA, made a panoramic sweep, and mounted it on a tripod 12 m (40 ft) from the LM. The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA. Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops. The PLSS backpack created a tendency to tip backwards, but neither astronaut had serious problems maintaining balance. Loping became the preferred method of movement. The astronauts reported that they needed to plan their movements six or seven steps ahead. The fine soil was quite slippery. Aldrin remarked that moving from sunlight into "Eagles shadow produced no temperature change inside the suit, though the helmet was warmer in sunlight, so he felt cooler in shadow. After the astronauts planted a U.S. flag on the lunar surface, they spoke with President Richard Nixon through a telephone-radio transmission which Nixon called "the most historic phone call ever made from the White House." Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief, out of respect of the lunar landing being Kennedy's legacy. The MESA failed to provide a stable work platform and was in shadow, slowing work somewhat. As they worked, the moonwalkers kicked up gray dust which soiled the outer part of their suits, the integrated thermal meteoroid garment. They deployed the EASEP, which included a passive seismograph and a laser ranging retroreflector. Then Armstrong loped about 120 m (400 ft) from the LM to snap photos at the rim of East Crater while Aldrin collected two core tubes. He used the geological hammer to pound in the tubes - the only time the hammer was used on "Apollo 11". The astronauts then collected rock samples using scoops and tongs on extension handles. Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 min. During this period Mission Control used a coded phrase to warn Armstrong that his metabolic rates were high and that he should slow down. He was moving rapidly from task to task as time ran out. However, as metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension.
Aldrin entered "Eagle" first. With some difficulty the astronauts lifted film and two sample boxes containing more than 22 kg (48 lb) of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor. Armstrong reminded Aldrin of a bag of memorial items in his suit pocket sleeve, and Aldrin tossed the bag down; Armstrong then jumped to the ladder's third rung and climbed into the LM. After transferring to LM life support, the explorers lightened the ascent stage for return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, one Hasselblad camera, and other equipment. They then repressurised the LM, and settled down to sleep. During this time another spacecraft, Luna 15 - an unmanned Soviet spacecraft in lunar orbit, began its own descent to the lunar surface. Launched only three days before the Apollo 11 mission, this was the third Soviet attempt to return lunar soil back to Earth. The Russian craft crashed on the moon at 15:50 UT – just a few hours before the scheduled American liftoff. In a race to reach the Moon and return to Earth, the parallel missions of Luna 15 and Apollo 11 were, in many ways, the culmination of the space race that underlay the space programs of both the United States and the Soviet Union in the 1960s. The simultaneous missions became one of the first instances of Soviet/American space cooperation as the USSR released Luna 15's flight plan to ensure it would not collide with Apollo 11, though its exact mission was unknown. While moving within the cabin, Aldrin accidentally broke the circuit breaker that would arm the main engine for lift off from the moon. There was concern this would prevent firing the engine, stranding them on the moon. Fortunately a felt-tip pen was sufficient to activate the switch. Had this not worked, the Lunar Module circuitry could have been reconfigured to allow firing the ascent engine. After about seven hours of rest, the crew were awakened by Houston to prepare for the return flight. Two and a half hours later, at 17:54 UTC, they lifted off in "Eagles ascent stage, carrying 21.5 kilograms of lunar samples with them, to rejoin CMP Michael Collins aboard "Columbia" in lunar orbit. After more than 2½ hours on the lunar surface, they had left behind scientific instruments which included a retroreflector array used for the Lunar Laser Ranging Experiment and a Passive Seismic Experiment used to measure moonquakes. They also left an American flag, an Apollo 1 mission patch, and a plaque (mounted on the LM Descent Stage ladder) bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Richard M. Nixon. The inscription read "Here Men From The Planet Earth First Set Foot Upon the Moon, July 1969 A.D. We Came in Peace For All Mankind." They also left behind a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace and a silicon message disk. The disk carries the goodwill statements by Presidents Eisenhower, Kennedy, Johnson and Nixon and messages from leaders of 73 countries around the world. The disc also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and present top management. (In his 1989 book, "Men from Earth", Aldrin says that the items included Soviet medals commemorating Cosmonauts Vladimir Komarov and Yuri Gagarin.) Also, according to Deke Slayton's book 'Moonshot', Armstrong carried with him a special diamond-studded Astronaut pin from Deke. Film taken from the LM Ascent Stage upon liftoff from the moon reveals the American flag, planted some from the descent stage, whipping violently in the exhaust of the ascent stage engine. Buzz Aldrin witnessed it topple: "The ascent stage of the LM separated...I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over." Subsequent Apollo missions usually planted the American flags at least from the LM to prevent its being blown over by the ascent engine exhaust. After rendezvous with "Columbia", "Eagle's" ascent stage was jettisoned into lunar orbit at July 21, 1969 at 23:41 UT (7:41 PM EDT). Just before the Apollo 12 flight, it was noted that "Eagle" was still likely to be orbiting the moon. Later NASA reports mentioned that "Eagle's" orbit had decayed resulting in it impacting in an "uncertain location" on the lunar surface. The location is uncertain because the "Eagle" ascent stage was not tracked after it was jettisoned and the lunar gravity field is sufficiently uncertain to make the orbit of the spacecraft unpredictable after a short time. NASA estimated that the orbit had decayed within months and would have impacted on the Moon. On July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented, On the return to Earth, the Guam tracking station failed, which would have prevented communication on the last segment of the Earth return. Repair was not possible until a staff member had his ten-year old son, Greg Force, do repairs made possible by his small hands. Force later was thanked by Armstrong.
On July 24, the astronauts returned home aboard the command module Columbia just before dawn at, in the Pacific Ocean 2,660 km (1,440 nm) east of Wake Island, or 380 km (210 nm) south of Johnston Atoll, and 24 km (15 mi) from the recovery ship, USS "Hornet". Initially the command module landed upside down but was righted in several minutes by flotation bags triggered by the astronauts. A diver from the Navy helicopter hovering above attached an anchor to the command module to prevent it from drifting. Additional divers attached additional flotation collars to stabilize the module and position rafts for astronaut extraction. Though the possibility of bringing back pathogen from the lunar surface was considered remote, it was not considered impossible and NASA took great precautions at the recovery site. Astronauts were provided Biological Isolation Garment (BIG suit) by divers which were worn until they reached isolation facilities onboard the Hornet. Additionally astronauts were rubbed down with a sodium-hypochlorite solution and the command module wiped with betadine to remove any lunar dust that might be present. The raft containing decontamination materials was then intentionally sunk. A second Sea King helicopter hoisted the astronauts aboard one by one where a NASA flight surgeon gave each a brief physical check during the half mile trip back to the Hornet. After touchdown on the Hornet, all crew exited the helicopter, leaving the flight surgeon and 3 crew. The helicopter was then lowered into hangar bay #2 where the astronauts walked the 30 feet to the Mobile Quarantine Facility (MQF) where they would begin their 21 days of quarantine, a practice that would continue for the next 3 Apollo missions before the moon was proven to be barren of life and quarantine process dropped for Apollo XV through XVII. President Richard Nixon was aboard "Hornet" to personally welcome the astronauts back to Earth. He told the astronauts: "As a result of what you've done, the world has never been closer together before." Years later, it was publicly revealed that Nixon had prepared a speech to be given if the mission resulted in death. The lunar module had not been tested to assess if it could launch from the moon surface. After Nixon departed, the "Hornet" was brought alongside the 5 ton command module where it was placed aboard by the ship's crane, placed on a dolly and moved next to the MQF. The Hornet steamed for Pearl Harbor where the command module and MQF were airlifted to the Johnson Space Center. The astronauts were placed in quarantine after their landing on the moon for fear that the moon might contain undiscovered pathogens, and that the astronauts might have been exposed to them during their moon walks. (The decision to do so was made in accordance with the recently passed Extra-Terrestrial Exposure Law). However, after almost three weeks in confinement (first in their trailer and later in the Lunar Receiving Laboratory at the Manned Spacecraft Center), the astronauts were given a clean bill of health. On August 13, 1969, the astronauts exited quarantine to the cheers of the American public. Parades were held in their honor in New York, Chicago, and Los Angeles on the same day. A few weeks later, they were invited by Mexico for a parade honoring them in Mexico City. That evening in Los Angeles there was an official State Dinner to celebrate "Apollo 11", attended by Members of Congress, 44 Governors, the Chief Justice, and ambassadors from 83 nations at the Century Plaza Hotel. President Richard Nixon and Vice President Spiro T. Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom. This celebration was the beginning of a 45-day "Giant Leap" tour that brought the astronauts to 25 foreign countries and included visits with prominent leaders such as Queen Elizabeth II of the United Kingdom. Many nations would honor the first manned moon landing by issuing "Apollo 11" commemorative postage stamps or coins. Also, a few POWs held in Vietnam received letters from home a few months after the landings with those stamps to covertly let the POWs know that the United States had landed men on the moon. On September 16, 1969, the three astronauts spoke before a joint meeting of Congress on Capitol Hill. They presented two U.S. flags, one to the House of Representatives and the other to the Senate, that had been carried to the surface of the moon with them.
The command module is displayed at the National Air and Space Museum, Washington, D.C.. It is placed in the central exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the "Spirit of St. Louis", the Bell X-1, the North American X-15, Mercury spacecraft "Friendship 7", and Gemini 4. The quarantine trailer, the flotation collar, and the righting spheres are displayed at the Smithsonian's Udvar-Hazy Center annex near Washington Dulles International Airport in Virginia. In 2009 the Lunar Reconnaissance Orbiter imaged the various Apollo landing sites on the surface of the moon with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts.
The patch of "Apollo 11" was designed by Collins, who wanted a symbol for "peaceful lunar landing by the United States." He chose an eagle as the symbol, put an olive branch in its beak, and drew a moon background with the earth in the distance. NASA officials said the talons of the eagle looked too "warlike" and after some discussion, the olive branch was moved to the claws. The crew decided the Roman numeral XI would not be understood in some nations and went with "Apollo 11"; they decided not to put their names on the patch, so it would "be representative of "everyone" who had worked toward a lunar landing." All colors are natural, with blue and gold borders around the patch. The LM was named "Eagle" to match the insignia. When the Eisenhower dollar coin was released a few years later, the patch design provided the eagle for its reverse side. The design was retained for the smaller Susan B. Anthony dollar which was unveiled in 1979, ten years after the Apollo 11 mission.
On July 15, 2009, LIFE.com published a photo gallery of never-before-seen photos of Aldrin, Collins, and Armstrong in the days before their mission. LIFE Photographer Ralph Morse covered the astronauts for years—especially in the months leading up to the July 16, 1969 launch—chronicling the crew's public and private lives. In the gallery, Morse talks with LIFE about the astronauts, the moon landing, quarantine, and rare and never-before-published photographs capturing that thrilling time. From July 16-24 2009 NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred. In addition, it is in the process of restoring the video footage and have released a preview of key moments. More events are listed at the website. The John F. Kennedy Library set up a Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon. It was carried out in a technically brilliant way with risks taken... that would be inconceivable in the risk-averse world of today...The Apollo programme is arguably the greatest technical achievement of mankind to date...nothing since Apollo has come close [to] the excitement that was generated by those astronauts - Armstrong, Aldrin and the 10 others who followed them. On May 1, 2009, Congress introduced a bill granting the three astronauts on Apollo 11 a Congressional Gold Medal, the highest civilian award in the United States. The bill was sponsored by Florida Senator Bill Nelson and Florida Congressman Alan Grayson.
Apollo 8 was the first human spaceflight mission to escape from the gravitational field of planet Earth; the first to be captured by and escape from the gravitational field of another celestial body; and the first crewed voyage to return to planet Earth from another celestial body - Earth's Moon. The three-man crew of Mission Commander Frank Borman, Command Module Pilot James Lovell, and Lunar Module Pilot William Anders became the first humans to see the far side of the Moon with their own eyes, as well as the first humans to see planet Earth from beyond low Earth orbit. The mission was accomplished with the first manned launch of a Saturn V rocket. Apollo 8 was the second manned mission of the Apollo Program. Originally planned as a low Earth orbit Lunar Module/Command Module test, the mission profile was changed to the more ambitious lunar orbital flight in August 1968 when the Lunar Module scheduled for the flight became delayed. The new mission's profile, procedures and personnel requirements left an uncharacteristically short time frame for training and preparation, thus placing more demands than usual on the time, talent, and discipline of the crew. After launching on December 21, 1968, the crew took three days to travel to the Moon. They orbited ten times over the course of 20 hours, during which the crew made a Christmas Eve television broadcast in which they read the first 10 verses from the Book of Genesis. At the time, the broadcast was the most watched TV program ever. Apollo 8's successful mission paved the way for Apollo 11 to fulfill U.S. President John F. Kennedy's goal of landing a man on the Moon before the end of the decade.
Lovell was originally the CMP on the back-up crew, with Michael Collins as the prime crew's CMP. However, Collins was replaced in July 1968, after suffering a cervical disc herniation that required surgery to repair. Aldrin was originally the backup LMP. When Lovell was rotated to the prime crew, no one with experience on CSM 103 (the specific spacecraft used for the mission) was available, so Aldrin was moved to CMP and Fred Haise brought in as backup LMP. Armstrong went on to command Apollo 11, where Aldrin was returned to the Lunar Module Pilot position. Michael Collins was assigned as Command Module Pilot, although Aldrin was seated in the CMP position for Apollo 11's launch due to his training advantage via Apollo 8. Fred Haise later flew on Apollo 13.
The Earth-based mission control teams for Apollo 8 consisted of astronauts assigned to the support crew, as well as non-astronaut flight directors and their staffs. The support crew members were not trained to fly the mission, but were able to stand in for astronauts in meetings and be involved in the minutiae of mission planning, while the prime and backup crews trained. They also served as capcoms during the mission. For Apollo 8, these crew members included astronauts John S. Bull, Vance D. Brand, Gerald P. Carr, and Ken Mattingly. The mission control teams on Earth rotated in three shifts, each led by a flight director. The directors for Apollo 8 included Cliff Charlesworth (Green team), Glynn Lunney (Black team), and Milton Windler (Maroon team).
The triangular shape of the insignia symbolizes the shape of the Apollo command module. It shows a red figure 8 looping around the earth and moon representing the mission number as well as the circumlunar nature of the mission. On the red number 8 are the names of the three astronauts. The initial design of the insignia was developed by Jim Lovell. Lovell reportedly sketched the initial design while riding in the backseat of a T-38 flight from California to Houston, shortly after learning of the re-designation of the flight to become a lunar orbital mission.
Apollo 4 and Apollo 6 had been "A" missions, each launching an unmanned Block I production model of the Apollo Command and Service Modules into Earth Orbit., scheduled for October 1968, would be a manned Earth Orbit flight of the CSM, completing the objectives for Mission "C". Further missions relied on the readiness of the Lunar Module (LM). Production of the LM was behind schedule, with the first model arriving at Cape Canaveral in June 1968. Even then, significant defects were discovered, leading Grumman, the lead contractor for the LM, to predict that the first mission-ready LM would not be ready until at least February 1969. This would mean delaying the proposed "D" mission and endangering the program's goal of a lunar landing before the end of 1969. Even more pressing was a CIA report that the Soviets were expected to attempt to send cosmonauts on a Zond circumlunar mission before the end of the year. If the Soviets were successful in being first to get humans around the Moon, then that would greatly detract from having Americans being first to land on the Moon. George Low, the Manager of the Apollo Spacecraft Program Office, proposed a solution in August. Since the Service Module (CSM) would be ready three months before the Lunar Module, a CSM-only mission could be flown in December 1968. Instead of just repeating the "C" mission flight of Apollo 7, this CSM could be sent all the way to the Moon, with the possibility of entering a lunar orbit. The new mission would also allow NASA to test lunar landing procedures that would otherwise have to wait until Apollo 10, the scheduled "F" mission. Almost every senior manager at NASA agreed with this new mission, citing both confidence in the hardware and personnel, and the potential for a significant morale boost provided by a circumlunar flight. The only person who needed some convincing was James E. Webb, the NASA administrator. With the rest of his agency in support of the new mission, Webb eventually approved the mission change. The mission was officially changed from a "D" mission to a "C-Prime" Lunar Orbit mission, but was still referred to in press releases as an Earth Orbit mission at Webb's direction. No public announcement was made about the change in mission until November 12, three weeks after Apollo 7's successful Earth Orbit mission and less than 40 days before launch. With the change in mission for Apollo 8, Director of Flight Crew Operations Deke Slayton decided to swap the crews of the D and E missions. James McDivitt, the original commander of the D mission, has said he was never offered the circumlunar flight, but would probably have turned it down, as he wanted to fly the lunar module. Borman, on the other hand, jumped at the chance: his original mission would just have been a repeat of the previous flight, except in a higher orbit. This swap also meant a swap of spacecraft, requiring Borman's crew to use CSM-103, while McDivitt's crew would use CSM-104. On September 9, the crew entered the simulators to begin their preparation for the flight. By the time the mission flew, the crew had spent seven hours training for every actual hour of flight. Although all crew members were trained in all aspects of the mission, it was necessary to specialize. Borman, as commander, was given training on controlling the spacecraft during the re-entry. Lovell was trained on navigating the spacecraft in case communication was lost with the Earth. Anders was placed in charge of checking that the spacecraft was in working order. The crew, now living in the crew quarters at Kennedy Space Center, received a visit from Charles Lindbergh and his wife, Anne Morrow Lindbergh, the night before the launch. They talked about how before his 1927 flight, Lindbergh had used a piece of string to measure the distance from New York City to Paris on a globe and from that calculated the fuel needed for the flight. The total was a tenth of the amount that the Saturn V would burn every second. The next day, the Lindberghs watched the launch of Apollo 8 from a nearby dune. Anne Morrow Lindbergh would later write a book about the Apollo program, entitled "Earth Shine", which mentions both the launch and the mission.
The Saturn V rocket used by Apollo 8 was designated SA-503, or the "03rd" model of the Saturn V ("5") Rocket to be used in the Saturn-Apollo ("SA") program. When it was erected in the Vertical Assembly Building on December 20, 1967, it was thought that the rocket would be used for an unmanned Earth-orbit test flight carrying a boilerplate Command/Service Module. Apollo 6 had suffered several major problems during its April 1968 flight, including severe pogo oscillation during its first stage, two second stage engine failures, and a third stage that failed to reignite in orbit. Without assurances that these problems had been rectified, NASA administrators could not justify risking a manned mission until additional unmanned test flights proved that the Saturn V was ready. Teams from the Marshall Space Flight Center (MSFC) went to work on the problems. Of primary concern was the pogo oscillation, which would not only hamper engine performance, but could exert significant g-forces on a crew. A task force of contractors, NASA agency representatives, and MSFC researchers concluded that the engines vibrated at a frequency similar to the frequency at which the spacecraft itself vibrated, causing a resonance effect that induced oscillations in the rocket. A system using helium gas to absorb some of these vibrations was installed. Of equal importance was the failure of three engines during flight. Researchers quickly determined that a leaking hydrogen fuel line ruptured when exposed to vacuum, causing a loss of fuel pressure in engine two. When an automatic shutoff attempted to close the liquid hydrogen valve and shut down engine two, it accidentally shut down engine three's liquid oxygen due to a miswired connection. As a result, engine three failed within one second of engine two's shutdown. Further investigation revealed the same problem for the third-stage engine — a faulty igniter line. The team modified the igniter lines and fuel conduits, hoping to avoid similar problems on future launches. The teams tested their solutions in August 1968 at the Marshall Space Flight Center. A Saturn stage IC was equipped with shock absorbing devices to demonstrate the team's solution to the problem of pogo oscillation, while a Saturn Stage II was retrofitted with modified fuel lines to demonstrate their resistance to leaks and ruptures in vacuum conditions. Once NASA administrators were convinced that the problems were solved, they gave their approval for a manned mission using SA-503. The Apollo 8 spacecraft was placed on top of the rocket on September 21 and the rocket made the slow 3-mile (5 km) journey to the launch pad on October 9. Testing continued all through December until the day before launch, including various levels of readiness testing from 5 December through 11 December. Final testing of modifications to address the problems of pogo oscillation, ruptured fuel lines, and bad igniter lines took place on 18 December, a mere three days before the scheduled launch.
Apollo 8 launched at 7:51:00 a.m. Eastern Standard Time on December 21, 1968, using the Saturn V's three stages, S-IC, S-II, and S-IVB, to achieve Earth orbit. The launch phase experienced only three minor problems: The engines of the first stage, S-IC, underperformed by 0.75%, causing the engines to burn for 2.45 seconds longer than planned, and toward the end of the second stage burn, S-II, the rocket underwent pogo oscillations. Frank Borman estimated the oscillations were approximately and (±2.5 m/s²). The apogee was also slightly higher than the planned circular orbit of. In its first manned mission, the Saturn V rocket placed Apollo 8 into a Earth orbit with a period of 88 minutes and 10 seconds. All three rocket stages fired during launch; the S-IC and S-II detached during launch. The S-IC impacted the Atlantic Ocean at and the S-II second stage at. The third stage of the rocket, S-IVB, assisted in driving the craft into Earth orbit but remained attached to later perform the Trans-Lunar Injection (TLI), the burn that would put the spacecraft on a trajectory to the Moon. Once in Earth orbit, both the Apollo 8 crew and Mission Control spent the next 2 hours and 38 minutes checking that the spacecraft was in proper working order and ready for TLI. The proper operation of third stage of the rocket, S-IVB was crucial; In the last unmanned test, the S-IVB had failed to re-ignite for TLI. During the flight, three fellow astronauts served on the ground as capsule communicators (usually referred to as "CAPCOMs") on a rotating schedule. The CAPCOMs were the only people who regularly communicated with the crew. Michael Collins was the first CAPCOM on duty and at 2 hours, 27 minutes and 22 seconds after launch radioed, "Apollo 8. You are Go for TLI". This communication signified that Mission Control had given official permission for Apollo 8 to go to the moon. Over the next twelve minutes before the TLI burn, the Apollo 8 crew continued to monitor the spacecraft and the rocket. The S-IVB third stage rocket ignited on time and burned perfectly for 5 minutes and 17 seconds. The burn increased the velocity of Apollo 8 to and the spacecraft's altitude at the end of the burn was. At this time, the crew also set the record for the highest speed humans had ever traveled. Although the S-IVB was sufficiently powerful to accelerate the CSM to the Earth's escape velocity, the TLI burn did not achieve this; the CSM remained in an elongated elliptical Earth orbit, and would have returned to the Earth if it had not encountered the Moon's gravitational field. After the S-IVB had performed its required tasks, it was jettisoned. The crew then rotated the spacecraft to take some photographs of the spent stage and then practiced flying in formation with it. As the crew rotated the spacecraft, they had their first views of the Earth as they moved away from it. This marked the first time humans could view the whole Earth at once. Borman became worried that the S-IVB was staying too close to the Command/Service Module and suggested to Mission Control that the crew perform a separation maneuver. Mission Control first suggested pointing the spacecraft towards Earth and using the Reaction Control System (RCS) thrusters on the Service Module to add away from the Earth, but Borman did not want to lose sight of the S-IVB. After discussion, the crew and Mission Control decided to burn in this direction, but at instead. These discussions put the crew an hour behind their flight plan. Five hours after launch, Mission Control sent a command to the S-IVB booster to vent its remaining fuel through its engine bell to change the booster's trajectory. This S-IVB would then pass the Moon and enter into a solar orbit, posing no further hazard to Apollo 8. The S-IVB subsequently went into a solar orbit with an inclination of 23.47° and a period of 340.80 days. The Apollo 8 crew were the first humans to pass through the Van Allen radiation belts, which extend up to from Earth. Scientists predicted that passing through the belts quickly at the spacecraft's high speed would cause a radiation dosage of no more than a chest X-ray, or 1 milligray (during the course of a year, the average human receives a dose of 2 to 3 mGy). To record the actual radiation dosages, each crew member wore a Personal Radiation Dosimeter that transmitted data to Earth as well as three passive film dosimeters that showed the cumulative radiation experienced by the crew. By the end of the mission, the crew experienced an average radiation dose of 1.6 mGy.
Jim Lovell's main job as Command Module Pilot was as navigator. Although Mission Control performed all of the actual navigation calculation, it was necessary to have a crew member serving as navigator so that the crew could successfully return to Earth in case of communication loss with Mission Control. Lovell navigated by star sightings using a sextant built into the spacecraft, measuring the angle between a star and the Earth's (or the Moon's) horizon. This task proved to be difficult, as a large cloud of debris around the spacecraft formed by the venting S-IVB made it hard to distinguish the stars. By seven hours into the mission, the crew was about one hour and 40 minutes behind flight plan due to the issues of moving away from the S-IVB and Lovell's obscured star sightings. The crew now placed the spacecraft into Passive Thermal Control (PTC), also known as "barbecue" mode. PTC involved the spacecraft rotating about once per hour along its long axis to ensure even heat distribution across the surface of the spacecraft. In direct sunlight, the spacecraft could be heated to over 200 °C while the parts in shadow would be −100 °C. These temperatures could cause the heat shield to crack or propellant lines to burst. As it was impossible to get a perfect roll, the spacecraft actually swept out a cone as it rotated. The crew had to make minor adjustments every half hour as the cone pattern got larger and larger. The first mid-course correction came 11 hours into the flight. Testing on the ground had shown that the Service Propulsion System (SPS) engine had a small chance of exploding when burned for long periods unless its combustion chamber was "coated" first. Burning the engine for a short period would accomplish coating. This first correction burn was only 2.4 seconds and added about prograde (in the direction of travel). This change was less than the planned due to a bubble of helium in the oxidizer lines causing lower than expected fuel pressure. The crew had to use the small Reaction Control System (RCS) thrusters to make up the shortfall. Two later planned mid-course corrections were canceled as the Apollo 8 trajectory was found to be perfect. Eleven hours into the flight, the crew had been awake for over 16 hours. Before launch, NASA had decided that at least one crew member should be awake at all times to deal with any issues that might arise. Borman started the first sleep shift, but between the constant radio chatter and mechanical noises, he found sleep difficult. About an hour after starting his sleep shift, Borman requested clearance to take a Seconal sleeping pill. However, the pill had little effect. Borman eventually fell asleep but then awoke feeling ill. He vomited twice and had a bout of diarrhea that left the spacecraft full of small globules of vomit and feces that the crew cleaned up to the best of their ability. Borman initially decided that he did not want everyone to know about his medical problems, but Lovell and Anders wanted to inform Mission Control. The crew decided to use the Data Storage Equipment (DSE), which could tape voice recordings and telemetry and dump them to Mission Control at high speed. After recording a description of Borman's illness they requested that Mission Control check the recording, stating that they "would like an evaluation of the voice comments". The Apollo 8 crew and Mission Control medical personnel held a conference using an unoccupied second floor control room (there were two identical control rooms in Houston on the second and third floor, only one of which was used during a mission). The conference participants decided that there was little to worry about and that Borman's illness was either a 24-hour flu, as Borman thought, or a reaction to the sleeping pill. Researchers now believe that he was suffering from space adaptation syndrome, which affects about a third of astronauts during their first day in space as their vestibular system adapts to weightlessness. Space adaptation syndrome had not been an issue on previous spacecraft (Mercury and Gemini), as those astronauts were unable to move freely in the comparatively smaller cabins of those spacecraft. The increased cabin space in the Apollo Command Module afforded astronauts greater freedom of movement, contributing to symptoms of spacesickness for Borman and, later, astronaut Russell Schweickart during Apollo 9. The cruise phase was a relatively uneventful part of the flight, except for the crew checking that the spacecraft was in working order and that they were on course. During this time, NASA scheduled a television broadcast at 31 hours after launch. The Apollo 8 crew used a 2 kg camera that broadcast in black-and-white only, using a Vidicon tube. The camera had two lenses, a very wide-angle (160°) lens, and a telephoto (9°) lens. During this first broadcast, the crew gave a tour of the spacecraft and attempted to show how the Earth appeared from space. However, difficulties aiming the narrow-angle lens without the aid of a monitor to show what it was looking at made showing the Earth impossible. Additionally, the Earth image became saturated by any bright source without proper filters. In the end, all the crew could show the people watching back on Earth was a bright blob. After broadcasting for 17 minutes, the rotation of the spacecraft took the high-gain antenna out of view of the receiving stations on Earth and they ended the transmission with Lovell wishing his mother a happy birthday. By this time, the crew had completely abandoned the planned sleep shifts. Lovell went to sleep 32½ hours into the flight — 3½ hours before he had planned to. A short while later, Anders also went to sleep after taking a sleeping pill. The crew was unable to see the Moon for much of the outward cruise. Two factors made the Moon almost impossible to see from inside the spacecraft: three of the five windows fogging up due to out-gassed oils from the silicone sealant, and the attitude required for the PTC. It was not until the crew had gone behind the Moon that they would be able to see it for the first time. The Apollo 8 made a second television broadcast at 55 hours into the flight. This time, the crew rigged up filters meant for the still cameras so they could acquire images of the Earth through the telephoto lens. Although difficult to aim, as they had to maneuver the entire spacecraft, the crew was able to broadcast back to Earth the first television pictures of the Earth. The crew spent the transmission describing the Earth and what was visible and the colors they could see. The transmission lasted 23 minutes.
At about 55 hours and 40 minutes into the flight, the crew of Apollo 8 became the first humans to enter the gravitational sphere of influence of another celestial body. In other words, the effect of the Moon's gravitational force on Apollo 8 became stronger than that of the Earth. At the time it happened, Apollo 8 was from the Moon and had a speed of relative to the Moon. This historic moment was of little interest to the crew since they were still calculating their trajectory with respect to the launch pad at Kennedy Space Center. They would continue to do so until they performed their last mid-course correction, switching to a reference frame based on ideal orientation for the second engine burn they would make in lunar orbit. It was only thirteen hours until they would be in lunar orbit. The last major event before Lunar Orbit Insertion was a second mid-course correction. It was in retrograde (against direction of travel) and slowed the spacecraft down by, effectively lowering the closest distance that the spacecraft would pass the moon. At exactly 61 hours after launch, about from the Moon, the crew burned the RCS for 11 seconds. They would now pass from the lunar surface. At 64 hours into the flight, the crew began to prepare for Lunar Orbit Insertion-1 (LOI-1). This maneuver had to be performed perfectly, and due to orbital mechanics had to be on the far side of the Moon, out of contact with the Earth. After Mission Control was polled for a Go/No Go decision, the crew was told at 68 hours, they were Go and "riding the best bird we can find". At 68 hours and 58 minutes, the spacecraft went behind the Moon and out of radio contact with the Earth. With 10 minutes before the LOI-1, the crew began one last check of the spacecraft systems and made sure that every switch was in the correct place. At that time, they finally got their first glimpses of the Moon. They had been flying over the unlit side, and it was Lovell who saw the first shafts of sunlight obliquely illuminating the lunar surface. The LOI burn was only two minutes away, so the crew had little time to appreciate the view.
The SPS ignited at 69 hours, 8 minutes, and 16 seconds after launch and burned for 4 minutes and 13 seconds, placing the Apollo 8 spacecraft in orbit around the Moon. The crew described the burn as being the longest four minutes of their lives. If the burn had not lasted exactly the correct amount of time, the spacecraft could have ended up in a highly elliptical lunar orbit or even flung off into space. If it lasted too long they could have impacted the Moon. After making sure the spacecraft was working, they finally had a chance to look at the Moon, which they would orbit for the next 20 hours. On Earth, Mission Control continued to wait. If the crew had not burned the engine or the burn had not lasted the planned length of time, the crew would appear early from behind the Moon. However, this time came and went without Apollo 8 reappearing. Exactly at the calculated moment, the signal was received from the spacecraft, indicating it was in a orbit about the Moon. Lovell continued to describe the terrain they were passing over. One of the crew's major tasks was reconnaissance of planned future landing sites on the Moon, especially one in Mare Tranquillitatis that would be the Apollo 11 landing site. The launch time of Apollo 8 had been chosen to give the best lighting conditions for examining the site. A film camera had been set up in one of the spacecraft windows to record a frame every second of the Moon below. Bill Anders spent much of the next 20 hours taking as many photographs as possible of targets of interest. By the end of the mission the crew had taken 700 photographs of the Moon and 150 of the Earth. Throughout the hour that the spacecraft was in contact with Earth, Borman kept asking how the data for the SPS looked. He wanted to make sure that the engine was working and could be used to return early to the Earth if necessary. He also asked that they receive a Go/No Go decision before they passed behind the Moon on each orbit. As they reappeared for their second pass in front of the Moon, the crew set up the equipment to broadcast a view of the lunar surface. Anders described the craters that they were passing over. At the end of this second orbit they performed the eleven-second LOI-2 burn of the SPS to circularize the orbit to. Through the next two orbits, the crew continued to keep check of the spacecraft and to observe and photograph the Moon. During the third pass, Borman read a small prayer for his church. He was scheduled to participate in a service at St. Christopher's Episcopal Church near Seabrook, Texas, but due to the Apollo 8 flight was unable. A fellow parishioner and engineer at Mission Control, Rod Rose, suggested that Borman read the prayer which could be recorded and then replayed during the service.
When the spacecraft came out from behind the Moon for its fourth pass across the front, the crew witnessed Earthrise for the first time in human history. Borman saw the Earth emerging from behind the lunar horizon and called in excitement to the others, taking a black-and-white photo as he did so. In the ensuing scramble Anders took the more famous colour photo, later picked by "Life" magazine as one of its hundred photos of the century. Due to the synchronous rotation of the Moon about the Earth, Earthrise is not generally visible from the Lunar surface. Earthrise is generally only visible when orbiting the Moon, other than at selected places near the Moon's limb, where libration carries the Earth slightly above and below the lunar horizon. Anders continued to take photographs while Lovell assumed control of the spacecraft so Borman could rest. Despite the difficulty resting in the cramped and noisy spacecraft, Borman was able to sleep for two orbits, awakening periodically to ask questions about their status. Borman awoke fully, however, when he started to hear his fellow crew members make mistakes. They were beginning to not understand questions and would have to ask for the answers to be repeated. Borman realized that everyone was extremely tired having not had a good night's sleep in over three days. Taking command, he ordered Anders and Lovell to get some sleep and that the rest of the flight plan regarding observing the Moon be scrubbed. At first Anders protested saying that he was fine, but Borman would not be swayed. At last Anders agreed as long as Borman would set up the camera to continue to take automatic shots of the Moon. Borman also remembered that there was a second television broadcast planned, and with so many people expected to be watching he wanted the crew to be alert. For the next two orbits Anders and Lovell slept while Borman sat at the helm. On subsequent Apollo missions, crews would avoid this situation by sleeping on the same schedule. As they rounded the Moon for the ninth time, the second television transmission began. Borman introduced the crew, followed by each man giving his impression of the lunar surface and what it was like to be orbiting the Moon. Borman described it as being "a vast, lonely, forbidding expanse of nothing." Then, after talking about what they were flying over, Anders said that the crew had a message for all those on Earth. Each man on board read a section from the Biblical creation story (verses 1-10) from the Book of Genesis. Borman finished the broadcast by wishing a Merry Christmas to everyone on Earth. His message appeared to sum up the feelings that all three crewmen had from their vantage point in lunar orbit. Borman said, "And from the crew of Apollo 8, we close with good night, good luck, and a Merry Christmas, God bless all of you, "all of you on the good Earth". The only task left for the crew at this point was to perform the Trans-Earth Injection (TEI), which was scheduled for 2½ hours after the end of the television transmission. The TEI was the most critical burn of the flight, as any failure of the SPS to ignite would strand the crew in Lunar orbit, with little hope of escape. As with the previous burn, the crew had to perform the maneuver above the far side of the Moon, out of contact with Earth. The burn occurred exactly on time. The spacecraft telemetry was reacquired as it re-emerged from behind the Moon at 89 hours, 28 minutes, and 39 seconds, the exact time calculated. When voice contact was regained, Lovell announced, "Please be informed, there is a Santa Claus", to which Ken Mattingly, the current CAPCOM, replied, "That's affirmative, you are the best ones to know". The spacecraft began its journey back to Earth on December 25, Christmas Day.
Later, Lovell used some otherwise idle time to do some navigational sightings, maneuvering the module to view various stars by using the computer keyboard. However, he accidentally erased some of the computer's memory, which caused the inertial measuring unit (IMU) to think the module was in the same relative position it had been in before lift-off and fire the thrusters to "correct" the module's attitude. Once the crew realized why the computer had changed the module's attitude, they realized they would have to re-enter data that would tell the computer its real position. It took Lovell ten minutes to figure out the right numbers, using the thrusters to get the stars Rigel and Sirius aligned, and another fifteen minutes to enter the corrected data into the computer. Sixteen months later, Lovell would once again have to perform a similar manual re-alignment, under more critical conditions, during the Apollo 13 mission, after that module's IMU had to be turned off to conserve energy. In his 1994 book, ', Lovell wrote, "My training [on Apollo 8] came in handy!". In that book he dismissed the incident as a "planned experiment", requested by the ground crew. However, in subsequent interviews Lovell has acknowledged that the incident was an accident, caused by his mistake. Cruise back to Earth and re-entry. The cruise back to Earth was mostly a time for the crew to relax and monitor the spacecraft. As long as the trajectory specialists had calculated everything correctly, the spacecraft would re-enter 2½ days after TEI and splashdown in the Pacific. On Christmas afternoon, the crew made their fifth television broadcast. This time they gave a tour of the spacecraft, showing how an astronaut lived in space. When they had finished broadcasting they found a small present from Deke Slayton in the food locker—real turkey with stuffing and three miniature bottles of brandy (which remained unopened). There were also small presents to the crew from their wives. The next day, at about 124 hours into the mission, the sixth and final TV transmission showed the mission's best video images of the earth, in a short four minute broadcast. After two uneventful days the crew prepared for re-entry. The computer would control the re-entry and all the crew had to do was put the spacecraft in the correct attitude, blunt end forward. If the computer broke down, Borman would take over. Once the Command Module was separated from the Service Module, the astronauts were committed to re-entry. Six minutes before they hit the top of the atmosphere, the crew saw the Moon rising above the Earth's horizon, just as had been predicted by the trajectory specialists. As they hit the thin outer atmosphere they noticed it was becoming hazy outside as glowing plasma formed around the spacecraft. The spacecraft started slowing down and the deceleration peaked at 6 g (59 m/s²). With the computer controlling the descent by changing the attitude of the spacecraft, Apollo 8 rose briefly like a skipping stone before descending to the ocean. At the drogue parachute stabilized the spacecraft and was followed at by the three main parachutes. The spacecraft splashdown position was estimated to be. When it hit the water, the parachutes dragged the spacecraft over and left it upside down, in what was termed Stable 2 position. As they were buffeted by a swell, Borman was sick, waiting for the three flotation balloons to right the spacecraft. It was 43 minutes after splashdown before the first frogman from the USS "Yorktown" arrived, as the spacecraft had landed before sunrise. Forty-five minutes later, the crew was safe on the deck of the aircraft carrier.
Apollo 8 came at the end of 1968, a year that had seen much upheaval around the world. Yet, "TIME" magazine chose the crew of Apollo 8 as their Men of the Year for 1968, recognizing them as the people who most influenced events in the preceding year. They had been the first people ever to leave the gravitational influence of the Earth and orbit another celestial body. They had survived a mission that even the crew themselves had rated as only having a fifty-fifty chance of fully succeeding. The effect of Apollo 8 can be summed up by a telegram from a stranger, received by Borman after the mission, that simply stated, "Thank you Apollo 8. You saved 1968." One of the most famous aspects of the flight was the Earthrise picture that was taken as they came around for their fourth orbit of the Moon. This was the first time that humans had taken such a picture whilst actually behind the camera, and it has been credited with a role in inspiring the first Earth Day in 1970. It was selected as the first of "Life" magazine's 'hundred photos that changed the world'. Apollo 8 is regarded by some as the most historically significant of all the Apollo missions. The mission was the most widely covered by the media since the first American orbital flight, Mercury-Atlas 6 by John Glenn in 1962. There were 1200 journalists covering the mission, with the BBC coverage being broadcast in 54 countries in 15 different languages. The Soviet newspaper "Pravda" featured a quote from Boris Nikolaevich Petrov, Chairman of the Soviet Intercosmos program, who described the flight as an "outstanding achievement of American space sciences and technology". It is estimated that a quarter of the people alive at the time saw — either live or delayed — the Christmas Eve transmission during the ninth orbit of the Moon. The Apollo 8 broadcasts won an Emmy, the highest honor given by the Academy of Television Arts and Sciences. Atheist Madalyn Murray O'Hair later caused controversy by bringing a lawsuit against NASA over the reading from "Genesis". O'Hair wished the courts to ban US astronauts — who were all Government employees — from public prayer in space. Though the case was rejected by the US Supreme Court for lack of jurisdiction, it caused NASA to be skittish about the issue of religion throughout the rest of the Apollo program. Buzz Aldrin, on Apollo 11, self-communicated Presbyterian Communion on the surface of the moon after landing; he refrained from mentioning this publicly for several years, and only obliquely referred to it at the time. In 1969, the US Postal Service issued a postage stamp (1371) commemorating the Apollo 8 flight around the moon. The stamp featured a detail of the famous photograph of the Earthrise over the moon taken by Anders on Christmas Eve, and the words, "In the beginning God..."
The mission parameters for Apollo 8 differed significantly from those of previous flights, for several reasons. As the first manned spacecraft to orbit multiple celestial bodies, the mission recorded two different sets of orbital parameters. The mission was also the first to execute a translunar injection. While in parking orbit around the Earth, Apollo 8 maintained altitude between a perigee of and an apogee of. The inclination of this orbit, or its angle in relation to the equator, was 32.51°. Each orbit had a period of 88.17 minutes. In contrast, the spacecraft orbited the Moon at more varying altitudes. At its lowest altitude above the moon's surface, the spacecraft had a pericynthion of, while the highest altitude, or apocynthion, was. The spacecraft took 128.7 minutes to complete each of its 10 circuits around the Moon, at an inclination of 12°. The spacecraft began its translunar injection burn on December 21, 1968, at 15:41:38 UTC. The burn represented the second of two burns on the Saturn V rocket's S-IVB third stage. The rocket burned for a total of 318 seconds, propelling the spacecraft from an Earth parking orbit velocity of to a translunar trajectory velocity of.
Apollo 8's historic mission has been shown and referred to in several forms, both documentary and fiction. The various television transmissions and 16 mm footage shot by the crew of Apollo 8 was compiled and released by NASA in the 1969 documentary, "Debrief: Apollo 8", which was hosted by Burgess Meredith. In addition, Spacecraft Films released a three-disc DVD set covering the mission in 2003. Portions of the Apollo 8 Mission can be seen in the 1989 documentary "For All Mankind", which won the Grand Jury Prize at the Sundance Film Festival for Outstanding Documentary. The Apollo 8 mission was well covered in the British documentary: 'In the Shadow of the Moon'. Apollo 8 was mentioned in the film "Apollo 13," though only briefly. Portions of the Apollo 8 mission are dramatized in the miniseries "From the Earth to the Moon" episode "1968". The S-IVB stage of Apollo 8 was also portrayed as the location of an alien device in the 1970 "UFO" episode "Conflict".
An astronaut or cosmonaut is a person trained by a human spaceflight program to command, pilot, or serve as a crew member of a spacecraft. While generally reserved for professional space travelers, the term is sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists. Until 2003, astronauts were sponsored and trained exclusively by governments, either by the military, or by civilian space agencies. With the sub-orbital flight of the privately-funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.
The criteria for what constitutes human spaceflight vary. The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of. In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings. As of September 19, 2009, a total of 505 humans from 38 countries have reached 100 km or more in altitude, of which 502 reached Low Earth orbit or beyond. Of these, 24 people have traveled beyond Low Earth orbit, to either lunar or trans-lunar orbit or to the surface of the moon; three of the 24 did so twice: Jim Lovell, John Young and Eugene Cernan. Under the U. S. definition, 496 people qualify as having reached space, above altitude. Of eight X-15 pilots who exceeded 50 miles in altitude, seven reached above but below 100 kilometers (about 62 miles). Space travelers have spent over 30,400 person-days (or a cumulative total of over 83 years) in space, including over 100 astronaut-days of spacewalks. As of 2008, the man with the longest time in space is Sergei K. Krikalev, who has spent 803 days, 9 hours and 39 minutes, or 2.2 years, in space. Peggy A. Whitson holds the record for most time in space by a woman, 377 days.
In the United States, Canada, United Kingdom, and many other English-speaking nations, a professional space traveler is called an "astronaut". The term derives from the Greek words "ástron" (ἄστρον), meaning "star", and "nautes" (ναύτης), meaning "sailor". The first known use of the term "astronaut" in the modern sense was by Neil R. Jones in his short story "The Death's Head Meteor" in 1930. The word itself had been known earlier. For example, in Percy Greg's 1880 book "Across the Zodiac", "astronaut" referred to a spacecraft. In "Les Navigateurs de l'Infini" (1925) of J.-H. Rosny aîné, the word "astronautique" (astronautic) was used. The word may have been inspired by "aeronaut", an older term for an air traveler first applied (in 1784) to balloonists. NASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond. NASA also uses the term as a title for those selected to join its Astronaut Corps. The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.
By convention, an astronaut employed by the Russian Federal Space Agency (or its Soviet predecessor) is called a cosmonaut in English texts. The word is an anglicisation of the Russian word "kosmonavt" (), which in turn derives from the Greek words "kosmos" (κόσμος), meaning "universe", and "nautes" (ναύτης), meaning "sailor". For the most part, "cosmonaut" and "astronaut" are synonyms in all languages, and the usage of choice is often dictated by political reasons. Yuri Gagarin, Russian, is the first human cosmonaut. Valentina Tereshkova, Russian, is the first woman cosmonaut. On March 14, 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, arguably becoming the first "American cosmonaut" in the process.
Official English-language texts issued by the government of the People's Republic of China use "astronaut" while texts in Russian use "космонавт" ("kosmonavt"). In China, the terms "yǔhángyuán" (, "sailing personnel in universe") or "hángtiānyuán" (, "sailing personnel in sky") have long been used for astronauts. The phrase "tàikōng rén" (, "spaceman") is often used in Taiwan and Hong Kong. The term taikonaut is used by some English-language news media organizations for professional space travelers from China. The word has featured in the Longman and Oxford English dictionaries, the latter of which describes it as "a hybrid of the Chinese term "taikong" (space) and the Greek "naut" (sailor)"; the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the "Shenzhou 5" spacecraft. This is the term used by Xinhua in the English version of the Chinese People's Daily since the advent of the Chinese space program. The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia, used it in newsgroups, while Chen Lan (), almost simultaneously, announced it at his "Go Taikonauts!" GeoCities page.
With the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term "spaceflight participant" to distinguish those space travelers from astronauts on missions coordinated by those two agencies. While no nation other than Russia (formerly the Soviet Union), the United States, and China has launched a manned spacecraft, several other nations have sent people into space in cooperation with one of these countries. Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage. For example, the term spationaut (French spelling: "spationaute") is sometimes used to describe French space travelers, from the Latin word "spatium" or space, and the Malay term "angkasawan" was used to describe participants in the Angkasawan program.
The first human in space was Russian Yuri Gagarin, who was launched into space on April 12, 1961 aboard Vostok 1 and orbited around the Earth for 108 minutes. There are allegations that Gagarin ejected from landing module after re-entering the atmosphere and parachuted back, due to safety concerns about the craft's landing systems. The first woman in space was Russian Valentina Tereshkova, launched in June 1963 aboard Vostok 6. Alan Shepard became the first American and second person in space on May 5, 1961 on a 15-minute sub-orbital flight. The first American woman in space was Sally Ride, during Space Shuttle Challenger's mission STS-7, on June 18, 1983. The first mission to orbit the moon was "Apollo 8", which included William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968. In April 1985, Taylor Wang became the first ethnic Chinese person in space. On 15 October 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft. The Soviet Union, through its Intercosmos program, allowed people from other "socialist" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions. An example is Vladimír Remek, a Czechoslovak, who became the first non-Soviet European in space in 1978 on a Russian Soyuz-U rocket. On July 23, 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37. Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, Guion Bluford became the first African American to fly into space. The first person born in Africa to fly in space was Patrick Baudry, in 1985. In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the Mir space station. With the larger number of seats available on the Space Shuttle, the U.S. began taking international astronauts. In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft. In 1985, Rodolfo Neri Vela became the first Mexican-born person in space. In 1991, Helen Sharman became the first Briton to fly in space. In 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant. In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.
The first civilian in space was Neil Armstrong, who had retired from the United States Navy before his first spaceflight on Gemini 8. The first person in space who had never been a member of any country's armed forces was Harrison Schmitt, a geologist who first flew in space on Apollo 17. Both Armstrong and Schmitt were directly employed by NASA. The first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983. In December 1990, Toyohiro Akiyama became the first paying space traveler as a reporter for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was "Research Cosmonaut". Akiyama suffered severe space-sickness during his mission, which affected his productivity. The first self-funded space tourist was Dennis Tito onboard the Russian spacecraft Soyuz TM-3 on 28 April 2001.
The first NASA astronauts were selected for training in 1959. Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection. Selection was initially limited to military pilots. The earliest astronauts for both America and Russia tended to be jet fighter pilots, and were often test pilots. Once selected, NASA astronauts go through 20 months of training in a variety of areas, including training for extra-vehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory. Astronauts-in-training may also experience short periods of weightlessness in aircraft called the "vomit comet", the nickname given to a pair of modified KC-135s (retired in 2000 and 2004 respectively, and replaced in 2005 with a C-9) which perform parabolic flights. Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft. This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center. Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are done out of Edwards Air Force Base.
Mission Specialist Educators, or "Educator Astronauts", were first selected in 2004, and as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger. Barbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist. The Educator Astronaut program is a successor to the Teacher in Space program from the 1980s. Health risks of space travel. Astronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, orthostatic intolerance due to volume loss, sleep disturbances, and radiation injury. A variety of large scale medical studies are being conducted in space via the National Space and Biomedical Research Institute (NSBRI) to address these issues. Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space. This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students. It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare. For more information on the health hazards faced by astronauts, go to the article entitled Space medicine.
At NASA, people who complete astronaut candidate training receive a silver lapel pin. Once they have flown in space, they receive a gold pin. U.S. astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight. The United States Air Force also presents an Astronaut Badge to its pilots who exceed 50 miles (80 km) in altitude.
Eighteen astronauts have lost their lives during spaceflight, on four missions. By nationality, they are thirteen Americans, three Russians, one Ukrainian, and one Israeli. Several others have died while training for space missions. The Space Mirror Memorial, which stands on the grounds of the John F. Kennedy Space Center Visitor Complex, commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States. In addition to twenty NASA career astronauts, the memorial includes the names of a U.S. Air Force X-15 test pilot, a U.S. Air Force officer who died while training for a then-classified military space program, a civilian spaceflight participant who died in the Challenger disaster, and an international astronaut who was killed in the Columbia disaster.
"A Modest Proposal: For Preventing the Children of Poor People in Ireland from Being a Burden to Their Parents or Country, and for Making Them Beneficial to the Publick", commonly referred to as "A Modest Proposal", is a Juvenalian satirical essay written and published anonymously by Jonathan Swift in 1729. Swift appears to suggest in his essay that the impoverished Irish might ease their economic troubles by selling children as food for rich gentlemen and ladies. By doing this he mocks the authority of the British officials.
Swift goes to great lengths to support his argument, including a list of possible preparation styles for the children, and calculations showing the financial benefits of his suggestion. He uses common methods of argument throughout his essay, such as appealing to the authority of "a very knowing American of my acquaintance in London" and "the famous Psalmanazar, a native of the island Formosa" (who had already confessed to "not" being from Formosa in 1706). Swift couches his arguments in then-current events, exploiting common prejudice against Catholics (misnomed "Papists") and pointing out their depredations of England. After enumerating the benefits of his proposal, Swift addresses possible objections including the depopulation of Ireland and a litany of other solutions which he dismisses as impractical. This essay is widely held to be one of the greatest examples of sustained irony in the history of the English language. Much of its shock value derives from the fact that the first portion of the essay describes the plight of starving beggars in Ireland, so that the reader is unprepared for the surprise of Swift's solution when he states, "A young healthy child well nursed, is, at a year old, a most delicious nourishing and wholesome food, whether stewed, roasted, baked, or boiled; and I make no doubt that it will equally serve in a fricassee, or a ragout." Readers unacquainted with its reputation as a satirical work often do not immediately realize that Swift was not seriously proposing cannibalism and infanticide, nor would readers unfamiliar with the satires of Horace and Juvenal recognize that Swift's essay follows the rules and structure of Latin satires. The satirical element of the pamphlet is often only understood after the reader notes the allusions made by Swift to the attitudes of landlords, such as the following: "I grant this food may be somewhat dear, and therefore very proper for Landlords, who as they have already devoured most of the Parents, seem to have the best Title to the Children." Swift extends the metaphor to get in a few jibes at England’s mistreatment of Ireland, noting that "For this kind of commodity will not bear exportation, and flesh being of too tender a consistence, to admit a long continuance in salt, although perhaps I could name a country, which would be glad to eat up our whole nation without it."
It has been argued that Swift’s main target in "A Modest Proposal" was not the conditions in Ireland, but rather the can-do spirit of the times that led people to devise a number of illogical schemes that would purportedly solve social and economic ills. Swift was especially insulted by projects that tried to fix population and labor issues with a simple cure-all solution. A memorable example of these sorts of schemes "involved the idea of running the poor through a joint-stock company". In response, Swift’s "Modest Proposal" was "a burlesque of projects concerning the poor", that were in vogue during the early 18th century. "A Modest Proposal" also targets the calculating way people perceived the poor in designing their projects. The pamphlet targets reformers who "regard people as commodities". In the piece, Swift adopts the "technique of a political arithmetician" to show the utter ridiculousness of trying to prove any proposal with dispassionate statistics. Critics differ about Swift’s intentions in using this faux-mathematical philosophy. Edmund Wilson argues that statistically "the logic of the 'Modest proposal' can be compared with defense of crime (arrogated to Marx) in which he argues that crime takes care of the superfluous population". Wittkowsky counters that Swift's satiric use of statistical analysis is an effort to enhance his satire that "springs from a spirit of bitter mockery, not from the delight in calculations for their own sake".
Charles K. Smith argues that Swift’s rhetorical style persuades the reader to detest the speaker and pity the Irish. Swift’s specific strategy is twofold, using a "trap" to create sympathy for the Irish and a dislike of the narrator who, in the span of one sentence, "details vividly and with rhetorical emphasis the grinding poverty" but feels emotion solely for members of his own class. Swift’s use of gripping details of poverty and his narrator’s cool approach towards them creates "two opposing points of view" which "alienate the reader, perhaps unconsciously, from a narrator who can view with 'melancholy' detachment a subject that Swift has directed us, rhetorically, to see in a much less detached way". Swift has his proposer further degrade the Irish by using language ordinarily reserved for animals. Lewis argues that the speaker uses "the vocabulary of animal husbandry" to describe the Irish. Once the children have been commoditized, Swift’s rhetoric can easily turn "people into animals, then meat, and from meat, logically, into tonnage worth a price per pound". Swift uses the proposer’s serious tone to highlight the absurdity of his proposal. In making his argument, the speaker uses the conventional, text book approved order of argument from Swift’s time. The contrast between the "careful control against the almost inconceivable perversion of his scheme" and "the ridiculousness of the proposal" create a situation in which the reader has "to consider just what perverted values and assumptions would allow such a diligent, thoughtful, and conventional man to propose so perverse a plan".
Some scholars have argued that "A Modest Proposal" was largely influenced and inspired by Tertullian’s "Apology". While Tertullian’s "Apology" is a satirical attack against early Roman persecution of Christianity, Swift’s "A Modest Proposal" addresses the Anglo-Irish situation in the 1720s. James William Johnson believes that Swift saw major similarities between the two situations. Johnson notes Swift’s obvious affinity for Tertullian and the bold stylistic and structural similarities between the works "A Modest Proposal" and "Apology". In structure, Johnson points out the same central theme; that of cannibalism and the eating of babies; and the same final argument; that "human depravity is such that men will attempt to justify their own cruelty by accusing their victims of being lower than human". Stylistically, Swift and Tertullian share the same command of sarcasm and language. In agreement with Johnson, Donald C. Baker points out the similarity between both authors' tones and use of irony. Baker notes the uncanny way that both authors imply an ironic "justification by ownership" over the subject of sacrificing children—Tertullian while attacking pagan parents, and Swift while attacking the English mistreatment of the Irish poor.
Robert Phiddian's article "Have you eaten yet? The Reader in A Modest Proposal" focuses on two aspects of "A Modest Proposal": the voice of Swift and the voice of the Proposer. Phiddian stresses that a reader of the pamphlet must learn to distinguish between the satiric voice of Jonathan Swift and the apparent economic projections of the Proposer. He reminds readers that "there is a gap between the narrator’s meaning and the text’s, and that a moral-political argument is being carried out by means of parody". While Swift’s proposal is obviously not a serious economic proposal, George Wittkowsky, author of "Swift’s Modest Proposal: The Biography of an Early Georgian Pamphlet", argues that it in order to understand the piece fully, it is important to understand the economics of Swift’s time. Wittowsky argues that not enough critics have taken the time to focus directly on the mercantilism and theories of labor in 18th century England. "[I]f one regards the "Modest Proposal" simply as a criticism of condition, about all one can say is that conditions were bad and that Swift's irony brilliantly underscored this fact". At the start of a new industrial age in the 18th century, it was believed that "people are the riches of the nation", and there was a general faith in an economy which paid its workers low wages because high wages would mean workers would work less. Furthermore, "in the mercantilist view no child was too young to go into industry". In those times, the "somewhat more humane attitudes of an earlier day had all but disappeared and the laborer had come to be regarded as a commodity". People are the riches of a nation. Louis A. Landa presents Swift’s "A Modest Proposal" as a critique of the popular and unjustified maxim of mercantilism in the eighteenth century that "people are the riches of a nation". Swift presents the dire state of Ireland and shows that mere population itself, in Ireland’s case, did not always mean greater wealth and economy. The uncontrolled maxim fails to take into account that a person that does not produce in an economic or political way makes a country poorer, not richer. Swift also recognizes the implications of such a fact in making mercantilist philosophy a paradox: the wealth of a country is based on the poverty of the majority of its citizens. Swift however, Landa argues, is not merely criticizing economic maxims but also addressing the fact that England was denying Irish citizens their natural rights and dehumanizing them by viewing them as a mere commodity.
"A Modest Proposal" is included in many literature programs as an example of early modern western satire. It also serves as an exceptional introduction to the concept and use of argumentative language, lending itself well to secondary and post-secondary essay courses. Outside of the realm of English studies, "A Modest Proposal" is a relevant piece included in many comparative and global literature and history courses, as well as those of numerous other disciplines in the arts, humanities, and even the social sciences. It has been emulated many times as well. In his book "A Modest Proposal" (1984), evangelical author Frank Schaeffer emulated Swift's work in social conservative polemic against abortion and euthanasia in a future dystopia that advocated recycling of aborted embryos and fetuses, as well as some disabled infants with compound intellectual, physical and physiological difficulties. (Such Baby Doe Rules cases were then a major concern of the pro-life movement of the early 1980s, which viewed selective treatment of those infants as disability discrimination.) In Hunter S. Thompson's, which contains hundreds of private letters written by Thompson over the years, contains a letter in which he uses "A Modest Proposals satire technique against the Vietnam War. Thompson writes a letter to a local Aspen newspaper informing them that, on Christmas Eve, he was going to use napalm to burn a number of dogs and hopefully any humans they find. This letter protests the burning of Vietnamese people occurring overseas.
The game "Orphan Feast" on Cartoon Network's Adult Swim website is loosely based on "A Modest Proposal". The show "Sealab 2021" references "A Modest Proposal" by the character of Jodene Sparks. It was suggested as recommended reading when Debbie wanted a child. "A Modest Proposal" is the name of The University of Texas at Dallas', the monthly opinion paper of the University; and was the name of a regular column in of Harvard University, a satire publication which also takes its name from Johnathan Swift. "A Modest Proposal" is mentioned in the 1996 film "The Birdcage". Controversial American political activist and disbarred attorney Jack Thompson's A Modest Video Game Proposal draws its title from "A Modest Proposal". One of the radio presenters in the game Saint's Row claims he has "A modest proposal" which is to apply shock collars to all immigrants in America.
The alkali metals are a series of chemical elements forming Group 1 (IUPAC style) of the periodic table: lithium (Li), sodium (Na), potassium (K), rubidium (Rb), caesium (Cs), and francium (Fr). (Hydrogen, although nominally also a member of Group 1, very rarely exhibits behavior comparable to the alkali metals). The alkali metals provide one of the best examples of group trends in properties in the periodic table, with well characterized homologous behavior down the group.
The alkali metals are all highly reactive and are never found in elemental form in nature. As a result, in the laboratory they are stored under mineral oil or paraffin oil. They also tarnish easily and have low melting points and densities. Potassium and rubidium possess a weak radioactive characteristic due to the presence of long duration radioactive isotopes. The alkali metals are silver-colored (caesium has a golden tinge), soft, low-density metals, which react readily with halogens to form ionic salts, and with water to form strongly alkaline (basic) hydroxides. These elements all have one electron in their outermost shell, so the energetically preferred state of achieving a filled electron shell is to lose one electron to form a singly charged positive ion, i.e. cation. Hydrogen, with a solitary electron, is usually placed at the top of Group 1 of the periodic table, but it is not considered an alkali metal; rather it exists naturally as a diatomic gas. Removal of its single electron requires considerably more energy than removal of the outer electron for the alkali metals. As in the halogens, only one additional electron is required to fill in the outermost shell of the hydrogen atom, so hydrogen can in some circumstances behave like a halogen, forming the negative hydride ion. Binary compounds of hydride with the alkali metals and some transition metals have been prepared. Under extremely high pressure, such as is found at the core of Jupiter, hydrogen does become metallic and behaves like an alkali metal; see metallic hydrogen. Alkali metals have the lowest ionization potentials in their respective periods, as removing the single electron from the outermost shell gives them the stable inert gas configuration. Their second ionization potentials are very high, as removing an electron from a species having a noble gas configuration is very difficult. Alkali metal + water → Alkali metal hydroxide + hydrogen gas
In logic, the argument form or "test form" of an argument results from replacing the different words, or sentences, that make up the argument with letters, along the lines of algebra; the letters represent logical "variables". This is of importance since the validity of an argument is determined solely by its form. The "sentence forms" which classify argument forms of common important arguments are studied in logic.
To demonstrate the important notion of the "form" of an argument, substitute letters for similar items throughout the sentences in the original argument. All we have done in the "Argument form" is to put 'S' for 'human' and 'humans', 'P' for 'mortal', and 'a' for 'Socrates'; what results, is the "form" of the original argument. Moreover, each individual sentence of the "Argument form" is the "sentence form" of its respective sentence in the original argument.
The word "alphabet" came into Middle English from the Late Latin word Alphabetum, which in turn originated in the Ancient Greek "Αλφάβητος" Alphabetos, from "alpha" and "beta," the first two letters of the Greek alphabet. "Alpha" and "beta" in turn came from the first two letters of the Phoenician alphabet, and meant "ox" and "house" respectively. There are dozens of alphabets in use today, the most common being Latin, deriving from the first true alphabet, Greek. Most of them are composed of lines (linear writing); notable exceptions are Braille, fingerspelling (Sign language), and Morse code.
The term alphabet prototypically refers to a writing system that has characters (graphemes) which represent both consonant and vowel sounds, even though there may not be a complete one-to-one correspondence between symbol and sound. A grapheme is an abstract entity which may be physically represented by different styles of glyphs. There are many written entities which do not form part of the alphabet, including numerals, mathematical symbols, and punctuation. Some human languages are commonly written using a combination of logograms (which represent morphemes or words) and syllabaries (which represent syllables) instead of an alphabet. Egyptian hieroglyphs and Chinese characters are two of the best-known writing systems with predominantly non-alphabetic representations. Non-written languages may also be represented alphabetically. For example, linguists researching a non-written language (such as some of the indigenous Amerindian languages) will use the International Phonetic Alphabet to enable them to write down the sounds they hear. Most, if not all, linguistic writing systems have some means for phonetic approximation of foreign words, usually using the native character set. The English alphabet has 26 letters in it.
The history of the alphabet started in ancient Egypt. By 2700 BC Egyptian writing had a set of some 24 hieroglyphs which are called uniliterals, to represent syllables that begin with a single consonant of their language, plus a vowel (or no vowel) to be supplied by the native speaker. These glyphs were used as pronunciation guides for logograms, to write grammatical inflections, and, later, to transcribe loan words and foreign names. However, although seemingly alphabetic in nature, the original Egyptian uniliterals were not a system and were never used by themselves to encode Egyptian speech. In the Middle Bronze Age an apparently "alphabetic" system known as the Proto-Sinaitic script is thought by some to have been developed in the Sinai peninsula during the 19th century BC, by Canaanite workers in the Egyptian turquoise mines. Others suggest the alphabet was developed in central Egypt during the 15th century BC for or by Semitic workers, but only one of these early writings has been deciphered and their exact nature remains open to interpretation. Based on letter appearances and names, it is believed to be based on Egyptian hieroglyphs. This script had no characters representing vowels. An alphabetic cuneiform script with 30 signs including 3 which indicate the following vowel was invented in Ugarit before the 15th century BC. This script was not used after the destruction of Ugarit. The Proto-Sinatic or Proto-Canaanite script eventually developed into the Proto-Canaanite alphabet, which in turn was refined into the Phoenician alphabet. The oldest text in Phoenician script is an inscription on the sarcophagus of King Ahiram.This script is the parent script of all western alphabets.At the tenth century two other forms can be distinguished namely Canaanite and Aramaic.The Aramaic gave rise to Hebrew. The South Arabian alphabet, a sister script to the Phoenician alphabet, is the script from which the Ge'ez alphabet (an abugida) is descended. Note that the scripts mentioned above are not considered proper alphabets, as they all lack characters representing vowels. These vowelless alphabets are called abjads, currently exemplified in scripts including Arabic, Hebrew, and Syriac.The omission of vowels was not a satisfactory solution and some "weak" consonants were used to indicate the vowel quality of a syllable.(matres lectionis).These had dual function since they were also used as pure consonants. The Proto-Sinatic or Proto Canaanite script and the Ugaritic script were the first scripts with limited number of signs, in contrast to the other widely used writing systems at the time, Cuneiform, Egyptian hieroglyphs, and Linear B. The Phoenecian script was probably the first phonemic script and it contained only about two dozen distinct letters, making it a script simple enough for common traders to learn. Another advantage of Phoenician was that it could be used to write down many different languages, since it recorded words phonemically. The script was spread by the Phoenicians, across the Mediterranean. In Greece, the script was modified to add the vowels, giving rise to the ancestor of all alphabets in the West. The indication of the vowels is the same way as the indication of the consonants, therefore it was the first true alphabet. The Greeks took letters which did not represent sounds that existed in Greek, and changed them to represent the vowels. The vowels are significant in the Greek language, and the syllabical Linear B script which was used by the Mycenean Greeks from the 16th century BC had 87 symbols including 5 vowels. In its early years, there were many variants of the Greek alphabet, a situation which caused many different alphabets to evolve from it.
The Cumae form of the Greek alphabet was carried over by Greek colonists from Euboea to the Italian peninsula, where it gave rise to a variety of alphabets used to inscribe the Italic languages. One of these became the Latin alphabet, which was spread across Europe as the Romans expanded their empire. Even after the fall of the Roman state, the alphabet survived in intellectual and religious works. It eventually became used for the descendant languages of Latin (the Romance languages) and then for most of the other languages of Europe. Another notable script is Elder Futhark, which is believed to have evolved out of one of the Old Italic alphabets. Elder Futhark gave rise to a variety of alphabets known collectively as the Runic alphabets. The Runic alphabets were used for Germanic languages from AD 100 to the late Middle Ages. Its usage was mostly restricted to engravings on stone and jewelry, although inscriptions have also been found on bone and wood. These alphabets have since been replaced with the Latin alphabet, except for decorative usage for which the runes remained in use until the 20th century. The Glagolitic alphabet was the initial script of the liturgical language Old Church Slavonic and became, together with the Greek uncial script, the basis of the Cyrillic alphabet. The Cyrillic alphabet is one of the most widely used modern alphabets, and is notable for its use in Slavic languages and also for other languages within the former Soviet Union. Variants include the Serbian, Macedonian, Bulgarian, and Russian alphabets. The Glagolitic alphabet is believed to have been created by Saints Cyril and Methodius, while the Cyrillic alphabet was invented by the Bulgarian scholar Clement of Ohrid, who was their disciple. They feature many letters that appear to have been borrowed from or influenced by the Greek alphabet and the Hebrew alphabet.
Beyond the logographic Chinese writing, many phonetic scripts are in existence in Asia. The Arabic alphabet, Hebrew alphabet, Syriac alphabet, and other abjads of the Middle East are developments of the Aramaic alphabet, but because these writing systems are largely consonant-based they are often not considered true alphabets. Most alphabetic scripts of India and Eastern Asia are descended from the Brahmi script, which is often believed to be a descendent of Aramaic. In Korea, the Hangul alphabet was created by Sejong the Great in 1443. Understanding of the phonetic alphabet of Mongolian Phagspa script aided the creation of a phonetic script suited to the spoken Korean language. Mongolian Phagspa script was in turn derived from the Brahmi script. Hangul is a unique alphabet in a variety of ways: it is a featural alphabet, where many of the letters are designed from a sound's place of articulation (P to look like widened mouth, L sound to look like tongue pulled in, etc.); its design was planned by the government of the time; and it places individual letters in syllable clusters with equal dimensions, in the same way as Chinese characters, to allow for mixed script writing (one syllable always takes up one type-space no matter how many letters get stacked into building that one sound-block). Zhuyin (sometimes called "Bopomofo") is a semi-syllabary used to phonetically transcribe Mandarin Chinese in the Republic of China. After the later establishment of the People's Republic of China and its adoption of Hanyu Pinyin, the use of Zhuyin today is limited, but it's still widely used in Taiwan where the Republic of China still governs. Zhuyin developed out of a form of Chinese shorthand based on Chinese characters in the early 1900s and has elements of both an alphabet and a syllabary. Like an alphabet the phonemes of syllable initials are represented by individual symbols, but like a syllabary the phonemes of the syllable finals are not; rather, each possible final (excluding the medial glide) is represented by its own symbol. For example, "luan" is represented as ㄌㄨㄢ ("l-u-an"), where the last symbol ㄢ represents the entire final "-an". While Zhuyin is not used as a mainstream writing system, it is still often used in ways similar to a romanization system that is, for aiding in pronunciation and as an input method for Chinese characters on computers and cell phones. European alphabets, especially Latin and Cyrillic, have been adapted for many languages of Asia. Arabic is also widely used, sometimes as an abjad (as with Urdu and Persian) and sometimes as a complete alphabet (as with Kurdish and Uyghur)
The term "alphabet" is used by linguists and paleographers in both a wide and a narrow sense. In the wider sense, an alphabet is a script that is "segmental" at the phoneme level that is, it has separate glyphs for individual sounds and not for larger units such as syllables or words. In the narrower sense, some scholars distinguish "true" alphabets from two other types of segmental script, abjads and abugidas. These three differ from each other in the way they treat vowels: abjads have letters for consonants and leave most vowels unexpressed; abugidas are also consonant-based, but indicate vowels with diacritics to or a systematic graphic modification of the consonants. In alphabets in the narrow sense, on the other hand, consonants and vowels are written as independent letters. The earliest known alphabet in the wider sense is the Wadi el-Hol script, believed to be an abjad, which through its successor Phoenician is the ancestor of modern alphabets, including Arabic, Greek, Latin (via the Old Italic alphabet), Cyrillic (via the Greek alphabet) and Hebrew (via Aramaic). Examples of present-day abjads are the Arabic and Hebrew scripts; true alphabets include Latin, Cyrillic, and Korean hangul; and abugidas are used to write Tigrinya Amharic, Hindi, and Thai. The Canadian Aboriginal syllabics are also an abugida rather than a syllabary as their name would imply, since each glyph stands for a consonant which is modified by rotation to represent the following vowel. (In a true syllabary, each consonant-vowel combination would be represented by a separate glyph.) The boundaries between the three types of segmental scripts are not always clear-cut. For example, Sorani Kurdish is written in the Arabic script, which is normally an abjad. However, in Kurdish, writing the vowels is mandatory, and full letters are used, so the script is a true alphabet. Other languages may use a Semitic abjad with mandatory vowel diacritics, effectively making them abugidas. On the other hand, the Phagspa script of the Mongol Empire was based closely on the Tibetan abugida, but all vowel marks were written after the preceding consonant rather than as diacritic marks. Although short "a" was not written, as in the Indic abugidas, one could argue that the linear arrangement made this a true alphabet. Conversely, the vowel marks of the Tigrinya abugida and the Amharic abugida (ironically, the original source of the term "abugida") have been so completely assimilated into their consonants that the modifications are no longer systematic and have to be learned as a syllabary rather than as a segmental script. Even more extreme, the Pahlavi abjad eventually became logographic. (See below.) Thus the primary classification of alphabets reflects how they treat vowels. For tonal languages, further classification can be based on their treatment of tone, though names do not yet exist to distinguish the various types. Some alphabets disregard tone entirely, especially when it does not carry a heavy functional load, as in Somali and many other languages of Africa and the Americas. Such scripts are to tone what abjads are to vowels. Most commonly, tones are indicated with diacritics, the way vowels are treated in abugidas. This is the case for Vietnamese (a true alphabet) and Thai (an abugida). In Thai, tone is determined primarily by the choice of consonant, with diacritics for disambiguation. In the Pollard script, an abugida, vowels are indicated by diacritics, but the placement of the diacritic relative to the consonant is modified to indicate the tone. More rarely, a script may have separate letters for tones, as is the case for Hmong and Zhuang. For most of these scripts, regardless of whether letters or diacritics are used, the most common tone is not marked, just as the most common vowel is not marked in Indic abugidas; in Zhuyin not only is one of the tones unmarked, but there is a diacritic to indicate lack of tone, like the virama of Indic. The number of letters in an alphabet can be quite small. The Book Pahlavi script, an abjad, had only twelve letters at one point, and may have had even fewer later on. Today the Rotokas alphabet has only twelve letters. (The Hawaiian alphabet is sometimes claimed to be as small, but it actually consists of 18 letters, including the ʻokina and five long vowels.) While Rotokas has a small alphabet because it has few phonemes to represent (just eleven), Book Pahlavi was small because many letters had been "conflated" that is, the graphic distinctions had been lost over time, and diacritics were not developed to compensate for this as they were in Arabic, another script that lost many of its distinct letter shapes. For example, a comma-shaped letter represented "g, d, y, k," or "j". However, such apparent simplifications can perversely make a script more complicated. In later Pahlavi papyri, up to half of the remaining graphic distinctions of these twelve letters were lost, and the script could no longer be read as a sequence of letters at all, but instead each word had to be learned as a whole that is, they had become logograms as in Egyptian Demotic. The largest segmental script is probably an abugida, Devanagari. When written in Devanagari, Vedic Sanskrit has an alphabet of 53 letters, including the "visarga" mark for final aspiration and special letters for "kš" and "jñ," though one of the letters is theoretical and not actually used. The Hindi alphabet must represent both Sanskrit and modern vocabulary, and so has been expanded to 58 with the "khutma" letters (letters with a dot added) to represent sounds from Persian and English. The largest known abjad is Sindhi, with 51 letters. The largest alphabets in the narrow sense include Kabardian and Abkhaz (for Cyrillic), with 58 and 56 letters, respectively, and Slovak (for the Latin alphabet), with 46. However, these scripts either count di- and tri-graphs as separate letters, as Spanish did with "ch" and "ll" until recently, or uses diacritics like Slovak "č". The largest true alphabet where each letter is graphically independent is probably Georgian, with 41 letters. Syllabaries typically contain 50 to 400 glyphs (though the Múra-Pirahã language of Brazil would require only 24 if it did not denote tone, and Rotokas would require only 30), and the glyphs of logographic systems typically number from the many hundreds into the thousands. Thus a simple count of the number of distinct symbols is an important clue to the nature of an unknown script.
It is not always clear what constitutes a distinct alphabet. French uses the same basic alphabet as English, but many of the letters can carry additional marks, such as é, à, and ô. In French, these combinations are not considered to be additional letters. However, in Icelandic, the accented letters such as á, í, and ö are considered to be distinct letters of the alphabet. In Spanish, ñ is considered a separate letter, but accented vowels such as á and é are not. The ll and ch were also considered single letters, distinct from a single l followed by an l and c followed by an h, respectively, but in 1994 the Real Academia Española changed them so that ll is between lk and lm in the dictionary and ch is between cg and ci. In German, words starting with "sch-" (constituting the German phoneme /ʃ/) would be intercalated between words with initial "sca-" and "sci-" (all incidentally loanwords) instead of this graphic cluster appearing after the letter s, as though it were a single letter – a lexicographical policy which would be de rigueur in a dictionary of Albanian, i.e. "dh-", "gj-", "ll-", "rr-", "th-", "xh-" and "zh-" (all representing phonemes and considered separate single letters) would follow the letters d, g, l, n, r, t, x and z respectively. Nor is, in a dictionary of English, the lexical section with initial "th-" reserved a place after the letter t, but is inserted between "te-" and "ti-". German words with umlaut would further be alphabetized as if there were no umlaut at all – contrary to Turkish which allegedly adopted the Swedish graphemes ö and ü, and where a word like tüfek, "gun", would come after tuz, "salt", in the dictionary. The Danish and Norwegian alphabets end with æ – ø – å, whereas the Swedish and the Finnish ones conventionally put å – ä – ö at the end. Some adaptations of the Latin alphabet are augmented with ligatures, such as æ in Old English and Icelandic and Ȣ in Algonquian; by borrowings from other alphabets, such as the thorn þ in Old English and Icelandic, which came from the Futhark runes; and by modifying existing letters, such as the eth ð of Old English and Icelandic, which is a modified "d". Other alphabets only use a subset of the Latin alphabet, such as Hawaiian, and Italian, which uses the letters "j, k, x, y" and "w" only in foreign words. It is unknown whether the earliest alphabets had a defined sequence. Some alphabets today, such as the Hanuno'o script, are learned one letter at a time, in no particular order, and are not used for collation where a definite order is required. However, a dozen Ugaritic tablets from the fourteenth century BC preserve the alphabet in two sequences. One, the "ABCDE" order later used in Phoenician, has continued with minor changes in Hebrew, Greek, Armenian, Gothic, Cyrillic, and Latin; the other, "HMĦLQ," was used in southern Arabia and is preserved today in Ethiopic. Both orders have therefore been stable for at least 3000 years. The historical order was abandoned in Runic and Arabic, although Arabic retains the traditional "abjadi order" for numbering. The Brahmic family of alphabets used in India use a unique order based on phonology: The letters are arranged according to how and where they are produced in the mouth. This organization is used in Southeast Asia, Tibet, Korean hangul, and even Japanese kana, which is not an alphabet. The Phoenician letter names, in which each letter is associated with a word that begins with that sound, continue to be used in Samaritan, Aramaic, Syriac, Hebrew, and Greek. However, they were abandoned in Arabic, Cyrillic and Latin.
Each language may establish rules that govern the association between letters and phonemes, but, depending on the language, these rules may or may not be consistently followed. In a perfectly phonological alphabet, the phonemes and letters would correspond perfectly in two directions: a writer could predict the spelling of a word given its pronunciation, and a speaker could predict the pronunciation of a word given its spelling. However, languages often evolve independently of their writing systems, and writing systems have been borrowed for languages they were not designed for, so the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language. National languages generally elect to address the problem of dialects by simply associating the alphabet with the national standard. However, with an international language with wide variations in its dialects, such as English, it would be impossible to represent the language in all its variations with a single phonetic alphabet. Some national languages like Finnish, Turkish and Bulgarian have a very regular spelling system with a nearly one-to-one correspondence between letters and phonemes. Strictly speaking, there is no word in the Finnish, Turkish and Bulgarian languages corresponding to the verb "to spell" (meaning to split a word into its letters), the closest match being a verb meaning to split a word into its syllables. Similarly, the Italian verb corresponding to 'spell', "compitare", is unknown to many Italians because the act of spelling itself is almost never needed: each phoneme of Standard Italian is represented in only one way. However, pronunciation cannot always be predicted from spelling in cases of irregular syllabic stress. In standard Spanish, it is possible to tell the pronunciation of a word from its spelling, but not vice versa; this is because certain phonemes can be represented in more than one way, but a given letter is consistently pronounced. French, with its silent letters and its heavy use of nasal vowels and elision, may seem to lack much correspondence between spelling and pronunciation, but its rules on pronunciation are actually consistent and predictable with a fair degree of accuracy. At the other extreme, are languages such as English, where the spelling of many words simply has to be memorized as they do not correspond to sounds in a consistent way. For English, this is partly because the Great Vowel Shift occurred after the orthography was established, and because English has acquired a large number of loanwords at different times, retaining their original spelling at varying levels. Even English has general, albeit complex, rules that predict pronunciation from spelling, and these rules are successful most of the time; rules to predict spelling from the pronunciation have a higher failure rate. Sometimes, countries have the written language undergo a spelling reform to realign the writing with the contemporary spoken language. These can range from simple spelling changes and word forms to switching the entire writing system itself, as when Turkey switched from the Arabic alphabet to the Roman alphabet. The sounds of speech of all languages of the world can be written by a rather small universal phonetic alphabet. A standard for this is the International Phonetic Alphabet.
In chemistry and physics, the atomic number (also known as the proton number) is the number of protons found in the nucleus of an atom and therefore identical to the charge number of the nucleus. It is conventionally represented by the symbol "Z". The atomic number uniquely identifies a chemical element. In an atom of neutral charge, the atomic number is also equal to the number of electrons. The atomic number, "Z", should not be confused with the mass number, "A", which is the total number of protons and neutrons in the nucleus of an atom. The number of neutrons, "N", is known as the neutron number of the atom; thus, "A" = "Z" + "N". Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes), the atomic mass of an atom is roughly equal to "A". Atoms having the same atomic number Z but different neutron number "N", and hence different atomic mass, are known as isotopes. Most naturally occurring elements exist as a mixture of isotopes, and the average atomic mass of this mixture determines the element's atomic weight. The current standard for the atomic mass unit (amu), also termed the dalton (Da) is defined to be exactly of the mass of a free (unbound) neutral atom in its ground (lowest-energy) state.
Loosely speaking, the existence of a periodic table creates an ordering for the elements. Such an ordering is not necessarily a numbering, but can be used to construct a numbering by fiat. Dmitri Mendeleev claimed he arranged his tables in order of atomic weight ("Atomgewicht") However, in deference to the observed chemical properties, he violated his own rule and placed tellurium (atomic weight 127.6) ahead of iodine (atomic weight 126.9). This placement is consistent with the modern practice of ordering the elements by proton number, "Z", but this number was not known or suspected at the time. A simple numbering based on periodic table position was never entirely satisfactory. Besides iodine and tellurium, several other pairs of elements (such as cobalt and nickel) were known to have nearly identical or reversed atomic weights, leaving their placement in the periodic table by chemical properties to be in violation of known physical properties. Another problem was that the gradual identification of more and more chemically similar and indistinguishable lanthanides, which were of an uncertain number, led to inconsistency and uncertainty in the numbering of all elements at least from lutetium (element 71) onwards (hafnium was not known at this time). In 1911 Ernest Rutherford gave a model of the atom in which a central core held most of the atom's mass and a positive charge which, in units of the electron's charge, was to be approximately equal to half of the atom's atomic weight, expressed in numbers of hydrogen atoms. This central charge would thus be approximately half the atomic weight (though it was almost 25% off the figure for the atomic number in gold (Z=79, A=197), the single element from which Rutherford made his guess). Nevertheless, in spite of Rutherford's estimation that gold had a central charge of about 100 (but was element Z=79 on the periodic table), a month after Rutherford's paper appeared, Antonius van den Broek first formally suggested that the central charge and number of electrons in an atom was "exactly" equal to its place in the periodic table (also known as element number, atomic number, and symbolized Z). This proved eventually to be the case. The experimental situation improved dramatically after research by Henry Moseley in 1913. Moseley, after discussions with Bohr who was at the same lab (and who had used Van den Broek's hypothesis in his Bohr model of the atom), decided to test Van den Broek and Bohr's hypothesis directly, by seeing if spectral lines emitted from excited atoms fit the Bohr theory's demand that the frequency of the spectral lines be proportional to a measure of the square of Z. To do this, Moseley measured the wavelengths of the innermost photon transitions (K and L lines) produced by the elements from aluminum (Z=13) to gold (Z= 79) used as a series of movable anodic targets inside an x-ray tube. The square root of the frequency of these photons (x-rays) increased from one target to the next in a linear fashion. This led to the conclusion (Moseley's law) that the atomic number does closely correspond (with an offset of one unit for K-lines, in Moseley's work) to the calculated electric charge of the nucleus, i.e. the proton number "Z". Among other things, Moseley demonstrated that the lanthanide series (from lanthanum to lutetium inclusive) must have 15 members — no fewer and no more — which was far from obvious from the chemistry at that time. The conventional symbol Z presumably comes from the German word "Atomz'"ahl" (atomic number).
Each element has a specific set of chemical properties as a consequence of the number of electrons present in the neutral atom, which is "Z". The configuration of these electrons follows from the principles of quantum mechanics. The number of electrons in each element's electron shells, particularly the outermost valence shell, is the primary factor in determining its chemical bonding behavior. Hence it is the atomic number alone that determines the chemical properties of an element; and it is for this reason that an element can be defined as consisting of "any" mixture of atoms with a given atomic number.
The quest for new elements is usually described using atomic numbers. As of early 2007, elements with atomic numbers 1 to 116 and 118 have been observed. Synthesis of new elements is accomplished by bombarding target atoms of heavy elements with ions, such that the sum of the atomic numbers of the target and ion elements equals the atomic number of the element being created. In general, the half-life becomes shorter as atomic number increases, though an "island of stability" may exist for undiscovered isotopes with certain numbers of protons and neutrons.
Anatomy (from the Greek " anatomia", from " ana: separate, apart from, and temnein", to cut up, cut open. Also from the Greek word "anatome"--ana: apart, tome: to cut-->To cut apart.) is a branch of biology and medicine that is the consideration of the structure of living things. It is a general term that includes human anatomy, animal anatomy (zootomy) and plant anatomy (phytotomy). In some of its facets anatomy is closely related to embryology, comparative anatomy and comparative embryology, through common roots in evolution. Anatomy is subdivided into gross anatomy (or macroscopic anatomy) and microscopic anatomy. Gross anatomy (also called topographical anatomy, regional anatomy, or anthropotomy) is the study of anatomical structures that can be seen by unaided vision with the naked eye. Microscopic anatomy is the study of minute anatomical structures assisted with microscopes, which includes histology (the study of the organization of tissues), and cytology (the study of cells). The history of anatomy has been characterized, over time, by a continually developing understanding of the functions of organs and structures in the body. Methods have also improved dramatically, advancing from examination of animals through dissection of cadavers (dead human bodies) to technologically complex techniques developed in the 20th century including X-ray, ultrasound, and MRI imaging. Anatomy should not be confused with anatomical pathology (also called morbid anatomy or histopathology), which is the study of the gross and microscopic appearances of diseased organs.
Human anatomy, including gross human anatomy and histology, is primarily the scientific study of the morphology of the adult human body. Generally, students of certain biological sciences, paramedics, physiotherapists, occupational therapy, nurses, and medical students learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures and tutorials. The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope; and in addition, medical students generally also learn gross anatomy with practical experience of dissection and inspection of cadavers (dead human bodies). Human anatomy, physiology and biochemistry are complementary basic medical sciences, which are generally taught to medical students in their first year at medical school. Human anatomy can be taught regionally or systemically; that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems. The major anatomy textbook, Gray's Anatomy, has recently been reorganized from a systems format to a regional format, in line with modern teaching methods. A thorough working knowledge of anatomy is required by all medical doctors, especially surgeons, and doctors working in some diagnostic specialities, such as histopathology and radiology. Academic human anatomists are usually employed by universities, medical schools or teaching hospitals. They are often involved in teaching anatomy, and research into certain systems, organs, tissues or cells.
An argument of this form is invalid, i.e., the conclusion can be false even when statements 1 and 2 are true. Since "P" was never asserted as the "only" sufficient condition for "Q", other factors could account for "Q" (while "P" was false). The name "affirming the consequent" derives from the premise "Q", which affirms the "then" clause of the conditional premise. Owning Fort Knox is not the "only" way to be rich. There are any number of other ways to be rich. But having the flu is not the "only" cause of a sore throat since many illnesses cause sore throat, such as the common cold or strep throat. The following is a more subtle version of the fallacy embedded into conversation. B attempts to falsify A's conditional statement ("if Republican then pro-life") by providing evidence he believes would contradict its implication. However, B's example of his uncle does not contradict A's statement, which says nothing about non-Republicans. What would be needed to disprove A's assertion are examples of Republicans who are not pro-life.
If claims "P" and "Q" express the same proposition, then the argument would be trivially valid, as it would beg the question. This is also the case for definitions. For example. In everyday discourse, however, such cases are rare. The validity of such definitions is due to the fact that definitions can be expressed as an if and only if (see below). Clearly if the definition of "bachelor" is "an unmarried male", then the propositional statement: "A is a bachelor" if and only if "A is an unmarried male", must be true. In normal speech it is awkward to use the phrase "if and only if", so we substitute the valid but less complete "if", giving the conventional form which is similar to the form of the formal fallacy.
The above argument may be valid, but only if the claim "if he's outside, then he's not inside" follows from the first premise. More to the point, the validity of the argument stems not from affirming the consequent, but affirming the antecedent. Such if and only if statements often make their way into detective mysteries. Use of the fallacy in science. However, such reasoning is still affirming the consequent and logically invalid (e.g., Let "P" = geocentrism and "Q" = sunrise and sunset.) The strength of such reasoning as an inductive inference depends on the likelihood of alternative hypotheses, which shows that such reasoning is based on additional premises, not merely on affirming the consequent. In addition, testing scientific theories involves repeated rounds of affirming the consequent as new data come in. The repetitive use eliminates competing theories (those that are inconsistent with the newest data: more technically, it is the law of contraposition that plays the role in elimination), leaving behind only theories that have proved to be consistent with all tests performed to date.
Andrei Arsenyevich Tarkovsky () (April 4, 1932–December 29, 1986) was a Soviet and Russian filmmaker, writer, film editor, film theorist and opera director. Tarkovsky's films include "Andrei Rublev", "Solaris", "The Mirror", and "Stalker". He directed the first five of his seven feature films in the Soviet Union; his last two films were produced in Italy and Sweden. They are characterized by spirituality and metaphysical themes, extremely long takes, lack of conventional dramatic structure and plot, and memorable cinematography. Ingmar Bergman said of him: "Tarkovsky for me is the greatest [director], the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream".
Tarkovsky was born in the village of Zavrazhye in Ivanovo Oblast, the son of poet and translator Arseny Alexandrovich Tarkovsky, native of Kirovohrad, Ukraine, and Maria Ivanova Vishnyakova, a graduate of the Maxim Gorky Literature Institute. Tarkovsky spent his childhood in Yuryevets. He was described by childhood friends as active and popular, having many friends and being typically in the center of action. In 1937, his father left the family, subsequently volunteering for the army in 1941. Tarkovsky stayed with his mother, moving with her and his sister Marina to Moscow, where she worked as a proofreader at a printing press. In 1939, Tarkovsky enrolled at the Moscow School № 554. During the war, the three evacuated to Yuryevets, living with his maternal grandmother. In 1943, the family returned to Moscow. Tarkovsky continued his studies at his old school, where the poet Andrey Voznesensky was one of his classmates. He learned the piano at a music school and attended classes at an art school. The family lived on Shshipok Street in the Zamoskvorechye District in Moscow. From November 1947 to spring 1948, he was in a hospital with tuberculosis. Many themes of his childhood - the evacuation, his mother and her two children, the withdrawn father, the time in the hospital - feature prominently in his film "The Mirror". Following high school graduation, from 1951 to 1952, Tarkovsky studied Arabic at the Oriental Institute in Moscow, a branch of the Academy of Sciences of the USSR. Although he already spoke some Arabic and was a successful student in his first semesters, he did not finish his studies and dropped out to work as a prospector for the Academy of Science Institute for Non-Ferrous Metals and Gold. He participated in a year-long research expedition to the river Kureikye near Turukhansk in the Krasnoyarsk Province. During this time in the Taiga Tarkovsky decided to study film.
Upon return from the research expedition in 1954, Tarkovsky applied at the State Institute of Cinematography (VGIK) and was admitted to the film-directing-program. He was in the same class as Irma Raush, whom he married in April 1957. The early Khrushchev era offered unique opportunities for young film directors. Before 1953, annual film production was low and most films were directed by veteran directors. After 1953, more films were produced, many of them by young directors. The Khrushchev Thaw opened Soviet society and allowed, to some degree, Western literature, films and music. This allowed Tarkovsky to see films of the Italian neorealists, French New Wave, and of directors such as Kurosawa, Buñuel, Bergman, Bresson and Mizoguchi. Tarkovsky absorbed the idea of the auteur as a necessary condition for creativity. Tarkovsky’s teacher and mentor was Mikhail Romm, who taught many film students who would later become influential film directors. In 1956, Tarkovsky directed his first student short film, "The Killers", from a short story of Ernest Hemingway. The short film "There Will Be No Leave Today" and the screenplay "Concentrate" followed in 1958 and 1959. An important influence on Tarkovsky was the film director Grigori Chukhrai, who was teaching at the VGIK. Impressed by the talent of his student, Chukhrai offered Tarkovsky a position as assistant director for his film "Clear Skies". Tarkovsky initially showed interest, but then decided to concentrate on his studies and his own projects. During his third year at the VGIK, Tarkovsky met Andrei Konchalovsky. They found much in common as they liked the same film directors and shared ideas on cinema and films. In 1959, they wrote the script "Antarctica - Distant Country", which was later published in the "Moskovskij Komsomolets". Tarkovsky submitted the script to Lenfilm, which was rejected. They were more successful with the script "The Steamroller and the Violin," which they sold to Mosfilm. This film became Tarkovsky’s diploma film, earning him his diploma in 1960 and winning first prize at the New York Student Film Festival in 1961. Film career in the Soviet Union. Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He had inherited the film from director Eduard Abalov, who had to abort the project. The film earned Tarkovsky international acclaim and won the Golden Lion award at the Venice Film Festival in 1962. In the same year, on September 30, his first son Arseny (called Senka in Tarkovsky's diaries) Tarkovsky was born. In 1965, he directed the film "Andrei Rublev" about the life of Andrei Rublev, the 15th century Russian icon painter. "Andrei Rublev" was not immediately released after completion due to problems with Soviet authorities. Tarkovsky had to cut the film several times, resulting in several different versions of varying lengths. A version of the film was presented at the Cannes Film Festival in 1969 and won the FIPRESCI prize. The film was officially released in the Soviet Union in a cut version in 1971. He divorced his wife, Irma Raush, in June 1970. In the same year, he married Larissa Kizilova (née Egorkina), who had been a production assistant for the film "Andrei Rublev" (they had been living together since 1965). Their son, Andrei Tarkovsky Jr., was born in the same year on August 7. In 1972, he completed "Solaris", an adaptation of the novel "Solaris" by Stanisław Lem. He had worked on this together with screenwriter Fridrikh Gorenshtein, as early as 1968. The film was presented at the Cannes Film Festival and won the Grand Prix Spécial du Jury and the FIPRESCI prize and was nominated for the Palme d'Or. From 1973 to 1974, he shot the film "The Mirror", a highly autobiographical film drawing on his childhood and incorporating some of his father's poems. Tarkovsky had worked on the screenplay for this film since 1967, under the consecutive titles "Confession", "White day" and "A white, white day". From the beginning the film was not well received by Soviet authorities due to its content and its perceived elitist nature. Russian authorities placed the film in the "third category" which meant severe limitations on its distribution, allowing it to be shown only in third class cinemas and workers' clubs. Few prints were made and the filmmakers received no returns. Third category films also placed the filmmakers in danger of being accused of wasting public funds, which could have serious effects on their future productivity. These difficulties are presumed to have made Tarkovsky play with the idea of going abroad and producing a film outside the Soviet film industry. During 1975, Tarkovsky also worked on the screenplay "Hoffmanniana", about the German writer and poet E. T. A. Hoffmann. In December 1976, he directed "Hamlet", his only stage play, at the Lenkom Theatre in Moscow. The main role was played by Anatoly Solonitsyn, who also acted in several of Tarkovsky's films. At the end of 1978, he also wrote the screenplay "Sardor" together with the writer Aleksandr Misharin. The last film Tarkovsky completed in the Soviet Union was "Stalker", inspired by the novel "Roadside Picnic" by the brothers Arkady and Boris Strugatsky. Tarkovsky had met the brothers first in 1971 and was in contact with them until his death in 1986. Initially he wanted to shoot a film based on their novel "Dead Mountaineer's Hotel" and he developed a raw script. Influenced by a discussion with Arkady Strugatsky he changed his plan and began to work on the script based on "Roadside Picnic". Work on this film began in 1976. The production was mired in troubles; improper development of the negatives had ruined all the exterior shots. Tarkovsky's relationship with cinematographer Georgy Rerberg deteriorated to the point where Tarkovsky hired Alexander Knyazhinsky as a new first cinematographer. Furthermore, Tarkovsky suffered a heart attack in April 1978, resulting in further delay. The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival. In the same year Tarkovsky also began the production of the film "The First Day" (Russian: Pervyy Dyen), based on a script by his friend and longterm collaborator Andrei Konchalovsky. The film was set in 18th century Russia during the reign of Peter the Great and starred Natalya Bondarchuk and Anatoli Papanov in the main role. To get the project approved by Goskino, Tarkovsky submitted a script that was different from the original script, leaving out several scenes that were critical of the official atheism in the Soviet Union. After finishing shooting of roughly one half of the film, the project was stopped by Goskino, after it became apparent that the film differed from the script submitted to the censors. Tarkovsky was reportedly infuriated by this interruption and destroyed most of the film. Film career outside the Soviet Union. During the summer of 1979, Tarkovsky traveled to Italy, where he shot the documentary "Voyage in Time", together with his longtime friend Tonino Guerra. Tarkovsky returned to Italy in 1980 for an extended trip during which he and Tonino Guerra completed the script for the film "Nostalghia". During 1981 he traveled to the United Kingdom and Sweden. During his trip to Sweden he had considered defecting from the Soviet Union, but ultimately decided to return because of his wife and his son. Tarkovsky returned to Italy in 1982 to start shooting "Nostalghia". He did not return to his home country. As Mosfilm withdrew from the project, he had to complete the film with financial support provided by the Italian RAI. Tarkovsky completed the film in 1983. "Nostalghia" was presented at the Cannes Film Festival and won the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury. Soviet authorities prevented the film from winning the Palme d'Or, a fact that hardened Tarkovsky's resolve to never work in the Soviet Union again. In the same year, he also arranged the opera "Boris Godunov" at the Royal Opera House in London under the musical direction of Claudio Abbado. He spent most of 1984 preparing the film "The Sacrifice". At a press conference in Milan on July 10, 1984, he announced that he would never return to the Soviet Union and would remain in the West. At that time, his son Andrei Jr. was still in the Soviet Union and not allowed to leave the country. During 1985, he shot the film "The Sacrifice" in Sweden. At the end of the year he was diagnosed with terminal lung cancer. In January 1986, he began treatment in Paris, and was joined there by his wife and his son, who were finally allowed to leave the Soviet Union. "The Sacrifice" was presented at the Cannes Film Festival and received the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury. As Tarkovsky was unable to attend due to his illness, the prizes were collected by his son, Andrei Jr. In Tarkovsky's last diary entry (December 15, 1986), he wrote: "But now I have no strength left - that is the problem". The diaries are sometimes also known as "Martyrolog" and were published posthumously in 1989 and in English in 1991. Tarkovsky died in Paris on December 29, 1986. He was buried on January 3, 1987 in the Russian Cemetery in Sainte-Geneviève-des-Bois in France. The inscription on his grave stone, which was created by the Russian sculptor Ernst Neizvestny, reads: "To the man who saw the Angel". A controversy emerged in Russia in the early 1990s when it was alleged that Tarkovsky did not die of natural causes, but was assassinated by the KGB. Evidence for this hypothesis includes several testimonies by former KGB agents, who claim that Viktor Chebrikov gave the order to irradiate Tarkovsky to prevent what the Soviet government and the KGB saw as anti-Soviet propaganda by Tarkovsky. Other evidence includes several memos that surfaced after the 1991 coup and the claim by one of Tarkovsky's doctors that his cancer could not have developed from a natural cause. As Tarkovsky, his wife Larisa Tarkovskaya and actor Anatoli Solonitsyn all died from the very same type of lung cancer, Vladimir Sharun, sound designer in "Stalker", is convinced that they were all poisoned when shooting the film near a chemical plant.
Tarkovsky is mainly known as a director of films. During his career he directed only seven feature films, and three short films during his time at the film school. He also wrote several screenplays, directed the play "Hamlet" for the stage in Moscow, the opera "Boris Godunov" in London, and directed a radio production of the short story "Turnabout" by William Faulkner. He also wrote "Sculpting In Time", a book on film theory. Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He then directed in the Soviet Union "Andrei Rublev" in 1966, "Solaris" in 1972, "The Mirror" in 1975 and "Stalker" in 1979. The documentary "Voyage in Time" was produced in Italy in 1982, as was "Nostalghia" in 1983. His last film "The Sacrifice" was produced in Sweden in 1986. Tarkovsky was personally involved in writing the screenplays for all his films, sometimes with a co-writer. To Tarkovsky a director who realizes somebody else's screenplay without being involved in it becomes a mere illustrator, resulting in dead and monotonous films.
Numerous awards were bestowed on Tarkovsky throughout his lifetime. At the Venice Film Festival he was awarded the "Golden Lion". At the Cannes Film Festival he won several times the "FIPRESCI prize", the "Prize of the Ecumenical Jury" and the "Grand Prix Spécial du Jury". He was also nominated for the "Palme d'Or" two times. In 1987, the British Academy of Film and Television Arts awarded the BAFTA Award for Best Foreign Language Film to "The Sacrifice". Under the influence of Glasnost and Perestroika, Tarkovsky was finally recognized in the Soviet Union in the fall of 1986, shortly before his death, by a retrospective of his films in Moscow. After his death, an entire issue of the film magazine "Iskusstvo Kino" was devoted to Tarkovsky. In their obituaries, the film committee of the Council of Ministers of the USSR and the Union of Soviet Film Makers expressed their sorrow that Tarkovsky had to spend the last years of his life in exile. Posthumously, he was awarded the Lenin Prize in 1990, one of the highest state honors in the Soviet Union. In 1989 the "Andrei Tarkovsky Memorial Prize" was established, with its first recipient being the Russian animator Yuriy Norshteyn. Since 1993, the Moscow International Film Festival awards the annual "Andrei Tarkovsky Award". In 1996 the Andrei Tarkovsky Museum opened in Yuryevets, his childhood town. A minor planet, 3345 Tarkovskij, discovered by Soviet astronomer Lyudmila Georgievna Karachkina in 1982, has also been named after him. Tarkovsky has been the subject of several documentaries. Most notable is the 1988 documentary "Moscow Elegy", by Russian film director Alexander Sokurov. Sokurov's own work has been heavily influenced by Tarkovsky. The film consists mostly of narration over stock footage from Tarkovsky's films. "Directed by Andrei Tarkovsky" is 1988 documentary film by Michal Leszczylowski, an editor of the film "The Sacrifice". Film director Chris Marker produced the television documentary "One Day in the Life of Andrei Arsenevich" as an homage to Andrei Tarkovsky in 2000. Tarkovsky is widely considered to be one of the greatest film makers of all time. Ingmar Bergman was quoted as saying: "Tarkovsky for me is the greatest [of us all], the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream". Film historian Steven Dillon claims that much of subsequent film was deeply influenced by the films of Tarkovsky.
Tarkovsky became a film director during the mid and late 1950s, a period during which Soviet society opened to foreign films, literature and music. This allowed Tarkovsky to see films of European, American and Japanese directors, an experience which influenced his own film making. His teacher and mentor at the film school, Mikhail Romm, allowed his students considerable freedom and emphasized the independence of the film director. Tarkovsky was, according to Shavka Abdusalmov, a fellow student at the film school, fascinated by Japanese films. He was amazed by how every character on the screen is exceptional and how everyday events such as a Samurai cutting bread with his sword are elevated to something special and put into the limelight. Tarkovsky has also expressed interest in the art of Haiku and its ability to create “images in such a way that they mean nothing beyond themselves.” In 1972, Tarkovsky told film historian Leonid Kozlov his ten favorite films. The list includes: "Diary of a Country Priest" and "Mouchette", by Robert Bresson; "Winter Light", "Wild Strawberries" and "Persona", by Ingmar Bergman; "Nazarin", by Luis Buñuel; "City Lights", by Charlie Chaplin; "Ugetsu", by Kenji Mizoguchi; "Seven Samurai", by Akira Kurosawa, and "Woman in the Dunes", by Hiroshi Teshigahara. Among his favorite directors were Luis Buñuel, Kenji Mizoguchi, Ingmar Bergman, Robert Bresson, Akira Kurosawa, Michelangelo Antonioni, Jean Vigo and Carl Theodor Dreyer. With the exception of "City Lights", the list does not contain any films of the early silent era. The reason is that Tarkovsky saw film as an art as only a relatively recent phenomenon, with the early film-making forming only a prelude. The list has also no films or directors from Tarkovsky's native Russia, although he rated Soviet directors such as Boris Barnet, Sergei Paradjanov and Alexander Dovzhenko highly. Although strongly opposed to commercial cinema, in a famous exception Tarkovsky praised the blockbuster film "The Terminator", saying its "vision of the future and the relation between man and its destiny is pushing the frontier of cinema as an art". He was critical of the "brutality and low acting skills", but nevertheless impressed by this film.
Tarkovsky's films are characterised by Christian and metaphysical themes, extremely long takes, and memorable images of exceptional beauty. Recurring motifs are dreams, memory, childhood, running water accompanied by fire, rain indoors, reflections, levitation, and characters re-appearing in the foreground of long panning movements of the camera. Tarkovsky included levitation scenes into several of his films, most notably "Solaris". To him these scenes possess great power and are used for their photogenic value and magical inexplicability. Water, clouds, and reflections were used by him for its surreal beauty and photogenic value, as well as its symbolism, such as waves or the form of brooks or running water. Bells and candles are also frequent symbols. These are symbols of film, sight and sound, and Tarkovsky's film frequently has themes of self reflection. Tarkovsky developed a theory of cinema that he called "sculpting in time". By this he meant that the unique characteristic of cinema as a medium was to take our experience of time and alter it. Unedited movie footage transcribes time in real time. By using long takes and few cuts in his films, he aimed to give the viewers a sense of time passing, time lost, and the relationship of one moment in time to another. Up to, and including, his film "The Mirror", Tarkovsky focused his cinematic works on exploring this theory. After "The Mirror", he announced that he would focus his work on exploring the dramatic unities proposed by Aristotle: a concentrated action, happening in one place, within the span of a single day. Several of Tarkovsky's films have color or black and white sequences, including for example "Andrei Rublev" which features an epilogue in color of religious icon paintings, as well as "Solaris", "The Mirror", and "Stalker", which feature monochrome and sepia sequences while otherwise being in color. In 1966, in an interview conducted shortly after finishing "Andrei Rublev", Tarkovsky dismissed color film as a "commercial gimmick" and cast doubt on the idea that contemporary films meaningfully use color. He claimed that in everyday life one does not consciously notice colors most of the time. Hence in film color should be used mainly to emphasize certain moments, but not all the time as this distracts the viewer. To him, films in color are like moving paintings or photographs, which are too beautiful to be a realistic depiction of life. The natural elements play a large role in Tarkovsky's films. The soundtracks often contain the sounds of water dripping while the earth seems to be perpetually damp. Fire and water are usually represented together, the burning barn from "The Mirror" and candle in "Nostalghia" being two examples. "The Mirror", "Stalker", and "Nostalghia" all contain scenes in which one or several characters lay on the earth in contemplation. Wind is also used often in "The Mirror". This emphasis of moments in nature, as well as the theory of "sculpting in time" has been cited by the remodernist film movement as a major influence on their own ideas on filmmaking.
Ambiguity is the property of being ambiguous, where a word, term, notation, sign, symbol, phrase, sentence, or any other form used for communication, is called ambiguous if it can be interpreted in more than one way. Ambiguity is different from vagueness, which arises when the boundaries of meaning are indistinct. Ambiguity is context-dependent: the same linguistic item (be it a word, phrase, or sentence) may be ambiguous in one context and unambiguous in another context. For a word, ambiguity typically refers to an unclear choice between different definitions as may be found in a dictionary. A sentence may be ambiguous due to different ways of parsing the same sequence of words.
The lexical ambiguity of a word or phrase consists in its having more than one meaning in the language to which the word belongs. "Meaning" hereby refers to whatever should be captured by a good dictionary. For instance, the word “bank” has several distinct lexical definitions, including “financial institution” and “edge of a river”. Another example is as in apothecary. You could say "I bought herbs from the apothecary." This could mean you actually spoke to the apothecary (pharmacist) or went to the apothecary (drug store). The context in which an ambiguous word is used often makes it evident which of the meanings is intended. If, for instance, someone says “I deposited $100 in the bank,” most people would not think you used a shovel to dig in the mud. However, some linguistic contexts do not provide sufficient information to disambiguate a used word. For example, "Biweekly" can mean "fortnightly" (once every two weeks - 26 times a year), OR "twice a week" (104 times a year). If "biweekly" is used in a conversation about a meeting schedule, it may be difficult to infer which meaning was intended. Many people believe that such lexically ambiguous, miscommunication-prone words should be avoided wherever possible, since the user generally has to waste time, effort, and attention span to define what is meant when they are used. The use of multi-defined words requires the author or speaker to clarify their context, and sometimes elaborate on their specific intended meaning (in which case, a less ambiguous term should have been used). The goal of clear concise communication is that the receiver(s) have no misunderstanding about what was meant to be conveyed. An exception to this could include a politician whose "wiggle words" and obfuscation are necessary to gain support from multiple constituents with mutually exclusive conflicting desires from their candidate of choice. Ambiguity is a powerful tool of political science. More problematic are words whose senses express closely related concepts. “Good,” for example, can mean “useful” or “functional” ("That’s a good hammer"), “exemplary” ("She’s a good student"), “pleasing” ("This is good soup"), “moral” ("a good person" versus "the lesson to be learned from a story"), "righteous", etc. “I have a good daughter” is not clear about which sense is intended. The various ways to apply prefixes and suffixes can also create ambiguity (“unlockable” can mean “capable of being unlocked” or “impossible to lock”). Syntactic ambiguity arises when a complex phrase or a sentence can be parsed in more than one way. “He ate the cookies on the couch,” for example, could mean that he ate those cookies which were on the couch (as opposed to those that were on the table), or it could mean that he was sitting on the couch when he ate the cookies. Spoken language can contain many more types of ambiguities, where there is more than one way to compose a set of sounds into words, for example “ice cream” and “I scream.” Such ambiguity is generally resolved according to the context. A mishearing of such, based on incorrectly resolved ambiguity, is called a mondegreen. Semantic ambiguity arises when a word or concept has an inherently diffuse meaning based on widespread or informal usage. This is often the case, for example, with idiomatic expressions whose definitions are rarely or never well-defined, and are presented in the context of a larger argument that invites a conclusion. For example, “You could do with a new automobile. How about a test drive?” The clause “You could do with” presents a statement with such wide possible interpretation as to be essentially meaningless. Lexical ambiguity is contrasted with semantic ambiguity. The former represents a choice between a finite number of known and meaningful context-dependent interpretations. The latter represents a choice between any number of possible interpretations, none of which may have a standard agreed-upon meaning. This form of ambiguity is closely related to vagueness. Linguistic ambiguity can be a problem in law (see Ambiguity (law)), because the interpretation of written documents and oral agreements is often of paramount importance.
Philosophers (and other users of logic) spend a lot of time and effort searching for and removing (or intentionally adding) ambiguity in arguments, because it can lead to incorrect conclusions and can be used to deliberately conceal bad arguments. For example, a politician might say “I oppose taxes that hinder economic growth.” Some will think he opposes taxes in general, because they hinder economic growth. Others may think he opposes only those taxes that he believes will hinder economic growth. In writing, the correct insertion or omission of a comma after “taxes” and the use of "which" can help reduce ambiguity here (for the first meaning, “, which” is properly used in place of “that”), or the sentence can be restructured to completely eliminate possible misinterpretation. The devious politician hopes that each constituent (politics) will interpret the above statement in the most desirable way, and think the politician supports everyone's opinion. However, the opposite can also be true - An opponent can turn a positive statement into a bad one, if the speaker uses ambiguity (intentionally or not). The logical fallacies of amphiboly and equivocation rely heavily on the use of ambiguous words and phrases. In literature and rhetoric, on the other hand, ambiguity can be a useful tool. Groucho Marx’s classic joke depends on a grammatical ambiguity for its humor, for example: “Last night I shot an elephant in my pajamas. What he was doing in my pajamas I’ll never know.” Ambiguity can also be used as a comic device through a genuine intention to confuse, as does Magic: The Gathering's Unhinged © Ambiguity, which makes puns with homophones, mispunctuation, and run-ons: “Whenever a player plays a spell that counters a spell that has been played[,] or a player plays a spell that comes into play with counters, that player may counter the next spell played[,] or put an additional counter on a permanent that has already been played, but not countered.” Songs and poetry often rely on ambiguous words for artistic effect, as in the song title “Don’t It Make My Brown Eyes Blue” (where “blue” can refer to the color, or to sadness). In narrative, ambiguity can be introduced in several ways: motive, plot, character. F. Scott Fitzgerald uses the latter type of ambiguity with notable effect in his novel "The Great Gatsby". All religions debate the orthodoxy or heterodoxy of ambiguity. Christianity and Judaism employ the concept of paradox synonymously with 'ambiguity'. Ambiguity within Christianity (and other religions) is resisted by the conservatives and fundamentalists, who regard the concept as equating with 'contradiction'. Non-fundamentalist Christians and Jews endorse Rudolf Otto's description of the sacred as 'mysterium tremendum et fascinans', the awe-inspiring mystery which fascinates humans. Metonymy involves the use of the name of a subcomponent part as an abbreviation, or jargon, for the name of the whole object (for example "wheels" to refer to a car, or "flowers" to refer to beautiful offspring, an entire plant, or a collection of blooming plants). In modern vocabulary critical semiotics, metonymy encompasses any potentially ambiguous word substitution that is based on contextual contiguity (located close together), or a function or process that an object performs, such as "sweet ride" to refer to a nice car. Metonym miscommunication is considered a primary mechanism of linguistic humour.
In sociology and social psychology, the term "ambiguity" is used to indicate situations that involve uncertainty. An increasing amount of research is concentrating on how people react and respond to ambiguous situations. Much of this focuses on ambiguity tolerance. A number of correlations have been found between an individual’s reaction and tolerance to ambiguity and a range of factors. Apter and Desselles (2001) for example, found a strong correlation with such attributes and factors like a greater preference for safe as opposed to risk-based sports, a preference for endurance-type activities as opposed to explosive activities, a more organized and less casual lifestyle, greater care and precision in descriptions, a lower sensitivity to emotional and unpleasant words, a less acute sense of humor, engaging a smaller variety of sexual practices than their more risk-comfortable colleagues, a lower likelihood of the use of drugs, pornography and drink, a greater likelihood of displaying obsessional behavior. In the field of leadership David Wilkinson (2006) found strong correlations between an individual leader's reaction to ambiguous situations and the Modes of Leadership they use, the type of creativity (Kirton (2003) and how they relate to others.
In music, pieces or sections which confound expectations and may be or are interpreted simultaneously in different ways are ambiguous, such as some polytonality, polymeter, other ambiguous meters or rhythms, and ambiguous phrasing, or (Stein 2005, p. 79) any aspect of music. The music of Africa is often purposely ambiguous. To quote Sir Donald Francis Tovey (1935, p. 195), “Theorists are apt to vex themselves with vain efforts to remove uncertainty just where it has a high aesthetic value.”
In visual art, certain images are visually ambiguous, such as the Necker cube, which can be interpreted in two ways. Perceptions of such objects remain stable for a time, then may flip, a phenomenon called multistable perception. The opposite of such ambiguous images are impossible objects. Pictures or photographs may also be ambiguous at the semantic level: the visual image is unambiguous, but the meaning and narrative may be ambiguous: is a certain facial expression one of excitement or fear, for instance?
Some languages have been created with the intention of avoiding ambiguity, especially lexical ambiguity. Lojban and Loglan are two related languages which have been created with this in mind. The languages can be both spoken and written. These languages are intended to provide a greater technical precision over big natural languages, although historically, such attempts at language improvement have been criticized. Languages composed from many diverse sources contain much ambiguity and inconsistency. The many exceptions to syntax and semantic rules are time-consuming and difficult to learn.
Ambiguous expressions often appear in physical and mathematical texts. It is common practice to omit multiplication signs in mathematical expressions. Also, it is common, to give the same name to a variable and a function, for example, formula_1. Then, if one sees formula_2, there is no way to distinguish, does it mean formula_1 multiplied by formula_4, or function formula_5 evaluated at argument equal to formula_4. In each case of use of such notations, the reader is supposed to be able to perform the deduction and reveal the true meaning. Creators of algorithmic languages try to avoid ambiguities. Many algorithmic languages (C++, MATLAB, Fortran) require the character * as symbol of multiplication. The language Mathematica allows the user to omit the multiplication symbol, but requires square brackets to indicate the argument of a function; square brackets are not allowed for grouping of expressions. Fortran, in addition, does not allow use of the same name (identifier) for different objects, for example, function and variable; in particular, the expression f=f(x) is qualified as an error. The order of operations may depend on the context. In most programming languages, the operations of division and multiplication have equal priority and are executed from left to right. Until the last century, many editorials assumed that multiplication is performed first, for example, formula_7 is interpreted as formula_8; in this case, the insertion of parentheses is required when translating the formulas to an algorithmic language. In addition, it is common to write an argument of a function without parenthesis, which also may lead to ambiguity. Sometimes, one uses "italics" letters to denote elementary functions. In the scientific journal style, the expression
formula_13, although in a slideshow, it may mean formula_14. Comma in subscripts and superscripts sometimes is omitted; it is also ambiguous notation. If it is written formula_15, the reader should guess from the context, does it mean a single-index object, evaluated while the subscript is equal to product of variables formula_16, formula_12 and formula_18, or it is indication to a three-valent tensor. The writing of formula_15 instead of formula_20 may mean that the writer either is stretched in space (for example, to reduce the publication fees, or aims to increase number of publications without considering readers. The same may apply to any other use of ambiguous notations. Subscripts are also used to denote the argument to a function, as in formula_21. Examples of potentially confusing ambiguous mathematical expressions. formula_22, which could be understood to mean either formula_23 or formula_24. In addition, formula_25 may mean formula_26, as formula_27 means formula_28 (see tetration). formula_29, which by convention means formula_30, though it might be thought to mean formula_31 since formula_32 means formula_33. formula_34, which arguably should mean formula_35 but would commonly be understood to mean formula_36 Notations in quantum optics and quantum mechanics. It is common to define the coherent states in quantum optics with formula_37 and states with fixed number of photons with formula_38. Then, there is an "unwritten rule": the state is coherent if there are more Greek characters than Latin characters in the argument, and formula_12photon state if the Latin characters dominate. The ambiguity becomes even worse, if formula_40 is used for the states with certain value of the coordinate, and formula_41 means the state with certain value of the momentum, which may be used in books on quantum mechanics. Such ambiguities easy lead to confusions, especially if some normalized adimensional, dimensionless variables are used. Expression formula_42 may mean a state with single photon, or the coherent state with mean amplitude equal to 1, or state with momentum equal to unity, and so on. The reader is supposed to guess from the context. Ambiguous terms in physics and mathematics. Some physical quantities do not yet have established notations; their value (and sometimes even dimension, as in the case of the Einstein coefficients) depends on the system of notations. Many terms are ambiguous. Each use of an ambiguous term should be preceded by the definition, suitable for a specific case. A highly confusing term is "gain". For example, the sentence "the gain of a system should be doubled", without context, means close to nothing. It may mean that the ratio of the output voltage of an electric circuit to the input voltage should be doubled. It may mean that the ratio of the output power of an electric or optical circuit to the input power should be doubled. It may mean that the gain of the laser medium should be doubled, for example, doubling the population of the upper laser level in a quasi-two level system (assuming negligible absorption of the ground-state). The term "intensity" is ambiguous when applied to light. The term can refer to any of irradiance, luminous intensity, radiant intensity, or radiance, depending on the background of the person using the term. Also, confusions may be related with the use of atomic percent as measure of concentration of a dopant, or resolution of an imaging system, as measure of the size of the smallest detail which still can be resolved at the background of statistical noise. See also Accuracy and precision and its talk. The Berry paradox arises as a result of systematic ambiguity in the meaning of terms such as "definable" or "nameable". Terms of this kind give rise to vicious circle fallacies. Other terms with this type of ambiguity are: satisfiable, true, false, function, property, class, relation, cardinal, and ordinal.
In mathematics and logic, ambiguity can be considered to be an "underdetermined system" (of equations or logic) – for example, formula_43 leaves open what the value of "X" is – while its opposite is a self-contradiction, also called inconsistency, paradoxicalness, or oxymoron, in an overdetermined system – such as formula_44, which has no solution – see also underdetermination. Logical ambiguity and self-contradiction is analogous to visual ambiguity and impossible objects, such as the Necker cube and impossible cube, or many of the drawings of M. C. Escher. Pedagogic use of ambiguous expressions. Ambiguity can be used as a pedagogical trick, to force students to reproduce the deduction by themselves. Some textbooks Rigorously speaking, such an expression requires that formula_46; even if function formula_47 is a self-Fourier function, the expression should be written as formula_48; however, it is assumed that the shape of the function (and even its norm formula_49) depend on the character used to denote its argument. If the Greek letter is used, it is assumed to be a Fourier transform of another function, The first function is assumed, if the expression in the argument contains more characters formula_50 or formula_51, than characters formula_52, and the second function is assumed in the opposite case. Expressions like formula_53 or formula_54 contain symbols formula_50 and formula_52 in equal amounts; they are ambiguous and should be avoided in serious deduction.
The Aardvark ("Orycteropus afer") (afer: from Africa) is a medium-sized, burrowing, nocturnal mammal native to Africa. It is the only living species of all Tubulidentata, but there are known other prehistoric species and genera of Tubulidentata. It is sometimes called "antbear", "anteater", "Cape anteater" (after the Cape of Good Hope), "earth hog" or "earth pig". The word "aardvark" is famous for being one of the first entries to appear in many encyclopaedias and even abridged dictionaries. The name comes from the Afrikaans/Dutch for "earth pig" or "ground pig" ("aarde" earth/ground, "varken" pig), because early settlers from Europe thought it resembled a domesticated pig. However, the aardvark is not closely related to the pig; rather, it is the sole recent representative of the obscure mammalian order Tubulidentata, in which it is usually considered to form a single variable species of the genus "Orycteropus", coextensive with the family Orycteropodidae. The aardvark is not closely related to the South American anteater, despite sharing some characteristics and a superficial resemblance. The closest living relatives of the aardvark are the elephant shrews, along with the sirenians, hyraxes, tenrecs, and elephants. Together, these animals form the superorder Afrotheria.
Genetically speaking, the aardvark is a living fossil, as its chromosomes are highly conserved, reflecting much of the early eutherian arrangement before the divergence of the major modern taxa. The aardvark is vaguely pig-like in appearance. Its body is stout with an arched back and is sparsely covered with coarse hairs. The limbs are of moderate length. The front feet have lost the pollex (or 'thumb') — resulting in four toes — but the rear feet have all five toes. Each toe bears a large, robust nail which is somewhat flattened and shovel-like, and appears to be intermediate between a claw and a hoof. The ears are disproportionately long, and the tail is very thick at the base and gradually tapers. The greatly elongated head is set on a short, thick neck, and the end of the snout bears a disc, which houses the nostrils. The mouth is small and tubular, typical of species that feed on termites. The aardvark has a long, thin, snakelike, protruding tongue and elaborate structures supporting a keen sense of smell. An aardvark's weight is typically between 40 and 65 kg. An aardvark's length is usually between 1 and 1.3 metres, and can reach lengths of 2.2 metres when its tail (which can be up to 70 centimetres) is taken into account. The aardvark is pale yellowish gray in color and often stained reddish-brown by soil. The aardvark's coat is thin and the animal's primary protection is its tough skin. The aardvark has been known to sleep in a recently excavated ant nest, which also serves as protection from its predators.
The aardvark is nocturnal and is a solitary creature that feeds almost exclusively on ants and termites (formicivore); the only fruit eaten by aardvarks is the aardvark cucumber. An aardvark emerges from its burrow in the late afternoon or shortly after sunset, and forages over a considerable home range encompassing 10 to 30 kilometers, swinging its long nose from side to side to pick up the scent of food. When a concentration of ants or termites is detected, the aardvark digs into it with its powerful front legs, keeping its long ears upright to listen for predators, and takes up an astonishing number of insects with its long, sticky tongue—as many as 50,000 in one night have been recorded. It is an exceptionally fast digger, but otherwise moves fairly slowly. Its claws enable it to dig through the extremely hard crust of a termite or ant mound quickly, avoiding the dust by sealing the nostrils. When successful, the aardvark's long (as long as 30 centimeters) tongue licks up the insects; the termites' biting, or the ants' stinging attacks are rendered futile by the tough skin. Its keen hearing warns it of predators: lions, leopards, hyenas, and pythons. Aside from digging out ants and termites, the aardvark also excavates burrows in which to live: temporary sites are scattered around the home range as refuges, and a main burrow is used for breeding. Main burrows can be deep and extensive, have several entrances and can be as long as 13 meters. The aardvark changes the layout of its home burrow regularly, and from time to time moves on and makes a new one; the old burrows are then inhabited by smaller animals like the African Wild Dog. Only mothers and young share burrows. If attacked in the tunnel, it will seal the tunnel off behind itself or turn around and attack with its claws. Aardvarks only pair during the breeding season; after a gestation period of 7 months, a single cub weighing around 2 kg is born, and is able to leave the burrow to accompany its mother after only two weeks, and is eating termites at 14 weeks and is weaned by 16 weeks. At six months of age it is able to dig its own burrows, but it will often remain with the mother until the next mating season, and is sexually capable by the season after that. Aardvarks can live to be over 24 years old in captivity. The aardvark's main predators are lions, leopards, hunting dogs and pythons. Aardvarks can dig fast or run in zigzag fashion to elude enemies, but if all else fails, they will strike with their claws, tail and shoulders, sometimes flipping onto their backs to lash with all fours. Their thick skin also protects them to some extent.
In African folklore the aardvark is much admired because of its diligent quest for food and its fearless response to soldier ants. Hausa magicians make a charm from the heart, skin, forehead, and nails of the aardvark, which they then proceed to pound together with the root of a certain tree. Wrapped in a piece of skin and worn on the chest the charm is said to give the owner the ability to pass through walls or roofs at night. The charm is said to be used by burglars and those seeking to visit young girls without their parents' permission. The main character of Arthur, a popular animated television series for children produced by WGBH-TV and shown in more than 100 countries, is an aardvark. One of the main characters of The Ant and the Aardvark, is a blue aardvark, voiced by John Byner, doing an impersonation of Jackie Mason. It depicts the Aardvark attempting, and failing, to catch and eat his antagonist, the Ant. Cerebus the Aardvark was the title character of a comic-book series by Dave Sim and Gerhard that ran from 1977 to 2004, and is still sold in collected volumes of reprints. During character development for what would eventually be Spiderman, Stan Lee and Steve Ditko originally wanted a mammalian themed superhero to add to the Marvel Universe. Preliminary ideas led to the creation of "Aardvarkman," a superhero with the ability to control hordes of soldier ants. However, in the words of Ditko "the idea of a long gross tongue seemed downright evil in nature; a tongue that long with a mouth that small seems ridiculous." The soldier ant theme eventually led to the creation of Spider-Man, and the supervillain Toad was eventually created from the failed artwork for Aardvarkman
The aardwolf ("Proteles cristata") is a small, insectivorous hyena-like mammal, native to Eastern and Southern Africa. The name means "earth wolf" in Afrikaans/Dutch. It is also called "maanhaar-jackal" and "protelid". Unlike other hyenas, the diet of the aardwolf almost completely consists of termites, other insect larvae and carrion. The aardwolf is the only surviving species of the subfamily Protelinae. Two subspecies are recognized: "Proteles cristatus cristatus" of Southern Africa, and "Proteles cristatus septentrionalis" of eastern and northeastern Africa. It is usually placed in the Hyaenidae, though formerly separated into a monotypic family, Protelidae. The aardwolf lives in the scrublands of eastern and southern Africa. These are the areas of land covered with stunted trees or shrubs. The aardwolf hides in a burrow during the day and comes out at night to search for food. It is related to hyenas, but unlike its relatives, it does not hunt large prey. This unusual animal is a mass killer-of insects. It feeds mainly on termites and can eat more than 200,000 in a single night, using its long, sticky tongue to collect them.
The aardwolf looks most like the Striped Hyena, but is significantly smaller with a more slender muzzle, sharper ears utilized in the hunt for harvester termites, black vertical stripes on a coat of yellowish fur, and a long, distinct mane down the middle line of the neck and back, which is raised during a confrontation to make the aardwolf's size appear bigger. It is 55–80 cm long, excluding its bushy 20–30 cm tail, stands about 40–50 cm at the shoulder, and weighs between 9 and 14 kg. Its front feet have 5 toes, unlike other hyenas which have four toes. Its teeth and skull are similar to that of the hyena, although the cheek teeth are specialised for eating insects, and its tongue for licking them up. As the aardwolf ages, it will normally lose some of its teeth, though this has little impact on their feeding habits due to the soft nature of the insects they consume. It has two glands at the rear that secrete a musky fluid for marking territory and communicating with other aardwolves.
The aardwolf lives on open, dry plains and bushland, while avoiding mountainous areas. Due to its specific food requirements, the animal is only found in regions where termites of the family Hodotermitidae occur. Termites of this family depend on dead and withered grass and are most populous in heavily grazed grasslands and savannahs, including farmland. For most of the year, aardwolves spend time in shared territories consisting of up to a dozen dens which are occupied for six weeks at a time. There are two distinct populations: one in Southern Africa, and another in East and Northeast Africa. The species does not occur in the intermediary miombo forests.
Aardwolves are shy and nocturnal, sleeping in underground burrows by day. They usually use existing burrows of aardvarks, Old World porcupines or springhares, despite being capable of creating their own. By night, an aardwolf can consume up to 200,000 harvester termites using its sticky, long tongue. They take special care not to destroy the termite mound or consume the entire colony, which ensures that the termites can rebuild and provide a continuous supply of food. They will often memorise and return to nests to save the trouble of finding a new one. They are also known to feed on other insects, larvae, and eggs, and occasionally small mammals and birds. Unlike other hyenas, aardwolves do not scavenge or kill larger animals. The adult aardwolf is primarily solitary while foraging for food, necessary because of the scarcity and homogeneous distribution of their insect prey. They have often been mistaken for solitary animals. In fact, they live as monogamous pairs, with their young, defending the same territory. Young aardwolves generally achieve sexual maturity after two years, and the breeding season varies depending on their location, but normally takes place during the autumn or spring. During the breeding season, unpaired male aardwolves will search their own territory as well as others' for a female to mate with. Dominant males will also mate opportunistically with the females of less dominant neighboring aardwolves. This can often result in conflict between two male aardwolves when one has wandered into another's territory. Gestation lasts between 90 and 110 days, producing one to five cubs (most often two or three) during the rainy season, when termites are active. The first six to eight weeks are spent in the den with the mother. After three months, they begin supervised foraging and by four months are normally independent. However, they will often use the same den as their mother until the next breeding season. They can achieve a lifespan of up to 15 years when in captivity.
Adobe is a natural building material made from sand, clay, horse manure and water, with some kind of fibrous or organic material (sticks and/or straw,), which is shaped into bricks using frames and dried in the sun. It is similar to cob and mudbrick. Adobe structures are extremely durable and account for some of the oldest extant buildings on the planet. In hot climates, compared to wooden buildings, adobe buildings offer significant advantages due to their greater thermal mass, but they are known to be particularly susceptible to seismic damage in an event such as an earthquake. During the Black Friday bushfires in Victoria, Australia in February 2009 there was no observed difference in building survivial rates between adobe construction and traditional methods. Buildings made of sun-dried earth are common in the West Asia, North Africa, South America, southwestern North America, and in Spain (usually in the Mudéjar style). Adobe had been in use by indigenous peoples of the Americas in the Southwestern United States, Mesoamerica, and the Andean region of South America for several thousand years, although often substantial amounts of stone are used in the walls of Pueblo buildings. (Also, the Pueblo people built their adobe structures with handfuls or basketfuls of adobe, until the Spanish introduced them to the making of bricks.) Adobe brickmaking was used in Spain already in the Late Bronze Age and Iron Age, from the eighth century B.C. on. Its wide use can be attributed to its simplicity of design and make, and the cheapness thereby in creating it. A distinction is sometimes made between the smaller "adobes", which are about the size of ordinary baked bricks, and the larger "adobines", some of which may be one to two yards (2 m) long.
The word "adobe" () has come to us over some 4000 years with little change in either pronunciation or meaning: the word can be traced from the Middle Egyptian (c. 2000 BC) word "dj-b-t" "mud ["i.e.", sun-dried] brick." As Middle Egyptian evolved into Late Egyptian, Demotic, and finally Coptic (c. 600 BC), "dj-b-t" became "tobe" "[mud] brick." This evolved into Arabic "al-tub" (الطّوب "al" "the" + "tub" "brick") "[mud] brick," which was assimilated into Old Spanish as "adobe", still with the meaning "mud brick." English borrowed the word from Spanish in the early 18th century. In more modern English usage, the term "adobe" has come to include a style of architecture that is popular in the desert climates of North America, especially in New Mexico. (Compare with stucco).
An adobe brick is a composite material made of clay mixed with water and an organic material such as straw or dung. The soil composition typically contains clay and sand. Straw is useful in binding the brick together and allowing the brick to dry evenly. Dung offers the same advantage and is also added to repel insects. The mixture is roughly half sand (50%), one-third clay (35%), and one-sixth straw (15%).
Bricks are made in an open frame, by being a reasonable size, but any convenient size is acceptable. The mixture is molded by the frame, and then the frame is removed quickly. After drying a few hours, the bricks are turned on edge to finish drying. Slow drying in shade reduces cracking. The same mixture to make bricks, without the straw, is used for mortar and often for plaster on interior and exterior walls. Some ancient cultures used lime-based cement for the plaster to protect against rain damage. The brick’s thickness is preferred partially due to its thermal capabilities, and partially due to the stability of a thicker brick versus a more standard size brick. Depending on the form that the mixture is pressed into, adobe can encompass nearly any shape or size, provided drying time is even and the mixture includes reinforcement for larger bricks. Reinforcement can include manure, straw, cement, rebar or wooden posts. Experience has shown that straw, cement, or manure added to a standard adobe mixture can all produce a strong brick. A general testing is done on the soil content first. To do so, a sample of the soil is mixed into a clear container with some water, creating an almost completely saturated liquid. After the jar is sealed the container is shaken vigorously for at least one minute. It is then allowed to sit on a flat surface for a day or so until the soil has settled into layers or remains in suspension. Heavier particles settle out first so gravel will be on the bottom, sand above, silt above that and very fine clay and organic matter will stay in suspension for days. After the water has cleared percentages of the various particles can be determined. Fifty to 60 percent sand and 35 to 40 percent clay will yield strong bricks. The New Mexico US Extension Service recommends a mix of not more than 1/3 clay, not less than 1/2 sand, and never more than 1/3 silt. The largest structure ever made from adobe (bricks) was the Bam Citadel, which suffered serious damage (up to 80%) by an earthquake on December 26, 2003. Other large adobe structures are the Huaca del Sol in Peru, with 100 million signed bricks, the ciudellas of Chan Chan and Tambo Colorado, both in Peru.
An adobe wall can serve as a significant heat reservoir due to the thermal properties inherent in the massive walls typical in adobe construction. In desert and other climates typified by hot days and cool nights, the high thermal mass of adobe levels out the heat transfer through the wall to the living space. The massive walls require a large and relatively long input of heat from the sun (radiation) and from the surrounding air (convection) before they warm through to the interior and begin to transfer heat to the living space. After the sun sets and the temperature drops, the warm wall will then continue to transfer heat to the interior for several hours due to the time lag effect. Thus a well-planned adobe wall of the appropriate thickness is very effective at controlling inside temperature through the wide daily fluctuations typical of desert climates, a factor which has contributed to its longevity as a building material. In addition, the exterior of an adobe wall can be covered with glass to increase heat collection. In a passive solar home, this is called a Trombe wall.
When building an adobe structure, the ground should be compressed because the weight of adobe bricks is significantly greater than a frame house and may cause cracking in the wall. The footing is dug and compressed once again. Footing depth depends on the region and its ground frost level. The footing and stem wall are commonly 24" and 14", much larger than a frame house because of the weight of the walls. Adobe bricks are laid by course. Each course is laid the whole length of the wall, overlapping at the corners on a layer of adobe mortar. Adobe walls usually never rise above 2 stories because they're load bearing and have low structural strength. When placing window and door openings, a lintel is placed on top of the opening to support the bricks above. Within the last courses of brick, bond beams are laid across the top of the bricks to provide a horizontal bearing plate for the roof to distribute the weight more evenly along the wall. To protect the interior and exterior adobe wall, finishes can be applied, such as mud plaster, whitewash or stucco. These finishes protect the adobe wall from water damage, but need to be reapplied periodically, or the walls can be finished with other nontraditional plasters providing longer protection.
The traditional adobe roof has been generally constructed using a mixture of soil/clay, water, sand, and other available organic materials. The mixture was then formed and pressed into wood forms producing rows of dried, earth bricks that would then be laid across a support structure of wood and plastered into place with more adobe. For a deeper understanding of adobe, one might examine a cob building. Cob, a close cousin to adobe, contains proportioned amounts of soil, clay, water, manure, and straw. This is blended, but not formed like adobe. Cob is spread and piled around a frame and allowed to air dry for several months before habitation. Adobe, then, can be described as dried bricks of cob, stacked and mortared together with more adobe mixture to create a thick wall and/or roof.
Depending on the materials available, a roof can be assembled using lengths of wood or metal to create a frame work to begin layering adobe bricks. Depending on the thickness of the adobe bricks, the frame work has been performed using a steel framing and a layering of a metal fencing or wiring over the framework to allow an even load as masses of adobe are spread across the metal fencing like cob and allowed to air dry accordingly. This method was demonstrated with an adobe blend heavily impregnated with cement to allow even drying and prevent major cracking.
More traditional adobe roofs were often flatter than the familiar steeped roof as the native climate yielded more sun and heat than mass amounts of snow or rain that would find use in precipitous roofs. Cement may be introduced to prevent moisture from penetrating the composite of mud and organic matter. Vigas are beams across the roof that support the roof. Raising a traditional adobe roof. To raise a flattened adobe roof, beams of wood or metal should be assembled and span the extent of the building. The ends of the beams should then be fixed to the tops of the walls using the builder’s preferred choice of attachments. Taking into account the material the beams and walls are made from, choosing the attachments may prove difficult. In combination to the bricks and adobe mortar that are laid across the beams creates an even load-bearing pressure that can last for many years depending on attrition. Once the beams are laid across the building, it is then time to begin the placing of adobe bricks to create the roof. An adobe roof is often laid with bricks slightly larger in width to ensure a larger expanse is covered when placing the bricks onto the beams. This wider shape also provides the future homeowner with thermal protection enough to stabilize an even temperature through out the year. Following each individual brick should be a layer of adobe mortar, recommended to be at least an inch thick to make certain there is ample strength between the brick’s edges and also to provide a relative moisture barrier during the seasons where the arid climate does produce rain.
Adobe roofs can be inherently fire-proof, an attribute well received when the fireplace is kept lit during the cold nights, depending on the materials used. This feature leads the homeowner and builders to begin thinking about the installation of a chimney, a feat regarded as a necessity in any adobe building. The construction of the chimney can also greatly influence the construction of the roof supports, creating an extra need for care in choosing the right materials. An adobe chimney can be made from simple adobe bricks and stacked in similar fashion as the surrounding walls. Basically outline the location and perimeter of the hearth, minding the safety elements common to a fireplace, and begin to stack and mortar the walls with pre-made adobe bricks, cut to size.
An adventure is an activity that is perceived to involve risky, dangerous or exciting experiences. The term is often used to refer to activities with some potential for physical danger, such as skydiving, mountain climbing, and extreme sports. However, the term also broadly refers to any enterprise that is potentially fraught with physical, financial or psychological risk, such as a business venture, a love affair, or other major life undertakings. Adventurous experiences create psychological and physiological arousal, which can be interpreted as negative (e.g. fear) or positive (e.g. flow), and which can be detrimental as stated by the Yerkes-Dodson law. For some people, adventure becomes a major pursuit in and of itself. According to adventurer André Malraux, in his "La Condition Humaine" (1933), "If a man is not ready to risk his life, where is his dignity?". Similarly, Helen Keller famously stated that "Life is either a daring adventure or nothing."
The oldest and most widespread stories in the world are adventure stories. Joseph Campbell discussed his notion of the monomyth in his book, "The Hero with a Thousand Faces". Campbell proposed that the heroic mythological stories from culture to culture comprised of a similar underlying pattern, starting with the "call to adventure", followed by a hazardous journey and eventual triumph. The adventure novel exhibits these "protagonist on adventurous journey" characteristics as do many popular feature films, such as Star Wars.
In fiction, the adventurer figure or Picaro may be regarded as a descendant of the knight-errant of Medieval romance. Like the knight, the adventurer roams through episodic encounters, usually involving wealth, romance, or fighting. Unlike the knight, the adventurer was a realistic figure, often lower class or otherwise impoverished, who is forced to make his way to fortune, often by deceit. Also, an adventurer is a roguish hero of low social class who lives by his or her wits in a corrupt society. The picaresque novel originated in Spain in the middle of the fifteenth century. Novels such as Lazarillo de Tormes were influential across Europe. Throughout the eighteenth century, a great number of novels featured bold, amoral, adventuring protagonists, who made their way into wealth and happiness, sometimes with and sometimes without the moral conversion that generally accompanies the Spanish model. Under Victorian morality the term, used without qualifiers, came to imply a person of low moral character, often someone trying to marry for money. In comic book handbooks such as "Official Handbook of the Marvel Universe" and ', the term "adventurer" is used as a synonym for "super-hero" when listing a character's occupation. In role-playing games, the player characters are often professional adventurers, who earn wealth and fame by adventure, such as undertaking hazardous missions, exploring ruins, and slaying monsters. This stereotype is strong enough that "the adventurers" can often be used as a synonym for "the player characters". However non-player character groups of adventurers can also exist, and can be an interesting encounter for the players.
Asia is the world's largest and most populous continent, located primarily in the eastern and northern hemispheres. It covers 8.6% of the Earth's total surface area (or 29.9% of its land area) and with approximately 4 billion people, it hosts 60% of the world's current human population. Asia is traditionally defined as part of the landmass of Eurasia — with the western portion of the latter occupied by Europe — located to the east of the Suez Canal, east of the Ural Mountains and south of the Caucasus Mountains (or the Kuma-Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Given its size and diversity, Asia — a toponym dating back to classical antiquity — is more a cultural concept incorporating a number of regions and peoples than a homogeneous physical entity (see "Subregions of Asia", "Asian people"). The wealth of Asia differs very widely among and within its regions, due to its vast size and huge range of different cultures, environments, historical ties and government systems. In terms of nominal GDP, Japan has the largest economy on the continent and the second largest in the world. In purchasing power parity terms, however, China has the largest economy in Asia and the second largest in the world.
The term "Asia" is originally a concept exclusively of Western civilization. The peoples of ancient "Asia" (Chinese, Japanese, Indians, Persians, Arabs etc.) never conceived the idea of "Asia", simply because they did not see themselves collectively. In their perspective, they were vastly varied civilizations, contrary to ancient European belief. The word "Asia" originated from the Greek word "Ἀσία", first attributed to Herodotus (about 440 BC) in reference to Anatolia or — in describing the Persian Wars — to the Persian Empire, in contrast to Greece and Egypt. Herodotus comments that he is puzzled as to why three women's names are used to describe one enormous and substantial land mass (Europa, Asia, and Libya, referring to Africa), stating that most Greeks assumed that Asia was named after the wife of Prometheus (i.e. Hesione), but that the Lydians say it was named after Asias, son of Cotys, who passed the name on to a tribe in Sardis. Even before Herodotus, Homer knew of two figures in the Trojan War named Asios; and elsewhere he describes a marsh as ασιος (Iliad 2, 461). Usage of the term soon became common in ancient Greece, and subsequently by the ancient Romans. Ancient and medieval European maps depict the Asian continent as a "huge amorphous blob" extending eastward. It was presumed in antiquity to end with India — the Macedonian king Alexander the Great believing he would reach the "end of the world" upon his arrival in the East.
Alternatively, the etymology of the term may be from the Akkadian word ', which means 'to go outside' or 'to ascend', referring to the direction of the sun at sunrise in the Middle East and also likely connected with the Phoenician word "asa" meaning east. This may be contrasted to a similar etymology proposed for "Europe", as being from Akkadian "erēbu(m)" 'to enter' or 'set' (of the sun). T.R. Reid supports this alternative etymology, noting that the ancient Greek name must have derived from "asu", meaning 'east' in Assyrian ("ereb" for "Europe" meaning 'west'). The ideas of "Occidental" (form Latin "Occidens" 'setting') and "Oriental" (from Latin "Oriens" for 'rising') are also European invention, synonymous with "Western" and "Eastern". Reid further emphasizes that it explains the Western point of view of placing all the peoples and cultures of Asia into a single classification, almost as if there were a need for setting the distinction between Western and Eastern civilizations on the Eurasian continent. Ogura Kazuo and Tenshin Okakura are two Japanese outspoken figures over the subject. However, this etymology is considered doubtful, because it does not explain how the term "Asia" first came to be associated with Anatolia, which is "west" of the Semitic-speaking areas, unless they refer to the viewpoint of a Phoenician sailor sailing through the straits between the Mediterranean Sea and the Black Sea.
Medieval Europeans considered Asia as a continent a distinct landmass. The European concept of the three continents in the Old World goes back to Classical Antiquity, but during the Middle Ages was notably due to 7th century Spanish scholar Isidore of Sevilla (see T and O map). The demarcation between Asia and Africa (to the southwest) is the Isthmus of Suez and the Red Sea. The boundary between Asia and Europe is conventionally considered to run through the Dardanelles, the Sea of Marmara, the Bosporus, the Black Sea, the Caucasus Mountains, the Caspian Sea, the Ural River to its source and the Ural Mountains to the Kara Sea near Kara, Russia. While this interpretation of tripartite continents (i.e., of Asia, Europe and Africa) remains common in modernity, discovery of the extent of Africa and Asia have made this definition somewhat anachronistic. This is especially true in the case of Asia, which has several regions that would be considered distinct landmasses if these criteria were used (for example, Southern Asia and Eastern Asia). In the far northeast of Asia, Siberia is separated from North America by the Bering Strait. Asia is bounded on the south by the Indian Ocean (specifically, from west to east, the Gulf of Aden, Arabian Sea and Bay of Bengal), on the east by the waters of the Pacific Ocean (including, counterclockwise, the South China Sea, East China Sea, Yellow Sea, Sea of Japan, Sea of Okhotsk and Bering Sea) and on the north by the Arctic Ocean. Australia (or Oceania) is to the southeast. Some geographers do not consider Asia and Europe to be separate continents, as there is no logical physical separation between them. For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely "the western excrescence of the continent of Asia." Geographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass – or of Afro-Eurasia: geologically, Asia, Europe and Africa comprise a single continuous landmass (save the Suez Canal) and share a common continental shelf. Almost all of Europe and most of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Cherskiy Range) on the North American Plate. In geography, there are two schools of thought. One school follows historical convention and treats Europe and Asia as different continents, categorizing subregions within them for more detailed analysis. The other school equates the word "continent" with a geographical region when referring to Europe, and use the term "region" to describe Asia in terms of physiography. Since, in linguistic terms, "continent" implies a distinct landmass, it is becoming increasingly common to substitute the term "region" for "continent" to avoid the problem of disambiguation altogether. Given the scope and diversity of the landmass, it is sometimes not even clear exactly what "Asia" consists of. Some definitions exclude Turkey, the Middle East, Central Asia and Russia while only considering the Far East, Southeast Asia and the Indian subcontinent to compose Asia, especially in the United States after World War II. The term is sometimes used more strictly in reference to the Asia-Pacific region, which does not include the Middle East or Russia, but does include islands in the Pacific Ocean—a number of which may also be considered part of Australasia or Oceania, although Pacific Islanders are not considered Asian.
Asia has the third largest nominal GDP of all continents, after North America and Europe, but the largest when measured in PPP. As of 2007, the largest national economy within Asia, in terms of gross domestic product (GDP), is that of China followed by that of Japan, India, South Korea and Indonesia. However, in nominal (exchange value) terms, they rank as follows: Japan, China, India, South Korea, Saudi Arabia, Taiwan, Indonesia. Since the 1960s, South Korea had maintained the highest economic growth rate in Asia, nicknamed as an Asian tiger, becoming a newly industrialized country in the 1980s and a developed country by the 21st century. In the late 1990s and early 2000s, the economies of the PRC and India have been growing rapidly, both with an average annual growth rate of more than 8%. Other recent very high growth nations in Asia include Malaysia, the Philippines, Pakistan, Vietnam, Mongolia, Uzbekistan, Cyprus, and mineral-rich nations such as Kazakhstan, Turkmenistan, Iran, Brunei, United Arab Emirates, Qatar, Kuwait, Saudi Arabia, Bahrain and Oman. China was the largest and most advanced economy on earth for much of recorded history, until the British Empire (excluding India) overtook it in the mid 19th century. Japan has had for only several dacades after WW2 the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1986 and Germany in 1968. (NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC). In the late 1980s and early 1990s, Japan's GDP was almost as large (current exchange rate method) as that of the rest of Asia combined. In 1995, Japan's economy nearly equalled that of the USA to tie as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/dollar. Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which have now all received developed country status, having the highest GDP per capita in Asia. It is forecasted that the People's Republic of China will surpass Japan to have the largest nominal and PPP-adjusted GDP in Asia within a decade. India is also forecast to overtake Japan in terms of Nominal GDP by 2020. In terms of GDP per capita, both nominal and PPP-adjusted, South Korea will become the second wealthiest country in Asia by 2025, overtaking Germany, the United Kingdom and France. By 2050, according to a 2006 report by Price Waterhouse Cooper, China will have the largest economy in the world (43% greater than the United States when PPP adjusted, although perhaps smaller than the United States in nominal terms).
Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in mainland China, Taiwan, South Korea, Japan, India, Philippines and Singapore. Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly mainland China, and India are making significant inroads. Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.
Asia has four main financial centres: Tokyo, Hong Kong, Singapore and Shanghai. Call centres and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly-skilled, English-speaking workers. The increased use of outsourcing has assisted the rise of India and the China as financial centres. Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.
The history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia and the Middle East, linked by the interior mass of the Central Asian steppes. The coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley and the Huanghe shared many similarities. These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel. Other innovations, such as writing, seem to have been developed individually in each area. Cities, states and empires developed in these lowlands. The central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes. The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into the Middle East, South Asia, and the borders of China, where the Tocharians resided. The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra. These areas remained very sparsely populated. The center and the peripheries were mostly kept separated by mountains and deserts. The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty. While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.
The polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate. He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas. He is also the writer of the national anthems of Bangladesh and India. Tagore is said to have named another Bengali Indian Nobel prize winner, the 1998 laureate in Economics, Amartya Sen. Sen's work has centered around global issues including famine, welfare, and third-world development. Amartya Sen was Master of Trinity College, Cambridge University, UK, from 1998–2004, becoming the first Asian to head an 'Oxbridge' College. Other Asian writers who won Nobel Prizes include Yasunari Kawabata (Japan, 1966), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (People's Republic of China, 2000) and Orhan Pamuk (Turkey, 2006). Also, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children. Ebadi is the first Iranian and the first Muslim woman to receive the prize. Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma. She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma(Myanmar) and a noted prisoner of conscience. She is a Buddhist and was awarded the Nobel Peace Prize in 1991. Sir C.V.Raman is the first Asian to get a Nobel prize in Sciences. He won the Nobel Prize in Physics "for his work on the scattering of light and for the discovery of the effect named after him". Other Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Shmuel Yosef Agnon, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Yaser Arafat, Jose Ramos Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and thirteen Japanese scientists. Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Salam (Pakistan), Arafat (Palestinian Territories) and Kim (South Korea). In 2006, Dr. Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women in Bangladesh. Dr. Yunus received his Ph.D. in economics from Vanderbilt University, United States. He is internationally known for the concept of micro credit which allows poor and destitutes with little or no collateral to borrow money. The borrowers typically pay back money within the specified period and the incidence of default is very low. The Dalai Lama has received approximately eighty-four awards over his spiritual and political career. On 22 June 2006, he became one of only four people ever to be recognized with Honorary Citizenship by the Governor General of Canada. On 28 May 2005, he received the Christmas Humphreys Award from the Buddhist Society in the United Kingdom. Most notable was the Nobel Peace Prize, presented in Oslo, Norway on 10 December 1989.
Asian mythology is complex and diverse. The story of the Great Flood for example, as presented to Christians in the Old Testament, is first found in Mesopotamian mythology, in the "Epic of Gilgamesh". Hindu mythology tells about an avatar of the God Vishnu in the form of a fish who warned Manu of a terrible flood. In ancient Chinese mythology, Shan Hai Jing, the Chinese ruler Da Yu, had to spend 10 years to control a deluge which swept out most of ancient China and was aided by the goddess Nüwa who literally fixed the broken sky through which huge rains were pouring.
Almost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings. Indian philosophy includes Hindu philosophy and Buddhist philosophy. They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of material world. Christianity is also present in most Asian countries.
The Abrahamic religions of Judaism, Christianity and Islam originated in West Asia. Judaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel (which has the world's largest Jewish population), though small communities exist in other countries, such as the Bene Israel in India. In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively. In Armenia, Cyprus, Georgia and Russia, Eastern Orthodoxy is the predominant religion. Various Christian denominations have adherents in portions of the Middle East, as well as China and India. The world's largest Muslim community (within the bounds of one nation) is in Indonesia. South Asia (mainly Pakistan, India and Bangladesh) holds 30% of Muslims. There are also significant Muslim populations in China, Iran, Malaysia, southern Philippines (Mindanao), Russia and most of West Asia and Central Asia.
Aruba () is a -long island of the Lesser Antilles in the southern Caribbean Sea, located a mere north of the coast of Venezuela. Together with Bonaire and Curaçao, it forms a group referred to as the ABC islands of the Leeward Antilles, the southern island chain of the Lesser Antilles. Aruba, which has no administrative subdivisions, is one of the three countries that form the Kingdom of the Netherlands, together with the Netherlands and the Netherlands Antilles. Aruban citizens hold Dutch passports. Unlike much of the Caribbean region, Aruba has a dry climate and an arid, cactus-strewn landscape. This climate has helped tourism as visitors to the island can reliably expect warm, sunny weather. It has a land area of and lies outside the hurricane belt.
Aruba's first inhabitants are thought to have been Caquetíos Amerinds from the Arawak tribe, who migrated there from Venezuela to escape attacks by the Caribs. Fragments of the earliest known Indian settlements date back from 1,000 AD. Sea currents made canoe travel to other Caribbean islands difficult, thus Caquetio culture remained closer to that of mainland South America. Europeans first learned of Aruba when Amerigo Vespucci and Alonso de Ojeda happened upon it in August 1499. Vespucci, in one of his four letters to Lorenzo di Pierfrancesco de' Medici, described his voyage to the islands along the coast of Venezuela. He wrote about an island where most trees are of brazil wood and, from this island, he went to one ten leagues away, where they had houses built as in Venice. In another letter he described a small island inhabited by very large people, which the expedition thought was not inhabited. Aruba was colonized by Spain for over a century. The Cacique or Indian Chief in Aruba, Simas, welcomed the first priests in Aruba and received from them a wooden cross as a gift. In 1508, Alonso de Ojeda was appointed as Spain's first Governor of Aruba, as part of "Nueva Andalucía." Another governor appointed by Spain was Juan Martinez de Ampíes. A "cédula real" decreed in November 1525 gave Ampíes, factor of Española, the right to repopulate the depopulated islands of Aruba, Curaçao and Bonaire. In 1528, Ampíes was replaced by a representative of the "House of Welser". Aruba has been under Dutch administration since 1636, initially under Peter Stuyvesant. Stuyvesant was on a special mission in Aruba in November and December 1642. Under the Dutch W.I.C. administration, as "New Netherland and Curaçao" from 1648 to 1664 and the Dutch government regulations of 1629, also applied in Aruba. The Dutch administration appointed an Irishman as "Commandeur" in Aruba in 1667. The United Kingdom occupied Aruba from the years 1799 to 1802 and from 1805 to 1816. In August 1806, General Francisco de Miranda and a group of 200 freedom fighters on their voyage to liberate Venezuela from Spain stayed in Aruba for several weeks. In 1933 Aruba sent its first petition for Aruba's separate status and autonomy to the Queen. During World War II, together with Curaçao the then world-class exporting oil refineries were the main suppliers of refined products to the Allies. Aruba became a British protectorate from 1940 to 1942 and a US protectorate from 1942 to 1945. On February 16, 1942, its oil processing refinery was attacked by a German submarine ("U-156") under the command of Werner Hartenstein, but the mission failed. "U-156" was later destroyed by a US plane as the crew was sunbathing; only one survived. In March 1944, Eleanor Roosevelt briefly visited American troops stationed in Aruba. In attendance were: His Excellency, Dr. P. Kasteel, the Governor of Curaçao, and his aide, Lieutenant Ivan Lansberg; Rear Admiral T. E. Chandler and his Aide, Lieutenant W. L. Edgington; Captain Jhr. W. Boreel and his aide, Lieutenant E. O. Holmberg; and the Netherlands aide to Mrs. Roosevelt, Lieutenant Commander v.d. Schatte Olivier. The island's economy has been dominated by five main industries: gold mining, phosphate mining (The Aruba Phosphaat Maatschappij), aloe export, petroleum refineries (The Lago Oil & Transport Company and the Arend Petroleum Maatschappij Shell Co.), and tourism.
As a Constituent Country of the Kingdom of the Netherlands, Aruba's politics take place within a framework of a 21-member Parliament and an eight-member Cabinet. The governor of Aruba is appointed for a six-year term by the monarch, and the prime minister and deputy prime minister are elected by the Staten (or "Parlamento") for four-year terms. The Staten is made up of 21 members elected by direct, popular vote to serve a four-year term. Together with the Netherlands, the country of the Netherlands Antilles, and the country of Aruba form the Kingdom of the Netherlands. As they share the same Dutch citizenship, these three countries still also share the Dutch passport as the Kingdom of the Netherlands passport. As Aruba and the Antilles have small populations, the two countries had to limit immigration. To protect their population, they have the right to control the admission of people from the Netherlands. There is the supervision of the admission and expulsion of people from the Netherlands and the setting of general conditions for the admission and expulsion of aliens. Aruba is officially not a part of the European Union.
In August 1947, Aruba presented its first "Staatsreglement" (constitution), for Aruba's "status aparte" as the status of an autonomous state within the Kingdom of the Netherlands. This would come to pass in 1986. In November 1955, J. Irausquin of Aruba's PPA political party spoke in front of the United Nations Trust Committee. He ended his speech saying that in the future there will be changes to come. In 1972, at a conference in Suriname, Betico Croes (MEP) proposed a "sui-generis" Dutch Commonwealth of four states: Aruba, the Netherlands, Suriname and the Netherlands Antilles, each with its own nationality. Mr. C. Yarzagaray, a parliamentary member representing the AVP political party, proposed a referendum for the people of Aruba to determine Aruba's separate status or "Status Aparte" as a full autonomous state under the crown. He proclaimed: "Aruba shall never accept a federation and a second class nationality." Betico Croes worked in Aruba to inform and prepare the people of Aruba for independence. In 1976, a committee appointed by Croes introduced the national flag and anthem as the symbols of Aruba's sovereignty and independence, and he also set 1981 as a target for Aruba's independence. In March 1977, the first Referendum for Self Determination was held with the support of the United Nations and 82% of the participants voted for independence. The Island Government of Aruba assigned the Institute of Social Studies in The Hague to prepare a study of Aruba's independence, which was published in 1978, titled "Aruba en Onafhankelijkheid, achtergronden, modaliteiten en mogelijkheden; een rapport in eerste aanleg". At the conference in the Hague in 1981, Aruba's independence was then set for the year 1991. In March 1983, based on the Referendum, Aruba finally reached an official agreement within the Kingdom, for Aruba's Independence, first becoming an autonomous country within the Kingdom of the Netherlands, with its own constitution, unanimously approved and proclaimed in August 1985, and after an election held for Aruba's first parliament, Aruba seceded from the Netherlands Antilles and officially became a country of the Kingdom of the Netherlands on January 1, 1986, with full independence set for 1996. This achievement is largely due to Betico Croes and the political support of other nations like the USA, Panama, Venezuela and various European countries. Croes was later proclaimed "Libertador di Aruba" after his death in 1986. In 1990, movement toward independence was postponed upon the request of Aruba's Prime Minister, Nelson O. Oduber. The article scheduling Aruba's complete independence was rescinded in 1995, although the process can begin again after a referendum. Since January 1, 1986, the Kingdom has consisted of three countries: Netherlands, Netherlands Antilles, and Aruba. Although the "equality" of the countries is explicitly laid down in the preamble to the Charter, which states "..considering that they have expressed freely their will to establish a new constitutional order in the Kingdom of the Netherlands, in which they will conduct their internal interests autonomously and their common interests on a basis of equality, and in which they will accord each other reciprocal assistance, have resolved by mutual consent", in practice, the Netherlands has considerably more power than either the Netherlands Antilles or Aruba.
The Aruban legal system is based on the Dutch model. Instead of juries or grand juries, in Aruba, legal jurisdiction lies with a "Gerecht in Eerste Aanleg" (Court of First Instance) on Aruba, a "Gemeenschappelijk Hof van Justitie voor de Nederlandse Antillen en Aruba" (Common Court of Justice of the Netherlands Antilles and Aruba) and the "Hoge Raad der Nederlanden" (Supreme Court of Justice of the Netherlands).
Aruba's educational system, patterned after the Dutch system, provides for education at all levels. The Government finances the national education system, except for private schools, such as the International School of Aruba (ISA), which finance their own activities. The percentage of money earmarked for education is higher than the average for the Caribbean/Latin American region. Arubans benefit from a strong primary school education. A segmented secondary school program includes vocational training (VMBO), basic education (MAVO), college prep (HAVO) and advanced placement (VWO). Higher education goals can be pursued through the Professional Education program (EPI), the teachers college (IPA) as well as through the University of Aruba (UA) which offers bachelors and masters programs in law, finance and economics and hospitality and tourism management. Since the choice for higher education on the island itself is limited, many students choose to study abroad in countries in North America, South America as well as Europe. There are 68 schools for primary education, 12 schools for secondary education, and 5 universities. In 2007, there were 22,930 fulltime students registered. There are two private medical schools in Aruba: All Saints University of Medicine, Aruba and Xavier University School of Medicine, Aruba. All courses are presented in English. School's curriculum is based on the United States medical school model and will lead to a Doctor of Medicine degree that is recognized in North America.
Aruba is a generally flat, riverless island in the Leeward Antilles island arc of the Lesser Antilles. Aruba is renowned for its white, sandy beaches on the western and southern coasts of the island, relatively sheltered from fierce ocean currents, and this is where most tourist development has taken place. The northern and eastern coasts, lacking this protection, are considerably more battered by the sea and have been left largely untouched by humans. The hinterland of the island features some rolling hills, the best known of which are called Hooiberg at 165 meters (541 ft) and Mount Jamanota, the highest on the island at 188 metres (617 ft) above sea level. Oranjestad, the capital, is located at. To the east of Aruba are Bonaire and Curaçao, two island territories which form the southwest part of the Netherlands Antilles; Aruba and these two Netherlands Antilles islands are sometimes called the ABC islands. The isothermal temperature of Aruba's pleasantly tropical marine climate attracts tourists to the island all year round. Temperature varies little from 28 °C (82 °F), moderated by constant trade winds from the Atlantic Ocean. Yearly precipitation barely reaches 500 mm (19.7 in), most of it falling in late autumn.
Aruba enjoys one of the highest standards of living in the Caribbean region; the low unemployment rate is also positive for Aruba. About three quarters of the Aruban gross national product is earned through tourism or related activities. Most of the tourists are from Venezuela and the United States (predominately from eastern and southern states), Aruba's largest trading partner. Before the "Status Aparte" (a separate completely autonomous country/state within the Kingdom), oil processing was the dominant industry in Aruba despite expansion of the tourism sector. Today, the influence of the oil processing business is minimal. The size of the agriculture and manufacturing sectors also remains minimal. The GDP per capita for Aruba is calculated to be $23,831 in 2007; among the highest in the Caribbean and the Americas. Its main trading partners are Venezuela, the United States and Netherlands. Deficit spending has been a staple in Aruba's history, and modestly high inflation has been present as well. Recent efforts at tightening monetary policy are correcting this and will have its first balanced budget in 2009. Aruba received some development aid from the Dutch government each year, up until 2009 as part of a deal (signed as "Aruba's Financial Independence") in which the Netherlands gradually reduced its financial help to the island each successive year. The Aruban florin is pegged to the United States dollar, with a fixed exchange rate where 1.77 Florin equals 1 U.S. dollar. In most stores near Oranjestad, the exchange rate is 1.75 florin equals U.S 1 dollar In 2006 the Aruban government has also changed several tax laws to further reduce the deficit. Direct taxes have been converted to indirect taxes as proposed by the IMF. A 3% tax has been introduced on sales and services, while income taxes have been lowered and revenue taxes for business reduced with 20%. The government compensated workers with 3.1% for the effect that the B.B.O. would have on the inflation for 2007. The inflation on Aruba in 2007 was 8,7%.
Aruba is situated in the deep southern part of the Caribbean. Because it has almost no rainfall, Aruba was saved from the plantation system and the economics of the slave trade. Aruba's population is estimated to be 80% mestizo and 20% other ethnicities. Arawaks spoke the "broken Spanish" which their ancestors had learned on Hispaniola. The Dutch took control 135 years after the Spanish, left the Arawaks to farm and graze livestock, and used the island as a source of meat for other Dutch possessions in the Caribbean. The Arawak heritage is stronger on Aruba than on most Caribbean islands. Although no full-blooded Aboriginals remain, the features of the islanders clearly indicate their genetic Arawak heritage. Most of the population is descended mostly from Arawak, and to a lesser extent Spanish, Italian, Dutch, and a few French, Portuguese, British, and African ancestors. Recently there has been substantial immigration to the island from neighboring American and Caribbean nations, possibly attracted by the higher paid jobs. In 2007, new immigration laws were introduced to help control the growth of the population by restricting foreign workers to a maximum of 3 years residency on the island. The demographics of Aruba far more than neighboring Curaçao and Bonaire has been impacted by its proximity to Venezuela. Much of Aruba's families are present by way of Venezuela and there is a seasonal increase of Venezuelans living in second homes.
On March 18 Aruba celebrates its National Day. In 1976, Aruba presented its National Anthem (Aruba Dushi Tera) and Flag. The origins of the population and location of the island give Aruba a mixed culture. Dutch influence can still be seen, as in the celebration of "Sinterklaas" on December 5 and 6 and other national holidays like April 30, when in Aruba and the rest of the Kingdom of the Netherlands the Queen's birthday or "Dia di La Reina" (Koninginnedag) is celebrated. Christmas and New Year are celebrated with the typical music and songs of gaitas for Christmas and the Dande for New Year, and the "ayaca", the "ponchi crema" and "ham", and other typical foods and drinks. Millions of dollars worth of fireworks are burnt at midnight on New Year's. On January 25, Betico Croes' birthday is celebrated. The holiday of Carnival is also an important one in Aruba, as it is in many Caribbean and Latin American countries, and, like Mardi Gras, that goes on for weeks. Its celebration in Aruba started, around the 1950s, influenced by the inhabitants from the nearby islands (Venezuela, St Vincent, Trinidad, Barbados, St. Maarten and Anguilla) who came to work for the Oil refinery. Over the years the Carnival Celebration has changed and now starts from the beginning of January till the Tuesday before Ash Wednesday with a large parade on the last Sunday of the festivities (Sunday before Ash Wednesday). In June there is the celebration of the "Dia di San Juan", with the song of "Dera Gai". Tourism from the United States has recently also increased the visibility of American culture on the island, with such celebrations as Halloween and Thanksgiving Day in November. Religion also has its influences; the days of Ascension and Good Friday are also two holidays on the island. According to the "Bureau Burgelijke Stand en Bevolkingsregister" (BBSB), as of 2005 there are ninety-two different nationalities living on the island.
Language can be seen as an important part of island culture in Aruba. The cultural mixture has given rise to a linguistic mixture known as Papiamento, the predominant language on Aruba. The official language is Dutch. The local language used by its inhabitants is Papiamento and is a language that has been evolving through the centuries and absorbed many words from other languages like Dutch, English, French, diverse African dialects, and most importantly, from Portuguese and Spanish. However, like many islands in the region, Spanish is also often spoken. English has historical connections (with the British Empire) and is known by many; English usage has also grown due to tourism. Other common languages spoken based on the size of their community are Portuguese, Chinese, German and French. The latter is offered in high school and college, since a high percentage of Aruban students continue their studies in Europe. In recent years, the government of Aruba has shown an increased interest in acknowledging the cultural and historical importance of its native language. Although spoken Papiamento is fairly similar among the several Papiamento-speaking islands, there is a big difference in written Papiamento. The orthography differs per island and even per group of people. Some are more oriented towards the Portuguese roots and use the equivalent spelling (e.g. "y" instead of "j"), where others are more oriented towards the Dutch roots. In a book "The Buccaneers of America", first published in 1678, it is stated by eyewitness account that the Indians on Aruba spoke "Spanish". The oldest government official statement written in Papiamento dates from 1803. Aruba has four newspapers published in Papiamento: "Diario", "Bon Dia", "Solo di Pueblo" and "Awe Mainta" and two in English: "Aruba Today" and "The News". Amigoe is the newspaper published in Dutch. Aruba also has 18 radio stations (2 AM and 16 FM) and three local television stations (Tele-Aruba, Aruba Broadcast Company and Channel 22).
Aruba's Queen Beatrix International Airport is located near Oranjestad. This airport has daily flights to various cities across the United States, to San Juan, Puerto Rico; Miami, Florida; Chicago, Illinois; Philadelphia and Pittsburgh Pennsylvania; Houston, Texas; Atlanta, Georgia; Charlotte, North Carolina; Washington DC; New York City; and Boston, Massachusetts. It also connects Aruba with Toronto, Ontario, and South America, with daily flights to the international airports of Venezuela, Colombia, Peru, Brazil, Germany, France, Spain, U.K and most of Europe through the Schiphol Airport in the Netherlands. Direct flights from Italy started in November 2008. According to the Aruba Airport Authority, almost 1.7 million travelers used the airport in 2005, of which 61% were Americans. In cooperation with the United States government, and for the facilitation for the passengers that arrive into the United States, the United States Department of Homeland Security (DHS), U.S. Customs and Border Protection (CBP) full pre-clearance facility in Aruba has been in effect since February 1, 2001 with the expansion in the Queen Beatrix Airport, United States and Aruba have the agreement since 1986 that begins as a USDA and Customs post, and since 2008, the only island to have this service for private flights. In 1999, the U.S. Department of Defense established a Forward Operating Location (FOL) at the airport. Aruba has two ports Barcadera and Playa (this one) which is located in Oranjestad, The Port of Playa welcomes all the cruise-ship lines, including Royal Caribbean, Carnival Cruise Lines, NCL, Holland America Line, Disney Cruiseships and many more; an estimated almost one million tourists enter in this port per year, Aruba Ports Authority, owned and operated by the Aruban government is the authority in these seaports. Aruba's public buses transportation services is in charge of Arubus, a government based company which operates from 3:30am until 12:30am 365 days a year. Small private vans also provide the transportation services in certain areas such Hotel Area, San Nicolaas, Santa Cruz and Noord. Aruba also counts two telecommunications providers, Setar the government based company and Digicel Irish ownership company based in Kingston, Jamaica. Setar is the provider of services such as Internet, video conference, GSM wireless tech and land lines and offer the latest in telecom services, Digicel is the Setar competitor in wireless technology using the GSM platform.
The Articles of Confederation and Perpetual Union, customarily referred to as the Articles of Confederation, was the first constitution of the United States of America and legally established the union of the states. The Second Continental Congress appointed a committee to draft the Articles in June 1776 and sent the draft to the states for ratification in November 1777. The ratification process was completed in March 1781, legally federating the sovereign and independent states, already cooperating through the Continental Congress, into a new federation styled the "United States of America". Under the Articles the states retained sovereignty over all governmental functions not specifically relinquished to the central government. On June 12, 1776, a day after appointing a committee to prepare a draft declaration of independence, the Second Continental Congress resolved to appoint a committee of thirteen to prepare a draft of a constitution for a confederate type of union. The last draft of the Articles was written in the summer of 1777 and the Second Continental Congress approved them for ratification by the States on November 15, 1777, in York, Pennsylvania after a year of debate. In practice the final draft of the Articles served as the de facto system of government used by the Congress ("the United States in Congress assembled") until it became de jure by final ratification on March 1, 1781; at which point Congress became the Congress of the Confederation. The "Articles" set the rules for operations of the "United States" confederation. The confederation was capable of making war, negotiating diplomatic agreements, and resolving issues regarding the western territories. An important element of the Articles was that Article XIII stipulated that "their provisions shall be inviolably observed by every state" and "the Union shall be perpetual". The Articles were created by the chosen representatives of the states in the Second Continental Congress out of a perceived need to have "a plan of confederacy for securing the freedom, sovereignty, and independence of the United States." Although serving a crucial role in the victory in the American Revolutionary War, a group of reformers, known as "federalists", felt that the Articles lacked the necessary provisions for a sufficiently effective government. Fundamentally, a federation was sought to replace the confederation. The key criticism by those who favored a more powerful central state (i.e. the federalists) was that the government (i.e. the Congress of the Confederation) lacked taxing authority; it had to request funds from the states. Also various federalist factions wanted a government that could impose uniform tariffs, give land grants, and assume responsibility for unpaid state war debts ("assumption".) Those opposed to the Constitution, known as "anti-federalists," considered these limits on government power to be necessary and good. Another criticism of the Articles was that they did not strike the right balance between large and small states in the legislative decision making process. Due to its "one-state, one-vote" plank, the larger states were expected to contribute more but had only one vote. The Articles were replaced by the U.S. Constitution on June 21, 1788.
The political push for the colonies to increase cooperation began in the French and Indian War in the mid 1750s. The American Revolution in response to lack of elected representation in the British government, followed by the beginning of the American Revolutionary War in 1775 and a proclamation by the monarchy that Congress were traitors in rebellion, induced the various states to cooperate in declaring their independence from the British Empire. Starting 1775, the Second Continental Congress acted as the provisional national government that ran the war. Congress presented the Articles for enactment by the states in 1777, while prosecuting the American Revolutionary War.
"Permit us, then, earnestly to recommend these articles to the immediate and dispassionate attention of the legislatures of the respective states. Let them be candidly reviewed under a sense of the difficulty of combining in one general system the various sentiments and interests of a continent divided into so many sovereign and independent communities, under a conviction of the absolute necessity of uniting all our councils and all our strength, to maintain and defend our common liberties..." The document could not become officially effective until it was ratified by all thirteen colonies. The first state to ratify was Virginia on December 16, 1777. The process dragged on for several years, stalled by the refusal of some states to rescind their claims to land in the West. Maryland was the last holdout; it refused to go along until Virginia and New York agreed to cede their claims in the Ohio River Valley. A little over three years passed before Maryland's ratification on March 1, 1781.
Even though the Articles of Confederation and the Constitution were established by many of the same people, the two documents are very different. The original five-paged Articles contained thirteen articles, a conclusion, and a signatory section. The following list contains short summaries of each of the thirteen articles. Still at war with Great Britain, the Founding Fathers were divided between those seeking a powerful, centralized national government, and those seeking a loosely-structured one. Jealously guarding their new independence, members of the Continental Congress arrived at a compromise solution dividing sovereignty between the states and the federal government, with a unicameral legislature that protected the liberty of the individual states. While calling on Congress to regulate military and monetary affairs, for example, the Articles of Confederation provided no mechanism to force the states to comply with requests for troops or revenue. At times, this left the military in a precarious position, as George Washington wrote in a 1781 letter to the governor of Massachusetts, John Hancock.
As Congress failed to act on the petitions, Knox wrote to Gouverneur Morris, four years before the Philadelphia Convention was convened, “As the present Constitution is so defective, why do not you great men call the people together and tell them so; that is, to have a convention of the States to form a better Constitution.” Once the war was won, the Continental Army was largely disbanded. A very small national force was maintained to man frontier forts and protect against Native American attacks. Meanwhile, each of the states had an army (or militia), and 11 of them had navies. The wartime promises of bounties and land grants to be paid for service were not being met. In 1783, Washington defused the Newburgh conspiracy, but riots by unpaid Pennsylvania veterans forced the Congress to leave Philadelphia temporarily.
Even after peace was achieved, the weakness of the government frustrated the ability of the government to conduct foreign policy. In 1786 Thomas Jefferson, concerned over the failure to fund a naval expedition against the Barbary pirates, wrote to James Monroe, "It will be said there is no money in the treasury. There never will be money in the treasury till the confederacy shows its teeth. The states must see the rod.” Also, the Jay-Gardoqui Treaty with Spain in 1786 also showed weakness in foreign policy. In the treaty (which was never ratified due to its immense unpopularity) the US had to give up rights to the Mississippi River for 20 years which would have economically strangled the settlers west of the Appalachian Mountains. Finally, due to the Confederation's military weakness, they could not force the British out of the frontier forts (which the British promised they would leave in 1783). This violation of the Treaty of Paris was amended with Jay's Treaty in 1795 under the new constitution.
Under the articles, Congress could make decisions, but had no power to enforce them. There was a requirement for unanimous approval before any modifications could be made to the Articles. Because the majority of lawmaking rested with the states, the central government was also kept limited. Congress was denied the power of taxation: it could only request money from the states. The states did not generally comply with the requests in full, leaving the Confederation Congress and the Continental Army chronically short of funds. As more money was printed, continental dollars depreciated. Washington in 1779 wrote to John Jay, serving as President of the Continental Congress, "that a wagon load of money will scarcely purchase a wagon load of provisions." Jay and the Congress responded in May by requesting $45 million from the states. In an appeal to the states to comply Jay wrote that the taxes were "the price of liberty, the peace and the safety of yourselves and posterity." He argued that Americans should avoid having it said "that America had no sooner become independent than she became insolvent" or that "her infant glories and growing fame were obscured and tarnished by broken contracts and violated faith." The states did not respond with the money requested. Congress was also denied the power to regulate commerce, and as a result, the states maintained control over their own trade policy as well. The states and the national congress had both incurred debts during the war, and how to pay the debts became a major issue after the war. Some states paid off their debts; however, the centralizers favored federal assumption of states' debts.
The Treaty of Paris left the United States independent and at peace but with an unsettled governmental structure. The Second Continental Congress had drawn up Articles of Confederation in November 15, 1777, to regularize its own status. These described a permanent confederation, but granted to the Congress—the only federal institution—little power to finance itself or to ensure that its resolutions were enforced. The Articles of Confederation were weak and did not give a strong political or economic base for the newly formed nation. However, the articles did serve as the lead up to the much stronger and more agreed upon Constitution. Although historians generally agree that the articles were a spectacular failure in terms of workable governance, they do give much credit to the Land Ordinance of 1785 and Northwest Ordinance that set up protocol for the admission of new states, the division of land into homesteads and states, as well as setting aside land in each township for public use. This system represented a sharp break from imperial colonization, as in Europe, and provided the basis for the rest of American continental expansion through the 19th Century. During the latter years of the war, most people were living in comparative comfort. Farmers found a ready market for their produce within the lines of the British and French armies. Blockade runners and the prizes from privateers added rich cargoes and merchandise to northern shops. Speculators went in debt in preparation for the economic boom which was sure to follow the war. These dreams vanished in the economic depression that followed the war. Orders in council closed the ports of the British West Indies to all staple products which were not carried in British ships. France and Spain established similar policies. Simultaneously, new manufacturers were stifled by British products which were suddenly filling American ports. Political unrest in several states and efforts by debtors to use popular government to erase their debts increased the anxiety of the political and economic elites which had led the Revolution. The apparent inability of the Congress to redeem the public obligations (debts) incurred during the war, or to become a forum for productive cooperation among the states to encourage commerce and economic development, only aggravated a gloomy situation. The Continental Congress had issued bills of credit, but by the end of the war its paper money had so far depreciated that it ceased to pass as currency, spawning the expression "not worth a continental". Congress could not levy taxes and could only make requisitions upon the States. Less than a million and a half dollars came into the treasury between 1781 and 1784, although the governors had been asked for two million in 1783 alone. When John Adams went to London in 1785 as the first representative of the United States, he found it impossible to secure a treaty for unrestricted commerce. Demands were made for favors and there was no assurance that individual states would agree to a treaty. Adams stated it was necessary for the States to confer the power of passing navigation laws to Congress, or that the States themselves pass retaliatory acts against Great Britain. Congress had already requested and failed to get power over navigation laws. Meanwhile, each State acted individually against Great Britain to little effect. When other New England states closed their ports to British shipping, Connecticut hastened to profit by opening its ports. Debtor's problems came to a head in Shays' Rebellion in Massachusetts. Congress was unable to protect manufacturing and shipping. State legislatures were unable or unwilling to resist attacks upon private contracts and public credit. Land speculators expected no rise in values when the government could not defend its borders nor protect its frontier population. The idea of a convention to revise the Articles of Confederation grew in favor. Alexander Hamilton, a Revolutionary War veteran who determined while serving as Washington's aide-de-camp that a strong central government was necessary to avoid the frustrations endured by the Army due to an ineffectual Congress, called for what would be referred to as the Annapolis Convention of 1786 to revise the Articles. Only five states sent delegates, but plans were made for another meeting in Philadelphia the next year.
The Second Continental Congress approved the Articles for distribution to the states on November 15, 1777. A copy was made for each state and one was kept by the Congress. The copies sent to the states for ratification were unsigned, and a cover letter had only the signatures of Henry Laurens and Charles Thomson, who were the President and Secretary to the Congress. The "Articles", however, were unsigned, and the date was blank. Congress began the signing process by examining their copy of the "Articles" on June 27, 1778. They ordered a final copy prepared (the one in the National Archives), and that delegates should inform the secretary of their authority for ratification. On July 9, 1778, the prepared copy was ready. They dated it, and began to sign. They also requested each of the remaining states to notify its delegation when ratification was completed. On that date, delegates present from New Hampshire, Massachusetts, Rhode Island, Connecticut, New York, Pennsylvania, Virginia and South Carolina signed the Articles to indicate that their states had ratified. New Jersey, Delaware and Maryland could not, since their states had not ratified. North Carolina and Georgia also didn't sign that day, since their delegations were absent. After the first signing, some delegates signed at the next meeting they attended. For example, John Wentworth of New Hampshire added his name on August 8. John Penn was the first of North Carolina's delegates to arrive (on July 10), and the delegation signed the "Articles" on July 21, 1778. The other states had to wait until they ratified the "Articles" and notified their Congressional delegation. Georgia signed on July 24, New Jersey on November 26, and Delaware on February 12, 1779. Maryland refused to ratify the "Articles" until every state had ceded its western land claims. On February 2, 1781, the much-awaited decision was taken by the Maryland General Assembly in Annapolis. As the last piece of business during the afternoon Session, "among engrossed Bills" was "signed and sealed by Governor Thomas Sim Lee in the Senate Chamber, in the presence of the members of both Houses... an Act to empower the delegates of this state in Congress to subscribe and ratify the articles of confederation" and perpetual union among the states. The Senate then adjourned "to the first Monday in August next." The decision of Maryland to ratify the Articles was reported to the Continental Congress on February 12. The formal signing of the "Articles" by the Maryland delegates took place in Philadelphia at noon time on March 1, 1781 and was celebrated in the afternoon. With these events, the Articles entered into force and the United States came into being as a united, sovereign and national state. Congress had debated the "Articles" for over a year and a half, and the ratification process had taken nearly three and a half years. Many participants in the original debates were no longer delegates, and some of the signers had only recently arrived. The "Articles of Confederation and Perpetual Union" were signed by a group of men who were never present in the Congress at the same time. Roger Sherman (Connecticut) was the only person to sign all four great state papers of the United States: the Continental Association, the United States Declaration of Independence, the Articles of Confederation and the United States Constitution. Robert Morris (Pennsylvania) was the only person besides Sherman to sign three of the great state papers of the United States: the United States Declaration of Independence, the Articles of Confederation and the United States Constitution. John Dickinson (Delaware), Daniel Carroll (Maryland) and Gouverneur Morris (New York), along with Sherman and Robert Morris, were the only five people to sign both the Articles of Confederation and the United States Constitution (Gouverneur Morris represented Pennsylvania when signing the Constitution).
The following list is of those who led the Congress of the Confederation under the "Articles of Confederation" as the Presidents of the United States in Congress Assembled. Under the Articles, the president was the presiding officer of Congress, chaired the Cabinet (the Committee of the States) when Congress was in recess, and performed other administrative functions. He was not, however, a "chief" executive in the way the successor President of the United States is a chief executive, but all of the functions he executed were under the auspices and in service of the Congress. "For a full list of Presidents of the Congress Assembled and Presidents under the two Continental Congresses before the Articles, see President of the Continental Congress."
In May 1786, Charles Pinckney of South Carolina proposed that Congress revise the Articles of Confederation. Recommended changes included granting Congress power over foreign and domestic commerce, and providing means for Congress to collect money from state treasuries. Unanimous approval was necessary to make the alterations, however, and Congress failed to reach a consensus. The weakness of the Articles in establishing an effective unifying government was underscored by the threat of internal conflict both within and between the states, especially after Shays' Rebellion threatened to topple the state government of Massachusetts. When approached after leaving the close of the Federal Convention, Benjamin Franklin was asked a question. This is the conversation as has been recorded, The lady asked "Well, Doctor, what have we got—a Republic or a Monarchy?" “A Republic, if you can keep it.” was the response of Benjamin Franklin. According to their own terms for modification (Article XIII), the Articles would still have been in effect until 1790, the year in which the last of the 13 states ratified the new Constitution. The Congress under the Articles continued to sit until November 1788, overseeing the adoption of the new Constitution by the states, and setting elections. By that date, 11 of the 13 states had ratified the new Constitution. Historians have given many reasons for the perceived need to replace the articles in 1787. Jillson and Wilson (1994) point to the financial weakness as well as the norms, rules and institutional structures of the Congress, and the propensity to divide along sectional lines. Rakove (1988) identifies several factors that explain the collapse of the Confederation. The lack of compulsory direct taxation power was objectionable to those wanting a strong centralized state or expecting to benefit from such power. It could not collect customs after the war because tariffs were vetoed by Rhode Island. Rakove concludes that their failure to implement national measures "stemmed not from a heady sense of independence but rather from the enormous difficulties that all the states encountered in collecting taxes, mustering men, and gathering supplies from a war-weary populace." The second group of factors Rakove identified derived from the substantive nature of the problems the Continental Congress confronted after 1783, especially the inability to create a strong foreign policy. Finally, the Confederation's lack of coercive power reduced the likelihood for profit to be made by political means, thus potential rulers were uninspired to seek power. When the war ended in 1783, certain special interests had incentives to create a new "merchant state," much like the British state people had rebelled against. In particular, holders of war scrip and land speculators wanted a central government to pay off scrip at face value and to legalize western land holdings with disputed claims. Also, manufacturers wanted a high tariff as a barrier to foreign goods, but competition among states made this impossible without a central government. Political scientist David C. Hendrickson writes that two prominent political leaders in the Confederation, John Jay of New York and Thomas Burke of North Carolina believed that "the authority of the congress rested on the prior acts of the several states, to which the states gave their voluntary consent, and until those obligations were fulfilled, neither nullification of the authority of congress, exercising its due powers, nor secession from the compact itself was consistent with the terms of their original pledges." However, what if one or more states do violate the compact? One view, not only about the Articles but also the later Constitution, was that the state or states injured by such a breach could rightfully secede. This position was held by, among others, Thomas Jefferson and John Calhoun. This view, among others, was presented against declarations of secession from the Union by southern slave states as the American Civil War began.
The Atlantic Ocean is the second-largest of the world's oceanic divisions. With a total area of about 106.4 million square kilometres (41.1 million square miles), it covers approximately twenty percent of the Earth's surface and about twenty-six percent of its water surface area. The first part of its name refers to the Atlas of Greek mythology, making the Atlantic the "Sea of Atlas". The oldest known mention of this name is contained in "The Histories" of Herodotus around 450 BC (I 202); see also: "Atlas Mountains". Another name historically used was the ancient term Ethiopic Ocean, derived from Ethiopia, whose name was sometimes used as a synonym for all of Africa and thus for the ocean. Before Europeans discovered other oceans, the term "ocean" itself was to them synonymous with the waters beyond Western Europe that we now know as the Atlantic and which the Greeks had believed to be a gigantic river encircling the world; see Oceanus. The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between the Americas to the west, and Eurasia and Africa to the east. As one component of the interconnected global ocean, it is connected in the north to the Arctic Ocean (which is sometimes considered a sea of the Atlantic), to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south. (Other definitions describe the Atlantic as extending southward to Antarctica.) The equator subdivides it into the North Atlantic Ocean and South Atlantic Ocean.
The Atlantic Ocean is bounded on the west by North and South America. It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea. To the east, the boundaries of the ocean proper are Europe, the Strait of Gibraltar (where it connects with the Mediterranean Sea, one of its marginal seas and, in turn, the Black Sea) and Africa. In the southeast, the Atlantic merges into the Indian Ocean. The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border. Some authorities show it extending south to Antarctica, while others show it bounded at the 60° parallel by the Southern Ocean. In the southwest, the Drake Passage connects it to the Pacific Ocean. The man-made Panama Canal links the Atlantic and Pacific. Besides those mentioned, other large bodies of water adjacent to the Atlantic are the Caribbean Sea, the Gulf of Mexico, Hudson Bay, the Arctic Ocean, the Mediterranean Sea, the North Sea, the Baltic Sea, and the Celtic Sea. Covering approximately 22% of Earth's surface, the Atlantic is second in size to the Pacific. With its adjacent seas it occupies an area of about; without them, it has an area of. The land that drains into the Atlantic covers four times that of either the Pacific or Indian oceans. The volume of the Atlantic with its adjacent seas is 354,700,000 cubic kilometers (85,100,000 cu mi) and without them 323,600,000 cubic kilometres (77,640,000 cu mi). The average depth of the Atlantic, with its adjacent seas, is; without them it is. The greatest depth, is in the Puerto Rico Trench. The Atlantic's width varies from between Brazil and Sierra Leone to over in the south.
"On the West." The Eastern limits of the Caribbean Sea, the Southeastern limits of the Gulf of Mexico from the North coast of Cuba to Key West, the Southwestern limit of the Bay of Fundy and the Southeastern and Northeastern limits of the Gulf of St. Lawrence. "On the North." The Southern limit of Davis Strait from the coast of Labrador to Greenland and the Southwestern limit of the Greenland Sea and Norwegian Sea from Greenland to the Shetland Islands. "On the East." The Northwestern limit of the North Sea, the Northern and Western limits of the Scottish Seas, the Southern limit of the Irish Sea, the Western limits of the Bristol and English Channels, of the Bay of Biscay and of the Mediterranean Sea. "On the South." The equator, from the coast of Brazil to the Southwestern limit of the Gulf of Guinea. "On the Southwest." The meridian of Cape Horn, Chile (67°16'W) from Tierra del Fuego to the Antarctic Continent; a line from Cape Virgins () to Cape Espiritu Santo, Tierra del Fuego, the Eastern entrance to Magellan Strait, Chile "On the West." The limit of the Rio de La Plata. "On the North." The Southern limit of the North Atlantic Ocean. "On the Northeast." The limit of the Gulf of Guinea. "On the Southeast." From Cape Agulhas along the meridian of 20° East to the Antarctic continent. "On the South." The Antarctic Continent. Note that these definitions exclude any marginal waterbodies that are separately defined by the IHO (such as the Bay of Biscay and Gulf of Guinea), though these are usually considered to be part of the Atlantic Ocean. In 2000 the IHO redefined the Atlantic Ocean, moving its southern limit to 60°S, with the waters south of that line identified as the Southern Ocean. This new definition has not yet been ratified (a reservation has been lodged by Australia) though it is in use by the IHO and others. If and when adopted, the 2000 definition will be published in the 4th edition of "Limits of Oceans and Seas", restoring the Southern Ocean as originally outlined in the 2nd edition and subsequently omitted from the 3rd edition.
The principal feature of the bathymetry (bottom topography) is a submarine mountain range called the Mid-Atlantic Ridge. It extends from Iceland in the north to approximately 58° South latitude, reaching a maximum width of about. A great rift valley also extends along the ridge over most of its length. The depth of water at the apex of the ridge is less than in most places, the bottom of the ridge is three times as deep and of course several peaks rise above the water and form islands. The South Atlantic Ocean has an additional submarine ridge, the Walvis Ridge. The Mid-Atlantic Ridge separates the Atlantic Ocean into two large troughs with depths from. Transverse ridges running between the continents and the Mid-Atlantic Ridge divide the ocean floor into numerous basins. Some of the larger basins are the Blake, Guiana, North American, Cape Verde, and Canaries basins in the North Atlantic. The largest South Atlantic basins are the Angola, Cape, Argentina, and Brazil basins. The deep ocean floor is thought to be fairly flat with occasional deeps, trenches, seamounts and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.
On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3 - 3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the salinity values are just north of the equator (because of heavy tropical rainfall), in general the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation. Surface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below. Maximum temperatures occur north of the equator, and minimum values are found in the polar regions. In the middle latitudes, the area of maximum temperature variations, values may vary by 7-8 °C (12-15 °F). The Atlantic Ocean consists of four major water masses. The North and South Atlantic central waters make up the surface. The sub-Antarctic intermediate water extends to depths of. The North Atlantic Deep Water reaches depths of as much as. The Antarctic Bottom Water occupies ocean basins at depths greater than 4,000 meters. Within the North Atlantic, ocean currents isolate the Sargasso Sea, a large elongated body of water, with above average salinity. The Sargasso Sea contains large amounts of seaweed and is also the spawning ground for both the European eel and the American eel. The Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise. The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur during each 24 lunar hours. In latitudes above 40° North some east-west oscillation occurs.
Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great heat retention capacity, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures. The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas. The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift, for example, warms the atmosphere of the British Isles and north-western Europe, and the cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas. Hurricanes develop in the southern part of the North Atlantic Ocean.
The Atlantic Ocean appears to be the second youngest of the five oceans. Apparently it did not exist prior to 130 million years ago, when the continents that formed from the breakup of the ancestral super continent, Pangaea, were drifting apart from seafloor spreading. The Atlantic has been extensively explored since the earliest settlements along its shores. The Vikings, the Portuguese, and Christopher Columbus were the most famous among early explorers. After Columbus, European exploration rapidly accelerated, and many new trade routes were established. As a result, the Atlantic became and remains the major artery between Europe and the Americas (known as transatlantic trade). Scientific explorations include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.
The Ethiopic Ocean or Ethiopian Ocean (Okeanos Aithiopos) is an old name for what is now called the South Atlantic Ocean, which is separated from the North Atlantic Ocean by a narrow region between Natal, Brazil and Monrovia, Liberia. Use of this term illustrates a past trend towards referring to the whole continent of Africa by the name "Aethiopia". The modern nation of Ethiopia, in northeast Africa, is nowhere near the Ethiopic Ocean, which would be said to lie off the west coast of Africa. The term "Ethiopian Ocean" sometimes appeared until the mid-19th century.
The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves. The Atlantic hosts the world's richest fishing resources, especially in the waters covering the shelves. The major fish are cod, haddock, hake, herring, and mackerel. The most productive areas include Newfoundland's Grand Banks, the Nova Scotia shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Dogger Bank of the North Sea, and the Falkland Banks. Eel, lobster, and whales appear in great quantities. Because environmental threats from oil spills, marine debris, and the incineration of toxic wastes at sea, various international treaties attempt to reduce pollution.
From October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea. A clockwise warm-water gyre occupies the northern Atlantic, and a counter-clockwise warm-water gyre appears in the southern Atlantic. The Mid-Atlantic Ridge, a rugged north-south centerline for the entire Atlantic basin, first discovered by the Challenger Expedition dominates the ocean floor. This was formed by the vulcanism that also formed the ocean floor and the islands rising from it. The Atlantic has irregular coasts indented by numerous bays, gulfs, and seas. These include the Norwegian Sea, Baltic Sea, North Sea, Labrador Sea, Black Sea, Gulf of Saint Lawrence, Bay of Fundy, Gulf of Maine, Mediterranean Sea, Gulf of Mexico, and Caribbean Sea. Islands include Greenland, Iceland, Faroe Islands, Great Britain (including numerous surrounding islands), Ireland, Rockall, Newfoundland, Sable Island, Azores, Madeira, Bermuda, Canary Islands, Caribbean, Cape Verde, São Tomé and Príncipe, Annobón Province, St. Peter Island, Fernando de Noronha, Rocas Atoll, Ascension Island, Saint Helena, The Islands of Trindad, Tristan da Cunha, Gough Island (Also known as Diego Alvarez), Falkland Islands, Tierra del Fuego, South Georgia Island, South Sandwich Islands, and Bouvet Island.
Icebergs are common from February to August in the Davis Strait, Denmark Strait, and the northwestern Atlantic and have been spotted as far south as Bermuda and Madeira. Ships are subject to superstructure icing in the extreme north from October to May. Persistent fog can be a maritime hazard from May to September, as can hurricanes north of the equator (May to December). The United States' southeast coast has a long history of shipwrecks due to its many shoals and reefs. The Virginia and North Carolina coasts were particularly dangerous. The Bermuda Triangle is popularly believed to be the site of numerous aviation and shipping incidents because of unexplained and supposedly mysterious causes, but Coast Guard records do not support this belief.
Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea. In 2005, there was some concern that warm northern European currents were slowing down, but no scientific consensus formed from that evidence. On June 7, 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature.
Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone. Marine debris, also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.
Arthur Schopenhauer (22 February 1788 – 21 September 1860) was a German philosopher known for his atheistic pessimism and philosophical clarity. At age 25, he published his doctoral dissertation, "On the Fourfold Root of the Principle of Sufficient Reason", which examined the fundamental question of whether reason alone can unlock answers about the world. Schopenhauer's most influential work, "The World as Will and Representation", emphasized the role of man's basic motivation, which Schopenhauer called will. His analysis of will led him to the conclusion that emotional, physical, and sexual desires can never be fulfilled. Consequently, he favored a lifestyle of negating human desires, similar to the teachings of ancient Greek Stoic philosophers, Buddhism, and Vedanta. Schopenhauer's metaphysical analysis of will, his views on human motivation and desire, and his aphoristic writing style influenced many well-known thinkers including Friedrich Nietzsche, Richard Wagner, Ludwig Wittgenstein, Erwin Schrödinger, Albert Einstein, Sigmund Freud, Otto Rank, Carl Gustav Jung, and Jorge Luis Borges.
Arthur Schopenhauer was born in the city of Danzig (Gdańsk) as the son of Heinrich Floris Schopenhauer and Johanna Schopenhauer, both descendants of wealthy German Patrician families. When the Kingdom of Prussia acquired the Polish-Lithuanian Commonwealth city of Danzig in 1793, Schopenhauer's family moved to Hamburg. In 1805, Schopenhauer's father committed suicide. Schopenhauer's mother Johanna shortly after moved to Weimar, then the centre of German literature, to pursue her writing career. After one year, Schopenhauer left the family business in Hamburg to join her. Schopenhauer became a student at the University of Göttingen in 1809. There he studied metaphysics and psychology under Gottlob Ernst Schulze, the author of "Aenesidemus", who advised him to concentrate on Plato and Kant. In Berlin, from 1811 to 1812, he had attended lectures by the prominent post-Kantian philosopher J. G. Fichte and the theologian Schleiermacher. In 1814, Schopenhauer began his seminal work "The World as Will and Representation" ("Die Welt als Wille und Vorstellung"). He would finish it in 1818 and publish it the following year. In Dresden in 1819, Schopenhauer fathered an illegitimate child who was born and died the same year. In 1820, Schopenhauer became a lecturer at the University of Berlin. He scheduled his lectures to coincide with those of the famous philosopher G. W. F. Hegel, whom Schopenhauer described as a "clumsy charlatan". However, only five students turned up to Schopenhauer's lectures, and he dropped out of academia. A late essay, "On University Philosophy", expressed his resentment towards university philosophy. While in Berlin, Schopenhauer was named as a defendant in an action at law initiated by a woman named Caroline Marquet. She asked for damages, alleging that Schopenhauer had pushed her. According to Schopenhauer's court testimony, she deliberately annoyed him by raising her voice while standing right outside his door. Marquet alleged that the philosopher had assaulted and battered her after she refused to leave his doorway. Her companion testified that she saw Marquet prostrate outside his apartment. Because Marquet won the lawsuit, he made payments to her for the next twenty years. When she died, he wrote on a copy of her death certificate, "Obit anus, abit onus" ("The old woman dies, the burden flies"). In 1821, he fell in love with nineteen-year old opera singer, Caroline Richter (called Medon), and had a relationship with her for several years. He discarded marriage plans, however, writing, "Marrying means to halve one's rights and double one's duties", and "Marrying means, to grasp blindfolded into a sack hoping to find out an eel out of an assembly of snakes." When he was forty-three years old, seventeen-year old Flora Weiss recorded rejecting him in her diary. Schopenhauer had a notably strained relationship with his mother Johanna Schopenhauer. After his father's death, Arthur Schopenhauer endured two long years of drudgery as a merchant, in honor of his dead father. Afterwards, his mother retired to Weimar, and Arthur dedicated himself wholly to studies in the gymnasium of Gotha. After he left it in disgust after seeing one of the masters lampooned, he went to live with his mother. But by that time she had already opened her infamous salon, and Arthur was not compatible with the vain, ceremonious ways of the salon. He was also disgusted by the ease with which Johanna had forgotten his father's memory. Therefore, he gave university life a shot. There, he wrote his first book, "On the Fourfold Root of the Principle of Sufficient Reason". She informed him that the book was incomprehensible and it was unlikely that anyone would ever buy a copy. In a fit of temper Arthur told her that his work would be read long after the rubbish she wrote would have been totally forgotten. In 1831, a cholera epidemic broke out in Berlin and Schopenhauer left the city. Schopenhauer settled permanently in Frankfurt in 1833, where he remained for the next twenty-seven years, living alone except for a succession of pet poodles named Atma and Butz. Schopenhauer had a robust constitution, but in 1860 his health began to deteriorate. He died of heart failure on 21 September 1860, while sitting in his armchair at home. He was 72.
A key focus of Schopenhauer was his investigation of individual motivation. Before Schopenhauer, Hegel had popularized the concept of "Zeitgeist", the idea that society consisted of a collective consciousness which moved in a distinct direction, dictating the actions of its members. Schopenhauer, a reader of both Kant and Hegel, criticized their logical optimism and the belief that individual morality could be determined by society and reason. Schopenhauer believed that humans were motivated only by their own basic desires, or "Wille zum Leben" (Will to Live), which directed all of mankind. For Schopenhauer, human desire was futile, illogical, directionless, and, by extension, so was all human action in the world. To Schopenhauer, the Will is a metaphysical existence which controls not only the actions of individual, intelligent agents, but ultimately all observable phenomena. Will, for Schopenhauer, is what Kant called the "thing-in-itself".
For Schopenhauer, human desiring, "willing," and craving cause suffering or pain. A temporary way to escape this pain is through aesthetic contemplation (a method comparable to Zapffe's "Sublimation"). This is the next best way, short of not willing at all, which is the best way. Total absorption in the world as representation prevents a person from suffering the world as will. Art diverts the spectator's attention from the grave everyday world and lifts him or her into a world that consists of mere play of images. With music, the auditor becomes engrossed with a playful form of the will, which is normally deadly serious. Music was also given a special status in Schopenhauer's aesthetics as it did not rely upon the medium of phenomenal representation. Music artistically presents the will itself, not the way that the will appears to an individual observer. According to Daniel Albright, "Schopenhauer thought that music was the only art that did not merely copy ideas, but actually embodied the will itself."
Schopenhauer was perhaps even more influential in his treatment of man's psychology than he was in the realm of philosophy. ...one ought rather to be surprised that a thing [sex] which plays throughout so important a part in human life has hitherto practically been disregarded by philosophers altogether, and lies before us as raw and untreated material. He gave a name to a force within man which he felt had invariably precedence over reason: the Will to Live or Will to Life ("Wille zum Leben"), defined as an inherent drive within human beings, and indeed all creatures, to stay alive and to reproduce. The ultimate aim of all love affairs... is more important than all other aims in man's life; and therefore it is quite worthy of the profound seriousness with which everyone pursues it. What is decided by it is nothing less than the composition of the next generation... These ideas foreshadowed Darwin's discovery of evolution and Freud's concepts of the libido and the unconscious mind.
Schopenhauer's politics were, for the most part, an echo of his system of ethics (the latter being expressed in "Die beiden Grundprobleme der Ethik", available in English as two separate books, "On the Basis of Morality" and "On the Freedom of the Will"). Ethics also occupies about one quarter of his central work, "The World as Will and Representation". In occasional political comments in his "Parerga and Paralipomena" and "Manuscript Remains", Schopenhauer described himself as a proponent of limited government. What was essential, he thought, was that the state should "leave each man free to work out his own salvation", and so long as government was thus limited, he would "prefer to be ruled by a lion than one of [his] fellow rats" — i.e., by a monarch, rather than a democrat. Schopenhauer did, however, share the view of Thomas Hobbes on the necessity of the state, and of state violence, to check the destructive tendencies innate to our species. Schopenhauer, by his own admission, did not give much thought to politics, and several times he writes proudly of how little attention he had paid "to political affairs of [his] day". In a life that spanned several revolutions in French and German government, and a few continent-shaking wars, he did indeed maintain his aloof position of "minding not the times but the eternities". He wrote many disparaging remarks about Germany and the Germans. A typical example is, "For a German it is even good to have somewhat lengthy words in his mouth, for he thinks slowly, and they give him time to reflect." The highest civilization and culture, apart from the ancient Hindus and Egyptians, are found exclusively among the white races; and even with many dark peoples, the ruling caste or race is fairer in colour than the rest and has, therefore, evidently immigrated, for example, the Brahmans, the Incas, and the rulers of the South Sea Islands. All this is due to the fact that necessity is the mother of invention because those tribes that emigrated early to the north, and there gradually became white, had to develop all their intellectual powers and invent and perfect all the arts in their struggle with need, want and misery, which in their many forms were brought about by the climate. This they had to do in order to make up for the parsimony of nature and out of it all came their high civilization. Despite this, he was adamantly against differing treatment of races, was fervently anti-slavery, and supported the abolitionist movement in the United States. He describes the treatment of "[our] innocent black brothers whom force and injustice have delivered into [the slave-master's] devilish clutches" as "belonging to the blackest pages of mankind's criminal record". While all other religions endeavor to explain to the people by symbols the metaphysical significance of life, the religion of the Jews is entirely immanent and furnishes nothing but a mere war-cry in the struggle with other nations. Je me croyais loin de la religion pourtant. Je ne songeais pas que, de Schopenhauer que j'admirais plus que de raison, à l'"Ecclésiaste", et au "Livre de Job", il n'y avait qu'un pas. Les prémisses sur le Pessimisme sont les mêmes, seulement lorsqu'il s'agit de conclure, le philosophe se dérobe. [...] L'Eglise, elle, explique les origines et les causes, signale les fins, présente les remèdes; elle ne se contente pas de vous donner une consultation d'âme, elle vous traite et elle vous guérit alors que le médicastre allemand, après vous avoir bien démontré que l'affection dont vous souffrez est incurable, vous tourne, en ricanant, le dos.
In Schopenhauer's 1851 essay "Of Women" ("Über die Weiber", full text), he expressed his opposition to what he called "Teutonico-Christian stupidity" on female affairs. He claimed that "woman is by nature meant to obey", and opposed Schiller's poem in honor of women, "Würde der Frauen" ("Dignity of Women"). The essay does give two compliments, however: that "women are decidedly more sober in their judgment than [men] are" and are more sympathetic to the suffering of others. However, the latter was discounted as weakness rather than humanitarian virtue. Schopenhauer's controversial writings have influenced many, from Friedrich Nietzsche to nineteenth-century feminists. Schopenhauer's biological analysis of the difference between the sexes, and their separate roles in the struggle for survival and reproduction, anticipates some of the claims that were later ventured by sociobiologists and evolutionary psychologists in the twentieth century. After the elderly Schopenhauer sat for a sculpture portrait by Elisabet Ney, he told Richard Wagner's friend Malwida von Meysenbug, "I have not yet spoken my last word about women. I believe that if a woman succeeds in withdrawing from the mass, or rather raising herself above the mass, she grows ceaselessly and more than a man."
With our knowledge of the complete unalterability both of character and of mental faculties, we are led to the view that a real and thorough improvement of the human race might be reached not so much from outside as from within, not so much by theory and instruction as rather by the path of generation. Plato had something of the kind in mind when, in the fifth book of his "Republic", he explained his plan for increasing and improving his warrior caste. If we could castrate all scoundrels and stick all stupid geese in a convent, and give men of noble character a whole harem, and procure men, and indeed thorough men, for all girls of intellect and understanding, then a generation would soon arise which would produce a better age than that of Pericles. In another context, Schopenhauer reiterated his antidemocratic-eugenic thesis: "If you want Utopian plans, I would say: the only solution to the problem is the despotism of the wise and noble members of a genuine aristocracy, a genuine nobility, achieved by mating the most magnanimous men with the cleverest and most gifted women. This proposal constitutes my Utopia and my Platonic Republic". Analysts (e.g., Keith Ansell-Pearson) have suggested that Schopenhauer's advocacy of anti-egalitarianism and eugenics influenced the neo-aristocratic philosophy of Friedrich Nietzsche, who initially considered Schopenhauer his mentor. Views on homosexuality & pederasty. Schopenhauer was also one of the first philosophers since the days of Greek philosophy to address the subject of male homosexuality. In the third, expanded edition of "The World as Will and Representation" (1856), Schopenhauer added an appendix to his chapter on the "Metaphysics of Sexual Love". He also wrote that homosexuality did have the benefit of preventing ill-begotten children. Concerning this, he stated, "... the vice we are considering appears to work directly against the aims and ends of nature, and that in a matter that is all important and of the greatest concern to her, it must in fact serve these very aims, although only indirectly, as a means for preventing greater evils." Shrewdly anticipating the interpretive distortion on the part of the popular mind of his attempted scientific "explanation" of pederasty as a personal "advocacy" of a phenomenon Schopenhauer otherwise describes, in terms of spiritual ethics, as an "objectionable aberration", Schopenhauer sarcastically concludes the appendix with the statement that "by expounding these paradoxical ideas, I wanted to grant to the professors of philosophy a small favour, for they are very disconcerted by the ever-increasing publicization of my philosophy which they so carefully concealed. I have done so by giving them the opportunity of slandering me by saying that I defend and commend pederasty."
Schopenhauer said he was influenced by the Upanishads, Immanuel Kant, and Plato. References to Eastern philosophy and religion appear frequently in Schopenhauer's writing. As noted above, he appreciated the teachings of the Buddha and even called himself a "Buddhist". He said that his philosophy could not have been conceived before these teachings were available. If the reader has also received the benefit of the Vedas, the access to which by means of the Upanishads is in my eyes the greatest privilege which this still young century (1818) may claim before all previous centuries, if then the reader, I say, has received his initiation in primeval Indian wisdom, and received it with an open heart, he will be prepared in the very best way for hearing what I have to tell him. It will not sound to him strange, as to many others, much less disagreeable; for I might, if it did not sound conceited, contend that every one of the detached statements which constitute the Upanishads, may be deduced as a necessary result from the fundamental thoughts which I have to enunciate, though those deductions themselves are by no means to be found there. He summarised the influence of the Upanishads thus: “It has been the solace of my life, it will be the solace of my death!” Other influences were: Shakespeare, Jean Jacques Rousseau, John Locke, Baruch Spinoza, Matthias Claudius, George Berkeley, David Hume, René Descartes.
Schopenhauer accepted Kant's double-aspect of the universe—the phenomenal (world of experience) and the noumenal (world independent of experience). Some commentators suggest that Schopenhauer claimed that the noumenon, or thing-in-itself, was the basis for Schopenhauer's concept of the will. Other commentators suggest that Schopenhauer considered will to be only a subset of the "thing-in-itself" class, namely that which we can most directly experience. Schopenhauer's identification of the Kantian "noumenon" (i.e., the actually existing entity) with what he termed "will" deserves some explanation. The noumenon was what Kant called the "Ding an Sich", the "Thing in Itself", the reality that is the foundation of our sensory and mental representations of an external world. In Kantian terms, those sensory and mental representations are mere phenomena. Schopenhauer departed from Kant in his description of the relationship between the phenomenon and the noumenon. According to Kant, things-in-themselves ground the phenomenal representations in our minds; Schopenhauer, on the other hand, believed phenomena and noumena to be two different sides of the same coin. Noumena do not "cause" phenomena, but rather phenomena are simply the way by which our minds perceive the noumena, according to the Principle of Sufficient Reason. This is explained more fully in Schopenhauer's doctoral thesis, "On the Fourfold Root of the Principle of Sufficient Reason". Schopenhauer's second major departure from Kant's epistemology concerns the body. Kant's philosophy was formulated as a response to the radical philosophical skepticism of David Hume, who claimed that causality could not be observed empirically. Schopenhauer begins by arguing that Kant's demarcation between external objects, knowable only as phenomena, and the Thing in Itself of noumenon, contains a significant omission. There is, in fact, one physical object we know more intimately than we know any object of sense perception: our own body. We know our human bodies have boundaries and occupy space, the same way other objects known only through our named senses do. Though we seldom think of our body as a physical object, we know even before reflection that it shares some of an object's properties. We understand that a watermelon cannot successfully occupy the same space as an oncoming truck; we know that if we tried to repeat the experiment with our own body, we would obtain similar results – we know this even if we do not understand the physics involved. We know that our consciousness inhabits a physical body, similar to other physical objects only known as phenomena. Yet our consciousness is not commensurate with our body. Most of us possess the power of voluntary motion. We usually are not aware of the breathing of our lungs or the beating of our heart unless somehow our attention is called to them. Our ability to control either is limited. Our kidneys command our attention on their schedule rather than one we choose. Few of us have any idea what our liver is doing right now, though this organ is as needful as lungs, heart, or kidneys. The conscious mind is the servant, not the master, of these and other organs; these organs have an agenda which the conscious mind did not choose, and over which it has limited power. When Schopenhauer identifies the "noumenon" with the desires, needs, and impulses in us that we name "will," what he is saying is that we participate in the reality of an otherwise unachievable world outside the mind through will. We cannot "prove" that our mental picture of an outside world corresponds with a reality by reasoning; through will, we know – without thinking – that the world can stimulate us. We suffer fear, or desire: these states arise involuntarily; they arise prior to reflection; they arise even when the conscious mind would prefer to hold them at bay. The rational mind is, for Schopenhauer, a leaf borne along in a stream of pre-reflective and largely unconscious emotion. That stream is will, and through will, if not through logic, we can participate in the underlying reality beyond mere phenomena. It is for this reason that Schopenhauer identifies the "noumenon" with what we call our will. In his criticism of Kant, Schopenhauer claimed that sensation and understanding are separate and distinct abilities. Yet, for Kant, an object is known through each of them. Kant wrote: "… [T]here are two stems of human knowledge... namely, sensibility and understanding, objects being given by the former [sensibility] and thought by the latter [understanding]." Schopenhauer disagreed. He asserted that mere sense impressions, not objects, are given by sensibility. According to Schopenhauer, objects are intuitively perceived by understanding and are discursively thought by reason (Kant had claimed that (1) the understanding thinks objects through concepts and that (2) reason seeks the unconditioned or ultimate answer to "why?"). Schopenhauer said that Kant's mistake regarding perception resulted in all of the obscurity and difficult confusion that is exhibited in the Transcendental Analytic section of his critique.
Schopenhauer has had a massive influence upon later thinkers, though more so in the arts (especially literature and music) and psychology than in philosophy. His popularity peaked in the early twentieth century, especially during the Modernist era, and waned somewhat thereafter. Nevertheless, a number of recent publications have reinterpreted and modernised the study of Schopenhauer. His theory is also being explored by some modern philosophers as a precursor to evolutionary theory and modern evolutionary psychology.
If I were to say that the so-called philosophy of this fellow Hegel is a colossal piece of mystification which will yet provide posterity with an inexhaustible theme for laughter at our times, that it is a pseudo-philosophy paralyzing all mental powers, stifling all real thinking, and, by the most outrageous misuse of language, putting in its place the hollowest, most senseless, thoughtless, and, as is confirmed by its success, most stupefying verbiage, I should be quite right. Further, if I were to say that this summus philosophus [...] scribbled nonsense quite unlike any mortal before him, so that whoever could read his most eulogized work, the so-called "Phenomenology of the Mind", without feeling as if he were in a madhouse, would qualify as an inmate for Bedlam, I should be no less right. In his Foreword to the first edition of his work "Die beiden Grundprobleme der Ethik", Schopenhauer suggested that he had shown Hegel to have fallen prey to the "Post hoc ergo propter hoc" fallacy. Schopenhauer thought that Hegel used deliberately impressive but ultimately vacuous verbiage. He suggested his works were filled with "castles of abstraction" that sounded impressive but ultimately had no content. He also thought that his glorification of church and state were designed for personal advantage and had little to do with the search for philosophical truth. For instance, the Right Hegelians interpreted Hegel as viewing the Prussian state of his day as perfect and the goal of all history up until then.
It is the most satisfying and elevating reading (with the exception of the original text) which is possible in the world; it has been the solace of my life and will be the solace of my death. It is well known that the book "Oupnekhat" (Upanishad) always lay open on his table, and he invariably studied it before sleeping at night. He called the opening up of Sanskrit literature "the greatest gift of our century", and predicted that the philosophy and knowledge of the Upanishads would become the cherished faith of the West.
As a consequence of his philosophy, Schopenhauer was very concerned about the rights of animals. For him, all animals, including humans, are phenomenal manifestations of Will. The word "will" designated, for him, force, power, impulse, energy, and desire; it is the closest word we have that can signify both the real essence of all external things and also our own direct, inner experience. Since everything is basically Will, then humans and animals are fundamentally the same and can recognize themselves in each other. For this reason, he claimed that a good person would have sympathy for animals, who are our fellow sufferers. Compassion for animals is intimately associated with goodness of character, and it may be confidently asserted that he, who is cruel to living creatures, cannot be a good man. Nothing leads more definitely to a recognition of the identity of the essential nature in animal and human phenomena than a study of zoology and anatomy. “The assumption that animals are without rights and the illusion that our treatment of them has no moral significance is a positively outrageous example of Western crudity and barbarity. Universal compassion is the only guarantee of morality." In 1841, he praised the establishment, in London, of the Society for the Prevention of Cruelty to Animals, and also the Animals' Friends Society in Philadelphia. Schopenhauer even went so far as to protest against the use of the pronoun "it" in reference to animals because it led to the treatment of them as though they were inanimate things. To reinforce his points, Schopenhauer referred to anecdotal reports of the look in the eyes of a monkey who had been shot and also the grief of a baby elephant whose mother had been killed by a hunter. He was very attached to his succession of pet poodles. Schopenhauer criticized Spinoza's belief that animals are to be used as a mere means for the satisfaction of humans.
Many Europeans, in the 1830s and 1840s, including Schopenhauer himself, found a correspondence between Schopenhauerian thought and the Four Noble Truths of Buddhism. Similarities centered on the principles that life involves suffering, that suffering is caused by desire, and that the extinction of desire leads to salvation. Thus three of the four "truths of the Buddha" correspond to Schopenhauer's doctrine of the will. In Buddhism, however, while greed and lust are always unskillful, desire is ethically variable - it can be skillful, unskillful, or neutral. In the Buddhist perspective, the enemy to be defeated is craving rather than desire in general. For Schopenhauer, Will had ontological primacy over the intellect; in other words, desire is understood to be prior to thought. Schopenhauer felt this was similar to notions of purushartha or goals of life in Vedanta Hinduism. If I wished to take the results of my philosophy as the standard of truth, I should have to concede to Buddhism pre-eminence over the others. In any case, it must be a pleasure to me to see my doctrine in such close agreement with a religion that the majority of men on earth hold as their own, for this numbers far more followers than any other. And this agreement must be yet the more pleasing to me, inasmuch as "in my philosophizing I have certainly not been under its influence" [emphasis added]. For up till 1818, when my work appeared, there was to be found in Europe only a very few accounts of Buddhism. Buddhist philosopher Nishitani Keiji, however, sought to distance Buddhism from Schopenhauer. Philosophy... is a science, and as such has no articles of faith; accordingly, in it nothing can be assumed as existing except what is either positively given empirically, or demonstrated through indubitable conclusions. This actual world of what is knowable, in which we are and which is in us, remains both the material and the limit of our consideration.
Angola, officially the Republic of Angola (,;), is a country in south-central Africa bordered by Namibia on the south, Democratic Republic of the Congo on the north, and Zambia on the east; its west coast is on the Atlantic Ocean. The exclave province of Cabinda has a border with the Republic of the Congo and the Democratic Republic of the Congo. Angola was a Portuguese overseas territory from the 16th century to 1975. After independence, Angola was the scene of an intense civil war from 1975 to 2002. The country is the second-largest petroleum and diamond producer in sub-Saharan Africa; however, its life expectancy and infant mortality rates are both among the worst ranked in the world. In August 2006, a peace treaty was signed with a faction of the FLEC, a separatist guerrilla group from the Cabinda exclave in the North, which is still active. About 65% of Angola's oil comes from that region.
Khoisan hunter-gatherers are some of the earliest known modern human inhabitants of the area. They were largely replaced by Bantu tribes during the Bantu migrations, though small numbers of Khoisans remain in parts of southern Angola to the present day. The Bantu came from the north, probably from somewhere near the present-day Republic of Cameroon. When they reached what is now Angola, they encountered the Khoisans, Bushmen and other groups considerably less technologically advanced than themselves, whom they easily dominated with their superior knowledge of metal-working, ceramics and agriculture. The establishment of the Bantus took many centuries and gave rise to various groups who took on different ethnic characteristics. The BaKongo kingdoms of Angola established trade routes with other trading cities and civilizations up and down the coast of southwestern and West Africa but engaged in little or no transoceanic trade. This contrasts with the Great Zimbabwe Mutapa civilization which traded with India, the Persian Gulf civilizations and China. The BaKongo engaged in limited trading with Great Zimbabwe, exchanging copper and iron for salt, food and raffia textiles across the Kongo River.
The geographical areas now designated as Angola, first became subject to incursions by the Portuguese in the late 15th century. In 1483, when Portugal established relations with the Kongo State, Ndongo and Lunda existed. The Kongo State stretched from modern Gabon in the north to the Kwanza River in the south. Angola became a link in European trade with India and Southeast Asia. The Portuguese explorer Paulo Dias de Novais founded Luanda in 1575 as "São Paulo de Loanda", with a hundred families of settlers and four hundred soldiers. Benguela, a Portuguese fort from 1587 which became a town in 1617, was another important early settlement they founded and ruled. The Portuguese would establish several settlements, forts and trading posts along the coastal strip of current-day Angola, which relied on slave trade, commerce in raw materials, and exchange of goods for survival. The African slave trade provided a large number of black slaves to Europeans and their African agents. For example, in what is now Angola, the Imbangala economy was heavily focused on the slave trade. European traders would export manufactured goods to the coast of Africa where they would be exchanged for slaves. Within the Portuguese Empire, most black African slaves were traded to Portuguese merchants who bought them to sell as cheap labour for use on Brazilian agricultural plantations. This trade would last until the first half of the 1800s. The Portuguese gradually took control of the coastal strip during the sixteenth century by a series of treaties and wars forming the Portuguese colony of Angola. Taking advantage of the Portuguese Restoration War, the Dutch occupied Luanda from 1641 to 1648, where they allied with local peoples, consolidating their colonial rule against the remaining Portuguese resistance. In 1648, a fleet under the command of Salvador de Sá retook Luanda for Portugal and initiated a conquest of the lost territories, which restored Portugal to its former possessions by 1650. Treaties regulated relations with Congo in 1649 and Njinga's Kingdom of Matamba and Ndongo in 1656. The conquest of Pungo Andongo in 1671 was the last great Portuguese expansion, as attempts to invade Congo in 1670 and Matamba in 1681 failed. Portugal expanded its territory behind the colony of Benguela in the eighteenth century, and began the attempt to occupy other regions in the mid-nineteenth century. The process resulted in few gains until the 1880s. Development of the hinterland began after the Berlin Conference in 1885 fixed the colony's borders, and British and Portuguese investment fostered mining, railways, and agriculture. Full Portuguese administrative control of the hinterland did not occur until the beginning of the twentieth century. In 1951, the colony was designated as an overseas province, called Overseas Province of Angola. Portugal had a presence in Angola for nearly five hundred years, and the population's initial reaction to calls for independence was mixed. More overtly political organisations first appeared in the 1950s, and began to make organised demands for their rights, especially in international forums such as the Non-Aligned Movement. The Portuguese regime, meanwhile, refused to accede to the nationalists' demands of separatism, provoking an armed conflict that started in 1961 when black guerrillas attacked both white and black civilians in cross-border operations in northeastern Angola. The war came to be known as the Colonial War. In this struggle, the principal protagonists were the MPLA (Popular Movement for the Liberation of Angola), founded in 1956, the FNLA (National Front for the Liberation of Angola), which appeared in 1961, and UNITA (National Union for the Total Independence of Angola), founded in 1966. After many years of conflict, Angola gained its independence on 11 November 1975, after the 1974 coup d'état in the metropole's capital city of Lisbon which overthrew the Portuguese regime headed by Marcelo Caetano. Portugal's new revolutionary leaders began a process of democratic change at home and acceptance of its former colonies' independence abroad. These events prompted a mass exodus of Portuguese citizens from Portugal's African territories (mostly from Portuguese Angola and Mozambique), creating over a million destitute Portuguese refugees — the "retornados".
After independence in November 1975, Angola faced a devastating civil war which lasted several decades and claimed millions of lives and refugees. Following negotiations held in Portugal, itself under severe social and political turmoil and uncertainty due to the April 1974 revolution, Angola's three main guerrilla groups agreed to establish a transitional government in January 1975. Within two months, however, the FNLA, MPLA and UNITA were fighting each other and the country was well on its way to being divided into zones controlled by rival armed political groups. The superpowers were quickly drawn into the conflict, which became a flash point for the Cold War. The United States, Portugal, Brazil and South Africa supported the FNLA and UNITA. The Soviet Union and Cuba supported the MPLA.
On February 22, 2002, Jonas Savimbi, the leader of UNITA, was killed in combat with government troops, and a cease-fire was reached by the two factions. UNITA gave up its armed wing and assumed the role of major opposition party. Although the political situation of the country began to stabilize, President Dos Santos has so far refused to institute regular democratic processes. Among Angola's major problems are a serious humanitarian crisis (a result of the prolonged war), the abundance of minefields, and the actions of guerrilla movements fighting for the independence of the northern exclave of Cabinda (Frente para a Libertação do Enclave de Cabinda). While most of the internally displaced have now returned home, the general situation for most Angolans remains desperate, and the development facing the government challenging as a consequence.
Angola's motto is "Virtus Unita Fortior", a Latin phrase meaning "Virtue is stronger when united." The executive branch of the government is composed of the President, the Prime Minister (currently Paulo Kassoma) and the Council of Ministers. For decades, political power has been concentrated in the Presidency. The Council of Ministers, composed of all government ministers and vice ministers, meets regularly to discuss policy issues. Governors of the 18 provinces are appointed by and serve at the pleasure of the president. The Constitutional Law of 1992 establishes the broad outlines of government structure and delineates the rights and duties of citizens. The legal system is based on Portuguese and customary law but is weak and fragmented, and courts operate in only twelve of more than 140 municipalities. A Supreme Court serves as the appellate tribunal; a Constitutional Court with powers of judicial review has never been constituted despite statutory authorization. Parliamentary elections held on 5 September 2008, announced MPLA as the winning party with 81% of votes. The closest opposition party was UNITA with 10%. These elections were the first since 1992 and were described as only partly free but certainly not as fair. A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008. Angola scored poorly on the 2008 Ibrahim Index of African Governance. It was ranked 44 from 48 sub-Saharan African countries, scoring particularly badly in the areas of Participation and Human Rights, Sustainable Economic Opportunity and Human Development. The Ibrahim Index uses a number of different variables to compile its list which reflects the state of governance in Africa.
With an area of approximately, the Northern Angolan province of Cabinda is unique in being separated from the rest of the country by a strip, some wide, of the Democratic Republic of Congo (DRC) along the lower Congo river. Cabinda borders the Congo Republic to the north and north-northeast and the DRC to the east and south. The town of Cabinda is the chief population center. According to a 1995 census, Cabinda had an estimated population of 600,000, approximately 400,000 of whom live in neighboring countries. Population estimates are, however, highly unreliable. Consisting largely of tropical forest, Cabinda produces hardwoods, coffee, cocoa, crude rubber and palm oil. The product for which it is best known, however, is its oil, which has given it the nickname, "the Kuwait of Africa". Cabinda's petroleum production from its considerable offshore reserves now accounts for more than half of Angola's output. Most of the oil along its coast was discovered under Portuguese rule by the Cabinda Gulf Oil Company (CABGOC) from 1968 onwards. Since Portugal handed over sovereignty of its former overseas province of Angola to the local independentist groups (MPLA, UNITA, and FNLA), the territory of Cabinda has been a focus of separatist guerrilla actions opposing the Government of Angola (which has employed its military forces, the FAA – Forças Armadas Angolanas) and Cabindan separatists. The Cabindan separatists, FLEC-FAC, announced a virtual Federal Republic of Cabinda under the Presidency of N'Zita Henriques Tiago. One of the characteristics of the Cabindan independence movement is its constant fragmentation, into smaller and smaller factions, in a process which although not totally fomented by the Angolan government, is undoubtedly encouraged and duly exploited by it.
The Angolan Armed Forces (AAF) is headed by a Chief of Staff who reports to the Minister of Defense. There are three divisions—the Army (Exército), Navy (Marinha de Guerra, MGA), and National Air Force (Força Aérea Nacional, FAN). Total manpower is about 110,000. The army is by far the largest of the services with about 100,000 men and women. The Navy numbers about 3,000 and operates several small patrol craft and barges. Air force personnel total about 7,000; its equipment includes Russian-manufactured fighters, bombers, and transport planes. There are also Brazilian-made EMB-312 Tucano for Training role, Czech-made L-39 for training and bombing role, Czech Zlin for training role and a variety of western made aircraft such as C-212\Aviocar, Sud Aviation Alouette III, etc. A small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville).
The National Police departments are: Public Order, Criminal Investigation, Traffic and Transport, Investigation and Inspection of Economic Activities, Taxation and Frontier Supervision, Riot Police and the Rapid Intervention Police. The National Police are in the process of standing up an air wing, which will provide helicopter support for police operations. The National Police are also developing their criminal investigation and forensic capabilities. The National Police has an estimated 6,000 patrol officers, 2,500 Taxation and Frontier Supervision officers, 182 criminal investigators and 100 financial crimes detectives and around 90 Economic Activity Inspectors. The National Police have implemented a modernization and development plan to increase the capabilities and efficiency of the total force. In addition to administrative reorganization; modernization projects include procurement of new vehicles, aircraft and equipment, construction of new police stations and forensic laboratories, restructured training programs and the replacement of AKM rifles with 9 mm UZIs for police officers in urban areas.
At, Angola is the world's twenty-third largest country (after Niger). It is comparable in size to Mali and is nearly twice the size of the US state of Texas, or five times the area of the United Kingdom. Angola is bordered by Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east, and the South Atlantic Ocean to the west. The exclave of Cabinda also borders the Republic of the Congo to the north. Angola's capital, Luanda, lies on the Atlantic coast in the north-west of the country. Angola's average temperature on the coast is in the winter and in the summer.
Angola's economy has undergone a period of transformation in recent years, moving from the disarray caused by a quarter century of civil war to being the fastest growing economy in Africa and one of the fastest in the world. In 2004, China's Eximbank approved a $2 billion line of credit to Angola. The loan is being used to rebuild Angola's infrastructure, and has also limited the influence of the International Monetary Fund in the country. Growth is almost entirely driven by rising oil production which surpassed in late-2005 and was expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate which is owned by the Angolan government. In December 2006, Angola was admitted as a member of OPEC. The economy grew 18% in 2005, 26% in 2006 and 17.6% in 2007 and it's expected to stay above 10% for the rest of the decade. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production. The country's economy has grown since achieving political stability in 2002. However, it faces huge social and economic problems as a result of the almost continual state of conflict from 1961 onwards, although the highest level of destruction and socio-economic damage took place after the 1975 independence, during the long years of civil war. The oil sector, with its fast-rising earnings has been the main driving force behind improvements in overall economic activity – nevertheless, poverty remains widespread. Anti-corruption watchdog Transparency International rated Angola one of the 10 most corrupt countries in the world in 2005. The capital city is the most developed and the only large economic centre worth mentioning in the country, however, slums called "musseques", stretch for miles beyond Luanda's former city limits. According to the Heritage Foundation, a conservative American think tank, oil production from Angola has increased so significantly that Angola now is China's biggest supplier of oil. Before independence in 1975, Angola was a breadbasket of southern Africa and a major exporter of bananas, coffee and sisal, but three decades of civil war (1975–2002) destroyed the fertile countryside, leaving it littered with landmines and driving millions into the cities. The country now depends on expensive food imports, mainly from South Africa and Portugal, while more than 90 percent of farming is done at family and subsistence level. Thousands of Angolan small-scale farmers are trapped in poverty.
Angola is composed of Ovimbundu 37%, Mbundu 25%, Bakongo 13%, "mestiços" (mixed European and native African) 2%, European 1%, and 22% 'other' ethnic groups. The two Mbundu and Ovimbundu nations combined form a majority of the population, at 62%. It is estimated that Angola was host to 12,100 refugees and 2,900 asylum seekers by the end of 2007. 11,400 of those refugees were originally from the Democratic Republic of Congo (Congo-Kinshasa) who arrived in the 1970s. As of 2008 there were an estimated 400,000 DRC migrant workers, at least 30,000 Portuguese, and 100,000+ Chinese living in Angola. Prior to independence in 1975, Angola had a community of approximately 500,000 Portuguese.
Christianity is the major religion in Angola. The World Christian Database states that the Angolan population is 93.5% Christian, 4.7% ethnoreligionist (indigenous), 0.6% Muslim, 0.9% Agnostic and 0.2% non-religious. However, other sources put the percent of Christians at 53% with the remaining population adhering to indigenous beliefs. According to these sources, of Christians in Angola, 72% are Roman Catholic, and 28% are Baptist, Presbyterian, Reformed Evangelical, Pentecostal, Methodists and a few small Christian sects. In a study assessing nations' levels of religious regulation and persecution with scores ranging from 0–10 where 0 represented low levels of regulation or persecution, Angola was scored 0.8 on Government Regulation of Religion, 4.0 on Social Regulation of Religion, 0 on Government Favoritism of Religion and 0 on Religious Persecution. The largest Protestant denominations include the Methodists, Baptists, Congregationalists (United Church of Christ), and Assemblies of God. The largest syncretic religious group is the Kimbanguist Church, whose followers believe that a mid-20th century Congolese pastor named Joseph Kimbangu was a prophet. A small portion of the country's rural population practices animism or traditional indigenous religions. There is a small Islamic community based around migrants from West Africa. In colonial times, the country's coastal populations primarily were Catholic while the Protestant mission groups were active inland. With the massive social displacement caused by 26 years of civil war, this rough division is no longer valid. Foreign missionaries were very active prior to independence in 1975, although the Portuguese colonial authorities expelled many Protestant missionaries and closed mission stations based on the belief that the missionaries were inciting pro-independence sentiments. Missionaries have been able to return to the country since the early 1990s, although security conditions due to the civil war have prevented them from restoring many of their former inland mission stations. The Roman Catholic denomination mostly keeps to itself in contrast to the major Protestant denominations which are much more active in trying to win new members. The major Protestant denominations provide help for the poor in the form of crop seeds, farm animals, medical care and education in the English language, math, history and religion.
A 2007 survey concluded that low and deficient niacin status was common in Angola. Epidemics of cholera, malaria, rabies and African hemorrhagic fevers like Marburg hemorrhagic fever, are common diseases in several parts of the country. Many regions in this country have high incidence rates of tuberculosis and high HIV prevalence rates. Dengue, filariasis, leishmaniasis, and onchocerciasis (river blindness) are other diseases carried by insects that also occur in the region. Angola has one of the highest infant mortality rates in the world and the world's 2nd lowest life expectancies.
Although by law, education in Angola is compulsory and free for 8 years, the government reports that a certain percentage of students are not attending school due to a lack of school buildings and teachers. Students are often responsible for paying additional school-related expenses, including fees for books and supplies. In 1999, the gross primary enrollment rate was 74 percent and in 1998, the most recent year for which data are available, the net primary enrollment rate was 61 percent. Gross and net enrollment ratios are based on the number of students formally registered in primary school and therefore do not necessarily reflect actual school attendance. There continue to be significant disparities in enrollment between rural and urban areas. In 1995, 71.2 percent of children ages 7 to 14 years were attending school. It is reported that higher percentages of boys attend school than girls. During the Angolan Civil War (1975–2002), nearly half of all schools were reportedly looted and destroyed, leading to current problems with overcrowding. The Ministry of Education hired 20,000 new teachers in 2005, and continued to implement teacher trainings. Teachers tend to be underpaid, inadequately trained, and overworked (sometimes teaching two or three shifts a day). Teachers also reportedly demand payment or bribes directly from their students. Other factors, such as the presence of landmines, lack of resources and identity papers, and poor health also prevent children from regularly attending school. Although budgetary allocations for education were increased in 2004, the education system in Angola continues to be extremely under-funded. Literacy is quite low, with 67.4% of the population over the age of 15 able to read and write in Portuguese. 82.9% of males and 54.2% of women are literate as of 2001. Since independence from Portugal in 1975, a number of Angolan students continued to be admitted every year at high schools, polytechnical institutes, and universities Portuguese, Brazilian and Cuban through bilateral agreements; in general these students belong to the Angolan elites.
This article is about the demographic features of the population of Angola, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population. The demographics of Angola consist of three main ethnic groups, each speaking a Bantu language: Ovimbundu 37%, Mbundu 25%, and Bakongo 13%. Other groups include Chokwe (or Lunda), Ganguela, Nhaneca-Humbe, Ambo, Herero, and Xindunga. In addition, mixed race (European and African) people amount to about 2%, with a small (1%) population of whites, mainly ethnically Portuguese. As a former overseas territory of Portugal (until 1975), the Portuguese make up the largest non-African population, with about 100,000 (though many other native-born Angolans can claim Portuguese nationality under Portuguese law). In 1975, 250,000 Cuban soldiers arrived in Angola to help the MPLA forces during the civil war. The largest denomination is Roman Catholicism, but there are also followers of Protestantism, increasingly Pentecostal communities, and African Initiated Churches. As of 2006, one out of 221 people were Jehovah's Witnesses. Blacks from Mali, Nigeria, and Senegal are mostly Muslims. By now few Angolans retain African Traditional Religion following different ethnic faiths. CIA World Factbook demographic statistics. The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
There are 12,799,293 Angolan citizens as of July 2009, which ranks 70th in the world. However the results of the voters' registration of 2007-2008, 8.256.000 adults (from 18 years onwards) combined with the data from 2000/2001 that show that only about 45% of the population is in adult age (Multi-Indicator Cluster Survey, INE/UNICEF), give a total population of at least 18,3 million in 2008.The population of the capital Luanda is about 5,3 million.
The population is growing by 2.184% annually. There are 44.51 births and 24.81 deaths per 1,000 citizens. The net migration rate is 2.14 migrants per 1,000 citizens. The fertility rate of Angola is 6.27 children born per woman as of 2006. The infant mortality rate is 184.44 deaths for every 1,000 live births with 196.55 deaths for males and 171.72 deaths for females for every 1,000 live births. Life expectancy at birth is 37.63 years; 36.73 years for males and 38.57 years for females.
The adult prevalence rate of HIV/AIDS infection is 3.9% as of 2003. There are 240,000 citizens living with AIDS and 21,000 die annually. The risk of contracting disease is very high. There are food and waterborne diseases, bacterial and protozoal diarrhea, hepatitis A, and typhoid fever; vectorborne diseases, malaria, African trypanosomiasis (sleeping sickness); respiratory disease: meningococcal meningitis, and schistosomiasis, a water contact disease, as of 2005.
Politics of Angola takes place in a framework of a presidential republic, whereby the President of Angola is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and parliament. Angola changed from a one-party Marxist-Leninist system ruled by the MPLA to a formal multiparty democracy following the 1992 elections. President dos Santos won the first round election with more than 49% of the vote to Jonas Savimbi's 40%. A runoff never has taken place. The subsequent renewal of civil war and collapse of the Lusaka Protocol have left much of this process stillborn, but democratic forms exist, notably the National Assembly. Currently, political power is concentrated in the Presidency. The executive branch of the government is composed of the President, the Prime Minister (currently Paulo Kassoma) and Council of Ministers. The Council of Ministers, composed of all government ministers and vice ministers, meets regularly to discuss policy issues. Governors of the 18 provinces are appointed by and serve at the pleasure of the president. The Constitutional Law of 1992 establishes the broad outlines of government structure and the rights and duties of citizens. The legal system is based on Portuguese and customary law but is weak and fragmented. Courts operate in only 12 of more than 140 municipalities. A Supreme Court serves as the appellate tribunal; a Constitutional Court with powers of judicial review has never been constituted despite statutory authorization. The 26-year long civil war has ravaged the country's political and social institutions. The UN estimates of 1.8 million internally displaced persons (IDPs), while generally the accepted figure for war-affected people is 4 million. Daily conditions of life throughout the country and specifically Luanda (population approximately 4 million) mirror the collapse of administrative infrastructure as well as many social institutions. The ongoing grave economic situation largely prevents any government support for social institutions. Hospitals are without medicines or basic equipment, schools are without books, and public employees often lack the basic supplies for their day-to-day work. Currently, a constitutional commission is putting together a new constitutions. According to the commission, which is composed of 45 permanent members from the main political parties, a new constitution will be ready by the end of the first quarter of 2010.
The National Assembly ("Assembleia Nacional") has 223 members, elected for a four year term, 130 members by proportional representation, 90 members in provincial districts, and 3 members to represent Angolans abroad. The next general elections, due for 1997, have been rescheduled for 5 September 2008. The ruling party MPLA won 82% (191 seats in the National Assembly) and the main opposition party won only 10% (16 seats). The elections however have been described as only partly free but certainly not fair. A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008.
The Economy of Angola is one of the fastest-growing economies in the world, but is still recovering from the Angolan Civil War that plagued Angola from independence in 1975 until 2002. Despite extensive oil and gas resources, diamonds, hydroelectric potential, and rich agricultural land, Angola remains poor, and a third of the population relies on subsistence agriculture. Since 2002, when the 27-year civil war ended, the country has worked to repair and improve ravaged infrastructure and weakened political and social institutions. High international oil prices and rising oil production have led to a very strong economic growth in recent years, but corruption and public-sector mismanagement remain, particularly in the oil sector, which accounts for over 50 percent of GDP, over 90 percent of export revenue, and over 80 percent of government revenue.
Kingdom of Portugal's explorers and settlers, founded trading posts and forts along the coast of Africa since the 15th century, and reached the Angolan coast in the 16th century. Portuguese explorer Paulo Dias de Novais founded Luanda in 1575 as "São Paulo de Loanda", and the region developed as a slave trade market with the help of local Imbangala and Mbundu peoples who were notable slave hunters. Trade was mostly with the Portuguese colony of Brazil; Brazilian ships were the most numerous in the ports of Luanda and Benguela. By this time, Angola, a Portuguese colony, was in fact like a colony of Brazil, paradoxically another Portuguese colony. A strong Brazilian influence was also exercised by the Jesuits in religion and education. War gradually gave way to the philosophy of trade. The great trade routes and the agreements that made them possible were the driving force for activities between the different areas; warlike states become states ready to produce and to sell. In the Planalto (the high plains), the most important states were those of Bié and Bailundo, the latter being noted for its production of foodstuffs and rubber. The colonial power, Portugal, becoming ever richer and more powerful, would not tolerate the growth of these neighbouring states and subjugated them one by one, so that by the beginning of this century the Portuguese had complete control over the entire area. During the period of the Iberian Union (1580–1640), Portugal lost influence and power and made new enemies. The Dutch, a major enemy of Castile, invaded many Portuguese overseas possessions, including Luanda. The Dutch ruled Luanda from 1640 to 1648 as Fort Aardenburgh. They were seeking black slaves for use in sugarcane plantations of Northeastern Brazil (Pernambuco, Olinda, Recife) which they had also seized from Portugal. John Maurice, Prince of Nassau-Siegen, conquered the Portuguese possessions of Saint George del Mina, Saint Thomas, and Luanda, Angola, on the west coast of Africa. After the dissolution of the Iberian Union in 1640, Portugal would reestablish its authority over the lost territories of the Portuguese Empire. The Portuguese started to develop townships, trading posts, logging camps and small processing factories. From 1764 onwards, there was a gradual change from a slave-based society to one based on production for domestic consumption and export. Meanwhile, with the independence of Brazil in 1822, the slave trade was abolished in 1836, and in 1844 Angola's ports were opened to foreign shipping. By 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside Mainland Portugal, full of trading companies, exporting (together with Benguela) palm and peanut oil, wax, copal, timber, ivory, cotton, coffee, and cocoa, among many other products. Maize, tobacco, dried meat and cassava flour also began to be produced locally. The Angolan bourgeoisie was born. From the 1920s to the 1960s, strong economic growth, abundant natural resources and development of infrastruture, led to the arrival of even more Portuguese settlers from the metropole. The Portuguese discovered petroleum in Angola in 1955. Production began in the Cuanza basin in the 1950s, in the Congo basin in the 1960s, and in the exclave of Cabinda in 1968. The Portuguese government granted operating rights for Block Zero to the Cabinda Gulf Oil Company, a subsidiary of ChevronTexaco, in 1955. Oil production surpassed the exportation of coffee as Angola's largest export in 1973. A leftist military-led coup d'état, started on April 25, 1974, in Lisbon, overthrew the Marcelo Caetano government in Portugal, and promised to hand over power to an independent Angolan government. Mobutu Sese Seko, the President of Zaire, met with António de Spínola, the transitional President of Portugal, on September 15, 1974 on Sal island in Cape Verde, crafting a plan to empower Holden Roberto of the National Liberation Front of Angola, Jonas Savimbi of UNITA, and Daniel Chipenda of the MPLA's eastern faction at the expense of MPLA leader Agostinho Neto while retaining the facade of national unity. Mobutu and Spínola wanted to present Chipenda as the MPLA head, Mobutu particularly preferring Chipenda over Neto because Chipenda supported autonomy for Cabinda. The Angolan exclave has immense petroleum reserves estimated at around 300 million tons (~300 kg) which Zaire, and thus the Mobutu government, depended on for economic survival. After independence thousands of white Portuguese left, most of them to Portugal and many travelling overland to South Africa. There was an immediate crisis because the indigenous African population lacked the skills and knowledge needed to run the country and maintain its well-developed infrastructure. The Angolan government created Sonangol, a state-run oil company, in 1976. Two years later Sonangol received the rights to oil exploration and production in all of Angola. After independence from Portugal in 1975, Angola was ravaged by a horrific civil war between 1975 and 2002.
United Nations Angola Verification Mission III and MONUA spent USD1.5 billion overseeing implementation of the Lusaka Protocol, a 1994 peace accord that ultimately failed to end the civil war. The protocol prohibited UNITA from buying foreign arms, a provision the United Nations largely did not enforce, so both sides continued to build up their stockpile. UNITA purchased weapons in 1996 and 1997 from private sources in Albania and Bulgaria, and from Zaire, South Africa, Republic of the Congo, Zambia, Togo, and Burkina Faso. In October 1997 the UN imposed travel sanctions on UNITA leaders, but the UN waited until July 1998 to limit UNITA's exportation of diamonds and freeze UNITA bank accounts. While the U.S. government gave USD250 million to UNITA between 1986 to 1991, UNITA made USD1.72 billion between 1994 and 1999 exporting diamonds, primarily through Zaire to Europe. At the same time the Angolan government received large amounts of weapons from the governments of Belarus, Brazil, Bulgaria, the People's Republic of China, and South Africa. While no arms shipment to the government violated the protocol, no country informed the U.N. Register on Conventional Weapons as required. Despite the increase in civil warfare in late 1998, the economy grew by an estimated 4% in 1999. The government introduced new currency denominations in 1999, including a 1 and 5 kwanza note.
An economic reform effort was launched in 1998. The Angolan economy ranked 160 out of 174 nations in the United Nations Human Development Index of 2000. In April 2000 Angola started an International Monetary Fund (IMF) Staff-Monitored Program (SMP). The program formally lapsed in June 2001, but the IMF remains engaged. In this context the Government of Angola has succeeded in unifying exchange rates and has raised fuel, electricity, and water rates. The Commercial Code, telecommunications law, and Foreign Investment Code are being modernized. A privatization effort, prepared with World Bank assistance, has begun with the BCI bank. Nevertheless, a legacy of fiscal mismanagement and corruption persists. The civil war internally displaced 3.8 million people, 32% of the population, by 2001. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production. Angola produced over 3 million carats of diamonds per year in 2003, with its production expected to grow to 10 million carats per year by 2007. In 2004 China's Eximbank approved a $2 billion line of credit to Angola to rebuild infrastructure. The economy grew 18% in 2005 and growth was expected to reach 26% in 2006 and stay above 10% for the rest of the decade. ChevronTexaco started pumping from Block 14 in January 2000, but production has decreased to in 2007 due to the poor quality of the oil. Angola joined the Organization of the Petroleum Exporting Countries on January 1, 2007. Cabinda Gulf Oil Company found Malange-1, an oil reservoir in Block 14, on August 9, 2007.
Despite its abundant natural resources, output per capita is among the world's lowest. Subsistence agriculture provides the main livelihood for 85% of the population. Oil production and the supporting activities are vital to the economy, contributing about 45% to GDP and 90% of exports. Growth is almost entirely driven by rising oil production which surpassed in late-2005 and which is expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate which is owned by the Angolan government. With revenues booming from oil exports, the government has started to implement ambitious development programs in building roads and other basic infrastructure for the nation. In the last decade of the colonial period, Angola was a major African food exporter but now imports almost all its food. Because of severe wartime conditions, including extensive planting of landmines throughout the countryside, agricultural activities have been brought to a near standstill. Some efforts to recover have gone forward, however, notably in fisheries. Coffee production, though a fraction of its pre-1975 level, is sufficient for domestic needs and some exports. In sharp contrast to a bleak picture of devastation and bare subsistence is expanding oil production, now almost half of GDP and 90% of exports, at. Diamonds provided much of the revenue for Jonas Savimbi's UNITA rebellion through illicit trade. Other rich resources await development: gold, forest products, fisheries, iron ore, coffee, and fruits. This is a chart of trend of nominal gross domestic product of Angola at market prices using International Monetary Fund data; figures are in millions of units.
Exports in 2004 reached US$10,530,764,911. The vast majority of Angola's exports, 92% in 2004, are petroleum products. US$785 million worth of diamonds, 7.5% of exports, were sold abroad that year. Nearly all of Angola's oil goes to the United States, in 2006, making it the eighth largest supplier of oil to the United States, and to the People's Republic of China, in 2006. In the first quarter of 2008, Angola became the main exporter of oil to China. The rest of its petroleum exports go to Europe and Latin America. U.S. companies account for more than half the investment in Angola, with Chevron-Texaco leading the way. The U.S. exports industrial goods and services, primarily oilfield equipment, mining equipment, chemicals, aircraft, and food, to Angola, while principally importing petroleum. Trade between Angola and South Africa exceeded USD 300 million in 2007.
Angola produces and exports more petroleum than any other nation in sub-Saharan Africa, surpassing Nigeria in the 2000s. In January 2007 Angola became a member of OPEC. By 2010 production is expected to double the 2006 output level with development of deep-water offshore oil fields. Oil sales generated USD 1.71 billion in tax revenue in 2004 and now makes up 80% of the government's budget, a 5% increase from 2003, and 45% of GDP. Chevron Corporation produces and receives, 27% of Angolan oil. Elf Oil, Texaco, ExxonMobil, Agip, Petrobras, and British Petroleum also operate in the country. Block Zero provides the majority of Angola's crude oil production with produced annually. The largest fields in Block Zero are Takula (Area A), Numbi (Area A), and Kokongo (Area B). ChevronTexaco operates in Block Zero with a 39.2% share. SONANGOL, the state oil company, Total, and ENI-Agip own the rest of the block. ChevronTexaco also operates Angola's first producing deepwater section, Block 14, with. The United Nations has criticized the Angolan government for using torture, rape, summary executions, arbitrary detention, and disappearances, actions which Angolan government has justified on the need to maintain oil output. Angola is the third-largest trading partner of the United States in Sub-Saharan Africa, largely because of its petroleum exports. The U.S. imports 7% of its oil from Angola, about three times as much as it imported from Kuwait just prior to the Gulf War in 1991. The U.S. Government has invested USD $4 billion in Angola's petroleum sector.
Angola is the third largest producer of diamonds in Africa and has only explored 40% of the diamond-rich territory within the country, but has had difficulty in attracting foreign investment because of corruption, human rights violations, and diamond smuggling. Production rose by 30% in 2006 and Endiama, the national diamond company of Angola, expects production to increase by 8% in 2007 to 10 million carats annually. The government is trying to attract foreign companies to the provinces of Bié, Malanje and Uíge. The Angolan government loses $375 million annually from diamond smuggling. In 2003 the government began Operation Brilliant, an anti-smuggling investigation that arrested and deported 250,000 smugglers between 2003 and 2006. Rafael Marques, a journalist and human rights activist, described the diamond industry in his 2006 "Angola's Deadly Diamonds" report as plagued by "murders, beatings, arbitrary detentions and other human rights violations." Marques called on foreign countries to boycott Angola's "conflict diamonds."
Under Portuguese rule, Angola began mining iron in 1957, producing 1.2 million tons in 1967 and 6.2 million tons by 1971. In the early 1970s, 70% of Portuguese Angola's iron exports went to Western Europe and Japan. After independence in 1975, the Angolan Civil War (1975 - 2002) destroyed most of the territory's mining infrastructure. The redevelopment of the Angolan mining industry started in the late 2000s.
There are three separate lines which do not link up. The major railway is the Benguela railway. A fourth system once linked Gunza and Gabala. Railways in Angola suffered a lot of damage in the civil war, particularly the Benguela railway. A $4b project is proposed to restore the lines, and even to extend the system. It was reported in January 2008 that the repair of the Northern Line (a.k.a. Luanda Railway), started in October 2003 will be completed by August 2008. The work was carried out by the Chinese firm MEC-TEC. A link to Namibia is partly under construction.
Travel on highways outside of towns and cities in Angola (and in some cases within) is often not best advised for those without four-by-four vehicles. Whilst a reasonable road infrastructure has existed within Angola, time and the war have taken their toll on the road surfaces, leaving many severely potholed, littered with broken asphalt. In many areas drivers have established alternate tracks to avoid the worst parts of the surface, although careful attention must be paid to the presence or absence of landmine warning markers by the side of the road. The Angolan government has contracted the restoration of many of the country's roads, though. Many companies are coming into the country from China and surrounding nations to help improve road surfaces. The road between Lubango and Namibe, for example, was completed recently with funding from the European Union, and is comparable to many European main routes. Progress to complete the road infrastructure is likely to take some decades, but substantial efforts are already being made in the right directions.
Angola had an estimated total of 43 airports as of 2004, of which 31 had paved runways as of 2005. There is an international airport at Luanda. International and domestic services are maintained by TAAG, Air France, Air Namibe, Sabena, South African Airways, TAP (Portugal) and several regional carriers. In 2003, domestic and international carriers carried 198,000 passengers. There are airstrips for domestic transport at Benguela, Cabinda, Huambo, Namibe, and Catumbela.
The Angolan Armed Forces (FAA) is headed by a Chief of Staff who reports to the Minister of Defense. There are three components, the Army, Navy (Marinha de Guerra, MdG), and National Air Force of Angola (FAPA). Total manpower is about 65,000. The army is by far the largest of the services with about 55,000+ men and women. The Navy numbers about 1,000 and operates several small patrol craft and barges. Air force personnel total about 3,500; its equipment includes Russian-manufactured fighters and transport planes. A small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville). In 2005, FAPLA had a total of 90,000 active personnel of which the Army had 100,000 members, whose major equipment included over 300 main battle tanks, 600 reconnaissance vehicles, over 250 armored infantry fighting vehicles, 170 armored personnel carriers and more than 1,396 artillery pieces. In 1990-91, the Army had ten military regions and an estimated 73+ 'brigades', each with a mean strength of 1,000 and comprising inf, tank, APC, arty, and AA units as required (IISS Military Balance 1990 or 1991). The Navy had an estimated 2,400 personnel whose major naval units consisted of nine patrol/coastal vessels. The Air Force /Air Defense Forces had 6,000 personnel and 90 combat capable aircraft, including 22 fighters, 59 fighter ground attack aircraft and 16 attack helicopters. The defense budget in 2005 totalled $1.16 billion. Fleet - Navy (Marinha de Guerra). Most of the craft detailed are from the 1980s or earlier, but the navy acquired new boats from Spain and France in the 1990s.
The foreign relations of Angola are based on Angola's strong support of U.S. foreign policy as the Angolan economy is dependent on U.S. foreign aid. From 1975 to 1989, Angola was aligned with the Eastern bloc, in particular the Soviet Union, Libya, and Cuba. Since then, it has focused on improving relationships with Western countries, cultivating links with other Portuguese-speaking countries, and asserting its own national interests in Central Africa through military and diplomatic intervention. In 1993, it established formal diplomatic relations with the United States. It has entered the Southern African Development Community as a vehicle for improving ties with its largely Anglophone neighbors to the south. Zimbabwe and Namibia joined Angola in its military intervention in the Democratic Republic of the Congo, where Angolan troops remain in support of the Joseph Kabila government. It also has intervened in the Republic of the Congo (Brazzaville) to support the existing government in that country. Since 1998, Angola has successfully worked with the UN Security Council to impose and carry out sanctions on UNITA. More recently, it has extended those efforts to controls on conflict diamonds, the primary source of revenue for UNITA. At the same time, Angola has promoted the revival of the Community of Portuguese-Speaking Countries (CPLP) as a forum for cultural exchange and expanding ties with Portugal (its former ruler) and Brazil (which shares many cultural affinities with Angola) in particular.
Cape Verde signed a friendship accord with Angola in December 1975, shortly after Angola gained its independence. Cape Verde and Guinea-Bissau served as stop-over points for Cuban troops on their way to Angola to fight UNITA rebels and South African troops. Prime Minister Pedro Pires sent FARP soldiers to Angola where they served as the personal bodyguards of Angolan President José Eduardo dos Santos. Democratic Republic of the Congo. Many thousands of Angolans fled the country after the civil war. More than 20,000 people were forced to leave the Democratic Republic of the Congo in 2009, an action the DR Congo said was in retaliation for regular expulsion of Congolese diamond miners who were in Angola illegally. Angola sent a delegation to DR Congo's capital Kinshasa and succeeded in stopping government-forced expulsions which had become a "tit-for-tat" immigration dispute. "Congo and Angola have agreed to suspend expulsions from both sides of the border," said Lambert Mende, DR Congo information minister, in October 2009. "We never challenged the expulsions themselves; we challenged the way they were being conducted — all the beating of people and looting their goods, even sometimes their clothes," Mende said.
Namibia borders Angola to the south. In 1999 Namibia signed a mutual defense pact with its northern neighbor Angola. This affected the Angolan Civil War that had been ongoing since Angola's independence in 1975. Namibia's ruling party SWAPO sought to support the ruling party MPLA in Angola against the rebel movement UNITA, whose stronghold is in southern Angola, bordering to Namibia. The defence pact allowed Angolan troops to use Namibian territory when attacking Jonas Savimbi's UNITA.
Angola-South Africa relations are quite strong as the ruling parties in both nations, the African National Congress in South Africa and the MPLA in Angola, fought together during the Angolan Civil War and South African Border War. They fought against UNITA rebels, based in Angola, and the apartheid-era government in South Africa who supported them. Nelson Mandela mediated between the MPLA and UNITA factions during the last years of Angola's civil war.
Angola-Portugal relations have significantly improved since the Angolan government abandoned communism and nominally embraced democracy in 1991, embracing a pro-U.S. and to a lesser degree pro-Europe foreign policy. Portugal ruled Angola for 400 years, colonizing the territory from 1483 until independence in 1975. Angola's war for independence did not end in a military victory for either side, but was suspended as a result of a coup in Portugal that replaced the Caetano regime.
From the mid-1980s through at least 1992, the United States was the primary source of military and other support for the UNITA rebel movement, which was led from its creation through 2002 by Jonas Savimbi. The U.S. refused to recognize Angola diplomatically during this period. Relations between the United States of America and the Republic of Angola (formerly the People's Republic of Angola) have warmed since Angola's ideological renunciation of Marxism before the 1992 elections.
Chinese Prime Minister Wen Jiabao visited Angola in June 2006, offering a US$9 billion loan for infrastructure improvements in return for petroleum. The PRC has invested heavily in Angola since the end of the civil war in 2002. João Manuel Bernardo, the current ambassador of Angola to China, visited the PRC in November 2007. In February 2006, Angola surpassed Saudi Arabia to become the number one supplier of oil to China.
Albert Sidney Johnston (February 2, 1803 – April 6, 1862) was a career United States Army officer, a Texas Army general, and a Confederate States general. He saw extensive combat during his military career, fighting actions in the Texas War of Independence, the Mexican-American War, the Utah War, as well as the American Civil War. Considered by Confederate President Jefferson Davis to be the finest general officer in the Confederacy before the emergence of Robert E. Lee, he was killed early in the Civil War at the Battle of Shiloh and was the highest ranking officer, Union or Confederate, killed during the entire war. Davis believed the loss of Johnston "was the turning point of our fate"
Johnston was born in Washington, Kentucky, the youngest son of Dr. John and Abigail Harris Johnston. His father was a native of Salisbury, Connecticut. Although Albert Johnston was born in Kentucky, he lived much of his life in Texas, which he considered his home. He was first educated at Transylvania University in Lexington, where he met fellow student Jefferson Davis. Both were appointed to the United States Military Academy, Davis two years behind Johnston. In 1826 Johnston graduated eighth of 41 cadets in his class from West Point with a commission as a brevet second lieutenant in the 2nd U.S. Infantry. Johnston was assigned to posts in New York and Missouri and served in the Black Hawk War in 1832 as chief of staff to Bvt. Brig. Gen. Henry Atkinson. In 1829 he married Henrietta Preston, sister of Kentucky politician and future civil war general William Preston. He resigned his commission in 1834 to return to Kentucky to care for his dying wife, who succumbed two years later to tuberculosis. They had one son, Col. William Preston Johnston, who would also serve in the Confederate Army.
In April 1834, Johnston took up farming in Texas, but enlisted as a private in the Texas Army during the Texas War of Independence against the Republic of Mexico in 1836. One month later, Johnston was promoted to major and the position of aide-de-camp to General Sam Houston. He was named Adjutant General as a colonel in the Republic of Texas Army on August 5, 1836. On January 31, 1837, he became senior brigadier general in command of the Texas Army. On February 7, 1837, he fought in a duel with Texas Brig. Gen. Felix Huston, challenging each other for the command of the Texas Army; Johnston refused to fire on Huston and lost the position after he was wounded in the pelvis. The second president of the Republic of Texas, Mirabeau B. Lamar, appointed him Secretary of War on December 22, 1838. Johnston was to provide the defense of the Texas border against Mexican invasion, and in 1839 conducted a campaign against Indians in northern Texas. In February 1840, he resigned and returned to Kentucky, where he married Eliza Griffin in 1843. They settled on a large plantation he named China Grove in Brazoria County, Texas.
Johnston returned to the Texas Army during the Mexican-American War under General Zachary Taylor as a colonel of the 1st Texas Rifle Volunteers. The enlistments of his volunteers ran out just before the Battle of Monterrey. Johnston managed to convince a few volunteers to stay and fight as he himself served as the inspector general of volunteers and fought at the battles of Monterrey and Buena Vista. Johnston remained on his plantation after the war until he was appointed by President Taylor to the U.S. Army as a major and was made a paymaster in December 1849. He served in that role for more than five years, making six tours, and traveling more than 4,000 miles annually on the Indian frontier of Texas. He served on the Texas frontier and elsewhere in the West. In 1855 President Franklin Pierce appointed him colonel of the new 2nd U.S. Cavalry (the unit that preceded the modern 5th U.S.), a new regiment, which he organized. As a key figure in the Utah War, he led U.S. troops who established a non-Mormon government in the formerly Mormon territory. He received a brevet promotion to brigadier general in 1857 for his service in Utah. He spent 1860 in Kentucky until December 21, when he sailed for California to take command of the Department of the Pacific.
At the outbreak of the Civil War, Johnston was the commander of the U.S. Army Department of the Pacific in California. He was approached by some Californians who urged him to take his forces east to join the Union against the Confederacy. He resigned his commission on April 9, 1861, as soon as he heard of the secession of Texas. He moved to Los Angeles where he had family and remained there until May when, suspected by local Union authorities, he evaded arrest and joined the Los Angeles Mounted Rifles as a private, leaving Warner's Ranch May 27. He participated in their trek across the southwestern deserts to Texas, crossing the Colorado River into the Confederate Territory of Arizona on July 4, 1861. He reached Richmond, Virginia, on or about September 1, 1861. There Johnston was appointed a full general by his friend, Jefferson Davis. On May 30, 1861, Johnston became the second highest ranking Confederate general (after the little-known Samuel Cooper) as commander of the Western Department. He raised the Army of Mississippi to defend Confederate lines from the Mississippi River to Kentucky and the Allegheny Mountains. Although the Confederate States Army won a morale-boosting victory at First Battle of Bull Run in the East in 1861, matters in the West turned ugly by early 1862. Johnston's subordinate generals lost Fort Henry on February 6, 1862, and Fort Donelson on February 16, 1862, to Union Brig. Gen. Ulysses S. Grant. Johnston has been faulted for poor judgment in selecting Brig. Gens. Lloyd Tilghman and John B. Floyd for those crucial positions and for not supervising adequate construction of the forts. Union Maj. Gen. Don Carlos Buell subsequently captured the vital city of Nashville, Tennessee. Gen. P.G.T. Beauregard was sent west to join Johnston and they organized their forces at Corinth, Mississippi, planning to ambush Grant's forces at Pittsburg Landing, Tennessee.
Johnston concentrated many of his forces from around the theater and launched a massive surprise attack against Grant at the Battle of Shiloh on April 6, 1862. As the Confederate forces overran the Union camps, Johnston seemed to be everywhere, personally leading and rallying troops up and down the line. At about 2:30 p.m., while leading one of those charges, he was wounded, taking a bullet behind his right knee. He did not think the wound serious at the time, and sent his personal physician to attend to some wounded Union soldiers instead. The bullet had in fact clipped his popliteal artery and his boot was filling up with blood. Within a few minutes Johnston was observed by his staff to be nearly fainting off his horse, and asked him if he was wounded, to which he replied "Yes, and I fear seriously." It is possible that Johnston's duel in 1837 had caused nerve damage or numbness to that leg and that he did not feel the wound to his leg as a result. Johnston was taken to a small ravine, where he bled to death in minutes. It is probable that a Confederate soldier fired the fatal round. No Union soldiers were observed to have ever gotten behind Johnston during the fatal charge, while it is known that many Confederates were firing at the Union lines while Johnston charged well in advance of his soldiers. He was the highest-ranking casualty of the war on either side, and his death was a strong blow to the morale of the Confederacy. Jefferson Davis considered him the best general in the country; this was two months before the emergence of Robert E. Lee as the pre-eminent general of the Confederacy.
Johnston was buried in New Orleans, Louisiana. In 1866, a joint resolution of the Texas Legislature was passed to have his body reinterred to the Texas State Cemetery in Austin The re-interment occurred in 1867. Forty years later, the state appointed Elisabet Ney to design a monument and sculpture of him to be erected at his gravesite. The Texas Historical Commission has erected a historical marker near the entrance of what was once his plantation. An adjacent marker was erected by the San Jacinto Chapter of the Daughters of The Republic of Texas and the Lee, Roberts, and Davis Chapter of the United Daughters of the Confederate States of America. The University of Texas at Austin has also recognized Johnston with a statue on the South Mall.
The word derives from ανδρός, the genitive of the Greek ανήρ "anēr", meaning "man", and the suffix "-eides", used to mean "of the species; alike" (from "eidos," "species"). Though the word derives from a gender-specific root, its usage in English is usually gender neutral. The term was first mentioned by St. Albertus Magnus in 1270 and was popularized by the French writer Villiers in his 1886 novel "L'Ève future", although the term "android" appears in US patents as early as 1863 in reference to miniature human-like toy automatons. The term "droid", invented by George Lucas in "Star Wars" (1977) but now used widely within science fiction, originated as an abbreviation of "android", but has been used by Lucas and others to mean any robot, including distinctly non-humaniform machines like R2-D2. Another abbreviation, "andy", coined as a pejorative by writer Philip K. Dick in his novel "Do Androids Dream of Electric Sheep?", has seen some limited further currency, e.g., in the TV series "Total Recall 2070".
The Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and Kokoro Co., Ltd. have demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan. In 2006, Kokoro Co. developed a new "DER 2" android. The height of the human body part of DER2 is 165 cm. There are 47 mobile points. DER2 can not only change its expression but also move its hands and feet and twist its body. The "air servosystem" which Kokoro Co. developed originally is used for the actuator. As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise. DER2 realized a slimmer body than that of the former version by using a smaller cylinder. Outwardly DER2 has a more beautiful proportion. Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions. The smoothness of her movement has also been improved, making it now even more likely for the uninitiated to confuse her with an actual human being. Once programmed, she is able to choreograph her motions and gestures with her voice. The Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called "Saya", which was exhibited at Robodex 2002 in Yokohama, Japan. There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future. Now Saya is "working" at the Science University of Tokyo as a guide. The Waseda University (Japan) and NTT Docomo's manufacturers have succeeded in creating a shape-shifting robot "WD-2". It is capable of changing its face. At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person. The robot expresses his/her face by moving all points to the decided positions, they say. The first version of the robot was first developed back in 2003. After that, a year later, they made a couple of major improvements to the design. The robot features an elastic mask made from the average head dummy. It uses a driving system with a 3DOF unit. The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom. This one has 17 facial points, for a total of 56 degrees of freedom. As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength. Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw. Apparently, the researchers can also modify the shape of the mask based on actual human faces. To "copy" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points. After that, they are then driven into position using a laptop and 56 motor control boards. In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D mask.
KITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial "musculature" and capable of rudimentary conversation, having a vocabulary of around 400 words. She is tall and weighs, matching the average figure of Korean women in their twenties. EveR-1's name derives from the Biblical Eve, plus the letter "r" for "robot". EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication has an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2009 at a cost of 500 billion won, of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.
Hanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body. This Einstein android, also called "Albert Hubo", thus represents the first full-body walking android in history (see video at). Hanson Robotics, the FedEx Institute of Technology, and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of "Do Androids Dream of Electric Sheep?", the basis for the film "Blade Runner"), with full conversational capabilities that incorporated thousands of pages of the author's works. In 2005, the PKD android won a first place artificial intelligence award from AAAI.
Although human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: "simulacra" (devices that exhibit likeness) and "automata" (devices that have independence). The term "android" was popularized by the French author Auguste Villiers de l'Isle-Adam in his work "Tomorrow's Eve" (1886), featuring an artificial humanlike robot named Hadaly. As said by the officer in the story, "In this age of Realien advancement, who knows what goes on in the mind of those responsible for these mechanical dolls." Although Karel Čapek's robots in "R.U.R. (Rossum's Universal Robots)" (1921) - the play that introduced the word "robot" to the world - were organic artificial humans, the word "robot" has come to primarily refer to mechanical humans, animals, and other beings. The term "android" can mean either one of these, while a cyborg ("cybernetic organism" or "bionic man") would be a creature that is a combination of organic and mechanical parts. The word "android" is a combination of Ancient Greek "andros" and the suffix "-oid", which literally means "in the form of a man (male human being)". This could be contrasted with the more general term "anthropoid", which means humanlike. According to this fashion, a female human-like robot would be a "gynoid".
Androids are a staple of science fiction. Authors have used the term "android" in more diverse ways than "robot" or "cyborg". In some fictional works, the difference between a robot and android is only their appearance, with androids being made to look like humans on the outside but with robot-like internal mechanics. In other stories, authors have used the word "android" to mean a wholly organic, yet artificial, creation. Other fictional depictions of androids fall somewhere in between. One thing common to most fictional androids, though, is that the real-life technological challenges associated with creating thoroughly human-like robots – such as the creation of strong artificial intelligence – are assumed to have been solved. Fictional androids are generally depicted as mentally and physically equal or superior to humans – moving, thinking and speaking as fluidly as them. The tension between the nonhuman substance and the human appearance – or even human ambitions – of androids is the dramatic impetus behind most of their fictional depictions. Some android heroes seek, like Pinocchio, to become human, as in the films "Bicentennial Man" and "A.I. Artificial Intelligence", or Data in the science-fiction show '. Others, as in the film "Westworld", rebel against abuse by careless humans. Android hunter Deckard in American writer Philip K. Dick's "Do Androids Dream of Electric Sheep?" discovers that his targets are, in some ways, more human than he is. Android stories, therefore, are not essentially stories "about" androids; they are stories about the human condition and what it means to be human. One aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in "Do Androids Dream of Electric Sheep?" and its film adaptation "Blade Runner". Perhaps the clearest example of such an exploration is John Brunner's 1968 novel "Into the Slave Nebula", where the blue-skinned android slaves are explicitly shown to be fully human. More recently, the androids Lance Bishop and Annalee Call in the films "Aliens" and "Alien Resurrection" are used as vehicles for exploring how humans deal with the presence of an "Other". Female androids, or "gynoids", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical "perfect woman". Examples include the Greek myth of "Pygmalion", the fembots in the Austin Powers series, and the female robot Maria in Fritz Lang's "Metropolis". Some gynoids, like Pris in "Blade Runner", are designed as sex-objects, with the intent of "pleasing men's violent sexual desires". Fiction about gynoids or female cyborgs has therefore been described as reinforcing "essentialist ideas of femininity", although others have suggested that the treatment of female androids is a way of exploring racism and misogyny in society.
Alberta is the most populous and fastest growing of Canada's three prairie provinces. It is approximately the same size as France or Texas and had a population of 3.7 million in 2009. It became a province on September 1, 1905, on the same day as Saskatchewan. It is economically important primarily because of its vast oil reserves, and its large tertiary and quaternary economic sector. Alberta is located in western Canada, bounded by the provinces of British Columbia to the west, Saskatchewan to the east, the Northwest Territories to the north, and the U.S. state of Montana to the south. Alberta is one of three Canadian provinces and territories to border only a single U.S. state (the others being New Brunswick and Yukon). It is also one of only two Canadian provinces that are landlocked (the other being Saskatchewan). The capital city of Alberta is Edmonton, located just south of the centre of the province. Roughly south of the capital is Calgary, Alberta's largest city and a major distribution and transportation hub as well as one of Canada's major commerce centres. Edmonton is the primary supply and service hub for Canada's oil sands and other northern resource industries. According to recent population estimates, these two metropolitan areas have now both exceeded 1 million people. Other municipalities in the province include Red Deer, Lethbridge, Medicine Hat, Fort McMurray, Grande Prairie, Camrose, Lloydminster, Brooks, Wetaskiwin, Banff, Cold Lake, and Jasper. Alberta is named after Princess Louise Caroline Alberta (1848–1939), the fourth daughter of Queen Victoria and her husband, Prince Albert. Princess Louise was the wife of the Marquess of Lorne, Governor General of Canada from 1878 to 1883. Lake Louise, the village of Caroline, and Mount Alberta were also named in honour of Princess Louise. Since December 14, 2006, the Premier of the province has been Ed Stelmach, a Progressive Conservative.
Alberta covers an area of, an area about 5% smaller than Texas or 20% larger than France. This makes it the fourth largest province after Quebec, Ontario, and British Columbia. To the south, the province borders on the 49th parallel north, separating it from the U.S. state of Montana, while on the north the 60th parallel north divides it from the Northwest Territories. To the east the 110th meridian west separates it from the province of Saskatchewan, while on the west its boundary with British Columbia follows the 120th meridian west south from the Northwest Territories at 60°N until it reaches the Continental Divide at the Rocky Mountains, and from that point follows the line of peaks marking the Continental Divide in a generally southeasterly direction until it reaches the Montana border at 49°N. The province extends north to south and east to west at its maximum width. Its highest point is at the summit of Mount Columbia in the Rocky Mountains along the southwest border, while its lowest point is on the Slave River in Wood Buffalo National Park in the northeast. With the exception of the semi-arid steppe of the southeastern section, the province has adequate water resources. Alberta contains numerous rivers and lakes used for swimming, water skiing, fishing and a full range of other water sports. There are three large lakes and a multitude of smaller lakes less than each. Part of Lake Athabasca () lies in the province of Saskatchewan. Lake Claire () lies just west of Lake Athabasca in Wood Buffalo National Park. Lesser Slave Lake () is northwest of Edmonton. The longest river in Alberta is the Athabasca River which travels from the Columbia Icefield in the Rocky Mountains to Lake Athabasca. Alberta's capital city, Edmonton, is located approximately in the geographic centre of the province, with most of western Canada's oil refinery capacity located nearby, in proximity to most of Canada's largest oil fields. Edmonton is the most northerly major city in Canada, and serves as a gateway and hub for resource development in northern Canada. Alberta's other major city, Calgary, is located approximately south of Edmonton and north of Montana, surrounded by extensive ranching country. Almost 75% of the province's population lives in the Calgary-Edmonton Corridor, in and between the two major cities. Most of the northern half of the province is boreal forest, while the Rocky Mountains along the southwestern boundary are largely forested. The southern quarter of the province is prairie, ranging from shortgrass prairie in the southeastern corner to mixed grass prairie in an arc to the west and north of it. The central aspen parkland region extending in a broad arc between the prairies and the forests, from Calgary, north to Edmonton, and then east to Lloydminster, contains the most fertile soil in the province and most of the population. Much of the unforested part of Alberta is given over either to grain or to dairy farming, with mixed farming more common in the north and centre, while ranching and irrigated agriculture predominate in the south. The Alberta badlands are located in southeastern Alberta, where the Red Deer River crosses the flat prairie and farmland, and features deep gorges and striking landforms. Dinosaur Provincial Park, near Brooks, Alberta, showcases the badlands terrain, desert flora, and remnants from Alberta's past when dinosaurs roamed the then lush landscape.
Alberta has a dry continental climate with warm summers and cold winters. The province is open to cold arctic weather systems from the north, which often produce extremely cold conditions in winter. As the fronts between the air masses shift north and south across Alberta, temperature can change rapidly. Arctic air masses in the winter produce extreme minimum temperatures varying from in northern Alberta to in southern Alberta. In the summer, continental air masses produce maximum temperatures from in the mountains to in southern Alberta. Because Alberta extends for over from north to south, its climate varies considerably. Average temperatures in January range from in the south to in the north, and in July from in the south to in the north. The climate is also influenced by the presence of the Rocky Mountains to the southwest, which disrupt the flow of the prevailing westerly winds and cause them to drop most of their moisture on the western slopes of the mountain ranges before reaching the province, casting a rain shadow over much of Alberta. The northerly location and isolation from the weather systems of the Pacific Ocean cause Alberta to have a dry climate with little moderation from the ocean. Annual precipitation ranges from in the southeast to in the north, except in the foothills of the Rocky Mountains where rainfall can reach annually. In the summer, the average daytime temperatures range from around in the Rocky Mountain valleys and far north to near in the dry prairie of the southeast. The northern and western parts of the province experience higher rainfall and lower evaporation rates caused by cooler summer temperatures. The south and east-central portions are prone to drought-like conditions sometimes persisting for several years, although even these areas can receive heavy precipitation. Alberta is a sunny province. Annual bright sunshine totals range between 1900 and 2500 hours per year. Northern Alberta receives about 18 hours of daylight in the summer. The long summer days make summer the sunniest season of the year in Alberta. In southwestern Alberta, the winter cold is frequently interrupted by warm, dry chinook winds blowing from the mountains, which can propel temperatures upward from frigid conditions to well above the freezing point in a very short period. During one chinook recorded at Pincher Creek, temperatures soared from to in one hour. The region around Lethbridge has the most chinooks, averaging 30 to 35 chinook days per year, while Calgary has a white Christmas only 59% of the time as a result of these winds. Northern Alberta is mostly covered by boreal forest and has fewer frost-free days than southern Alberta due to its subarctic climate. The agricultural area of southern Alberta has a semi-arid steppe climate because the annual precipitation is less than the water that evaporates or is used by plants. The southeastern corner of Alberta, part of the Palliser Triangle, experiences greater summer heat and lower rainfall than the rest of the province, and as a result suffers frequent crop yield problems and occasional severe droughts. Western Alberta is protected by the mountains and enjoys the mild temperatures brought by winter chinook winds. Central and parts of northwestern Alberta in the Peace River region are largely aspen parkland, a biome transitional between prairie to the south and boreal forest to the north. After southern Ontario, Central Alberta is the most likely region in Canada to experience tornadoes. Thunderstorms, some of them severe, are frequent in the summer, especially in central and southern Alberta. The region surrounding the Calgary-Edmonton Corridor is notable for having the highest frequency of hail in Canada, which is caused by orographic lifting from the nearby Rocky Mountains, enhancing the updraft/downdraft cycle necessary for the formation of hail.
The province of Alberta, as far north as about 53° north latitude, was a part of Rupert's Land from the time of the incorporation of the Hudson's Bay Company (1670). After the arrival in the North-West of the French around 1731 they settled the prairies of the west, establishing communities such as Lac La Biche and Bonnyville. Fort La Jonquière was established near what is now Calgary in 1752. The North West Company of Montreal occupied the northern part of Alberta territory before the Hudson's Bay Company arrived from Hudson Bay to take possession of it. The first explorer of the Athabasca region was Peter Pond, who, on behalf of the North West Company of Montreal, built Fort Athabasca on Lac La Biche in 1778. Roderick Mackenzie built Fort Chipewyan on Lake Athabasca ten years later in 1788. His cousin, Sir Alexander Mackenzie, followed the North Saskatchewan River to its northernmost point near Edmonton, then setting northward on foot, trekked to the Athabasca River, which he followed to Lake Athabasca. It was there he discovered the mighty outflow river which bears his name—the Mackenzie River—which he followed to its outlet in the Arctic Ocean. Returning to Lake Athabasca, he followed the Peace River upstream, eventually reaching the Pacific Ocean, and so he became the first white man to cross the North American continent north of Mexico. Most of Alberta's territory was included in Rupert's Land, transferred to Canada in 1870. The southernmost portion of Alberta was part of the French (and Spanish) territory of Louisiana, sold to the United States in 1803; in 1818, the portion of Louisiana north of the Forty-Ninth Parallel was ceded to Great Britain. Northern Alberta was included in the North-Western Territory until 1870, when it and Rupert's land became Canada's Northwest Territories. The district of Alberta was created as part of the North-West Territories in 1882. As settlement increased, local representatives to the North-West Legislative Assembly were added. After a long campaign for autonomy, in 1905 the district of Alberta was enlarged and given provincial status, with the election of Alexander Cameron Rutherford as the first premier.
Alberta has enjoyed a relatively high rate of growth in recent years, mainly because of its burgeoning economy. Between 2003 and 2004, the province had high birthrates (on par with some larger provinces such as British Columbia), relatively high immigration, and a high rate of interprovincial migration when compared to other provinces. Approximately 81% of the population live in urban areas and only about 19% live in rural areas. The Calgary-Edmonton Corridor is the most urbanized area in the province and is one of the most densely populated areas of Canada. Many of Alberta's cities and towns have also experienced very high rates of growth in recent history. Over the past century, Alberta's population rose from 73,022 in 1901 to 2,974,807 in 2001 and 3,290,350 according to the 2006 census.
The 2006 census found that English, with 2,576,670 native speakers, was the mother tongue of 79.99% of Albertans. The next most common mother tongues were Chinese languages with 97,275 native-speakers (3.02%); followed by German with 84,505 native-speakers (2.62%); and French with 61,225 (1.90%); then Punjabi 36,320 (1.13%); Tagalog 29,740 (0.92%); Ukrainian 29,455 (0.91%); Spanish 29,125 (0.90%); and Polish 21,990 (0.68%); Arabic 20,495 (0.64%); Dutch 19,980 (0.62%); and Vietnamese 19,350 (0.60%). The most common aboriginal language is Cree 17,215 (0.53%). Other common mother tongues include Italian with 13,095 speakers (0.41%); Urdu with 11,275 (0.35%); and Korean with 10,845 (0.33%); then Hindi 8,985 (0.28%); Persian 7,700 (0.24%); Portuguese 7,205 (0.22%); and Hungarian 6,770 (0.21%)."(Figures shown are for the number of single language responses and the percentage of total single-language responses.)"
Alberta has considerable ethnic diversity. In line with the rest of Canada, many immigrants originated from Scotland, Ireland and Wales, but large numbers also came from other parts of Europe, notably Germans, French, Ukrainians and Scandinavians. According to Statistics Canada, Alberta is home to the second highest proportion (two percent) of Francophones in western Canada (after Manitoba). Many of Alberta's French-speaking residents live in the central and northwestern regions of the province. As reported in the 2001 census, the Chinese represented nearly four percent of Alberta's population, and East Indians represented more than two percent. Both Edmonton and Calgary have historic Chinatowns, and Calgary has Canada's third largest Chinese community. The Chinese presence began with workers employed in the building of the Canadian Pacific Railway in the 1880s. Aboriginal Albertans make up approximately three percent of the population. In the 2006 Canadian census, the most commonly reported ethnic origins among Albertans were: 885,825 English (27.2%); 679,705 German (20.9%); 667,405 Canadian (20.5%); 661,265 Scottish (20.3%); 539,160 Irish (16.6%); 388,210 French (11.9%); 332,180 Ukrainian (10.2%); 172,910 Dutch (5.3%); 170,935 Polish (5.2%); 169,355 North American Indian (5.2%); 144,585 Norwegian (4.4%); and 137,600 Chinese (4.2%). (Each person could choose more than one ethnicity.)" Amongst those of British origins, the Scots have had a particularly strong influence on place-names, with the names of many cities and towns including Calgary, Airdrie, Canmore, and Banff having Scottish origins.
As of the Canada 2001 Census the largest religious group was Roman Catholic, representing 25.7% of the population. Alberta had the second highest percentage of non-religious residents in Canada (after British Columbia) at 23.1% of the population. Of the remainder, 13.5% of the population identified themselves as belonging to the United Church of Canada, while 5.9% were Anglican. Lutherans made up 4.8% of the population while Baptists comprised 2.5%. The remainder had a wide variety of different religious affiliations, although no individual group constituted more than 2% of the population. The Mormons of Alberta reside primarily in the extreme south of the province and made up 1.7% of the population. Alberta has a population of Hutterites, a communal Anabaptist sect similar to the Mennonites (Hutterites represented 0.4% of the population while Mennonites were 0.8%), and has a significant population of Seventh-day Adventists at 0.3%.Alberta is home to several Byzantine Rite Churches as part of the legacy of Eastern European immigration, including the Ukrainian Catholic Eparchy of Edmonton, and the Ukrainian Orthodox Church of Canada's Western Diocese which is based in Edmonton. Muslims, Sikhs, and Hindus live in Alberta. Muslims constituted 1.7% of the population, Sikhs 0.8% and Hindus 0.5%. Many of these are recent immigrants, but others have roots that go back to the first settlers of the prairies. Canada's oldest mosque the Al-Rashid Mosque is located in Edmonton. Jews constituted 0.4% of Alberta's population. Most of Alberta's 13,000 Jews live in Calgary (7,500) and Edmonton (5,000). Visible Minorities and Aboriginal Peoples. Alberta is the third most diverse province in terms of visible minorities after British Columbia and Ontario with 13.9% of the population consisting of visible minorities. Calgary and Edmonton are very diverse cities in Canada with almost one quarter of their population belonging a visible minorities group. Alberta has been attracting immigrants who are for the most part are visible minorities with the opportunities available in a booming economy. Aboriginal Identity Peoples make up 5.8% of the population with half that consisting North American Indians and the other half consisting of Metis. There are also small number of Inuit people in Alberta. The number of Aboriginal Identity Peoples have been increasing at a rate greater than the population of Alberta.
Alberta's economy is one of the strongest in Canada, supported by the burgeoning petroleum industry and to a lesser extent, agriculture and technology. The per capita GDP in 2007 was by far the highest of any province in Canada at C$74,825. This was 61% higher than the national average of C$46,441 and more than twice that of some of the Atlantic provinces. In 2006 the deviation from the national average was the largest for any province in Canadian history. According to the 2006 census, the median annual family income after taxes was $70,986 in Alberta (compared to $60,270 in Canada as a whole). The Calgary-Edmonton Corridor is the most urbanized region in the province and one of the densest in Canada. The region covers a distance of roughly 400 kilometres north to south. In 2001, the population of the Calgary-Edmonton Corridor was 2.15 million (72% of Alberta's population). It is also one of the fastest growing regions in the country. A 2003 study by TD Bank Financial Group found the corridor to be the only Canadian urban centre to amass a U.S. level of wealth while maintaining a Canadian style quality of life, offering universal health care benefits. The study found that GDP per capita in the corridor was 10% above average U.S. metropolitan areas and 40% above other Canadian cities at that time. According to the Fraser Institute, Alberta also has very high levels of economic freedom. It is by far the most free economy in Canada, and is rated as the 2nd most free economy of U.S. states and Canadian provinces.
Alberta is the largest producer of conventional crude oil, synthetic crude, natural gas and gas products in the country. Alberta is the world’s 2nd largest exporter of natural gas and the 4th largest producer. Two of the largest producers of petrochemicals in North America are located in central and north central Alberta. In both Red Deer and Edmonton, world class polyethylene and vinyl manufacturers produce products shipped all over the world, and Edmonton's oil refineries provide the raw materials for a large petrochemical industry to the east of Edmonton. The Athabasca Oil Sands (sometimes known as the Athabasca Tar Sands) have estimated unconventional oil reserves approximately equal to the conventional oil reserves of the rest of the world, estimated to be 1.6 trillion barrels (254 km³). With the development of new extraction methods such as steam assisted gravity drainage, which was developed in Alberta, bitumen and synthetic crude oil can be produced at costs close to those of conventional crude. Many companies employ both conventional strip mining and non-conventional in situ methods to extract the bitumen from the oil sands. With current technology and at current prices, about 315 billion barrels (50 km³) of bitumen are recoverable. Fort McMurray, one of Canada's fastest growing cities, has grown enormously in recent years because of the large corporations which have taken on the task of oil production. As of late 2006 there were over $100 billion in oil sands projects under construction or in the planning stages in northeastern Alberta. Another factor determining the viability of oil extraction from the Tar Sands is the price of oil. The oil price increases since 2003 have made it more than profitable to extract this oil, which in the past would give little profit or even a loss. With concerted effort and support from the provincial government, several high-tech industries have found their birth in Alberta, notably patents related to interactive liquid crystal display systems. With a growing economy, Alberta has several financial institutions dealing with civil and private funds.
Agriculture has a significant position in the province's economy. The province has over three million head of cattle, and Alberta beef has a healthy worldwide market. Nearly one half of all Canadian beef is produced in Alberta. Alberta is one of the prime producers of plains buffalo (bison) for the consumer market. Sheep for wool and mutton are also raised. Wheat and canola are primary farm crops, with Alberta leading the provinces in spring wheat production; other grains are also prominent. Much of the farming is dryland farming, often with fallow seasons interspersed with cultivation. Continuous cropping (in which there is no fallow season) is gradually becoming a more common mode of production because of increased profits and a reduction of soil erosion. Across the province, the once common grain elevator is slowly being lost as rail lines are decreasing; farmers typically truck the grain to central points. Alberta is the leading beekeeping province of Canada, with some beekeepers wintering hives indoors in specially designed barns in southern Alberta, then migrating north during the summer into the Peace River valley where the season is short but the working days are long for honeybees to produce honey from clover and fireweed. Hybrid canola also requires bee pollination, and some beekeepers service this need. The vast northern forest reserves of softwood allow Alberta to produce large quantities of lumber, oriented strand board (OSB) and plywood, and several plants in northern Alberta supply North America and the Pacific Rim nations with bleached wood pulp and newsprint.
Alberta has been a tourist destination from the early days of the twentieth century, with attractions including outdoor locales for skiing, hiking and camping, shopping locales such as West Edmonton Mall, Calgary Stampede, outdoor festivals, professional athletic events, international sporting competitions such as the Commonwealth Games and Olympic Games, as well as more eclectic attractions. There are also natural attractions like Elk Island National Park, Wood Buffalo National Park, and the Columbia Icefield. According to Alberta Economic Development, Calgary and Edmonton both host over four million visitors annually. Banff, Jasper and the Rocky Mountains are visited by about three million people per year. Alberta tourism relies heavily on Southern Ontario tourists, as well as tourists from other parts of Canada, the United States, and many international countries. Alberta's Rocky Mountains include well known tourist destinations Banff National Park and Jasper National Park. The two mountain parks are connected by the scenic Icefields Parkway. Banff is located west of Calgary on Highway 1, and Jasper is located west of Edmonton on Yellowhead Highway. Five of Canada's fourteen UNESCO World heritage sites are located within the province: Canadian Rocky Mountain Parks, Waterton-Glacier International Peace Park, Wood Buffalo National Park, Dinosaur Provincial Park and Head-Smashed-In Buffalo Jump. About 1.2 million people visit the of Calgary Stampede, a celebration of Canada's own Wild West and the cattle ranching industry. About 800,000 people enjoy Edmonton's Capital Ex (formerly Klondike Days). Edmonton was the gateway to the only all-Canadian route to the Yukon gold fields, and the only route which did not require gold-seekers to travel the exhausting and dangerous Chilkoot Pass. Another tourist destination that draws more than 650,000 visitors each year is the Drumheller Valley, located northeast of Calgary. Drumheller, "Dinosaur Capital of The World", offers the Royal Tyrrell Museum of Palaeontology. Drumheller also had a rich mining history being one of Western Canada's largest coal producers during the war years. The Canadian Badlands has much to offer in the way of attractions, cultural events, celebrations, accommodations and service. Located in east-central Alberta is Alberta Prairie Railway Excursions, a popular tourist attraction operated out of Stettler. It boasts one of the few operable steam trains in the world, offering trips through the rolling prairie scenery. Alberta Prairie Railway Excursions caters to tens of thousands of visitors every year. Alberta is an important destination for tourists who love to ski and hike; Alberta boasts several world-class ski resorts such as Sunshine Village, Lake Louise, Marmot Basin, Norquay and Nakiska. Hunters and fishermen from around the world are able to take home impressive trophies and tall tales from their experiences in Alberta's wilderness.
The province's revenue comes mainly from royalties on non-renewable natural resources (30.4%), personal income taxes (22.3%), corporate and other taxes (19.6%), and grants from the federal government primarily for infrastructure projects (9.8%). Albertans are the lowest-taxed people in Canada, and Alberta is the only province in Canada without a provincial sales tax (though residents are still subject to the federal sales tax, the Goods and Services Tax of 5%.) It is also the only Canadian province to have a single rate of taxation for personal income taxes which is 10% of taxable income. The Alberta tax system maintains a progressive flavour by allowing residents to earn $16,161 before becoming subject to provincial taxation in addition to a variety of tax deductions for persons with disabilities, students, and the aged. Alberta's municipalities and school jurisdictions have their own governments which (usually) work in co-operation with the provincial government.
Alberta has over of highways and roads, of which nearly are paved. The main north-south corridor is Highway 2, which begins south of Cardston at the Carway border crossing and is part of the CANAMEX Corridor. Highway 4, which effectively extends Interstate 15 into Alberta and is the busiest U.S. gateway to the province, begins at the Coutts border crossing and ends at Lethbridge. Highway 3 joins Lethbridge to Fort Macleod and links Highway 4 to Highway 2. Highway 2 travels northward through Fort Macleod, Calgary, Red Deer, and Edmonton. North of Edmonton the highway continues to Athabasca, then northwesterly along the south shore of Lesser Slave Lake into High Prairie, north to Peace River, west to Fairview and finally south to Grande Prairie. The section of Highway 2 between Calgary and Edmonton has been named the Queen Elizabeth II Highway to commemorate the visit of the monarch in 2005. Highway 2 is supplemented by two more highways that run parallel to it: Highway 22, west of highway 2, known as "the Cowboy Trail," and Highway 21, east of highway 2. Highway 43 travels northwest into Grande Prairie and the Peace River Country; Highway 63 travels northeast to Fort McMurray, the location of the Athabasca Oil Sands. Alberta has two main east-west corridors. The southern corridor, part of the Trans-Canada Highway system, enters the province near Medicine Hat, runs westward through Calgary, and leaves Alberta through Banff National Park. The northern corridor, also part of the Trans-Canada network and known as the Yellowhead Highway (Highway 16), runs west from Lloydminster in eastern Alberta, through Edmonton and Jasper National Park into British Columbia. One of the most scenic drives is along the Icefields Parkway, which runs for between Jasper and Lake Louise, with mountain ranges and glaciers on either side of its entire length. Another major corridor through central Alberta is Highway 11 (also known as the David Thompson Highway), which runs east from the Saskatchewan River Crossing in Banff National Park through Rocky Mountain House and Red Deer, connecting with Highway 12 west of Stettler. The highway connects many of the smaller towns in central Alberta with Calgary and Edmonton, as it crosses Highway 2 just west of Red Deer. Urban stretches of Alberta's major highways and freeways are often called "trails". For example, Highway 2, the main north-south highway in the province, is called Deerfoot Trail as it passes through Calgary but becomes Calgary Trail as it enters Edmonton and then turns into Saint Albert Trail as it leaves Edmonton for the city of St. Albert. Calgary, in particular, has a tradition of calling its largest urban expressways "trails" and naming many of them after prominent First Nations individuals and tribes, such as Crowchild Trail, Deerfoot Trail, and Stoney Trail. Calgary, Edmonton, Red Deer, Medicine Hat, and Lethbridge have substantial public transit systems. In addition to buses, Calgary and Edmonton operate light rail transit (LRT) systems. Edmonton LRT, which is underground in the downtown core and on the surface outside of it, was the first of the modern generation of light rail systems to be built in North America, while the Calgary C-Train, although operating mostly on the surface, has almost 4 times more track than the Edmonton LRT and the highest ridership of any LRT system in North America. Alberta is well-connected by air, with international airports in both Calgary and Edmonton. Calgary International Airport and Edmonton International Airport are the fourth and fifth busiest in Canada respectively. Calgary's airport is a hub for WestJet Airlines and a regional hub for Air Canada. Calgary's airport primarily serves the Canadian prairie provinces (Alberta, Saskatchewan and Manitoba) for connecting flights to British Columbia, eastern Canada, 15 major US centres, nine European airports, and four destinations in Mexico and the Caribbean. Edmonton's airport acts as a hub for the Canadian north and has connections to all major Canadian airports as well as 10 major US airports, 3 European airports and 6 Mexican and Caribbean airports. There are over of operating mainline railway, and many tourists see Alberta aboard Via Rail or Rocky Mountaineer. The Canadian Pacific Railway and Canadian National Railway companies operate railway freight across the province.
The government of Alberta is organized as a parliamentary democracy with a unicameral legislature. Its unicameral legislature—the Legislative Assembly—consists of eighty-three members. Locally municipal governments and school boards are elected and operate separately. Their boundaries do not necessarily coincide. Municipalities where the same body act as both local government and school board are formally referred to as "counties" in Alberta. As Canada's head of state, Queen Elizabeth II is the head of state for the Government of Alberta. Her duties in Alberta are carried out by Lieutenant Governor Norman Kwong. Although the lieutenant governor is technically the most powerful person in Alberta, he is in reality a figurehead whose actions are restricted by custom and constitutional convention. The government is therefore headed by the premier. The current premier is Ed Stelmach who was elected as leader of the governing Progressive Conservatives on December 2, 2006. Stelmach was sworn in as the 13th Premier of Alberta on December 15, 2006. The Premier is a Member of the Legislative Assembly, and he draws all the members of his Cabinet from among the members of the Legislative Assembly. The City of Edmonton is the seat of the provincial government—the capital of Alberta. Alberta's elections tend to yield results which are much more conservative than those of other Canadian provinces. Alberta has traditionally had three political parties, the Progressive Conservatives ("Conservatives" or "Tories"), the Liberals, and the social democratic New Democrats. A fourth party, the strongly conservative Social Credit Party, was a power in Alberta for many decades, but fell from the political map after the Progressive Conservatives came to power in 1971. Since that time, no other political party has governed Alberta. In fact, only four parties have governed Alberta: the Liberals, from 1905 to 1921; the United Farmers of Alberta, from 1921 to 1935; the Social Credit Party, from 1935 to 1971, and the currently governing Progressive Conservative Party, from 1971 to the present. Alberta has had occasional surges in separatist sentiment. Even during the 1980s, when these feelings were at their strongest, there has never been enough interest in secession to initiate any major movements or referendums. There are several groups wishing to promote the independence of Alberta in some form currently active in the province. In the 2008 provincial election, held on March 3, 2008, the Progressive Conservative Party was re-elected as a majority government with 72 of 83 seats, the Alberta Liberal Party was elected as the Official Opposition with nine members, and the Alberta New Democratic Party elected two members.
As with all Canadian provinces, Alberta provides for all citizens and residents through a publicly funded health care system. Alberta became Canada's second province (after Saskatchewan) to adopt a Tommy Douglas-style program in 1950, a precursor to the modern medicare system. Alberta's health care budget is currently $13.2 billion during the 2008-2009 fiscal year (approximately 36% of all government spending), making it the best funded health care system per-capita in Canada. Every hour more than $1.5 million is spent on health care in the province. A highly educated population and burgeoning economy have made Alberta a national leader in health education, research, and resources. Many notable facilities include the Foothills Medical Centre, the Peter Lougheed Centre, Rockyview General Hospital, Alberta Children's Hospital, Grace Women's Health Centre, The University of Calgary Medical Centre (UCMC), Tom Baker Cancer Centre and Libin Cardiovascular Institute of Alberta, in Calgary; In Edmonton, the University of Alberta Hospital, the Royal Alexandra Hospital, the Mazankowski Alberta Heart Institute, the Lois Hole Hospital for Women, the Stollery Children's Hospital, the Alberta Diabetes Institute, the Cross Cancer Institute, and the Rexall Centre for Pharmacy and Health Research in Edmonton. Currently under construction in Edmonton is the new $909 million Edmonton Clinic, which will provide a similar research, education, and care environment as the Mayo Clinic in the United States. Health Care in Alberta is administered by the unified Alberta Health Services Board. Prior to July 1, 2008 Alberta was divided into nine health regions: Aspen Regional Health Authority: Calgary Health Region, Capital Health (Edmonton), Chinook Health, David Thompson Regional Health Authority, East Central Health, Northern Lights Health Region, Palliser Health Region and Peace Country Health Region. The Shock Trauma Air Rescue Society, a nonprofit organization, provides an air ambulance service to all but the most remote areas of Alberta, and some adjoining areas of British Columbia.
As with any Canadian province, the Alberta Legislature has (almost) exclusive authority to make laws respecting education. Since 1905 the Legislature has used this capacity to continue the model of locally elected public and separate school boards which originated prior to 1905, as well as to create and/or regulate universities, colleges, technical institutions and other educational forms and institutions (public charter schools, private schools, home schooling).
There are forty-two public school jurisdictions in Alberta, and seventeen operating separate school jurisdictions. Sixteen of the operating separate school jurisdictions have a Catholic electorate, and one (St. Albert) has a Protestant electorate. In addition, one Protestant separate school district, Glen Avon, survives as a ward of the St. Paul Education Region. The City of Lloydminster straddles the Alberta/Saskatchewan border, and both the public and separate school systems in that city are counted in the above numbers: both of them operate according to Saskatchewan law. For many years the provincial government has funded the greater part of the cost of providing K–12 education. Prior to 1994 public and separate school boards in Alberta had the legislative authority to levy a local tax on property, as supplementary support for local education. In 1994 the government of the province eliminated this right for public school boards, but not for separate school boards. Since 1994 there has continued to be a tax on property in support of K–12 education; the difference is that the mill rate is now set by the provincial government, the money is collected by the local municipal authority and remitted to the provincial government. The relevant legislation requires that all the money raised by this property tax must go to the support of K–12 education provided by school boards. The provincial government pools the property tax funds from across the province and distributes them, according to a formula, to public and separate school jurisdictions and Francophone authorities. Public and separate school boards, charter schools, and private schools all follow the Program of Studies and the curriculum approved by the provincial department of education (Alberta Education). Home schoolers may choose to follow the Program of Studies or develop their own Program of Studies. Public and separate schools, charter schools, and approved private schools all employ teachers who are certificated by Alberta Education, they administer Provincial Achievement Tests and Diploma Examinations set by Alberta Education, and they may grant high school graduation certificates endorsed by Alberta Education.
Alberta's oldest and largest university is Edmonton's University of Alberta established in 1908. The University of Calgary, once affiliated with the University of Alberta, gained its autonomy in 1966 and is now the second largest university in Alberta. There is also Athabasca University, which focuses on distance learning, and the University of Lethbridge, both of which are located in their title cities. In early September, 2009, Mount Royal University became Calgary's second public university, and in late September, 2009, a similar move made Grant MacEwan University Edmonton's second public university. There are 15 colleges that receive direct public funding, along with two technical institutes, Northern Alberta Institute of Technology and Southern Alberta Institute of Technology. There is also a large and active private sector of post-secondary institutions, mostly Christian Universities, bringing the total number of universities to twelve, plus a DeVry University location in Calgary. Students may also receive government loans and grants while attending selected private institutions. There has been some controversy in recent years over the rising cost of post-secondary education for students (as opposed to taxpayers). In 2005, Premier Ralph Klein made a promise that he would freeze tuition and look into ways of reducing schooling costs. So far, no plan has been released by the government of Alberta.
Summer brings many festivals to the province of Alberta, especially in Edmonton. The Edmonton Fringe Festival is the world's second largest after Edinburgh's. The Folk music festivals in both Calgary and Edmonton are two of Canada's largest and both cities host a number of annual multicultural events. With a large number of summer and winter events, Edmonton prides itself as being the "Festival City". The city's "heritage days" festival sees the participation of over 70 ethnic groups. Edmonton's Churchill Square is home to a large number of the festivals, including the large Taste of Edmonton & The Works Art & Design Festival throughout the summer months. Calgary is also home to Carifest, the second largest Caribbean festival in the nation (after Caribana in Toronto). Edmonton has Cariwest, a smaller Caribbean Parade in the downtown streets. Both Edmonton and Calgary are also known for decent Film festivals. The city of Calgary is also famous for its Calgary Stampede, dubbed "The Greatest Outdoor Show on Earth." The Stampede is Canada's biggest rodeo festival and features various races and competitions, such as calf roping and bull riding. In line with the western tradition of rodeo are the cultural artisans that reside and create unique Alberta western heritage crafts. The Banff Centre also hosts a range of festivals and other events including the internationally known Mountain Film Festival. These cultural events in Alberta highlight the province's cultural diversity and love of entertainment. Most of the major cities have several performing theatre companies who entertain in venues as diverse as Edmonton's Arts Barns and the Francis Winspear Centre for Music. Both Calgary and Edmonton are home to Canadian Football League and National Hockey League teams. Soccer, rugby union and lacrosse are also played professionally in Alberta.
The Arctic Circle is one of the five major circles of latitude that mark maps of the Earth. For Epoch 2010, it is the parallel of latitude that runs 66º 33′ 44″ (or 66.56222°) north of the Equator. The region north of this circle is known as the Arctic, and the zone just to the south is called the Northern Temperate Zone. The equivalent polar circle in the Southern Hemisphere is called the Antarctic Circle. The Arctic Circle marks the southern extremity of the polar day (24-hour sunlit day, often referred to as the "midnight sun") and polar night (24-hour sunless night). North of the Arctic Circle, the sun is above the horizon for 24 continuous hours at least once per year and below the horizon for 24 continuous hours at least once per year. On the Arctic Circle those events occur, in principle, exactly once per year, at the June and December solstices, respectively. In fact, because of atmospheric refraction and because the sun appears as a disk and not a point, part of the midnight sun may be seen on the night of the summer solstice up to about 50′ () south of the Arctic Circle; similarly, on the day of the winter solstice, part of the sun may be seen up to about 50′ north of the Arctic Circle. That is true at sea level; those limits increase with elevation above sea level although in mountainous regions, there is often no direct view of the horizon. The position of the Arctic Circle is not fixed, but directly depends on the Earth's axial tilt, which fluctuates within a margin of 2° over a 40,000 year period, notably due to tidal forces resulting from the orbit of the Moon. The Arctic Circle is currently drifting northwards at a speed of about per year, see "Circle of latitude" for more information. __TOC__ Murmansk Oblast Karelia again Murmansk again
The Actinopterygii constitute the class or sub-class of the ray-finned fishes. The ray-finned fishes are so called because they possess lepidotrichia or "fin rays", their fins being webs of skin supported by bony or horny spines ("rays"), as opposed to the fleshy, lobed fins that characterize the class Sarcopterygii which also, however, possess lepidotrichia. These actinopterygian fin rays attach directly to the proximal or basal skeletal elements, the radials, which represent the link or connection between these fins and the internal skeleton (e.g., pelvic and pectoral girdles). In terms of numbers, actinopterygians are the dominant class of vertebrates, comprising nearly 95% of the 25,000 species of fish. They are ubiquitous throughout fresh water and marine environments from the deep sea to the highest mountain streams. Extant species can range in size from "Paedocypris", at, to the massive Ocean Sunfish, at, and the long-bodied Oarfish, to at least.
Traditionally three grades of actinopterygians have been recognised: the Chondrostei, Holostei, and Teleostei. Some morphological evidence suggests that the second is paraphyletic and should be abandoned; however, recent work based on more complete sampling of fossil taxa, and also an analysis of DNA sequence data from the complete mitochondrial genome, supports its recognition. Nearly all living bony fishes are teleosts. A listing of the different groups is given below, down to the level of orders, arranged in what has been suggested to represent the evolutionary sequence down to the level of order based primarily on the long history of morphological studies. This classification, like any other taxonomy based on phylogenetic research is in a state of flux. Many of these ordinal and higher-level groupings have not been supported in both the recent morphological and molecular literature. Examples of demonstrably paraphyletic or unnatural groups include the Paracanthopterygii, Scorpaeniformes, and Perciformes. The listing follows FishBase with notes when this differs from Nelson and ITIS.
Albert Einstein (;; 14 March 1879–18 April 1955) was a German-born Swiss-American theoretical physicist, philosopher and author who is widely regarded as one of the most influential and best known scientists and intellectuals of all time. He is often regarded as the father of modern physics. He received the 1921 Nobel Prize in Physics "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect." His many contributions to physics include the special and general theories of relativity, the founding of relativistic cosmology, the first post-Newtonian expansion, explaining the perihelion advance of Mercury, prediction of the deflection of light by gravity and gravitational lensing, the first fluctuation dissipation theorem which explained the Brownian movement of molecules, the photon theory and wave-particle duality, the quantum theory of atomic motion in solids, the zero-point energy concept, the semiclassical version of the Schrödinger equation, and the quantum theory of a monatomic gas which predicted Bose–Einstein condensation. Einstein published more than 300 scientific and over 150 non-scientific works. Einstein additionally wrote and commentated prolifically on numerous philosophical and political issues.
Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire on 14 March 1879. His father was Hermann Einstein, a salesman and engineer. His mother was Pauline Einstein (née Koch). In 1880, the family moved to Munich, where his father and his uncle founded " Elektrotechnische Fabrik J. Einstein & Cie," a company that manufactured electrical equipment based on direct current. The Einsteins were non-observant Jews. Their son attended a Catholic elementary school from the age of five until ten. Although Einstein had early speech difficulties, he was a top student in elementary school. As he grew, Einstein built models and mechanical devices for fun and began to show a talent for mathematics. In 1889 Max Talmud (later changed to Max Talmey) introduced the ten-year old Einstein to key texts in science, mathematics and philosophy, including Kant’s "Critique of Pure Reason" and Euclid’s "Elements" (which Einstein called the "holy little geometry book"). Talmud was a poor Jewish medical student from Poland. The Jewish community arranged for Talmud to take meals with the Einsteins each week on Thursdays for six years. During this time Talmud wholeheartedly guided Einstein through many secular educational interests. In 1894, his father’s company failed: Direct current (DC) lost the War of Currents to alternating current (AC). In search of business, the Einstein family moved to Italy, first to Milan and then, a few months later, to Pavia. When the family moved to Pavia, Einstein stayed in Munich to finish his studies at the Luitpold Gymnasium. His father intended for him to pursue electrical engineering, but Einstein clashed with authorities and resented the school’s regimen and teaching method. He later wrote that the spirit of learning and creative thought were lost in strict rote learning. In the spring of 1895, he withdrew to join his family in Pavia, convincing the school to let him go by using a doctor’s note. During this time, Einstein wrote his first scientific work, "The Investigation of the State of Aether in Magnetic Fields". Einstein applied directly to the Eidgenössische Polytechnische Schule (ETH) in Zürich, Switzerland. Lacking the requisite Matura certificate, he took an entrance examination, which he failed, although he got exceptional marks in mathematics and physics. The Einsteins sent Albert to Aarau, in northern Switzerland to finish secondary school. While lodging with the family of Professor Jost Winteler, he fell in love with the family’s daughter, Marie. (His sister Maja later married the Winteler son, Paul.) In Aarau, Einstein studied Maxwell’s electromagnetic theory. At age 17, he graduated, and, with his father’s approval, renounced his citizenship in the German Kingdom of Württemberg to avoid military service, and enrolled in 1896 in the mathematics and physics program at the Polytechnic in Zurich. Marie Winteler moved to Olsberg, Switzerland for a teaching post. In the same year, Einstein’s future wife, Mileva Marić, also entered the Polytechnic to study mathematics and physics, the only woman in the academic cohort. Over the next few years, Einstein and Marić’s friendship developed into romance. In a letter to her, Einstein called Marić “a creature who is my equal and who is as strong and independent as I am.” Einstein graduated in 1900 from the Polytechnic with a diploma in mathematics and physics; Although historians have debated whether Marić influenced Einstein’s work, the majority of academic historians of science agree that she did not.
In early 1902, Einstein and Mileva Marić had a daughter they named Lieserl in their correspondence, who was born in Novi Sad where Marić's parents lived. Her full name is not known, and her fate is uncertain after 1903. Einstein and Marić married in January 1903. In May 1904, the couple’s first son, Hans Albert Einstein, was born in Bern, Switzerland. Their second son, Eduard, was born in Zurich in July 1910. In 1914, Einstein moved to Berlin, while his wife remained in Zurich with their sons. Marić and Einstein divorced on 14 February 1919, having lived apart for five years. Einstein married Elsa Löwenthal (née Einstein) on 2 June 1919, after having had a relationship with her since 1912. She was his first cousin maternally and his second cousin paternally. In 1933, they emigrated permanently to the United States. In 1935, Elsa Einstein was diagnosed with heart and kidney problems and died in December 1936.
After graduating, Einstein spent almost two frustrating years searching for a teaching post, but a former classmate’s father helped him secure a job in Bern, at the Federal Office for Intellectual Property, the patent office, as an assistant examiner. He evaluated patent applications for electromagnetic devices. In 1903, Einstein’s position at the Swiss Patent Office became permanent, although he was passed over for promotion until he "fully mastered machine technology". Much of his work at the patent office related to questions about transmission of electric signals and electrical-mechanical synchronization of time, two technical problems that show up conspicuously in the thought experiments that eventually led Einstein to his radical conclusions about the nature of light and the fundamental connection between space and time. With friends he met in Bern, Einstein formed a weekly discussion club on science and philosophy, which he jokingly named "The Olympia Academy." Their readings included the works of Henri Poincaré, Ernst Mach, and David Hume, which influenced his scientific and philosophical outlook.
In 1901, Einstein had a paper on the capillary forces of a straw published in the prestigious "Annalen der Physik". In 1905, he received his doctorate from the University of Zurich. His thesis was titled "On a new determination of molecular dimensions". That same year, which has been called Einstein's "annus mirabilis" or "miracle year", he published four groundbreaking papers, on the photoelectric effect, Brownian motion, special relativity, and the equivalence of matter and energy, which were to bring him to the notice of the academic world. By 1908, he was recognized as a leading scientist, and he was appointed lecturer at the University of Berne. The following year, he quit the patent office and the lectureship to take the position of physics professor at the University of Zurich. He became a full professor at Karl-Ferdinand University in Prague in 1911. In 1914, he returned to Germany after being appointed director of the Kaiser Wilhelm Institute for Physics and professor at the University of Berlin. In 1911, he had calculated that, based on his new theory of general relativity, light from another star would be bent by the Sun's gravity. That prediction was claimed confirmed by observations made by a British expedition led by Sir Arthur Eddington during the solar eclipse of May 29, 1919. International media reports of this made Einstein world famous. (Much later, questions were raised whether the measurements were accurate enough to support such a claim.) In 1921, Einstein was awarded the Nobel Prize in Physics. Because relativity was still considered somewhat controversial, it was officially bestowed for his explanation of the photoelectric effect. He also received the Copley Medal from the Royal Society in 1925. Emigration to the United States. In 1933, Einstein emigrated because of the rise to power of the Nazis and took up a position at the Institute for Advanced Study at Princeton, New Jersey, an affiliation that lasted until his death in 1955. There, he tried unsuccessfully to develop a unified field theory and to refute the accepted interpretation of quantum physics. He and Kurt Gödel, another Institute member, became close friends. They would take long walks together discussing their work. Just prior to the beginning of World War II in Europe, Einstein was persuaded to lend his enormous prestige to a letter sent to President Franklin D. Roosevelt on August 2, 1939, alerting him to the possibility that Nazi Germany might be developing an atomic bomb. In 1940, he became an American citizen. In 1952, he was offered the position of President of Israel, but declined.
On 17 April 1955, Albert Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Dr. Rudolph Nissen in 1948. He took the draft of a speech he was preparing for a television appearance commemorating the State of Israel’s seventh anniversary with him to the hospital, but he did not live long enough to complete it. Einstein refused surgery, saying: "I want to go when I want. It is tasteless to prolong life artificially. I have done my share, it is time to go. I will do it elegantly." He died in Princeton Hospital early the next morning at the age of 76, having continued to work until near the end. Einstein’s remains were cremated and his ashes were scattered around the grounds of the Institute for Advanced Study, Princeton, New Jersey. During the autopsy, the pathologist of Princeton Hospital, Thomas Stoltz Harvey removed Einstein’s brain for preservation, without the permission of his family, in hope that the neuroscience of the future would be able to discover what made Einstein so intelligent.
Einstein’s early papers all come from attempts to demonstrate that atoms exist and have a finite nonzero size. At the time of his first paper in 1902, it was not yet completely accepted by physicists that atoms were real, even though chemists had good evidence ever since Antoine Lavoisier’s work a century earlier. The reason physicists were skeptical was because no 19th century theory could fully explain the properties of matter from the properties of atoms. Ludwig Boltzmann was a leading 19th century atomist physicist, who had struggled for years to gain acceptance for atoms. Boltzmann had given an interpretation of the laws of thermodynamics, suggesting that the law of entropy increase is statistical. In Boltzmann’s way of thinking, the entropy is the logarithm of the number of ways a system could be configured inside. The reason the entropy goes up is only because it is more likely for a system to go from a special state with only a few possible internal configurations to a more generic state with many. While Boltzmann’s statistical interpretation of entropy is universally accepted today, and Einstein believed it, at the turn of the 20th century it was a minority position. The statistical idea was most successful in explaining the properties of gases. James Clerk Maxwell, another leading atomist, had found the distribution of velocities of atoms in a gas, and derived the surprising result that the viscosity of a gas should be independent of density. Intuitively, the friction in a gas would seem to go to zero as the density goes to zero, but this is not so, because the mean free path of atoms becomes large at low densities. A subsequent experiment by Maxwell and his wife confirmed this surprising prediction. Other experiments on gases and vacuum, using a rotating slitted drum, showed that atoms in a gas had velocities distributed according to Maxwell’s distribution law. In addition to these successes, there were also inconsistencies. Maxwell noted that at cold temperatures, atomic theory predicted specific heats that are too large. In classical statistical mechanics, every spring-like motion has thermal energy "k"B"T" on average at temperature "T", so that the specific heat of every spring is Boltzmann’s constant "k"B. A monatomic solid with "N" atoms can be thought of as "N" little balls representing "N" atoms attached to each other in a box grid with 3"N" springs, so the specific heat of every solid is 3"Nk"B, a result which became known as the Dulong–Petit law. This law is true at room temperature, but not for colder temperatures. At temperatures near zero, the specific heat goes to zero. Similarly, a gas made up of a molecule with two atoms can be thought of as two balls on a spring. This spring has energy "k"B"T" at high temperatures, and should contribute an extra "k"B to the specific heat. It does at temperatures of about 1000 degrees, but at lower temperature, this contribution disappears. At zero temperature, all other contributions to the specific heat from rotations and vibrations also disappear. This behavior was inconsistent with classical physics. The most glaring inconsistency was in the theory of light waves. Continuous waves in a box can be thought of as infinitely many spring-like motions, one for each possible standing wave. Each standing wave has a specific heat of "k"B, so the total specific heat of a continuous wave like light should be infinite in classical mechanics. This is obviously wrong, because it would mean that all energy in the universe would be instantly sucked up into light waves, and everything would slow down and stop. These inconsistencies led some people to say that atoms were not physical, but mathematical. Notable among the skeptics was Ernst Mach, whose positivist philosophy led him to demand that if atoms are real, it should be possible to see them directly. Mach believed that atoms were a useful fiction, that in reality they could be assumed to be infinitesimally small, that Avogadro’s number was infinite, or so large that it might as well be infinite, and "k"B was infinitesimally small. Certain experiments could then be explained by atomic theory, but other experiments could not, and this is the way it will always be. Einstein opposed this position. Throughout his career, he was a realist. He believed that a single consistent theory should explain all observations, and that this theory would be a description of what was really going on, underneath it all. So he set out to show that the atomic point of view was correct. This led him first to thermodynamics, then to statistical physics, and to the theory of specific heats of solids. In 1905, while he was working in the patent office, the leading German language physics journal "Annalen der Physik" published four of Einstein’s papers. The four papers eventually were recognized as revolutionary, and 1905 became known as Einstein’s "Miracle Year", and the papers as the "Annus Mirabilis Papers". Thermodynamic fluctuations and statistical physics. Einstein’s earliest papers were concerned with thermodynamics. He wrote a paper establishing a thermodynamic identity in 1902, and a few other papers which attempted to interpret phenomena from a statistical atomic point of view. His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena. As in Maxwell’s work, the finite nonzero size of atoms leads to effects which can be observed. This research, and the thermodynamic identity, were well within the mainstream of physics in his time. They would eventually form the content of his PhD thesis. His first major result in this field was the theory of thermodynamic fluctuations. When in equilibrium, a system has a maximum entropy and, according to the statistical interpretation, it can fluctuate a little bit. Einstein pointed out that the statistical fluctuations of a macroscopic object, like a mirror suspended on spring, would be completely determined by the second derivative of the entropy with respect to the position of the mirror. Searching for ways to test this relation, his great breakthrough came in 1905. The theory of fluctuations, he realized, would have a visible effect for an object which could move around freely. Such an object would have a velocity which is random, and would move around randomly, just like an individual atom. The average kinetic energy of the object would be formula_1, and the time decay of the fluctuations would be entirely determined by the law of friction. The law of friction for a small ball in a viscous fluid like water was discovered by George Stokes. He showed that for small velocities, the friction force would be proportional to the velocity, and to the radius of the particle (see Stokes’ law). This relation could be used to calculate how far a small ball in water would travel due to its random thermal motion, and Einstein noted that such a ball, of size about a micron, would travel about a few microns per second. This motion could be easily detected with a microscope and indeed, as Brownian motion, had actually been observed by the botanist Robert Brown. Einstein was able to identify this motion with that predicted by his theory. Since the fluctuations which give rise to Brownian motion are just the same as the fluctuations of the velocities of atoms, measuring the precise amount of Brownian motion using Einstein’s theory would show that Boltzmann’s constant is non-zero and would measure Avogadro’s number. These experiments were carried out a few years later, and gave a rough estimate of Avogadro’s number consistent with the more accurate estimates due to Max Planck’s theory of blackbody light, and Robert Millikan’s measurement of the charge of the electron. Unlike the other methods, Einstein’s required very few theoretical assumptions or new physics, since it was directly measuring atomic motion on visible grains. Einstein’s theory of Brownian motion was the first paper in the field of statistical physics. It established that thermodynamic fluctuations were related to dissipation. This was shown by Einstein to be true for time-independent fluctuations, but in the Brownian motion paper he showed that dynamical relaxation rates calculated from classical mechanics could be used as statistical relaxation rates to derive dynamical diffusion laws. These relations are known as Einstein relations. The theory of Brownian motion was the least revolutionary of Einstein’s Annus mirabilis papers, but it had an important role in securing the acceptance of the atomic theory by physicists. Thought experiments and a-priori physical principles. Einstein’s thinking underwent a transformation in 1905. He had come to understand that quantum properties of light mean that Maxwell’s equations were only an approximation. He knew that new laws would have to replace these, but he did not know how to go about finding those laws. He felt that guessing formal relations would not go anywhere. So he decided to focus on a-priori principles instead, which are statements about physical laws which can be understood to hold in a very broad sense even in domains where they have not yet been shown to apply. A well accepted example of an a-priori principle is rotational invariance. If a new force is discovered in physics, it is assumed to be rotationally invariant almost automatically, without thought. Einstein sought new principles of this sort, to guide the production of physical ideas. Once enough principles are found, then the new physics will be the simplest theory consistent with the principles and with previously known laws. The first general a-priori principle he found was the principle of relativity, that uniform motion is indistinguishable from rest. This was understood by Hermann Minkowski to be a generalization of rotational invariance from space to space-time. Other principles postulated by Einstein and later vindicated are the principle of equivalence and the principle of adiabatic invariance of the quantum number. Another of Einstein’s general principles, Mach’s principle, is fiercely debated, and whether it holds in our world or not is still not definitively established. The use of a-priori principles is a distinctive unique signature of Einstein’s early work, and has become a standard tool in modern theoretical physics.
His 1905 paper on the electrodynamics of moving bodies introduced his theory of special relativity, which showed that the observed independence of the speed of light on the observer’s state of motion required fundamental changes to the notion of simultaneity. Consequences of this include the time-space frame of a moving body slowing down and contracting (in the direction of motion) relative to the frame of the observer. This paper also argued that the idea of a luminiferous aether – one of the leading theoretical entities in physics at the time – was superfluous. In his paper on "mass–energy equivalence", which had previously been considered to be distinct concepts, Einstein deduced from his equations of special relativity what has been called the twentieth century’s best-known equation: "E" = "mc"2. This equation suggests that tiny amounts of mass could be converted into huge amounts of energy and presaged the development of nuclear power. Einstein’s 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.
In a 1905 paper, Einstein postulated that light itself consists of localized particles ("quanta"). Einstein’s light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr. This idea only became universally accepted in 1919, with Robert Millikan’s detailed experiments on the photoelectric effect, and with the measurement of Compton scattering. Einstein’s paper on the light particles was almost entirely motivated by thermodynamic considerations. He was not at all motivated by the detailed experiments on the photoelectric effect, which did not confirm his theory until fifteen years later. Einstein considers the entropy of light at temperature "T", and decomposes it into a low-frequency part and a high-frequency part. The high-frequency part, where the light is described by Wien’s law, has an entropy which looks exactly the same as the entropy of a gas of classical particles. Since the entropy is the logarithm of the number of possible states, Einstein concludes that the number of states of short wavelength light waves in a box with volume "V" is equal to the number of states of a group of localizable particles in the same box. Since (unlike others) he was comfortable with the statistical interpretation, he confidently postulates that the light itself is made up of localized particles, as this is the only reasonable interpretation of the entropy. This leads him to conclude that each wave of frequency "f" is associated with a collection of photons with energy "hf" each, where "h" is Planck’s constant. He does not say much more, because he is not sure how the particles are related to the wave. But he does suggest that this idea would explain certain experimental results, notably the photoelectric effect.
Einstein continued his work on quantum mechanics in 1906, by explaining the specific heat anomaly in solids. This was the first application of quantum theory to a mechanical system. Since Planck’s distribution for light oscillators had no problem with infinite specific heats, the same idea could be applied to solids to fix the specific heat problem there. Einstein showed in a simple model that the hypothesis that solid motion is quantized explains why the specific heat of a solid goes to zero at zero temperature. Einstein’s model treats each atom as connected to a single spring. Instead of connecting all the atoms to each other, which leads to standing waves with all sorts of different frequencies, Einstein imagined that each atom was attached to a fixed point in space by a spring. This is not physically correct, but it still predicts that the specific heat is 3"Nk"B, since the number of independent oscillations stays the same. Einstein then assumes that the motion in this model is quantized, according to the Planck law, so that each independent spring motion has energy which is an integer multiple of hf, where f is the frequency of oscillation. With this assumption, he applied Boltzmann’s statistical method to calculate the average energy of the spring. The result was the same as the one that Planck had derived for light: for temperatures where "k"B"T" is much smaller than "hf", the motion is frozen, and the specific heat goes to zero. So Einstein concluded that quantum mechanics would solve the main problem of classical physics, the specific heat anomaly. The particles of sound implied by this formulation are now called phonons. Because all of Einstein’s springs have the same stiffness, they all freeze out at the same temperature, and this leads to a prediction that the specific heat should go to zero exponentially fast when the temperature is low. The solution to this problem is to solve for the independent normal modes individually, and to quantize those. Then each normal mode has a different frequency, and long wavelength vibration modes freeze out at colder temperatures than short wavelength ones. This was done by Debye, and after this modification Einstein’s quantization method reproduced quantitatively the behavior of the specific heats of solids at low temperatures. This work was the foundation of condensed matter physics. Adiabatic principle and action-angle variables. Throughout the 1910s, quantum mechanics expanded in scope to cover many different systems. After Ernest Rutherford discovered the nucleus and proposed that electrons orbit like planets, Niels Bohr was able to show that the same quantum mechanical postulates introduced by Planck and developed by Einstein would explain the discrete motion of electrons in atoms, and the periodic table of the elements. Einstein contributed to these developments by linking them with the 1898 arguments Wilhelm Wien had made. Wien had shown that the hypothesis of adiabatic invariance of a thermal equilibrium state allows all the blackbody curves at different temperature to be derived from one another by a simple shifting process. Einstein noted in 1911 that the same adiabatic principle shows that the quantity which is quantized in any mechanical motion must be an adiabatic invariant. Arnold Sommerfeld identified this adiabatic invariant as the action variable of classical mechanics. The law that the action variable is quantized was the basic principle of the quantum theory as it was known between 1900 and 1925.
Although the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on "academia." In 1908, he became a "privatdozent" at the University of Bern. In "über die Entwicklung unserer Anschauungen über das Wesen und die Konstitution der Strahlung" ("The Development of Our Views on the Composition and Essence of Radiation"), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck’s energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. This paper introduced the "photon" concept (although the name "photon" was introduced later by Gilbert N. Lewis in 1926) and inspired the notion of wave-particle duality in quantum mechanics.
Einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. At the critical point, this derivative is zero, leading to large fluctuations. The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. Einstein relates this to Raleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue.
Einstein’s physical intuition led him to note that Planck’s oscillator energies had an incorrect zero point. He modified Planck’s hypothesis by stating that the lowest energy state of an oscillator is equal to "hf", to half the energy spacing between levels. This argument, which was made in 1913 in collaboration with Otto Stern, was based on the thermodynamics of a diatomic molecule which can split apart into two free atoms.
In 1907, while still working at the patent office, Einstein had what he would call his "happiest thought". He realized that the principle of relativity could be extended to gravitational fields. He thought about the case of a uniformly accelerated box not in a gravitational field, and noted that it would be indistinguishable from a box sitting still in an unchanging gravitational field. He used special relativity to see that the rate of clocks at the top of a box accelerating upward would be faster than the rate of clocks at the bottom. He concludes that the rates of clocks depend on their position in a gravitational field, and that the difference in rate is proportional to the gravitational potential to first approximation. Although this approximation is crude, it allowed him to calculate the deflection of light by gravity, and show that it is nonzero. This gave him confidence that the scalar theory of gravity proposed by Gunnar Nordström was incorrect. But the actual value for the deflection that he calculated was too small by a factor of two, because the approximation he used doesn’t work well for things moving at near the speed of light. When Einstein finished the full theory of general relativity, he would rectify this error and predict the correct amount of light deflection by the sun. From Prague, Einstein published a paper about the effects of gravity on light, specifically the gravitational redshift and the gravitational deflection of light. The paper challenged astronomers to detect the deflection during a solar eclipse. German astronomer Erwin Finlay-Freundlich publicized Einstein’s challenge to scientists around the world. Einstein thought about the nature of the gravitational field in the years 1909–1912, studying its properties by means of simple thought experiments. A notable one is the rotating disk. Einstein imagined an observer making experiments on a rotating turntable. He noted that such an observer would find a different value for the mathematical constant pi than the one predicted by Euclidean geometry. The reason is that the radius of a circle would be measured with an uncontracted ruler, but, according to special relativity, the circumference would seem to be longer because the ruler would be contracted. Since Einstein believed that the laws of physics were local, described by local fields, he concluded from this that spacetime could be locally curved. This led him to study Riemannian geometry, and to formulate general relativity in this language. Hole argument and Entwurf theory. While developing general relativity, Einstein became confused about the gauge invariance in the theory. He formulated an argument that led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations, and searched for equations that would be invariant under general linear transformations only. The Entwurf ("draft") theory was the result of these investigations. As its name suggests, it was a sketch of a theory, with the equations of motion supplemented by additional gauge fixing conditions. Simultaneously less elegant and more difficult than general relativity, Einstein abandoned the theory after realizing that the hole argument was mistaken.
In 1912, Einstein returned to Switzerland to accept a professorship at his "alma mater," the ETH. Once back in Zurich, he immediately visited his old ETH classmate Marcel Grossmann, now a professor of mathematics, who introduced him to Riemannian geometry and, more generally, to differential geometry. On the recommendation of Italian mathematician Tullio Levi-Civita, Einstein began exploring the usefulness of general covariance (essentially the use of tensors) for his gravitational theory. For a while Einstein thought that there were problems with the approach, but he later returned to it and, by late 1915, had published his general theory of relativity in the form in which it is used today. This theory explains gravitation as distortion of the structure of spacetime by matter, affecting the inertial motion of other matter. During World War I, the work of Central Powers scientists was available only to Central Powers academics, for national security reasons. Some of Einstein’s work did reach the United Kingdom and the United States through the efforts of the Austrian Paul Ehrenfest and physicists in the Netherlands, especially 1902 Nobel Prize-winner Hendrik Lorentz and Willem de Sitter of Leiden University. After the war ended, Einstein maintained his relationship with Leiden University, accepting a contract as an "Extraordinary Professor"; for ten years, from 1920 to 1930, he travelled to Holland regularly to lecture. In 1917, several astronomers accepted Einstein ’s 1911 challenge from Prague. The Mount Wilson Observatory in California, U.S., published a solar spectroscopic analysis that showed no gravitational redshift. In 1918, the Lick Observatory, also in California, announced that it too had disproved Einstein’s prediction, although its findings were not published. However, in May 1919, a team led by the British astronomer Arthur Stanley Eddington claimed to have confirmed Einstein’s prediction of gravitational deflection of starlight by the Sun while photographing a solar eclipse with dual expeditions in Sobral, northern Brazil, and Príncipe, a west African island. Nobel laureate Max Born praised general relativity as the "greatest feat of human thinking about nature"; fellow laureate Paul Dirac was quoted saying it was "probably the greatest scientific discovery ever made". The international media guaranteed Einstein’s global renown. There have been claims that scrutiny of the specific photographs taken on the Eddington expedition showed the experimental uncertainty to be comparable to the same magnitude as the effect Eddington claimed to have demonstrated, and that a 1962 British expedition concluded that the method was inherently unreliable. The deflection of light during a solar eclipse was confirmed by later, more accurate observations. Some resented the newcomer’s fame, notably among some German physicists, who later started the "Deutsche Physik" (German Physics) movement.
In 1917, Einstein applied the General theory of relativity to model the structure of the universe as a whole. He wanted the universe to be eternal and unchanging, but this type of universe is not consistent with relativity. To fix this, Einstein modified the general theory by introducing a new notion, the cosmological constant. With a positive cosmological constant, the universe could be an eternal static sphere Einstein believed a spherical static universe is philosophically preferred, because it would obey Mach’s principle. He had shown that general relativity incorporates Mach’s principle to a certain extent in frame dragging by gravitomagnetic fields, but he knew that Mach’s idea would not work if space goes on forever. In a closed universe, he believed that Mach’s principle would hold. Mach’s principle has generated much controversy over the years.
In 1917, at the height of his work on relativity, Einstein published an article in "Physikalische Zeitschrift" that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser. This article showed that the statistics of absorption and emission of light would only be consistent with Planck’s distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws. Einstein discovered Louis de Broglie’s work, and supported his ideas, which were received skeptically at first. In another major paper from this era, Einstein gave a wave equation for de Broglie waves, which Einstein suggested was the Hamilton–Jacobi equation of mechanics. This paper would inspire Schrödinger’s work of 1926.
In 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose’s statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose’s paper to the "Zeitschrift für Physik". Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures. It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder. Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein’s sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.
General relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. Noether’s theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether’s presecriptions do not make a real tensor for this reason. Einstein argued that this is true for fundamental reasons, because the gravitational field could be made to vanish by a choice of coordinates. He maintained that the non-covariant energy momentum pseudotensor was in fact the best description of the energy momentum distribution in a gravitational field. This approach has been echoed by Lev Landau and Evgeny Lifshitz, and others, and has become standard. The use of non-covariant objects like pseudotensors was heavily criticized in 1917 by Erwin Schrödinger and others.
Following his research on general relativity, Einstein entered into a series of attempts to generalize his geometric theory of gravitation, which would allow the explanation of electromagnetism. In 1950, he described his "unified field theory" in a "Scientific American" article entitled "On the Generalized Theory of Gravitation." Although he continued to be lauded for his work, Einstein became increasingly isolated in his research, and his efforts were ultimately unsuccessful. In his pursuit of a unification of the fundamental forces, Einstein ignored some mainstream developments in physics, most notably the strong and weak nuclear forces, which were not well understood until many years after his death. Mainstream physics, in turn, largely ignored Einstein’s approaches to unification. Einstein’s dream of unifying other laws of physics with gravity motivates modern quests for a theory of everything and in particular string theory, where geometrical fields emerge in a unified quantum-mechanical setting.
Einstein collaborated with others to produce a model of a wormhole. His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper "Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?". These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches. If one end of a wormhole was positively charged, the other end would be negatively charged. These properties led Einstein to believe that pairs of particles and antiparticles could be described in this way.
In 1935, Einstein returned to the question of quantum mechanics. He considered how a measurement on one of two entangled particles would affect the other. He noted, along with his collaborators, that by performing different measurements on the distant particle, either of position or momentum, different properties of the entangled partner could be discovered without disturbing it in any way. He then used a hypothesis of local realism to conclude that the other particle had these properties already determined. The principle he proposed is that if it is possible to determine what the answer to a position or momentum measurement would be, without in any way disturbing the particle, then the particle actually has values of position or momentum. This principle distilled the essence of Einstein’s objection to quantum mechanics. As a physical principle, it has since been shown to be incompatible with experiments.
The theory of general relativity has two fundamental laws – the Einstein equations which describe how space curves, and the geodesic equation which describes how particles move. Since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein equations themselves, not by a new law. So Einstein proposed that the path of a singular solution, like a black hole, would be determined to be a geodesic from general relativity itself. This was established by Einstein, Infeld and Hoffmann for pointlike objects without angular momentum, and by Roy Kerr for spinning objects.
Einstein himself considered the use of the "fudge factor" lambda in his 1917 paper founding cosmology as a "blunder". The theory of general relativity predicted an expanding or contracting universe, but Einstein wanted a universe which is an unchanging three dimensional sphere, like the surface of a three dimensional ball in four dimensions. He wanted this for philosophical reasons, so as to incorporate Mach’s principle in a reasonable way. He stabilized his solution by introducing a cosmological constant, and when the universe was shown to be expanding, he retracted the constant as a blunder. This is not really much of a blunder – the cosmological constant is necessary within general relativity as it is currently understood, and it is widely believed to have a nonzero value today. Einstein took the wrong side in a few scientific debates. In addition to these well known mistakes, it is sometimes claimed that the general line of Einstein’s reasoning in the 1905 relativity paper is flawed, or the photon paper, or one or another of the most famous papers. None of these claims are widely accepted.
Einstein and De Haas demonstrated that magnetization is due to the motion of electrons, nowadays known to be the spin. In order to show this, they reversed the magnetization in an iron bar suspended on a torsion pendulum. They confirmed that this leads the bar to rotate, because the electron’s angular momentum changes as the magnetization changes. This experiment needed to be sensitive, because the angular momentum associated with electrons is small, but it definitively established that electron motion of some kind is responsible for magnetization.
Einstein suggested to Erwin Schrödinger that he might be able to reproduce the statistics of a Bose–Einstein gas by considering a box. Then to each possible quantum motion of a particle in a box associate an independent harmonic oscillator. Quantizing these oscillators, each level will have an integer occupation number, which will be the number of particles in it. This formulation is a form of second quantization, but it predates modern quantum mechanics. Erwin Schrödinger applied this to derive the thermodynamic properties of a semiclassical ideal gas. Schrödinger urged Einstein to add his name as co-author, although Einstein declined the invitation.
In 1926, Einstein and his former student Leó Szilárd co-invented (and in 1930, patented) the Einstein refrigerator. This Absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input. On 11 November 1930, was awarded to Albert Einstein and Leó Szilárd for the refrigerator. Their invention was not immediately put into commercial production, as the most promising of their patents were quickly bought up by the Swedish company Electrolux to protect its refrigeration technology from competition.
In the 1920s, quantum mechanics developed into a more complete theory. Einstein was unhappy with the Copenhagen interpretation of quantum theory developed by Niels Bohr and Werner Heisenberg. In this interpretation, quantum phenomena are inherently probabilistic, with definite states resulting only upon interaction with classical systems. A public debate between Einstein and Bohr followed, lasting on and off for many years (including during the Solvay Conferences). Einstein formulated thought experiments against the Copenhagen interpretation, which were all rebutted by Bohr. In a 1926 letter to Max Born, Einstein wrote: "I, at any rate, am convinced that He [God] does not throw dice." Einstein was never satisfied by what he perceived to be quantum theory’s intrinsically incomplete description of nature, and in 1935 he further explored the issue in collaboration with Boris Podolsky and Nathan Rosen, noting that the theory seems to require non-local interactions; this is known as the EPR paradox. The EPR experiment has since been performed, with results confirming quantum theory’s predictions. Repercussions of the Einstein–Bohr debate have found their way into philosophical discourse.
The question of scientific determinism gave rise to questions about Einstein’s position on theological determinism, and whether or not he believed in God, or in a god. In 1929, Einstein told Rabbi Herbert S. Goldstein "I believe in Spinoza’s God, who reveals Himself in the lawful harmony of the world, not in a God Who concerns Himself with the fate and the doings of mankind." In a 1954 letter, he wrote, "I do not believe in a personal God and I have never denied this but have expressed it clearly.” In a letter to philosopher Erik Gutkind, Einstein remarked, "The word God is for me nothing more than the expression and product of human weakness, the Bible a collection of honorable, but still purely primitive, legends which are nevertheless pretty childish."
Throughout the November Revolution in Germany Einstein signed an appeal for the foundation of a nationwide liberal and democratic party, which was published in the Berliner Tageblatt on 16 November 1918, and became a member of the German Democratic Party. Einstein flouted the ascendant Nazi movement, tried to be a voice of moderation in the tumultuous formation of the State of Israel and braved anti-communist politics and resistance to the civil rights movement in the United States. He participated in the 1927 congress of the League against Imperialism in Brussels. He was a socialist Zionist who supported the creation of a Jewish national homeland in the British mandate of Palestine. After World War II, as enmity between the former allies became a serious issue, Einstein wrote, “I do not know how the third World War will be fought, but I can tell you what they will use in the Fourth – rocks!” In a 1949 "Monthly Review" article entitled “Why Socialism?” Albert Einstein described a chaotic capitalist society, a source of evil to be overcome, as the “predatory phase of human development”. With Albert Schweitzer and Bertrand Russell, Einstein lobbied to stop nuclear testing and future bombs. Days before his death, Einstein signed the Russell–Einstein Manifesto, which led to the Pugwash Conferences on Science and World Affairs. Einstein was a member of several civil rights groups, including the Princeton chapter of the NAACP. When the aged W. E. B. Du Bois was accused of being a Communist spy, Einstein volunteered as a character witness, and the case was dismissed shortly afterward. Einstein’s friendship with activist Paul Robeson, with whom he served as co-chair of the American Crusade to End Lynching, lasted twenty years.
While travelling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse. The letters were included in the papers bequeathed to The Hebrew University. Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986). Barbara Wolff, of The Hebrew University’s Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955. Einstein bequeathed the royalties from use of his image to The Hebrew University of Jerusalem. Corbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.
In the period before World War II, Albert Einstein was so well-known in America that he would be stopped on the street by people wanting him to explain "that theory." He finally figured out a way to handle the incessant inquiries. He told his inquirers "Pardon me, sorry! Always I am mistaken for Professor Einstein." Albert Einstein has been the subject of or inspiration for many novels, films, and plays. Einstein is a favorite model for depictions of mad scientists and absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. "Time" magazine’s Frederic Golden wrote that Einstein was "a cartoonist’s dream come true." Einstein’s association with great intelligence and originality has made the name "Einstein" synonymous with genius.
In 1922, Einstein was awarded the 1921 Nobel Prize in Physics, "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect". This refers to his 1905 paper on the photoelectric effect, "On a Heuristic Viewpoint Concerning the Production and Transformation of Light", which was well supported by the experimental evidence by that time. The presentation speech began by mentioning "his theory of relativity [which had] been the subject of lively debate in philosophical circles [and] also has astrophysical implications which are being rigorously examined at the present time." It was long reported that Einstein gave the Nobel prize money directly to his first wife, Mileva Marić, in compliance with their 1919 divorce settlement. However, personal correspondence made public in 2006 shows that he invested much of it in the United States, and saw much of it wiped out in the Great Depression. Einstein traveled to New York City in the United States for the first time on 2 April 1921. When asked where he got his scientific ideas, Einstein explained that he believed scientific work best proceeds from an examination of physical reality and a search for underlying axioms, with consistent explanations that apply in all instances and avoid contradicting each other. He also recommended theories with visualizable results. In 1999, Albert Einstein was named Person of the Century by "Time" magazine.
The Islamic Republic of Afghanistan is a landlocked country in South-Central Asia. It is variously described as being located within Central Asia, South Asia, Western Asia, or the Middle East. It is bordered by Iran in the west, Pakistan in the south and east, Turkmenistan, Uzbekistan and Tajikistan in the north, and China in the far northeast. Afghanistan has a long history, and has been an ancient focal point of the Silk Road and migration. It is an important geostrategic location, connecting East and West Asia or the Middle East. The land has been a target of various invaders, as well as a source from which local powers invaded neighboring regions to form their own empires. Ahmad Shah Durrani created the Durrani Empire in 1747, which is considered the beginning of modern Afghanistan. Its capital was shifted in 1776 from Kandahar to Kabul and most of its territories ceded to neighboring empires. In the late 19th century, Afghanistan became a buffer state in "The Great Game" played between the British Empire and Russian Empire. On August 19, 1919, following the third Anglo-Afghan war, the country regained independence from the United Kingdom over its foreign affairs. Since the late 1970s Afghanistan has experienced a continuous state of civil war punctuated by foreign occupations in the forms of the 1979 Soviet invasion and the October 2001 US-led invasion that overthrew the Taliban government. In December 2001, the United Nations Security Council authorized the creation of an International Security Assistance Force (ISAF) to help maintain security and assist the Karzai administration. The country is being rebuilt slowly with support from the international community and dealing with a strong Taliban insurgency.
The first part of the name, "Afghan", is an alternative name for the Pashtuns who are the founders and the largest ethnic group of the country. They probably began using the term "Afghan" as a name for themselves since at least the Islamic period and onwards. According to W. K. Frazier Tyler, M. C. Gillet and several other scholars "the word Afghan first appears in history in the Ḥudūd al-ʿĀlam in 982 AD." Al-Biruni referred to Afghans as various tribes living on the western frontier mountains of the Indus River, which would be the Sulaiman Mountains. The last part of the name, "-stān" is an ancient Iranian languages suffix for "place", prominent in many languages of the region. The term "Afghanistan", meaning the "Land of Afghans", was mentioned by the 16th century Mughal Emperor Babur in his memoirs, referring to the territories south of Kabul that were inhabited by Pashtuns (called "Afghans" by Babur). Until the 19th century the name was only used for the traditional lands of the Pashtuns, while the kingdom as a whole was known as the "Kingdom of Kabul", as mentioned by the British statesman and historian Mountstuart Elphinstone. Other parts of the country were at certain periods recognized as independent kingdoms, such as the "Kingdom of Balkh" in the late 18th and early 19th centuries. With the expansion and centralization of the country, Afghan authorities adopted and extended the name "Afghanistan" to the entire kingdom, after its English translation had already appeared in various treaties between the British Raj and Qajarid Persia, referring to the lands subject to the Pashtun Barakzai Dynasty of Kabul. "Afghanistan" as the name for the entire kingdom was mentioned in 1857 by Friedrich Engels. It became the official name when the country was recognized by the world community in 1919, after regaining full independence over its foreign affairs from the British, and was confirmed as such in the nation's 1923 constitution.
Afghanistan is landlocked and mountainous, with plains in the north and southwest. The highest point is Nowshak, at 7,485 m (24,557 ft) above sea level. The climate varies by region and tends to change quite rapidly. Large parts of the country are dry, and fresh water supplies are limited. The endorheic Sistan Basin is one of the driest regions in the world. Afghanistan has a continental climate with very harsh winters in the central highlands, the glacierized northeast (around Nuristan) and the Wakhan Corridor, where the average temperature in January is below −15°C, and hot summers in the low-lying areas of Sistan Basin of the southwest, the Jalalabad basin of the east, and the Turkistan plains along the Amu River of the north, where temperature averages over 35°C in July. The country is frequently subject to minor earthquakes, mainly in the northeast of Hindu Kush mountain areas. Some 125 villages were damaged and 4000 people killed by the May 31, 1998 earthquake. At 249,984 sq mi (647,500 km²), Afghanistan is the world's 41st-largest country (after Burma). Tajikistan, Turkmenistan and Uzbekistan border Afghanistan to the north, Iran to the west, Pakistan to the south and the People's Republic of China to the east. The country's natural resources include gold, silver, copper, zinc, and iron ore in the Southeast; precious and semi-precious stones (such as lapis, emerald, and azure) in the Northeast; and potentially significant petroleum and natural gas reserves in the North. The country also has uranium, coal, chromite, talc, barites, sulfur, lead, and salt. However, these significant mineral and energy resources remain largely untapped, due to the effects of the Soviet invasion and the subsequent civil war. Plans are under way to begin extracting them in the near future.
Though the modern nation state of Afghanistan was founded or created in 1747 by Ahmad Shah Durrani, the land has an ancient history and various timelines of different civilizations. Excavation of prehistoric sites by Louis Dupree, the University of Pennsylvania, the Smithsonian Institution and others suggest that humans were living in what is now Afghanistan at least 50,000 years ago, and that farming communities of the area were among the earliest in the world. Afghanistan is a country at a unique nexus point where numerous Indo-European civilizations have interacted and often fought, and was an important site of early historical activity. The region has been home to various people through the ages, among them the Aryan tribes, such as the Pactyans, Arians, Scythians, Bactrians, and etc. It also has been conquered by a host of people, including the Medes, Achaemenids, Alexander the Great, Seleucids, Indo-Greeks, Samanids, Arabs, Turks, Mongols, and others. On the other hand, native entities such as Kushans, Saffarids, Ghaznavids, Ghurids, Timurids, Mughals, Hotakis, Durranis and others have risin to power in what is now Afghanistan and invaded the surrounding regions to form empires of their own.
After 2000 BC, waves of Indo-European-speaking Aryans from Central Asia moved south into the area of Afghanistan. These Indo-Iranians later migrated further south to India, west to what is now Iran, and towards Europe via north of the Caspian. They set up a nation which became known as Airyānem Vāejah. During the rule of the Parthian, Sasanian and later, it was called Erānshahr (– "Īrānšahr") meaning "Dominion of the Aryans". The ancient Zoroastrianism religion is believed to have originated in what is now Afghanistan between 1800 to 800 BC, as Zoroaster lived and died in Balkh. Ancient Eastern Iranian languages, such as Avestan, may have been spoken in the region around the time of the rise of Zoroastrianism. By the middle of the sixth century BC, the Achaemenid Persian Empire overthrew the Medes and incorporated Afghanistan (known as Arachosia Aria, and Bactria to the Greeks) within its boundaries. Alexander the Great entered and conquered Afghanistan in 330 BCE. Following Alexander's brief occupation, the successor state of the Seleucid Empire controlled the area until 305 BCE, when they gave most of the area to the Hindu Maurya Empire as part of an alliance treaty. During the Mauryan rule, Hinduism and Buddhism was widely practiced in the area. The Mauryans were overthrown in about 185 BCE, leading to the Hellenistic reconquest of Afghanistan by the Greco-Bactrians by 180 BCE. Much of Afghanistan soon broke away from the Greco-Bactrians and became part of the Indo-Greek Kingdom. The Indo-Greeks were defeated by the Indo-Scythians and expelled from most of Afghanistan by the end of the 2nd century BCE. During the first century, the Parthian Empire subjugated Afghanistan, but lost it to their Indo-Parthian vassals. In the mid to late 1st century AD the vast Kushan Empire, centered in modern Afghanistan, became great patrons of Buddhist culture. The Kushans were defeated by the Sassanids in the third century. Although various rulers calling themselves Kushanshas (generally known as Indo-Sassanids) continued to rule at least parts of the region, they were probably more or less subject to the Sassanids. The late Kushans were followed by the Kidarite Huns who, in turn, were replaced by the short-lived but powerful Hephthalites, as rulers of the region in the first half of the fifth century. The Hephthalites were defeated by the Sasanian king Khosrau I in AD 557, who re-established Sassanid power in Persia. However, the successors of Kushans and Hepthalites established a small dynasty in Kabulistan called Kushano-Hephthalites or Kabul-Shahan, who were defeated by the Muslim Arab armies in the 7th century and conquered later by the Ghaznavids. Islamic conquests and Mongol invasion. In the Middle Ages, upto the 19th century, part of Afghanistan was known as Khorasan. Several important centers of Khorāsān are thus located in modern Afghanistan, such as Balkh, Herat, Ghazni and Kabul. It was during the 7th century to the 9th century that Islam was introduced and spread in the area. Prior to the arrival of Islam, the area was inhabited by people of multi-religions, which included Zoroastrians, Hindus, Buddhists, possibly Jews, shamanists and others. The region of Afghanistan became the center of various important empires, including that of the Samanids (875–999), Ghaznavids (977–1187), Seljukids (1037–1194), Ghurids (1149–1212), Ilkhanate (1225–1335), and Timurids (1370–1506). Among them, the periods of the Ghaznavids and Timurids are considered as some of the most brilliant eras of the region's history. Afghanistan was overrun in 1219 by Genghis Khan and his Mongols army, who devastated much of the land. For example, his troops are said to have exterminated or annihilated all living creatures in the ancient Khorāsānian cities of Herat and Balkh. The destruction caused by the Mongols depopulated major cities and caused much of the population to revert to an agrarian rural society. Their rule continued with the Ilkhanate, and was extended further following the invasion of Timur (Tamerlane). In 1504, Babur, a descendant of both Timur and Genghis Khan, established the Mughal Empire with its capital at Kabul. By the early 1700s, Afghanistan was controlled by several ruling groups: Uzbeks to the north, Safavid Persians to the west and the remaining larger area by the Mughals or self-ruled by local Afghan tribes. Some Urdu-speaking Muhajir and Indian Muslims claim descent from Pashtun soldiers who settled in India and married local Muslim women during the Muslim conquest in the Indian subcontinent. Notably, the Rohilla Pashtuns are known to have settled in parts of northern India.
In 1709, Mir Wais Hotak, a local Afghan (Pashtun) from the Ghilzai clan, overthrew and killed Gurgin Khan, the Safavid governor of Kandahar. Mir Wais successfully defeated a Safavid army sent for retaliation and held the region of Kandahar until his death in 1715. He was succeeded by his son Mir Mahmud Hotaki. In 1722, Mir Mahmud led an Afghan army to Isfahan (Iran), sacked the city and proclaimed himself King of Persia. However, the great majority still rejected the Afghan regime as usurping, and after the massacre of thousands of civilians in Isfahan by the Afghans – including more than three thousand religious scholars, nobles, and members of the Safavid family – the Hotaki dynasty was eventually removed from power by a new ruler, Nadir Shah of Persia. Durrani Empire: beginning of the Afghan state. In 1738, Nadir Shah and his army, which included Ahmad Khan and four thousand of his Pashtun soldiers of the Abdali tribe, conquered the region of Kandahar from the Hotak Ghilzais; in the same year he occupied Ghazni, Kabul and Lahore. On June 19, 1747, Nadir Shah was assassinated by the Persians and Ahmad Shah Abdali called for a loya jirga ("grand assembly") to select a leader among his people. The Afghans gathered near Kandahar in October 1747 and chose him as their new head of state. Ahmad Shah Durrani is often regarded as the founder of modern Afghanistan. After the inauguration, Ahmad Shah adopted the title "padshah durr-i dawran" ('King, "pearl of the age") and the Abdali tribe became known as the Durrani tribe there after. By 1751, Ahmad Shah Durrani and his Afghan army conquered the entire present-day Afghanistan, Pakistan, Khorasan and Kohistan provinces of Iran, along with Delhi in India. He defeated the Sikhs of the Maratha Empire in the Punjab region nine times, one of the biggest battles was the 1761 Battle of Panipat. In October 1772, Ahmad Shah retired to his home in Kandahar where he died peacefully and was buried there at a site that is now adjacent to the Mosque of the Cloak of the Prophet Mohammed. He was succeeded by his son, Timur Shah Durrani, who transferred the capital of their Afghan Empire from Kandahar to Kabul. Timur died in 1793 and was finally succeeded by his son Zaman Shah Durrani. Zaman Shah and his brothers had a weak hold on the legacy left to them by their famous ancestor. They sorted out their differences through a "round robin of expulsions, blindings and executions", which resulted in the deterioration of the Afghan hold over far-flung territories, such as Attock and Kashmir. Durrani's other grandson, Shuja Shah Durrani, fled the wrath of his brother and sought refuge with the Sikhs. Not only had Durrani and his Afghans invaded the Punjab region many times, but have destroyed the holiest shrine of the Sikhs – the Golden Temple in Amritsar, defiling its "sarowar" with the blood of cows and then killing Baba Deep Singh in 1757. The Sikhs, under Ranjit Singh, rebelled in 1809 and eventually wrest a large part of the Kingdom of Kabul (present day Pakistan, but not including Sindh) from the Afghans. Hari Singh Nalwa, the Commander-in-Chief of the Sikh Empire along its Afghan frontier, invaded the Afghan territory as far as the city of Jalalabad. In 1837, the Afghan Army descended through the Khyber Pass on Sikh forces at Jamrud. Hari Singh Nalwa's forces held off the Afghan offensive for over a week – the time it took reinforcements to reach Jamrud from Lahore.
During the nineteenth century, following the Anglo-Afghan wars (fought 1839–42, 1878–80, and lastly in 1919) and the ascension of the Barakzai dynasty, Afghanistan saw much of its territory and autonomy ceded to the United Kingdom. The UK exercised a great deal of influence, and it was not until King Amanullah Khan acceded to the throne in 1919 that Afghanistan re-gained complete independence over its foreign affairs (see "The Great Game"). During the period of British intervention in Afghanistan, ethnic Pashtun territories were divided by the Durand Line. This would lead to strained relations between Afghanistan and British India – and later the new state of Pakistan – over what came to be known as the Pashtunistan debate.
King Amanullah Khan moved to end his country's traditional isolation in the years following the Third Anglo-Afghan War. He established diplomatic relations with most major countries and, following a 1927 tour of Europe and Turkey (during which he noted the modernization and secularization advanced by Atatürk), introduced several reforms intended to modernize Afghanistan. A key force behind these reforms was Mahmud Tarzi, Amanullah's Foreign Minister and father-in-law – and an ardent supporter of the education of women. He fought for Article 68 of Afghanistan's first constitution (declared through a Loya Jirga), which made elementary education compulsory. Some of the reforms that were actually put in place, such as the abolition of the traditional Muslim veil for women and the opening of a number of co-educational schools, quickly alienated many tribal and religious leaders. Faced with overwhelming armed opposition, Amanullah was forced to abdicate in January 1929 after Kabul fell to forces led by Habibullah Kalakani. Prince Mohammed Nadir Shah, a cousin of Amanullah's, in turn defeated and killed Habibullah Kalakani in October of the same year, and with considerable Pashtun tribal support he was declared King Nadir Shah. He began consolidating power and regenerating the country. He abandoned the reforms of Amanullah Khan in favour of a more gradual approach to modernisation. In 1933, however, he was assassinated in a revenge killing by a Kabul student. Mohammed Zahir Shah, Nadir Shah's 19-year-old son, succeeded to the throne and reigned from 1933 to 1973. The longest period of stability in Afghanistan was when the country was under the rule of King Zahir Shah. Until 1946 Zahir Shah ruled with the assistance of his uncle, who held the post of Prime Minister and continued the policies of Nadir Shah. In 1946, another of Zahir Shah's uncles, Shah Mahmud Khan, became Prime Minister and began an experiment allowing greater political freedom, but reversed the policy when it went further than he expected. In 1953, he was replaced as Prime Minister by Mohammed Daoud Khan, the king's cousin and brother-in-law. Daoud sought a closer relationship with the Soviet Union and a more distant one towards Pakistan. During this period Afghanistan remained neutral. It was not a participant in World War II, nor aligned with either power bloc in the Cold War. However, it was a beneficiary of the latter rivalry as both the Soviet Union and the U.S. vied for influence by building such works as hotels and sewer systems. A good two lane road was constructed from Iran. Running through Herat, Kandahar, and Kabul, it ended at the Pakistani border. By the late 1960s large numbers of travelers were using it as part of the hippie trail.
In 1973, Zahir Shah's brother-in-law, Mohammed Daoud Khan, launched a bloodless coup and became the first President of Afghanistan while Zahir Shah was on an official overseas visit. Mohammed Daoud Khan jammed Afghan radio with anti-Pakistani broadcasts and looked to the Soviet Union and the United States for aid for development. In 1978 a prominent member of the People's Democratic Party of Afghanistan (PDPA), Mir Akbar Khyber (or "Kaibar"), was killed by the government. The leaders of PDPA apparently feared that Daoud was planning to exterminate them all, especially since most of them were arrested by the government shortly after. Hafizullah Amin and a number of military wing officers of the PDPA managed to remain at large and organised an uprising. The PDPA, led by Nur Mohammad Taraki, Babrak Karmal and Amin overthrew the regime of Mohammad Daoud, who was killed along with his family. The uprising was known as the Khalq, or Great Saur Revolution ('Saur' means 'April' in Pashto). On May 1, 1978, Taraki became President, Prime Minister and General Secretary of the PDPA. The country was then renamed the Democratic Republic of Afghanistan (DRA), and the PDPA regime lasted, in some form or another, until April 1992. The 1978 Khalq uprising against the government of Daoud Khan was essentially a resurgence by the Ghilzai tribe of the Pashtun against the Durrani (the tribe of Daoud Khan and the previous monarchy). Once in power, the PDPA moved to permit freedom of religion and carried out an ambitious land reform, waiving farmers' debts countrywide. They also made a number of statements on women's rights and introduced women to political life. A prominent example was Anahita Ratebzad, who was a major Marxist leader and a member of the Revolutionary Council. Ratebzad wrote the famous May 28, 1978 "New Kabul Times" editorial which declared: "Privileges which women, by right, must have are equal education, job security, health services, and free time to rear a healthy generation for building the future of the country... Educating and enlightening women is now the subject of close government attention." Many people in the cities including Kabul either welcomed or were ambivalent to these policies. However, the secular nature of the government made it unpopular with religiously conservative Afghans in the villages and the countryside, who favoured traditionalist 'Islamic' law. The U.S. saw the situation as a prime opportunity to weaken the Soviet Union. As part of a Cold War strategy, in 1979 the United States government (under President Jimmy Carter) began to covertly fund forces ranged against the pro-Soviet government, although warned that this might prompt a Soviet intervention, according to President Carter's National Security Advisor, Zbigniew Brzezinski. Brzezinski described the U.S. activities as the successful setting of a trap that drew the Soviet Union into "its Vietnam War" and brought about the breakup of the Soviet empire. Regarding U.S. support for Islamic fundamentalism, Brzezinski said, "What is most important to the history of the world? The Taliban or the collapse of the Soviet empire? Some stirred-up Moslems or the liberation of Central Europe and the end of the cold war?" The Mujahideen belonged to various different factions, but all shared, to varying degrees, a similarly conservative 'Islamic' ideology. In March 1979 Hafizullah Amin took over as prime minister, retaining the position of field marshal and becoming vice-president of the Supreme Defence Council. Taraki remained President and in control of the Army. On September 14, Amin overthrew Taraki, who died or was killed. Amin's tenure as prime minister lasted only a few months. Soviet invasion and civil war. In order to bolster the Parcham faction, the Soviet Union – citing the 1978 Treaty of Friendship, Cooperation and Good Neighborliness that had been signed between the two countries – intervened on December 24, 1979. Over 100,000 Soviet troops took part in the invasion backed by another one hundred thousand and by members of the Parcham faction. Amin was killed and replaced by Babrak Karmal. In response to the Soviet occupation of Afghanistan and part of its overall Cold War strategy, the United States responded by arming and otherwise supporting the Afghan mujahideen, which had taken up arms against the Soviet occupiers. U.S. support began during the Carter administration, but increased substantially during the Reagan administration, in which it became a centerpiece of the so-called Reagan Doctrine under which the U.S. provided support to anti-communist resistance movements in Afghanistan and also in Angola, Nicaragua, and other nations. The New York Times reported that the Reagan administration delivered several hundred FIM-92 Stinger surface-to-air missiles to Afghan resistance groups, including the Taliban. In addition to U.S. support, the mujahideen received support from Pakistan, Saudi Arabia and other nations. The Soviet occupation resulted in the killings of between 600,000 and two million Afghan civilians. Over 5million fled as Afghan refugees, mostly to Pakistan and Iran. Over 38,000 made it to the United States and many more to the European Union. Faced with mounting international pressure and great number of casualties on both sides, the Soviets withdrew in 1989. The Soviet withdrawal from the DRA was seen as an ideological victory in the U.S., which had backed the Mujahideen through three U.S. presidential administrations in order to counter Soviet influence in the vicinity of the oil-rich Persian Gulf. Following the removal of the Soviet forces, the U.S. and its allies lost interest in Afghanistan and did little to help rebuild the war-ravaged country or influence events there. The USSR continued to support President Mohammad Najibullah (former head of the Afghan secret service, "KHAD") until 1992 when the new Russian government refused to sell oil products to the Najibullah regime. Because of the fighting, a number of elites and intellectuals fled to take refuge abroad. This led to a leadership imbalance in Afghanistan. Fighting continued among the victorious Mujahideen factions, which gave rise to a state of warlordism. The most serious fighting during this period occurred in 1994, when over 10,000 people were killed in Kabul alone. It was at this time that the Taliban developed as a politico-religious force, eventually seizing Kabul in 1996 and establishing the Islamic Emirate of Afghanistan. By the end of 2000 the Taliban had captured 95% of the country. During the Taliban's seven-year rule, much of the population experienced restrictions on their freedom and violations of their human rights. Women were banned from jobs, girls forbidden to attend schools or universities. Communists were systematically eradicated and thieves were punished by amputating one of their hands or feet. Opium production was nearly wiped out by the Taliban by 2001.
Following the September 11, 2001 attacks in the United States, the U.S. and British air forces began bombing Afghanistan during Operation Enduring Freedom. On the ground, American and British special forces along with CIA Special Activities Division teams worked with the Tajik-dominated Northern Alliance to begin a military offensive to overthrow the Taliban. These attacks led to the fall of Mazar-i-Sharif and then Kabul in November 2001, as the Taliban retreated from most of northern Afghanistan. The International Security Assistance Force (ISAF) was established by the UN Security Council in December 2001 to secure Kabul and the surrounding areas. In the same month the Karzai administration was also established to run the country. As more coalition troops entered the war and the Northern Alliance forces fought their way southwards, the Taliban and al-Qaida retreated toward the mountainous Durand Line border region between Afghanistan and Pakistan. From 2002 onward, the Taliban focused on survival and on rebuilding its forces. Meanwhile NATO assumed control of ISAF in 2003. From 2003 onwards, the Taliban increased its attacks using insurgency tactics. Firmly entrenched in the borders between Pakistan and Afghanistan the Taliban enjoyed a resurgence, showing it could launch large, coordinated and effective attacks on coalition and Afghan forces. Over the course of the years, NATO-lead troops lead several offensives against the entrenched Taliban, but proved unable to completely dislodge their presence. By 2009, a Taliban lead shadow government began to form complete with their own verson of mediation court. On December 1, 2009, U.S. President Barack Obama announced that he would escalate U.S. military involvement by deploying an additional 30,000 soldiers over a period of six months. He also proposed to begin troop withdrawals 18 months from that date. On January 26, 2010, at the International Conference on Afghanistan in London which brought together some 70 countries and organizations, Afghan President Hamid Karzai told world leaders that he intends to reach out to the top echelons of the Taliban within a few weeks with a peace initiative. Karzai set the framework for dialogue with Taliban leaders when he called on the group's leadership to take part in a "loya jirga" -- or large assembly of elders—to initiate peace talks.
Politics in Afghanistan has historically consisted of power struggles, bloody coups and unstable transfers of power. With the exception of a military junta, the country has been governed by nearly every system of government over the past century, including a monarchy, republic, theocracy and communist state. The constitution ratified by the 2003 Loya jirga restructured the government as an Islamic republic consisting of three branches, executive, legislative and judicial. The nation is currently led by the Karzai administration with Hamid Karzai as the President and leader since December 20, 2001. The current parliament was elected in 2005. Among the elected officials were former mujahadeen, Taliban members, communists, reformists, and Islamic fundamentalists. 28% of the delegates elected were women, three points more than the 25% minimum guaranteed under the constitution. This made Afghanistan, long known under the Taliban for its oppression of women, 30th amongst nations in terms of female representation. Construction for a new parliament building began on August 29, 2005. The Supreme Court of Afghanistan is currently led by Chief Justice Abdul Salam Azimi, a former university professor who had been legal advisor to the president. The previous court, appointed during the time of the interim government, had been dominated by fundamentalist religious figures, including Chief Justice Faisal Ahmad Shinwari. The court issued several rulings, such as banning cable television, seeking to ban a candidate in the 2004 presidential election and limiting the rights of women, as well as overstepping its constitutional authority by issuing rulings on subjects not yet brought before the court. The current court is seen as more moderate and led by more technocrats than the previous court. The 2004 Afghan presidential election went relatively smooth in which Hamid Karzai won in the first round with 55.4% of the votes. However, the 2009 presidential election was characterized by lack of security, low voter turnout and widespread electoral fraud. The vote, along with elections for 420 provincial council seats, took place in August 2009, but remained unresolved during a lengthy period of vote counting and fraud investigation. Two months later, under U.S. and ally pressure, a second round run-off vote between Karzai and remaining challenger Abdullah was announced for November 7, 2009, but on the 1st of November Abdullah announced that he would no longer be participating in the run-off because his demands for changes in the electoral commission had not been met, and claiming a transparent election would not be possible. A day later, officials of the election commission cancelled the run-off and declared Hamid Karzai as President of Afghanistan for another 5 year term. Corruption is many Afghans’ chief grievance against their leaders, pervading nearly all aspects of daily life. A number of government ministries are believed to be rife with corruption, including Interior, Education and Health. They either tolerate widespread malfeasance or have been powerless to stop it. A January 2010 report published by the United Nations Office on Drugs and Crime revealed that bribery consumes an amount equal to 23 percent of the Gross Domestic Product (GDP) of Afghanistan. Afghans are forced by corrupt government culture to pay more than a third of their income in bribes. Women in public life in many parts of the country are subject to routine threats and intimidation, according to a December, 2009 report by Human Rights Watch. Several high profile women have been assassinated, but their killers have not been brought to justice. When Sitara Achakzai, an outspoken and courageous human rights defender and politician, was murdered by the Taliban in April 2009, her death was seen as another warning to all women who are active in public life. In the aftermath of the election, Peter Galbraith – a senior UN official in Kabul who was fired after pushing for the UN to reveal the extent of the preparation for fraud before the first vote – wrote that before the election, Karzai was seen as ineffectual and corrupt, and that now he was ineffectual, corrupt and illegitimate. Later that month, the U.S. ambassador in Kabul sent two classified cables to Washington expressing deep concerns about sending more U.S. troops to Afghanistan until President Hamid Karzai's government demonstrates that it is willing to tackle the corruption and mismanagement that has fueled the Taliban's rise. In November 2009, Afghanistan slipped three places in Transparency International's annual index of corruption perceptions, becoming the world's second most-corrupt country ahead of Somalia. In January 2010, President Karzai reinstated Abdul Rashid Dostum to a high ranking army post despite Western demands for sweeping reform. Dostum is among Afghanistan's most notorious warlords, accused of widespread abuses including the massacre of thousands of Taliban prisoners, something he denies.
Afghanistan currently has more than 90,000 national police officers, with plans to recruit more so that the total number can reach 160,000. They are being trained by and through the Afghanistan Police Program. In many areas, crimes have gone uninvestigated because of insufficient police or lack of equipment. Afghan National Army soldiers have been sent to quell fighting in some regions lacking police protection. Many of the police officers are illiterate due to the 30 years of civil unrest in the country. Approximately 17 percent of them test positive for illegal drugs. They are widely accused of demanding bribes, which is not surprising to see in most developing countries. Every year many Afghan police officers are killed by militants, and in some cases by NATO forces due to friendly fire incidents. See List of Afghan security forces fatality reports in Afghanistan Attempts to build a credible Afghan police force are faltering badly, according to NATO officials, even as they acknowledge that the force will be a crucial piece of the effort to have Afghans manage their own security so American forces can begin leaving. Taliban infiltration is a constant worry; incompetence an even bigger one. A quarter of the officers quit every year, making the Afghan government's goals of substantially building up the police force even harder to achieve. Helmand is the most dangerous place in Afghanistan due to its distance from Kabul as well as the drug trade that flourishes there. Other turbulent provinces in Afghanistan include Kandahar and Oruzgan, although security in the latter has improved recently due to Dutch and Afghan counter offensives. The Afghan Border Police are responsible for protecing the nation's borders, especially the Durand Line border which is often used by criminals and terrorists. Women and girls in Afghanistan suffer high levels of violence and discrimination and have poor access to justice and education, Human Rights Watch concluded in a December, 2009 report. One recent nationwide survey of levels of violence against Afghan women found that 52 percent of respondents experienced physical violence and 17 percent reported sexual violence. Yet because of social and legal obstacles to accessing justice, few women and girls report violence to the authorities. These barriers are particularly formidable in rape cases. The Afghan government rates 121 out of 160 countries in terms of corruption. In 2009, President Hamid Karzai created two anti-corruption units within the Afghan Interior Ministry at the insistence of the United States, Europe and Iran. Afghan Interior Minister Hanif Atmar told reporters in Kabul on November 16, 2009 that security officials from the U.S. (FBI), Britain (Scotland Yard) and the European Union (ELOPE) will train prosecutors in the unit.
The Afghan National Army currently has about 100,000 soldiers, with plans to increase this number to 260,000 in the coming years. It is plagued by inefficiency and endemic corruption. U.S. training efforts have been drastically slowed by the corruption, widespread illiteracy, vanishing supplies, and lack of discipline. U.S. trainers report missing vehicles, weapons and other military equipment, and outright theft of fuel provided by the U.S. Death threats have been leveled against U.S. officers who try to stop Afghan soldiers from stealing. Afghan soldiers often find improvised explosive devices and snip the command wires instead of marking them and waiting for U.S. forces to come to detonate them. The Americans say this just allows the insurgents to return and reconnect them. U.S. trainers frequently must remove the cell phones of Afghan soldiers hours before a mission for fear that the operation will be compromised. American trainers often spend large amounts of time verifying that Afghan rosters are accurate – that they are not padded with "ghosts" being "paid" by Afghan commanders who quietly collect the bogus wages. The Afghan Army has severely limited fighting capacity. Even the best Afghan units lack training, discipline and adequate reinforcements. In one new unit in Baghlan Province, soldiers have been found cowering in ditches rather than fighting. Some are suspected of collaborating with the Taliban against the Americans. "They don’t have the basics, so they lay down," said Capt. Michael Bell, who is one of a team of U.S. and Hungarian mentors tasked with training Afghan soldiers. "I ran around for an hour trying to get them to shoot, getting fired on. I couldn’t get them to shoot their weapons." In addition, 9 out of 10 soldiers in the Afghan National Army cannot read. In multiple firefights during the February, 2010 NATO offensive in Helmand Province, many Afghan soldiers did not aim — they pointed their American-issued M-16 rifles in the rough direction of the incoming small-arms fire and pulled their triggers without putting rifle sights to their eyes. Their rifle muzzles were often elevated several degrees high. Desertion is a significant problem in the Afghan Army. One in every four combat soldiers quit the Afghan Army during the 12-month period ending in September, 2009, according to data from the U.S. Defense Department and the Inspector General for Reconstruction in Afghanistan.
Afghanistan is administratively divided into thirty-four (34) provinces ("welayats"), and for each province there is a capital. Each province is then divided into many provincial districts, and each district normally covers a city or several townships. The Governor of the province is appointed by the Ministry of Interior, and the Prefects for the districts of the province will be appointed by the provincial Governor. The Governor is the representative of the central government of Afghanistan, and is responsible for all administrative and formal issues. The provincial Chief of Police is appointed by the Ministry of Interior, who works together with the Governor on law enforcement for all the cities or districts of that province. There is an exception in the capital city (Kabul) where the Mayor is selected by the President of Afghanistan, and is completely independent from the prefecture of Kabul Province.
Since the overthrow of the Taliban regime, Afghanistan's new government has maintained strong relations with the United States and other members of NATO. More than 22 NATO nations deploy thousands of troops in Afghanistan as a part of the International Security Assistance Force (ISAF). Apart from close military links, Afghanistan also enjoys strong economic relations with NATO members and other allies. The United States is the largest donor to Afghanistan, followed by Japan, United Kingdom, Germany and India. Relations between Afghanistan and neighboring Pakistan often fluctuate. During the Taliban regime, Pakistan had strong influence in Afghanistan due to close links with most Taliban leaders. However, Pakistan's influence has gradually waned since the overthrow of the Taliban. Though Pakistan maintains strong security and economic links with Afghanistan, dispute between the two countries remain due to Pakistani concerns over growing influence of rival India in Afghanistan and the continuing border dispute over the Durand Line. Since 2007, Afghan and Pakistani forces have been involved in a number of border skirmishes. Relations between the two strained further after Afghan officials alleged that Pakistani intelligence agencies were involved in some terrorist attacks on Afghanistan. Afghanistan has strong historical and cultural links with neighboring Iran as both the countries were a part of Greater Persia. Relations between the two, which had previously soured after the rise of radical Sunni Islamist Taliban regime in Afghanistan, rebounded after the establishment of Hamid Karzai government. Iran has also actively participated in Afghan reconstruction efforts. Afghanistan also enjoys good relations with Russia and neighboring Central Asian nations, especially Uzbekistan, Tajikistan and Turkmenistan. India is often regarded as one of Afghanistan's most influential allies. India is the largest regional donor to Afghanistan and has extensively participated in several Afghan reconstruction efforts, including power, agricultural and educational projects. Since 2002, India has extended more than US$1.2 billion in aid to Afghanistan. Strong military ties also exist – Afghan security forces regularly get counter-insurgency training in India and India is also considering the deployment of troops in Afghanistan.
Other minor languages include Nuristani (Ashkunu, Kamkata-viri, Vasi-vari, Tregami and Kalasha-ala), Pamiri (Shughni, Munji, Ishkashimi and Wakhi), Brahui, Hindko, Kyrgyz, etc. According to older numbers in the Encyclopædia Iranica, the Persian language is the most widely used language of the country, spoken by most of the population (although ca. 25% native), while Pashto is spoken and understood by around 60% of the population (50–55% native). According to "A survey of the Afghan people – Afghanistan in 2006", Persian is the first language of 49% of the population, while additional 37% speak the language as a second language (combined 86%). Pashto is the first language of 40% of the population, while additional 27% know the language (combined 67%). Uzbek is spoken or understood by 6% of the population, Turkmen by 3%. In the survey "Afghanistan: Where Things Stand" (average numbers from 2005 to 2009), 69% of the interviewed people preferred Persian, while 31% preferred Pashto. Additionally, 45% of the polled people said that they can read Persian, while 36% said that they can read Pashto.
Afghans display pride in their religion, country, ancestry, and above all, their independence. Like other highlanders, Afghans are regarded with mingled apprehension and condescension, for their high regard for personal honor, for their clan loyalty and for their readiness to carry and use arms to settle disputes. As clan warfare and internecine feuding has been one of their chief occupations since time immemorial, this individualistic trait has made it difficult for foreign invaders to hold the region. Afghanistan has a complex history that has survived either in its current cultures or in the form of various languages and monuments. However, many of the country's historic monuments have been damaged in recent wars. The two famous statues of Buddha in Bamyan Province were destroyed by the Taliban, who regarded them as idolatrous. Other famous sites include the cities of Kandahar, Herat, Ghazni and Balkh. The Minaret of Jam, in the Hari River valley, is a UNESCO World Heritage site. A cloak reputedly worn by Muhammad is stored inside the famous Mosque of the Cloak of the Prophet Mohammed in Kandahar City. Buzkashi is a national sport in Afghanistan. It is similar to polo and played by horsemen in two teams, each trying to grab and hold a goat carcass. Afghan hounds (a type of running dog) also originated in Afghanistan. Although literacy levels are very low, classic Persian poetry plays a very important role in the Afghan culture. Poetry has always been one of the major educational pillars in Iran and Afghanistan, to the level that it has integrated itself into culture. Persian culture has, and continues to, exert a great influence over Afghan culture. Private poetry competition events known as "musha’era" are quite common even among ordinary people. Almost every homeowner owns one or more poetry collections of some sort, even if they are not read often. The eastern dialects of the Persian language are popularly known as "Dari". The name itself derives from "Pārsī-e Darbārī", meaning "Persian of the royal courts". The ancient term "Darī" – one of the original names of the Persian language – was revived in the Afghan constitution of 1964, and was intended "to signify that Afghans consider their country the cradle of the language. Hence, the name "Fārsī", the language of Fārs, is strictly avoided." Many of the famous Persian poets of the tenth to fifteenth centuries stem from Khorasan where is now known as Afghanistan. They were mostly also scholars in many disciplines like languages, natural sciences, medicine, religion and astronomy. Most of these individuals were of Persian (Tājīk) ethnicity who still form the second-largest ethnic group in Afghanistan. Also, some of the contemporary Persian language poets and writers, who are relatively well-known in Persian-speaking world, include Khalilullah Khalili, Sufi Ghulam Nabi Ashqari, Sarwar Joya, Parwin Pazwak and others. In 2003, Khaled Hosseini published The Kite Runner which though fiction, captured much of the history, politics and culture experienced in Afghanistan from the 1930s to present day. In addition to poets and authors, numerous Persian scientists were born or worked in the region of present-day Afghanistan. Most notable was Avicenna (Abu Alī Hussein ibn Sīnā) whose father hailed from Balkh. Ibn Sīnā, who travelled to Isfahan later in life to establish a medical school there, is known by some scholars as "the father of modern medicine". George Sarton called ibn Sīnā "the most famous scientist of Islam and one of the most famous of all races, places, and times." His most famous works are "The Book of Healing" and "The Canon of Medicine", also known as the Qanun. Ibn Sīnā's story even found way to the contemporary English literature through Noah Gordon's "The Physician", now published in many languages. Moreover, according to Ibn al-Nadim, Al-Farabi, a well-known philosopher and scientist, was from Faryab Province in Afghanistan. Before the Taliban gained power, the city of Kabul was home to many musicians who were masters of both traditional and modern Afghan music, especially during the Nauroz-celebration. Kabul in the middle part of the twentieth century has been likened to Vienna during the eighteenth and nineteenth centuries. There are an estimated 60 major Pashtun tribes. The tribal system, which orders the life of most people outside metropolitan areas, is potent in political terms. Men feel a fierce loyalty to their own tribe, such that, if called upon, they would assemble in arms under the tribal chiefs and local clan leaders. In theory, under Islamic law, every believer has an obligation to bear arms at the ruler's call. Heathcote considers the tribal system to be the best way of organizing large groups of people in a country that is geographically difficult, and in a society that, from a materialistic point of view, has an uncomplicated lifestyle. The population of nomads in Afghanistan is estimated at about 2-3 million. Nomads contribute importantly to the national economy in terms of meat, skins and wool.
Religiously, Afghans are over 99% Muslims: approximately 80% Sunni, 19% Shi'a, and 1% other. Until the 1890s, the region around Nuristan was known as Kafiristan (land of the kafirs) because of its inhabitants: the Nuristani, an ethnically distinctive people who practiced animism, polytheism and shamanism. Up until the mid-1980s, there were possibly about 50,000 Hindus and Sikhs living in different cities, mostly in Kabul, Kandahar, Jalalabad, and Ghazni. There was also a small Jewish community in Afghanistan who immigrated to Israel and the United States by the end of the last century, and only one individual, Zablon Simintov, remains today.
Afghanistan is a member of the South Asian Association for Regional Cooperation (SAARC), Economic Cooperation Organization (ECO) and the Organization of the Islamic Conference (OIC). It is an impoverished country, one of the world's poorest and least developed. In 2010, 40% of Afghans live below the poverty ine. Two-thirds of the population lives on fewer than 2 US dollars a day. Its economy has suffered greatly from the 1979 Soviet invasion and subsequent conflicts, while severe drought added to the nation's difficulties in 1998–2001. According to the World Bank, "economic growth has been strong and has generated better livelihoods" since 2001. The economically active population in 2002 was about 11 million (out of a total of an estimated 29 million). As of 2005, the official unemployment rate is at 40%. The number of non-skilled young people is estimated at 3 million, which is likely to increase by some 300,000 per annum. The nation's economy began to improve since 2002 due to the infusion of multi-billion US dollars in international assistance and investments, as well as remittances from expats. It is also due to dramatic improvements in agricultural production and the end of a four-year drought in most of the country. The real value of "non-drug" GDP increased by 29% in 2002, 16% in 2003, 8% in 2004 and 14% in 2005. As much as one-third of Afghanistan's GDP comes from growing poppy and illicit drugs including opium and its two derivatives, morphine and heroin, as well as hashish production. Opium production in Afghanistan has soared to a new record in 2007, with an increase on last year of more than a third, the United Nations has said. Some 3.3 million Afghans are now involved in producing opium. In a recent article in the Washington Quarterly, Peter van Ham and Jorrit Kamminga argue that the international community should establish a pilot project and investigate a licensing scheme to start the production of medicines such as morphine and codeine from poppy crops to help it escape the economic dependence on opium. According to a 2004 report by the Asian Development Bank, the present reconstruction effort is two-pronged: first it focuses on rebuilding critical physical infrastructure, and second, on building modern public sector institutions from the remnants of Soviet style planning to ones that promote market-led development. In 2006, two U.S. companies, Black & Veatch and the Louis Berger Group, have won a US 1.4 billion dollar contract to rebuild roads, power lines and water supply systems of Afghanistan. One of the main drivers for the current economic recovery is the return of over 5 million Afghan refugees from neighbouring countries, who brought with them fresh energy, entrepreneurship and wealth-creating skills as well as much needed funds to start up businesses. What is also helping is the estimated US 2–3 billion dollars in international assistance every year, the partial recovery of the agricultural sector, and the reestablishment of market institutions. Private developments are also beginning to get underway. In 2006, a Dubai-based Afghan family opened a $25 million Coca Cola bottling plant in Afghanistan. While the country's current account deficit is largely financed with the donor money, only a small portion – about 15% – is provided directly to the government budget. The rest is provided to non-budgetary expenditure and donor-designated projects through the United Nations system and non-governmental organizations. The government had a central budget of only $350 million in 2003 and an estimated $550 million in 2004. The country's foreign exchange reserves totals about $500 million. Revenue is mostly generated through customs, as income and corporate tax bases are negligible. Inflation had been a major problem until 2002. However, the depreciation of the Afghani in 2002 after the introduction of the new notes (which replaced 1,000 old Afghani by one new Afghani) coupled with the relative stability compared to previous periods has helped prices to stabilize and even decrease between December 2002 and February 2003, reflecting the turnaround appreciation of the new Afghani currency. Since then, the index has indicated stability, with a moderate increase toward late 2003. The Afghan government and international donors seem to remain committed to improving access to basic necessities, infrastructure development, education, housing and economic reform. The central government is also focusing on improved revenue collection and public sector expenditure discipline. The rebuilding of the financial sector seems to have been so far successful. Money can now be transferred in and out of the country via official banking channels. Since 2003, over sixteen new banks have opened in the country, including Afghanistan International Bank, Kabul Bank, Azizi Bank, Standard Chartered Bank, First Micro Finance Bank, and others. A new law on private investment provides three to seven-year tax holidays to eligible companies and a four-year exemption from exports tariffs and duties. Some private investment projects, backed with national support, are also beginning to pick up steam in Afghanistan. An initial concept design called the City of Light Development, envisioned by Dr. Hisham N. Ashkouri, Principal of ARCADD, Inc. for the development and the implementation of a privately based investment enterprise has been proposed for multi-function commercial, historic and cultural development within the limits of the Old City of Kabul along the Southern side of the Kabul River and along Jade Meywand Avenue, revitalizing some of the most commercial and historic districts in the City of Kabul, which contains numerous historic mosques and shrines as well as viable commercial activities among war damaged buildings. Also incorporated in the design is a new complex for the Afghan National Museum. According to the U.S. Geological Survey and the Afghan Ministry of Mines and Industry, Afghanistan may be possessing up to of natural gas, of petroleum and up to of natural gas liquids. This could mark the turning point in Afghanistan's reconstruction efforts. Energy exports could generate the revenue that Afghan officials need to modernize the country's infrastructure and expand economic opportunities for the beleaguered and fractious population. Other reports show that the country has huge amounts of gold, copper, coal, iron ore and other minerals. The government of Afghanistan is in the process of extracting and exporting its copper reserves, which will be earning $1.2 billion US dollars in royalties and taxes every year for the next 30 years. It will also provide permanent labor to 3,000 of its citizens.
Ariana Afghan Airlines is the national airlines carrier, with domestic flights between Kabul, Kandahar, Herat and Mazar-e Sharif. International flights include to Dubai, Frankfurt, Istanbul and a number of other destinations. There are also limited domestic and international flight services available from Kam Air, Pamir Airways and Safi Airways. The country has limited rail service with Turkmenistan. There are two railway projects currently in progress, one is between Herat and the Iranian city Mashad while another is between Kandahar and Quetta in Pakistan. Most people who travel from one city to another use bus services. Automobiles have recently become more widely available, with Toyota, Nissan and Hyundai dealerships in Kabul. A large number of second-hand vehicles are also arriving from the UAE. Nearly all highways and roads are being rebuilt in the country.
The media was tightly controlled under the Taliban and other periods in its history, and was relatively free in others. Under the Taliban, television was shut down in 1996, and print media were forbidden to publish commentary, photos or readers letters. The only radio station broadcast religious programmes and propaganda, and aired no music. After the overthrow of the Taliban in 2001, press restrictions were gradually relaxed and private media diversified. Freedom of expression and the press is promoted in the 2004 constitution and censorship is banned, though defaming individuals or producing material contrary to the principles of Islam is prohibited. In 2008, Reporters Without Borders listed the media environment as 156 out of 173, with 1st being most free. 400 publications are now registered and 60 radio stations, a major source of information, currently exist. Foreign radio stations, such as the BBC World Service, also broadcast into the country.
Telecommunication services in the country are provided by Afghan Wireless, Etisalat, Roshan, Areeba and Afghan Telecom. In 2006, the Afghan Ministry of Communications signed a US$64.5 million agreement with ZTE Corporation for the establishment of a countrywide fibre optic cable network. This will improve telephone, internet, television and radio broadcast services throughout the country. Around 500,000 (1.5% of the population) had internet access by the end of 2008. Television and radio broadcastings are available in most parts of the country, with local and international channels or stations. The nation's post service is also operating. Package delivery services such as FedEx, DHL and others are also available.
As of 2006 more than four million male and female students were enrolled in schools throughout the country. However, there are still significant obstacles to education in Afghanistan, stemming from lack of funding, unsafe school buildings and cultural norms. A lack of women teachers is an issue that concerns some Afghan parents, especially in more conservative areas. Some parents will not allow their daughters to be taught by men. UNICEF estimates that more than 80 percent of females and around 50 percent of males lack access to education centers. According to the United Nations, 700 schools have been closed in the country because of poor security. Literacy of the entire population is estimated at 34%. Female literacy is 10%. Another aspect of education that is rapidly changing in Afghanistan is the face of higher education. Following the fall of the Taliban, Kabul University was reopened to both male and female students. In 2006, the American University of Afghanistan also opened its doors, with the aim of providing a world-class, English-language, co-educational learning environment in Afghanistan. The university accepts students from Afghanistan and the neighboring countries. Construction work will soon start at the new site selected for University of Balkh in Mazari Sharif. The new building for the university, including the building for the Engineering Department, would be constructed at 600 acres (2.4 km²) of land at the cost of 250 million US dollars. Since the 1930s there have been two French lycées (secondary schools) (AEFE contracted school) in Kabul, the "Lycée Esteqlal" and "Lycée Malalaï". A new military school has been set up to properly train and educate Afghan soldiers.
Every half hour, an average of one Afghan woman dies from pregnancy-related complications, another dies of tuberculosis and 14 children die, largely from preventable causes. Eight years after the fall of the Taliban, the humanitarian and development needs in Afghanistan remain acute. According to a November, 2009 UNICEF report, Afghanistan is now the most dangerous place in the world for a child to be born. Afghanistan has the highest infant mortality rate in the world – 257 deaths per 1,000 live births – and 70 percent of the population lacks access to clean water. The Afghan government has ambitious plans to cut the infant mortality rate to 400 from 1,600 for every 100,000 live births by 2020. Before the start of the Afghan wars in 1978, Afghanistan had an improving health care system and a semi-modernized health care system in cities like Kabul. Ibn Sina Hospital in Kabul and Ali Abad Hospital in Kabul were two of the leading health institutions in Central Asia at the time. Following the Soviet invasion and the civil war that followed, the health care system was limited only to urban areas and was eventually destroyed. The Taliban made some improvements, but health care was not available for women during their six year rule. Following the establishment of the Islamic Republic in 2002, the health system began to improve dramatically in Afghanistan due to international aid and all institutions accepted women for the first time since 1996. Non-governmental charities such as Mahboba's promise assist orphans in association with governmental structures. According to Reuters, "Afghanistan's healthcare system is widely believed to be one of the country's success stories since reconstruction began." An estimated 80,000 Afghans have lost limbs, mainly as a result of landmines. After years of war in Afghanistan, there are an estimated one million handicapped people. This is one of the highest percentages anywhere in the world. According to the Human Development Index Afghanistan is the second least developed country in the world.
Albania (, Gheg Albanian: "Shqipnia" or "Shqypnia"), officially the Republic of Albania (,), is a country in South Eastern Europe. It is bordered by Montenegro to the northwest, Kosovo to the northeast, the Republic of Macedonia to the east and Greece to the south and southeast. It has a coast on the Adriatic Sea to the west, and on the Ionian Sea to the southwest. It is less than from Italy, across the Strait of Otranto which links the Adriatic Sea to the Ionian Sea. Albania is a member of the United Nations, NATO, the Organization for Security and Co-operation in Europe, Council of Europe, World Trade Organisation, Organisation of the Islamic Conference and one of the founding members of the Union for the Mediterranean. Albania has been a potential candidate for accession to the European Union since January 2003, and it formally applied for EU membership on 28 April 2009. Albania is a parliamentary democracy and a transition economy. The Albanian capital, Tirana, is home to approximately 727,000 of the country's 3.6 million people, and it is also the financial capital of the country. Free-market reforms have opened the country to foreign investment, especially in the development of energy and transportation infrastructure.
"Albania" is the Medieval Latin name of the country which is called "Shqipëri" by its inhabitants. In Medieval Greek, the country's name is "Albania" () besides variants "Albanitia", "Arbanitia". The name may be derived from the Illyrian tribe of the Albani recorded by Ptolemy, the geographer and astronomer from Alexandria who drafted a map in 150 AD that shows the city of Albanopolis (located northeast of Durrës). The name may have a continuation in the name of a medieval settlement called Albanon and Arbanon, although it is not certain this was the same place. In his "History" written in 1079–1080, Byzantine historian Michael Attaliates was the first to refer to "Albanoi" as having taken part in a revolt against Constantinople in 1043 and to the "Arbanitai" as subjects of the Duke of Dyrrachium. During the Middle Ages, the Albanians called their country "Arbër" or "Arbën" and referred to themselves as Arbëresh or "Arbnesh". As early as the 16th century, a new name for their home evolved among Albanian people: "Shqipëria", popularly interpreted as "Land of the Eagles" or "Land of the Mountain Eagle" hence the two-headed bird on the national flag, though most likely the origin lies in Skanderbeg's use of the Byzantine double-headed eagle on his seals. Under the Ottoman Empire Albania was referred to officially as "Arnavutluk" and its inhabitants as arnaut.
The first recorded inhabitants in the territory of Albania were the Illyrians, an Indo-European people that inhabited the area corresponding to northern and central Albania. The Illyrian tribes that resided in the region of modern Albania were the Taulantii the Parthini, the Abri, the Caviii, the Enchelei, and several others. In the westernmost parts of the territory of Albania there lived the Bryges, a Phrygian people, and in the south were the Greek Chaonians. Beginning in the 8th century BC, Greek colonies were established on the Illyrian coast. The most important were Apollonia, Avlona (modern-day Vlorë), Epidamnos (modern-day Durrës), and Lissus (modern-day Lezhë). The rediscovered Greek city of Buthrotum (modern-day Butrint), a UNESCO World Heritage Site, is probably more significant today than it was when Julius Caesar used it as a provisions depot for his troops during his campaigns in the 1st century BC. At that time, it was considered an unimportant outpost, overshadowed by Apollonia and Epidamnos. In the 4th century BC, the Illyrian king Bardyllis united several Illyrian tribes and engaged in conflict with Macedon to the southeast, but was defeated. Bardyllis was succeeded by Grabos, then by Bardyllis II, and then by Cleitus the Illyrian, who was defeated by Alexander the Great. Later on, in 229 BC, Queen Teuta of the Ardiaei clashed with the Romans and initiated the Illyrian Wars, which resulted in defeat and in the end of Illyrian independence by 168 B.C., when King Gentius was defeated by a Roman army. The lands comprising modern-day Albania were incorporated into the Roman empire as part of the province of Illyricum above the river Drin, and Roman Macedonia (specifically as Epirus Nova) below it. The western part of the Via Egnatia ran inside modern Albania, ending at Dyrrachium. Illyricum was later divided into the provinces of Dalmatia and Pannonia. When the Roman Empire was divided into East and West in 395, the territories of modern Albania became part of the Byzantine Empire. Beginning in the first decades of Byzantine rule (until 461), the region suffered devastating raids by Visigoths, Huns, and Ostrogoths. In the 6th and 7th centuries, the region was overrun by the Slavs. The territory of Albania would remain under Byzantine and Bulgarian rule until the 14th century, when the Ottoman Turks began to make incursions into the Empire. The Ottomans captured Constantinople in 1453, and by 1460 most former Byzantine territories were in the hands of the Turks.
The new administrative system of the themes, or military provinces created by the Byzantine Empire, contributed to the eventual rise of feudalism in Albania, as peasant soldiers who served military lords became serfs on their landed estates. Among the leading families of the Albanian feudal nobility were the Thopia, Shpata, Muzaka, Dukagjini and Kastrioti. The first three of these rose to become rulers of principalities were vassals of Byzantium, and Albania mostly neglected by their Greek masters at Constantinople. Many Albanians converted to the Roman Catholic Church at this time. During the Byzantine Era the powerful Serbs had occupied almost all of Northern Albania and Kosovo, and the Venetians had gained control and colonized the coastal regions of Albania.
In the Middle Ages, the name "Arberia" (see "Origin and history of the name Albania") began to be increasingly applied to the region now comprising the nation of Albania. Beginning in the late-14th century, the Ottoman Turks expanded their empire from Anatolia to the Balkans (Rumelia). By the 15th century, the Ottomans ruled all of the Balkan Peninsula. Many Albanians had been recruited into the Janissary, including the feudal heir Gjergj Kastrioti who was renamed Skanderbeg (Iskandar Bey) by his Turkish trainers at Edrine. After some Ottoman defeats at the hands of the Serbs, Skanderbeg deserted and began a rebellion against the Ottoman Empire. After deserting, Gjergj Kastrioti Skanderbeg re-converted to Roman Catholicism and declared a holy war against the Ottoman Empire, which he led from 1443 to 1468. Under a red flag bearing Skanderbeg's heraldic emblem, an Albanian force of about 30,000 men at Krujë held off Ottoman campaigns against their lands for twenty-four years. Thrice the Albanians overcame sieges of Krujë (see Siege of Krujë) led by many Ottoman commanders, including the influential Iljaz Hoxha and his Albanian Janissary led by Hamza Kastrioti. However, Skanderbeg was unable to receive any of the help which had been promised him by the popes. He later abandoned Christianity and died in 1468, leaving no worthy successor. After his death the rebellion continued, but without its former success. The loyalties and alliances created and nurtured by Skanderbeg faltered and fell apart, and the Ottomans reconquered the territory of Albania in 1478. Shortly after the fall of Kruje's castle, some Albanians fled to neighboring Italy, giving rise to the modern Arbëreshë communities. After the defeat of Skanderbeg, Albania completely transformed under Ottoman rule, and its culture and society closely resembled that of neighboring Bosnia. The Ottomans had urbanized the landscape creating new cities, Bazaars, garrisons and Mosques throughout the Albanian regions. The majority of the remaining Albanian population converted to Islam, with many joining the Sufi Order of the Bektashi. Converting from Christianity to Islam brought considerable advantages, including access to Ottoman trade networks, bureaucratic positions and the army. As a result many Albanians came to serve in the elite Janissary and the administrative Devşirme system. Among these were important historical figures, including Iljaz Hoxha, Hamza Kastrioti, Köprülü Mehmed Pasha (head of the Köprülü family of Grand Viziers), the Bushati family, Sulejman Pasha, Ethem Pasha, Nezim Frakulla, Ali Pasha of Tepelena, Hasan Zyko Kamberi, Ali-paša Šabanagić, and Mehmet Ali ruler of Egypt. and Emin Pasha. Many Albanians gained prominent positions in the Ottoman government, Albanians highly active during the Ottoman Era and leaders such as Ali Pasha of Tepelena is known to have aided the Bosnian Hero Husein Gradaščević on various occasions, no fewer than 42 Grand Viziers of the Empire were Albanian in origin, including Mehmet Akif Ersoy (1873–1936) an Albanian from Peć who composed the Turkish National Anthem in 1921, "İstiklâl Marşı" (The Independence March). As Hupchik states, "Albanians had little cause of unrest" and "if anything, grew important in Ottoman internal affairs", and sometimes persecuted Christians harshly on behalf of their Turkish allies. Albania became pivotal for the Ottomans in the Balkans, although Albanians never rested, always having small rebellions wchich were put down by the Ottomans. As a cosequene of the continuous rebellions, the Albanians got the nickname "Arnauts" by the Ottomans, which meant "stubborn". Anyway, this period saw the rising of semi-autonomous Albanian ruled Pashaliks, and Albanians were also an important part of the Ottoman army and Ottoman administration like the case of Köprülü family. Albania would remain a part of the Ottoman Empire as the provinces of Shkodra, Manastir and Yanya until 1912.
After five hundred years of Ottoman domination, an independent Albania was proclaimed on November 28, 1912. The initial sparks of the first Balkan War in 1912 were ignited by the Albanian uprising between 1908 and 1910 which were directed at opposing the Young Turk policies of consolidation of the Ottoman Empire. Following the eventual weakening of the Ottoman Empire in the Balkans, Serbia, Greece and Bulgaria declared war and sought to aggrandize their respective boundaries on the remaining territories of the Empire. Albania was thus invaded by Serbia in the north and Greece in the south, restricting the country to only a patch of land around the southern coastal city of Vlora. In 1912 Albania, still under foreign occupation declared its independence and with the aid of Austria-Hungary, the Great Powers drew its present borders leaving more than half of the Albanian population outside the new country. The border between Albania and its neighbours was delineated in 1913 following the dissolution of most of the Ottoman Empire's territories in the Balkans. The delineation of the new state's borders left a significant number of Albanian communities outside Albania. This population was largely divided between Montenegro and Serbia (which then included what is now the Republic of Macedonia). A substantial number of Albanians thus found themselves under Serbian rule. At the same time, an uprising in the country's south by local Greeks, led to the formation of an autonomous region inside its borders (1914). After a period of political instability caused during World War I, the country adopted a republican form of government in 1920.
Starting in 1928, but especially during the Great Depression, the government of King Zog, which brought law and order to the country, began to cede Albania's sovereignty to Italy. Despite some strong resistance, especially at Durrës, Italy invaded Albania on 7 April 1939 and took control of the country, with the Italian Fascist dictator Benito Mussolini proclaiming Italy's figurehead King Victor Emmanuel III of Italy as King of Albania. The nation thus became one of the first to be occupied by the Axis Powers in World War II. As Hitler began his aggressions, Mussolini decided to occupy Albania as a means to compete with Hitler's territorial gains. Mussolini and the Italian Fascists saw Albania as a historical part of the Roman Empire, and the occupation was intended to fulfill Mussolini's dream of creating an Italian Empire. During the Italian occupation, Albania's population was subject to a policy of forced Italianisation by the kingdom's Italian governors, in which the use of the Albanian language was discouraged in schools while the Italian language was promoted. At the same time, the colonization of Albania by Italians was encouraged. Mussolini, in October 1940, used his Albanian base to launch an attack on Greece, which led to the defeat of the Italian forces and the Greek occupation of Southern Albania in what was seen by the Greeks as the liberation of Northern Epirus. While preparing for the Invasion of Russia, Hitler decided to attack Greece in December 1940 to prevent a British attack on his southern flank. During World War II, the Party of Labour was created on 8 November 1941. With the intention of organizing a partisan resistance, they called a general conference in Pezë on 16 September 1942 where the Albanian National Liberation Front was set up. The Front included nationalist groups, but it was dominated by communist partisans. In December 1942, more Albanian nationalist groups were organized under Visar Kola. Albanians fought against the Italians while, during Nazi German occupation, Balli Kombëtar allied itself with the Germans and clashed with Albanian communists, which continued their fight against Germans and Balli Kombëtar at the same time. With the collapse of the Mussolini government in line with the Allied invasion of Italy, Germany occupied Albania in September 1943, dropping paratroopers into Tirana before the Albanian guerrillas could take the capital. The German Army soon drove the guerrillas into the hills and to the south. The Nazi German government subsequently announced it would recognize the independence of a neutral Albania and set about organizing a new government, police and armed forces. Many Balli Kombëtar units cooperated with the Germans against the communists and several Balli Kombëtar leaders held positions in the German-sponsored regime. The partisans entirely liberated Albania from German occupation on 28 November 1944. The Albanian partisans also liberated Kosovo, part of Montenegro, and southern Bosnia and Herzegovina. By November 1944, they had thrown out the Germans, one of the few East European nations to do so without any assistance from Soviet troops. Enver Hoxha became the leader of the country by virtue of his position as Secretary General of the Albanian Communist Party. Albania was one of the European countries occupied by the Axis powers that ended World War II with a larger Jewish population than before the war. Some 1,200 Jewish residents and refugees from other Balkan countries were hidden by Albanian families during World War II, according to official records.
Albania became an ally of the Soviet Union, but this came to an end in 1960 over the advent of de-Stalinization. A strong political alliance with China followed, leading to several billion dollars in aid, which was curtailed after 1974. China cut off aid in 1978 when Albania attacked its policies after the death of the Chinese ruler Mao Zedong. Large-scale purges of officials occurred during the 1970s. Enver Hoxha, a dictator who ruled Albania for four decades with an iron fist, died on 11 April 1985. Eventually the new regime introduced some liberalization, and granting the freedom to travel abroad in 1990. The new government made efforts to improve ties with the outside world. The elections of March 1991 left the former Communists in power, but a general strike and urban opposition led to the formation of a coalition cabinet that included non-Communists. Recent history – 1992 to present. Albania's former Communists were routed in elections March 1992, causing economic collapse and social unrest. The blood feud has returned in rural areas after more than 40 years of being abolished by Albanian communists, with nearly 10,000 Albanians being killed due to blood feuds since 1991. Sali Berisha was elected as the first non-Communist president since World War II. The next crisis occurred in 1997, during his presidency, as riots ravaged the country. The state institutions collapsed and an EU military mission led by Italy was sent to stabilize the country. In summer 1997, Berisha was defeated in elections, winning just 25 seats out of a total of 156. His return to power in the elections of 3 July 2005 ended eight years of Socialist Party rule. In 2009, Albania – along with Croatia – joined NATO. Government, politics and armed forces. The Albanian republic is a parliamentary democracy established under a constitution renewed in 1998. Elections are now held every four years to a unicameral 140-seat chamber, the People's Assembly. In June 2002, a compromise candidate, Alfred Moisiu, former Army General, was elected to succeed President Rexhep Meidani. Parliamentary elections in July 2005 brought Sali Berisha, as leader of the Democratic Party, back to power. The Euro-Atlantic integration of Albania has been the ultimate goal of the post-communist governments. Albania's EU membership bid has been set as a priority by the European Commission. Albania, along with Croatia, joined NATO on 1 April 2009 becoming the 27th and 28th members of the alliance. The workforce of Albania has continued to migrate to Greece, Italy, Germany, other parts of Europe, and North America. However, the migration flux is slowly decreasing, as more and more opportunities are emerging in Albania itself as its economy steadily develops.
The head of state in Albania is the President of the Republic. The President is elected to a 5-year term by the Assembly of the Republic of Albania by secret ballot, requiring a 50%+1 majority of the votes of all deputies. The next election will run in 2012. The current President of the Republic is Bamir Topi. The President has the power to guarantee observation of the constitution and all laws, act as commander in chief of the armed forces, exercise the duties of the Assembly of the Republic of Albania when the Assembly is not in session, and appoint the Chairman of the Council of Ministers (prime minister). Executive power rests with the Council of Ministers (cabinet). The Chairman of the Council (prime minister) is appointed by the president; ministers are nominated by the president on the basis of the prime minister's recommendation. The People's Assembly must give final approval of the composition of the Council. The Council is responsible for carrying out both foreign and domestic policies. It directs and controls the activities of the ministries and other state organs.
The Assembly of the Republic of Albania ("Kuvendi i Republikës së Shqipërisë") is the lawmaking body in Albania. There are 140 deputies in the Assembly, which are elected through a party-list proportional representation system. The President of the Assembly (or Speaker) has two deputies and chairs the Assembly. There are 15 permanent commissions, or committees. Parliamentary elections are held at least every four years. The Assembly has the power to decide the direction of domestic and foreign policy; approve or amend the constitution; declare war on another state; ratify or annul international treaties; elect the President of the Republic, the Supreme Court and the Attorney General and his or her deputies; and control the activity of state radio and television, state news agency and other official information media.
The Albanian Armed Forces ("Forcat e Armatosura të Shqipërisë") first formed after independence in 1912. Albania reduced the number of active troops from a 1988 number of 65,000 to a 2009 number of 14,500 with a small fleet of aircraft and sea vessels. In the 1990s, the country scrapped enormous amount of obsolete hardware, such as tanks and SAM systems from China. Today, it consists of the General Staff Headquarters, the Albanian Joint Forces Command, the Albanian Support Command and the Albanian Training and Doctrine Command. Increasing the military budget was one of the most important conditions for NATO integration. Military spending accounted for about 2.7% of GDP in 2008. Since February 2008, Albania participates officially in NATO's Operation Active Endeavor in the Mediterranean Sea. and received a NATO membership invitation on 3 April 2008. Albania became a full member of NATO on 1 April, 2009.
Albania has a total area of 28,748 square kilometers. Its coastline is 362 kilometers long and extends along the Adriatic and Ionian Seas. The lowlands of the west face the Adriatic Sea. The 70% of the country that is mountainous is rugged and often inaccessible from the outside. The highest mountain is Korab situated in the district of Dibër, reaching up to. The climate on the coast is typically Mediterranean with mild, wet winters and warm, sunny, and rather dry summers. Inland conditions vary depending on altitude but the higher areas above 1,500 m/5,000 ft are rather cold and frequently snowy in winter; here cold conditions with lying snow may linger into spring. Besides the capital city of Tirana, which has 800,000 inhabitants, the principal cities are Durrës, Korçë, Elbasan, Shkodër, Gjirokastër, Vlorë and Kukës. In Albanian grammar, a word can have indefinite and definite forms, and this also applies to city names: both "Tiranë" and "Tirana", "Shkodër" and "Shkodra" are used. The three largest and deepest tectonic lakes of the Balkan Peninsula are partly located in Albania. Lake Shkodër in the country's northwest has a surface which can vary between and 530 km2, out of which one third belongs to Albania and rest to Montenegro. The Albanian shoreline of the lake is. Ohrid Lake is situated in the country's southeast and is shared between Albania and Republic of Macedonia. It has a maximal depth of 289 meters and a variety of unique flora and fauna can be found there, including "living fossils" and many endemic species. Because of its natural and historical value, Ohrid Lake is under the protection of UNESCO. Over a third of the territory of Albania – about 10,000 square kilometers (2.5 million acres) – is forested and the country was very rich in flora. About 3.000 different species of plants grow in Albania, many of which are used for medicinal purposes. Phytogeographically, Albania belongs to the Boreal Kingdom and is shared between the Adriatic and East Mediterranean provinces of the Mediterranean Region and the Illyrian province of the Circumboreal Region. According to the World Wide Fund for Nature and Digital Map of European Ecological Regions by the European Environment Agency, the territory of Albania can be subdivided into three ecoregions: the Illyrian deciduous forests, Pindus Mountains mixed forests and Dinaric Alpine mixed forests. The forests are home to a wide range of mammals, including wolves, bears, wild boars and chamois. Lynx, wildcats, pine martens and polecats are rare, but survive in some parts of the country.
With its coastline facing the Adriatic and Ionian seas, its highlands backed upon the elevated Balkan landmass, and the entire country lying at a latitude subject to a variety of weather patterns during the winter and summer seasons, Albania has a high number of climatic regions for so small an area. The coastal lowlands have typically Mediterranean weather; the highlands have a Mediterranean continental climate. In both the lowlands and the interior, the weather varies markedly from north to south. The lowlands have mild winters, averaging about. Summer temperatures average. In the southern lowlands, temperatures average about higher throughout the year. The difference is greater than during the summer and somewhat less during the winter. Inland temperatures are affected more by differences in elevation than by latitude or any other factor. Low winter temperatures in the mountains are caused by the continental air mass that dominates the weather in Eastern Europe and the Balkans. Northerly and northeasterly winds blow much of the time. Average summer temperatures are lower than in the coastal areas and much lower at higher elevations, but daily fluctuations are greater. Daytime maximum temperatures in the interior basins and river valleys are very high, but the nights are almost always cool. Average precipitation is heavy, a result of the convergence of the prevailing airflow from the Mediterranean Sea and the continental air mass. Because they usually meet at the point where the terrain rises, the heaviest rain falls in the central uplands. Vertical currents initiated when the Mediterranean air is uplifted also cause frequent thunderstorms. Many of these storms are accompanied by high local winds and torrential downpours. When the continental air mass is weak, Mediterranean winds drop their moisture farther inland. When there is a dominant continental air mass, cold air spills onto the lowland areas, which occurs most frequently in the winter. Because the season's lower temperatures damage olive trees and citrus fruits, groves and orchards are restricted to sheltered places with southern and western exposures, even in areas with high average winter temperatures. Lowland rainfall averages from 1,000 millimeters to more than 1,500 millimeters annually, with the higher levels in the north. Nearly 95% of the rain falls in the winter. Rainfall in the upland mountain ranges is heavier. Adequate records are not available, and estimates vary widely, but annual averages are probably about 1,800 millimeters and are as high as 2,550 millimeters in some northern areas. The western Albanian Alps (valley of Boga) are among the most wet areas in Europe, receiving some of rain annually. The seasonal variation is not quite as great in the coastal area. The higher inland mountains receive less precipitation than the intermediate uplands. Terrain differences cause wide local variations, but the seasonal distribution is the most consistent of any area.
Although a small country, Albania is distinguished for its rich biological diversity. The variation of geomorphology, climate and terrain create favorable conditions for a number of endemic and sub-endemic species with 27 endemic and 160 subendemic vascular plants present in the country. The total number of plants is over 3250 species, approximately 30% of the entire flora species found in Europe. Coastal regions and lowlands have typical Mediterranean macchia vegetation, whereas oak forests and vegetation are found on higher altitudes. Vast forests of black pine, beech and fir are found on higher mountains and alpine grasslands grow at altitudes above 1800 meters a.s.l. There are around 760 vertebrate species found so far in Albania. Among these there are over 350 bird species, 330 freshwater and marine fish and 80 mammal species. There are some 91 globally threatened species found within the country, among which the Dalmatian pelican, Pygmy cormorant, and the European sea sturgeon. Rocky coastal regions in the south provide good habitats for the endangered Mediterranean monk seal. Some of the most significant bird species found in the country include the golden eagle – known as the national symbol of Albania – vulture species, capercaillie and numerous waterfowl. The Albanian forests still maintain significant communities of large mammals such as the brown bear, gray wolf, chamois and wild boar. The north and eastern mountains of the country are home to the last remaining Balkan lynx – a critically endangered population of the Eurasian lynx.
Albania remains a poor country by Western European standards. Its GDP per capita (expressed in PPS—Purchasing Power Standards) stood at 25 percent of the EU average in 2008. Still, Albania has shown potential for economic growth, as more and more businesses relocate there and consumer goods are becoming available from emerging market traders as part of the current massive global cost-cutting exercise. Albania and Cyprus are the only countries in Europe that recorded economic growth in the first quarter of 2009. In its latest report, the International Monetary Fund (IMF) said Albania and Cyprus recorded increases of 0.4% and 0.3%, respectively. However, the country is still of low interest for major foreign investors due to frequent power shortages, occasional lack of water supplies and ubiquitous illegal activities. Albania and Croatia have discussed the possibility of jointly building a nuclear power plant at Lake Shkoder, close to the border with Montenegro, a plan that has gathered criticism from the latter due to seismicity in the area. In addition, there is some doubt whether Albania would be able to finance a project of such a scale with a total national budget of less than $ 5 billion. However, in February 2009 Italian company Enel announced plans to build an 800 MW coal-fired power plant in Albania, to diversify electricity sources. Nearly 100% of the electricity is generated by ageing hydroelectric power plants, which are becoming more ineffective due to increasing droughts. The country has some deposits of petroleum and natural gas, but produces only 6,425 barrels of oil per day. Natural gas production, estimated at about 30 million cubic meters, is sufficient to meet consumer demands. Other natural resources include coal, bauxite, copper and iron ore. Agriculture is the most significant sector, employing some 58% of the labor force and generating about 21% of GDP. Albania produces significant amounts of wheat, corn, tobacco, figs (13th largest producer in the world) and olives.
In the early 1990s, the rock-strewn roadways, unstable rail lines and obsolete telephone network crisscrossing Albania represented the remnants of the marked improvements that were made after World War II. Enver Hoxha's xenophobia and lust for control had kept Albania isolated, however, as the communications revolution transformed the wider world into a global village. Even internal travel amounted to something of a luxury for many Albanians during communism's ascendancy.
Currently the major cities of the country are linked with first class national roads. There is a four lane highway connecting the city of Durrës with Tirana and the city of Durrës with the city of Lushnje. Albania is partaking in the construction of what it sees as three major corridors of transportation. The major priority as of present is the construction of the four lane Durrës-Pristina highway which will link Kosovo with Albania's Adriatic coast. The portion of the highway which links Albania's north east border with Kosovo was completed in June 2009, as a result, cutting the time it takes to get from Kosovo to Durrës from six hours to two. Indeed the roads in northwestern Albania remain in poor condition as of summer 2009. It takes approximately 1H30 to drive the from the border of Montenegro to Shkodër. It is also worth noting that there are no road signs and no traffic lights within and around this city. The second priority is the construction of European corridor 8 linking Albania with the Republic of Macedonia and Greece. The third priority for the government is the construction of the north-south axis of the country; it is sometimes referred to as the Adriatic–Ionian motorway as it is part of a larger regional highway connecting Croatia with Greece along the Adriatic and Ionian coasts. By the end of the decade it is expected that the majority of the sections of these three corridors will have been built. When all three corridors are completed Albania will have an estimated 759 kilometers of highway linking it with its neighbors. There has been much discussion, debate and interest in the small Durrës–Kukës–Morinë Highway Albanian highway to Kosovo, which is intended to create a new, super-fast connection between Durrës on the Adriatic coast to Morinë at the border of Kosovo. The current drive time between Kukës and Durrës is 6–7 hours, but once the new highway is completed the drive time will only be two hours. The whole road will be around, when completed to Pristina. The objective for constructing the road, according to the transport ministry, is to reduce transport costs and accidents, and improve traffic flow. It is the biggest, most expensive infrastructure project undertaken in Albania. There has been much controversy and scandal surrounding this project as well, due to the spiralling cost of construction leading to various corruption allegations. Originally the highway was forecast to cost around EUR400 million, and now the cost appears to have breached EUR800 million, although the exact cost for the total highway has yet to be confirmed by the government. Currently there is a display in Tirana's centre on Bvld Dëshmorët e Kombit.
The civil air transport in Albania marked its beginnings in November 1924, when the Republic of Albania signed a Governmental Agreement with German Air Company Lufthansa. On the basis of a ten-year concession agreement, the Albanian Airlines with the name Adria Aero Lloyd Company was established. In the spring of 1925, the first domestic flights from Tirana to Shkoder and Vlora began. In August 1927, the office of Civil Aviation of Air Traffic Ministry of Italy purchased Adria Aero Lloyd. The company, now in Italian hands, expanded its flights to other cities, such as Elbasan, Korça, Kukësi, Peshkopia and Gjirokastra, and opened up international lines to Rome, Milan, Thessaloniki, Sofia, Belgrade, and Podgorica. The construction of a more modern airport construction in present Lapraka) started in 1934 and was completed by the end of 1935. This new airport, which was later officially named "Airport of Tirana", was constructed in conformity with optimal technological parameters of that time, with reinforced concrete runway of 1200, and complemented with technical equipment and appropriate buildings. During 1955–1957, the Rinasi Airport was constructed for military purposes. Later, its administration was shifted to the Ministry of Transport. On 25 January 1957 the State-owned Enterprise of International Air Transport (Albtransport) established its headquarters in Tirana. Aeroflot, Jat, Malev, Tarom and Interflug were the air companies that started to have flights with Albania until 1960. During 1960–1978, several airlines ceased to operate in Albania due to the impact of the politics, resulting to a decrease of influx of flights and passengers. In 1977 Albania's government signed an agreement with Greece to open the country's first air links with non-communist Europe. As a result, Olympic Airways was the first non-communist airline to commercially fly into Albania after WWII. By 1991 Albania had air links with many major European cities, including Paris, Rome, Zürich, Vienna and Budapest, but no regular domestic air service. A French-Albanian joint venture Ada Air, was launched in Albania's as the first private airline, in 1991. The company offered flights in a thirty-six-passenger airplane four days each week between Tirana and Bari, Italy and a charter service for domestic and international destinations. From 1989 to 1991, because of political changes in the Eastern European countries, Albania adhered to the International Civil Aviation Organization (ICAO), opened its air space to international flights, and had its duties of Air Traffic Control defined. As premises of these developments, conditions were created to separate the activities of air traffic control from Albtransport. Instead, the National Agency of Air Traffic (NATA) was established as an independent enterprise. In addition, during these years, governmental agreements of civil air transport were established with Bulgaria, Germany, Slovenia, Italy, Russia, Austria, England, Macedonia, etc. The Directory General of Civil Aviation (DGCA) was established on 3 February 1991, to cope with the development required by the time. As of 2007 Albania has one international airport: Tirana International Airport Nënë Tereza. The airport is linked to 29 destinations by 14 airlines. It has seen a dramatic rise in terms of passenger numbers and aircraft movements since the early 1990s. The data for 2009 is 1.3 million passengers served and an average of 44 landings and takeoffs per day.
The railway system was extensively promoted by the totalitarian regime of Enver Hoxha, during which time the use of private transport was effectively prohibited. Since the collapse of the former regime, there has been a considerable increase in car ownership and bus usage. Whilst some of the country's roads are still in a very poor condition, there have been other developments (such as the construction of a motorway between Tirana and Durrës) which have taken much traffic away from the railways. The railways in Albania are administered by the national railway company "Hekurudha Shqiptare" (HSH) (which means "Albanian Railways"). It operates a gauge (standard gauge) rail system in Albania. All trains are hauled by Czech-built ČKD diesel-electric locomotives.
The Albanian population is considered a very young population, with an average age of 28.9 years. After 1990 the Albanian population has faced new phenomena like migration, which greatly affected the distribution by districts and prefectures. Districts in the North have seen a decreasing population, while Tirana and Durrës districts have increased their population. Albania's population was 3,152,600 on 1 January 2007 and 3,170,048 on 1 January 2008. Alternative sources estimate the population in July 2009 at 3,639,453 with an annual growth rate of 0.546%. Albania is a largely ethnically homogeneous country with only small minorities. The vast majority of the population is ethnically Albanian (98.6%). Minorities include Greeks 1.17% and others 0.23% (Vlachs, Macedonians, Serbs, Bulgarians, Balkan Egyptians, Roma and former Yugoslavians). The size of the Greek minority is contentious, with the Albanian government claiming it is only 60,000, while the Greek government is claiming 300,000. Most Western sources put the size of the Greek minority at around 200,000, or ~6% of the population, while the CIA Factbook estimates the Greek minority at 3% of the total population. The dominant language is Albanian, with two main dialects, Gheg and Tosk. Many Albanians are also fluent in English, Italian, Greek, Turkish or German.
Estimates of the religious allegiance of the population of Albania vary, with some sources suggesting that the majority do not follow any religion. A second study of religion in Albania under the "International Religious Freedom Report 2009", performed by the Bureau of Democracy, Human Rights, and Labor of the United States's State Department, found that a majority of Albania's population is nonreligious. A recent study by the Pew Research Center puts the percentage of nominal Muslims in Albania at 79.9%, with the remaining 20% consisting of Christians. The CIA World Factbook gives a distribution of 70% Muslims, 20% Eastern Orthodox, and 10% Roman Catholics. According to the World Christian Encyclopedia, roughly 39% of Albanians are Muslim, and 35% Christian The Albanians first appear in the historical record in Byzantine sources of the late-11th century. At this point, they are already fully Christianised. Christianity was later overshadowed by Islam, which kept the scepter of the major religion during the period of Ottoman Turkish rule from the 15th century until year 1912. After independence (1912) from the Ottoman Empire, the Albanian republican, monarchic and later communist regimes followed a systematic policy of separating religion from official functions and cultural life. Albania never had an official state religion either as a republic or as a kingdom. In the 20th century, the clergy of all faiths was weakened under the monarchy, and ultimately eradicated during the 1940s and 1950s, under the state policy of obliterating all organised religion from Albanian territories. The Communist regime that took control of Albania after World War II suppressed religious observance and institutions and entirely banned religion to the point where Albania was officially declared to be the world's first atheist state. Religious freedom has returned to Albania since the regime's change in 1992. Albanian Muslim populations (mainly secular and of the Sunni rite) are found throughout the country whereas Orthodox Christians are concentrated in the south and Roman Catholics are found in the north of the country. No reliable data are available on active participation in formal religious services, and estimates range from 25% to 40%. There are about 4,000 active Jehovah's witnesses in Albania. Among other religious organizations making inroads into this nation is The Church of Jesus Christ of Latter-day Saints (LDS or 'Mormons'). The Church's involvement in Albania began with Humanitarian Aid during the 1990s. The first missionaries were sent in 1992 with the Albania Tirana Mission being opened in 1996. As of 2008, there were nearly 2,000 members of the Church in Albania, spread throughout ten branches with two purpose-built Chapels and one Family History Center.
Albanian folk music falls into three sylistic groups, with other important music areas around Shkodër and Tirana; the major groupings are the Ghegs of the north and southern Labs and Tosks. The northern and southern traditions are contrasted by the "rugged and heroic" tone of the north and the "relaxed, gentle and exceptionally beautiful" form of the south. These disparate styles are unified by "the intensity that both performers and listeners give to their music as a medium for patriotic expression and as a vehicle carrying the narrative of oral history", as well as certain characteristics like the use of obscure rhythms such as 3/8, 5/8 and 10/8. The first compilation of Albanian folk music was made by Pjetër Dungu in 1940. Albanian folk songs can be divided into major groups, the heroic epics of the north, and the sweetly melodic lullabies, love songs, wedding music, work songs and other kinds of song. The music of various festivals and holidays is also an important part of Albanian folk song, especially those that celebrate St. Lazarus Day ("the llazore"), which inaugurates the springtime. Lullabies and laments are very important kinds of Albanian folk song, and are generally performed by solo women.
Albanian was proven to be an Indo-European language in 1854 by the German philologist Franz Bopp. The Albanian language comprises its own branch of the Indo-European language family. Some scholars believe that Albanian derives from Illyrian while others, claim that it derives from Daco-Thracian. (Illyrian and Daco-Thracian, however, might have been closely related languages; see Thraco-Illyrian.) Establishing longer relations, Albanian is often compared to Balto-Slavic on the one hand and Germanic on the other, both of which share a number of isoglosses with Albanian. Moreover, Albanian has undergone a vowel shift in which stressed, long "o" has fallen to "a", much like in the former and opposite the latter. Likewise, Albanian has taken the old relative "jos" and innovatively used it exclusively to qualify adjectives, much in the way Balto-Slavic has used this word to provide the definite ending of adjectives. The cultural resistance was first of all expressed through the elaboration of the Albanian language in the area of church texts and publications, mainly of the Catholic confessional region in the North, but also of the Orthodox in the South. The Protestant reforms invigorated hopes for the development of the local language and literary tradition when cleric Gjon Buzuku brought into the Albanian language the Catholic liturgy, trying to do for the Albanian language what Luther did for German. "Meshari" (The Missal) by Gjon Buzuku, published by him in 1555, is considered to date as the first literary work of written Albanian. The refined level of the language and the stabilised orthography must be a result of an earlier tradition of writing Albanian, a tradition that is not known. But there are some fragmented evidence, dating earlier than Buzuku, which indicate that Albanian was written at least since 14th century AD. The first known evidence dates from 1332 AD and deals with the French Dominican Guillelmus Adae, Archbishop of Antivari, who in a report in Latin writes that Albanians use Latin letters in their books although their language is quite different from Latin. Of special importance in supporting this are: a baptizing formula ("Unte paghesont premenit Atit et Birit et spertit senit") of 1462, written in Albanian within a text in Latin by the Bishop of Durrës, Pal Engjëlli; a glossary with Albanian words of 1497 by Arnold von Harff, a German who had travelled through Albania, and a 15th century fragment from the Bible from the Gospel of Matthew, also in Albanian, but in Greek letters. Albanian writings of these centuries must not have been religious texts only, but historical chronicles too. They are mentioned by the humanist Marin Barleti, who, in his book "Rrethimi i Shkodrës" (The Siege of Shkodër) (1504), confirms that he leafed through such chronicles written in the language of the people ("in vernacula lingua"). Despite the obstacles generated by the Counter-Reformation which was opposed to the development of national languages in Christian liturgy, this process went on uninterrupted. During the 16th to 17th centuries, the catechism "E mbësuame krishterë" (Christian Teachings) (1592) by Lekë Matrënga, "Doktrina e krishterë" (The Christian Doctrine) (1618) and "Rituale romanum" (1621) by Pjetër Budi, the first writer of original Albanian prose and poetry, an apology for George Castriot (1636) by Frang Bardhi, who also published a dictionary and folklore creations, the theological-philosophical treaty "Cuneus Prophetarum" (The Band of Prophets) (1685) by Pjetër Bogdani, the most universal personality of Albanian Middle Ages, were published in Albanian. The most famous Albanian writer is probably Ismail Kadare.
Before the Communist regime, Albania's illiteracy rate was as high as 85%. Schools were scarce between World War I and World War II. When the Communist regime over took the country in 1944, the regime wanted to wipe out illiteracy. The regulations became so strict that anyone between the ages of 12 and 40 who could not read or write was mandated to attend classes to learn. Since these times of struggle the country's literacy rate has improved remarkably. Today the overall literacy rate in Albania is 98.7%, the male literacy rate is 99.2% and female literacy rate is 98.3%. Since the rather large population movements in the 1990s to urban areas, education has moved as well. Thousands of teachers moved to urban areas to follow students. The University of Tirana is the first university in Albania and was founded in October 1957.
Radio Televizioni Shqiptar, (RTSH), is Albania's leading television network. RTSH runs a national television station "TVSH", (standing for "Televizioni Shqiptar"), and two national radio stations, using the name "Radio Tirana". An international service broadcasts radio programmes in Albanian and seven other languages via medium wave (AM) and short wave (SW). The international service has used the theme from the song "Keputa një gjethe dafine" as its signature tune. Since 1999, RTSH has been a member of the European Broadcasting Union. Since 1993, RTSH has also run an international television service via satellite, aimed at Albanian language communities in Kosovo, Macedonia, Montenegro and Greece, plus the Albanian diaspora in the rest of Europe. According the National Council of Radio and Television Albania has an estimated 257 media outlets, including 66 radio stations and 65 television stations, with three national and 62 local stations.
Health care has been in a steep decline after the collapse of socialism in the country, but a process of modernization has been taking place since 2000. As of the early 2000s, there were 51 hospitals in the country, including a military hospital and specialist facilities. Albania has successfully removed diseases such as malaria. Life expectancy is estimated at 77.43 years, ranking 51st worldwide, and outperforming a number of European Union countries, such as Hungary and the Czech Republic. The most common causes of death are circulatory disease followed by cancerous illnesses. The medical school, Faculty of Medicine at Tirana University, is in Tirana. There are also nursing schools in many other cities.
The cuisine of Albania – as with most Mediterranean and Balkan nations – is strongly influenced by its long history. At different times, the territory which is now Albania has been claimed or occupied by Greece, Italy and the Ottoman Turks and each group has left its mark on Albanian cuisine. The main meal of the Albanians is lunch, and it is usually accompanied by a salad of fresh vegetables, such as tomatoes, cucumbers, green peppers and olives with olive oil, vinegar and salt. Lunch also includes a main dish of vegetables and meat. Seafood specialties are also common in the coastal areas of Durrës, Vlorë and Sarandë.
Allah (, ',) is the standard Arabic word for God. While the term is best known in the West for its use by Muslims as a reference to God, it is used by Arabic-speakers of all Abrahamic faiths, including Christians and Jews, in reference to "God". The term was also used by pagan Meccans as a reference to the creator-god, possibly the supreme deity in pre-Islamic Arabia. The concepts associated with the term "Allah" (as a deity) differ among the traditions. In pre-Islamic Arabia amongst pagan Arabs, "Allah" was not considered the sole divinity, having associates and companions, sons and daughters - a concept which Islam thoroughly and resolutely abrogated. In Islam, the name Allah is the supreme and all-comprehensive divine name. All other divine names are believed to refer back to Allah. "Allah" is unique, the only Deity, creator of the universe and omnipotent. Arab Christians today use terms such as "Allāh al-ʼAb" (الله الأب, "God the Father") to distinguish their usage from Muslim usage. There are both similarities and differences between the concept of God as portrayed in the Qur'an and the Hebrew Bible. Unicode has a codepoint reserved for "Allāh", = U+FDF2. Many Arabic type fonts feature special ligatures for Allah.
The term "Allāh" is derived from a contraction of the Arabic definite article "al-" "the" and ' "deity, god" to ' meaning "the [sole] deity, God" ("ho theos monos"). Cognates of the name "Allāh" exist in other Semitic languages, including Hebrew and Aramaic. The corresponding Aramaic form is אֱלָהָא ' in Biblical Aramaic and ܐܰܠܳܗܳܐ ' or ' in Syriac. The contraction of "al-" and ' in forming the term "Allāh" ("the god", masculine form) parallels the contraction of "al-" and ' in forming the term "Allāt" ("the goddess", feminine form).
In pre-Islamic Arabia, Allah was used by Meccans as a reference to the creator-god, possibly the supreme deity. Allah was not considered the sole divinity; however, Allah was considered the creator of the world and the giver of rain. The notion of the term may have been vague in the Meccan religion. Allah was associated with companions, whom pre-Islamic Arabs considered as subordinate deities. Meccans held that a kind of kinship existed between Allah and the jinn. Allah was thought to have had sons and that the local deities of al-ʻUzzá, Manāt and al-Lāt were His daughters. The Meccans possibly associated angels with Allah. Allah was invoked in times of distress. Muhammad's father's name was meaning the “servant of Allāh.” or "the slave of Allāh"
According to Islamic belief, Allah is the proper name of God, and humble submission to His Will, Divine Ordinances and Commandments is the pivot of the Muslim faith. "He is the only God, creator of the universe, and the judge of humankind." "He is unique ("wahid") and inherently one ("ahad"), all-merciful and omnipotent." The Qur'an declares "the reality of Allah, His inaccessible mystery, His various names, and His actions on behalf of His creatures." In Islamic tradition, there are 99 Names of God ("al-asma al-husna" lit. meaning: "The best names") each of which evoke a distinct characteristic of Allah. All these names refer to Allah, the supreme and all-comprehensive divine name. Among the 99 names of God, the most famous and most frequent of these names are "the Merciful" ("al-rahman") and "the Compassionate" ("al-rahim"). Most Muslims use the untranslated Arabic phrase "insha' Allah" (meaning "God willing") after references to future events. Muslim discursive piety encourages beginning things with the invocation of "bismillah"(meaning "In the name of God"). There are certain phrases in praise of God that are favored by Muslims, including "Subhan-Allah" (Holiness be to God), "Alhamdulillah" (Praise be to God), "La-il-la-ha-illa-Allah" (There is no deity but God) and "Allāhu Akbar" (God is great) as a devotional exercise of remembering God (zikr). In a Sufi practice known as "zikr Allah" (lit. remembrance of God), the Sufi repeats and contemplates on the name "Allah" or other divine names while controlling his or her breath. Some scholars have suggested that Muhammad used the term Allah in addressing both pagan Arabs and Jews or Christians in order to establish a common ground for the understanding of the name for God, a claim Gerhard Böwering says is doubtful. According to Böwering, in contrast with Pre-Islamic Arabian polytheism, God in Islam does not have associates and companions nor is there any kinship between God and jinn. Pre-Islamic pagan Arabs believed in a blind, powerful, inexorable and insensible fate over which man had no control. This was replaced with the Islamic notion of a powerful but provident and merciful God. According to Francis Edwards Peters, "The Qur'an insists, Muslims believe, and historians affirm that Muhammad and his followers worship the same God as the Jews (). The Quran's Allah is the same Creator God who covenanted with Abraham". Peters states that the Qur'an portrays Allah as both more powerful and more remote than Yahweh, and as a universal deity, unlike Yahweh who closely follows Israelites.
Arabic-speakers of all Abrahamic faiths, including Christians and Jews, use the word "Allah" to mean "God". The Christian Arabs of today have no other word for 'God' than 'Allah'. (Even the Arabic-descended Maltese language of Malta, whose population is almost entirely Roman Catholic, uses "Alla" for 'God'.) Arab Christians for example use terms "Allāh al-ʼab" (الله الأب) meaning God the Father, "Allāh al-ibn" (الله الابن) mean God the Son, and ' (الله الروح القدس) meaning God the Holy Spirit (See God in Christianity for the Christian concept of God). Arab Christians have used two forms of invocations that were affixed to the beginning of their written works. They adopted the Muslim "basm-Allah", and also created their own Trinitized "basm-Allah" as early as the eight century CE. The Muslim "basm-Allah" reads: "In the name of God, the Compassionate, the Merciful." The Trinitized "basm-Allah" reads: "In the name of Father and the Son and the Holy Spirit, One God." The Syriac, Latin and Greek invocations do not have the words "One God" at the end. This addition was made to emphasize the monotheistic aspect of Trinitian belief and also to make it more palatable to Muslims. According to Marshall Hodgson, it seems that in the pre-Islamic times, some Arab Christians made pilgrimage to the Kaaba, a pagan temple at that time, honoring Allah there as God the Creator. English and other European languages. The history of the word "Allāh" in English was probably influenced by the study of comparative religion in 19th century; for example, Thomas Carlyle (1840) sometimes used the term Allah but without any implication that Allah was anything different from God. However, in his biography of Muhammad (1934), Tor Andræ always used the term Allah, though he allows that this 'conception of God' seems to imply that it is different from that of the Jewish and Christian theologies. By this time Christians were also becoming accustomed to retaining the Hebrew term "Yahweh" untranslated (it was previously translated as 'the Lord'). Languages which may not commonly use the term "Allah" to denote a deity may still contain popular expressions which use the word. For example, because of the centuries long Muslim presence in the Iberian Peninsula, the word ojalá in the Spanish language and oxalá in the Portuguese language exist today, borrowed from Arabic (Arabic: إن شاء الله). This word literally means "God willing" (in the sense of "I hope so"). Some Muslims leave the name "Allāh" untranslated in English. Sometimes this comes from a zeal for the Arabic text of the Qur'an and sometimes with a more or less conscious implication that the Jewish and Christian concept of God is not completely true in its details. Conversely, the usage of the term Allah by English speaking non-Muslims in reference to the God in Islam, Marshall G. S. Hodgson says, can imply that Muslims are worshiping a mythical god named 'Allah' rather than God, the creator. This usage is therefore appropriate, Hodgson says, only for those who are prepared to accept its theological implications.
Christians in Indonesia and Malaysia also use "Allah" to refer to God in the Malay language and Indonesian Language (both language although different referred to as "Bahasa"). Mainstream Bible translations in Bahasa use "Allah" as the translation of Hebrew Elohim (translated in English Bibles as "God"). This goes back to early Bahasa translation work by Francis Xavier in the 16th century. The government of Malaysia in 2007 outlawed usage of the term "Allah" in any other but Muslim contexts, but the High Court in 2009 revoked the law, ruling that it was unconstitutional. While "Allah" had been used for the Christian God in Malay for more than four centuries, the contemporary controversy was triggered by usage of "Allah" by the Roman Catholic newspaper "The Herald". The government has in turn appealed the court ruling, and the High Court has suspended implementation of its verdict until the appeal is heard.
The word "Allāh" is always written without an alif to spell the "ā" vowel. This is because the spelling was settled before Arabic spelling started habitually using alif to spell "ā". However, in vocalized spelling, a small diacritic "alif" is added on top of the "shaddah" to indicate the pronunciation. One exception may be in the pre-Islamic Zabad inscription, where it ends with an ambiguous sign that may be a lone-standing "h" with a lengthened start, or may be a non-standard conjoined "l-h":-
Unicode has a codepoint reserved for "Allāh", = U+FDF2. This character according to the official Unicode specification is a ligature of "alif-lām-lām-shadda-(superscript alif)-hā" (U+0627 U+0644 U+0644 U+0651 U+0670 U+0647). There is, however some confusion arising from the fact that Arabic typography usually features a ' glyph without the preceding alif, which only occurs phrase-initially (or with in Qur'anic orthography). Consequently, the majority of Arabic Unicode fonts do not conform with the specification and have a glyph without the alif at this position (e.g. those provided by, the great majority of those licensed to or developed by, those of, SIL's and the fonts of developed in Pakistan), while others have the prescribed form with alif (e.g., distributed with the Middle-Eastern version of the, Arial Unicode MS, and, distributed with and with). The calligraphic variant of the word used as the Coat of arms of Iran is encoded in Unicode, in the Miscellaneous Symbols range, at codepoint U+262B ().
Azerbaijan (;), formally the Republic of Azerbaijan (), is a country in the Caucasus region of Eurasia. Located at the crossroads of Eastern Europe and Western Asia, it is bounded by the Caspian Sea to the east, Russia to the north, Georgia to the northwest, Armenia to the west, and Iran to the south. The exclave of Nakhichevan is bounded by Armenia to the north and east, Iran to the south and west, while having a short borderline with Turkey to the northwest. The majority-Armenian populated Nagorno-Karabakh region in the southwest of Azerbaijan declared itself independent from Azerbaijan in 1991, but it is not diplomatically recognised by any nation and is still considered a "de-jure" part of Azerbaijan. Azerbaijan, a nation with a majority Turkic and Shi‘ite Muslim population, is a secular and a unitary republic with an ancient and historic cultural heritage. Azerbaijan was the first successful attempt to establish a democratic and secular republic in the Muslim world. Azerbaijan is one of the founder members of GUAM and the Organisation for the Prohibition of Chemical Weapons, and joined the Commonwealth of Independent States in September 1993. A Special Envoy of the European Commission is present in the country, which is also a member of the United Nations, the OSCE, the Council of Europe, and the NATO Partnership for Peace (PfP) program.
The name of Azerbaijan derives from "Atropates", a satrap of Persia under the Achaemenid Empire, that was later reinstated as the satrap of Media under Alexander of Macedonia. The original etymology of this name is thought to have its roots in the ancient Iranian religion of Zoroastrianism. In Avestan Frawardin Yasht ("Hymn to the Guardian Angels"), there is a mention of "âterepâtahe ashaonô fravashîm ýazamaide", which literally translates from Old Persian as "we worship the Fravashi of the holy Atare-pata". Atropates ruled over the region of Atropatene (present-day Iranian Azerbaijan). The name "Atropates" itself is the Greek transliteration of an Old-Iranian, probably Median, compounded name with the meaning "Protected by the (Holy) Fire". The Greek name is mentioned by Diodorus Siculus and Strabo, and it is continued as "ādurbādagān" in the Pahlavi geographical text Shahrestānihā i Erānshahr. The word is translatable as both "the treasury" and "the treasurer" of fire in Modern Persian.
The earliest evidence of human settlement in the territory of Azerbaijan dates to the late Stone Age and is related to the Guruchay culture of the Azykh Cave, where archeological evidences promoted the inclusion of Azerbaijan into the map of the ascent man sites of Europe. The Upper Paleolithic and late Bronze Age cultures are attested in the caves of Tağılar, Damcılı, Zar, Yataq-yeri and in the necropolises of Leylatepe and Saraytepe. The area was conquered by the Achaemenids around 550 B.C.E., leading to the spread of Zoroastrianism. Later it became part of Alexander the Great's Empire and its successor Seleucid Empire. Caucasian Albanians, the original inhabitants of the area, established an independent kingdom around the fourth century B.C.E. Early Iranian settlements included the Scythians in the ninth century BC. Following the Scythians, Iranian Medes came to dominate the area to the south of the Aras. The Medes forged a vast empire between 900–700 BC, which was integrated into the Achaemenids Empire around 550 BC. During this period, Zoroastrianism spread in the Caucasus and Atropatene. Ancient Azaris spoke Ancient Azari language, which belonged to Iranian branch of Indo-European languages.
In 252 C.E., the Sassanids turned it into a vassal state, while King Urnayr officially adopted Christianity as the state religion in the fourth century. Despite numerous conquests by the Sassanids and Byzantines, Albania remained an entity in the region until the ninth century. The Islamic Umayyad Caliphate repulsed both the Sassanids and Byzantines from the region and turned Caucasian Albania into a vassal state after the Christian resistance, led by Prince Javanshir, was suppressed in 667. The power vacuum left by the decline of the Abbasid Caliphate was filled by numerous local dynasties such as the Sallarids, Sajids, Shaddadids, Rawadids and Buyids. At the beginning of the eleventh century, the territory was gradually seized by waves of Turkic Oghuz tribes from Central Asia. The first of these Turkic dynasties was the Ghaznavids, which entered the area now known as Azerbaijan by 1030. Turkification of Azaris was completed only By the late 1800s. The old Iranic speakers found solely in tiny isolated recesses of the mountains or other remote areas (such as Harzand, Galin Guya, Shahrud villages in Khalkhal and Anarjan). Today, this Turkic speaking population is also known as Azeris. Locally, the possessions of the subsequent Seljuq Empire were ruled by atabegs, who were technically vassals of the Seljuq sultans, being sometimes de facto rulers themselves. Under the Seljuq Turks, local poets such as Nizami Ganjavi and Khagani Shirvani gave rise to a blossoming of Persian literature on the territory of present-day Azerbaijan. The next ruling state of the Jalayirids was short-lived and fell under the conquests of Timur. The local dynasty of Shirvanshahs became a vassal state of Timur's Empire and assisted him in his war with the ruler of the Golden Horde Tokhtamysh. Following Timur's death two independent and rival states emerged: Kara Koyunlu and Ak Koyunlu. The Shirvanshahs returned, maintaining a high degree of autonomy as local rulers and vassals from 861 until 1539. During their persecution by the Safavids, the last dynasty imposed Shia Islam upon the formerly Sunni population, as it was battling against the Sunni Ottoman Empire.
After the Safavids, the area was ruled by the Iranian dynasties of Afshar and Zand and briefly by the Qajars. However, while under Persian sovereignty de facto self-ruling khanates emerged in the area, especially following the collapse of the Zand dynasty and in the early Qajar era. The brief and successful Russian campaign of 1812 was concluded with the Treaty of Gulistan, in which the shah's claims to some of the Khanates of the Caucasus were dismissed by Russia on the ground that they had been de facto independent long before their Russian occupation. The khanates exercised control over their affairs via international trade route between Central Asia and the West. Engaged in constant warfare, these khanates were eventually incorporated into the Russian Empire in 1813, following two Russo-Persian Wars. The area to the North of the river Arax, amongst which the territory of the contemporary republic of Azerbaijan were Iranian territory until they were occupied by Russia. Under the Treaty of Turkmenchay, Persia recognized Russian sovereignty over the Erivan Khanate, the Nakhchivan Khanate and the remainder of the Lankaran Khanate.
After the collapse of the Russian Empire during World War I, Azerbaijan, together with Armenia and Georgia became part of the short-lived Transcaucasian Democratic Federative Republic. When the republic dissolved in May 1918, Azerbaijan declared independence as the Azerbaijan Democratic Republic (ADR). The ADR was the first modern parliamentary republic in the Muslim World. Among the important accomplishments of the Parliament was the extension of suffrage to women, making Azerbaijan the first Muslim nation to grant women equal political rights with men. In this accomplishment, Azerbaijan also preceded the United Kingdom and the United States. Another important accomplishment of ADR was the establishment of Baku State University, which was the first modern-type university founded in Muslim East. By March 1920, it was obvious that Soviet Russia would attack the much-needed Baku. Vladimir Lenin said that the invasion was justified as Soviet Russia could not survive without Baku oil. Independent Azerbajian lasted only 23 months until the Bolshevik 11th Soviet Red Army invaded it and establishing the Azerbaijan SSR on April 28, 1920. Although the bulk of the newly formed Azerbaijani army was engaged in putting down an Armenian revolt that had just broken out in Karabakh, Azeris did not surrender their brief independence of 1918–20 quickly or easily. As many as 20,000 Azerbaijani soldiers died resisting what was effectively a Russian reconquest. Despite existing for only two short years, the multi party Azerbaijani Parliamentary republic and the coalition governments managed to achieve a number of measures on national and state building, education, creation of an army, independent financial and economic systems, international recognition of the ADR as a de facto state pending de jure recognition, official recognitions and diplomatic relations with a number of states, and preparing of a Constitution, equal rights for all. This has laid an important foundation for the re-establishment of independence in 1991.
In October 13, 1921, the Soviet republics of Russia, Armenia, Azerbaijan, and Georgia signed an agreement with Turkey known as the Treaty of Kars. The previously independent Naxicivan SSR would also become autonomous ASSR within Azerbaijan by the treaty of Kars. On the other hand, Armenia was awarded the region of Zhangezur and Turkey agreed to return Alexandropol (Gymri). In March 12, 1922, under heavy pressure from Moscow, the leaders of Azerbaijan, Armenian, and Georgian Soviet Socialist Republics established a union known as the Transcaucasian SFSR. This was the first attempt at a union of Soviet republics, preceding the USSR. The Union Council of TSFSR consisted of the representatives of the three republics – Nariman Narimanov (Azerbaijan), Polikarp Mdivani (Georgia), and Aleksandr Fyodorovich Miasnikyan (Armenia). The First Secretary of the Transcaucasian Communist Party was Sergo Ordzhonikidze. In 1936, TSFSR was dissolved and Azerbaijan SSR became one of the constituent member states of the Soviet Union. During World War II, Azerbaijan played a crucial role in the strategic energy policy of Soviet Union, much of the Soviet Union's oil on the Eastern Front was supplied by Baku. By the Decree of the Supreme Soviet of the USSR in February 1942, the commitment of more than 500 workers and employees of the oil industry of Azerbaijan was awarded orders and medals. Operation Edelweiss carried out by the German Wehrmacht targeted Baku because of its importance as the energy (petroleum) dynamo of the USSR. Some 800,000 Azerbaijanis fought well in the ranks of the Soviet Army of which 400,000 died and Azeri Major-General Azi Aslanov was awarded twice Hero of the Soviet Union.
Following the politics of "glasnost", initiated by Mikhail Gorbachev, civil unrest and ethnic strife grew in various regions of the Soviet Union, including Nagorno-Karabakh, a region of the Azerbaijan SSR. The disturbances in Azerbaijan, in response to Moscow's indifference to already heated conflict, resulted in calls for independence and secession, then led to the Pogrom of Armenians in Baku, and subsequently culminated in the events of Black January in Baku. At this time, Ayaz Mütallibov was appointed as the First Secretary of the Azerbaijan Communist Party. Later in 1990, the Supreme Council of the Azerbaijan SSR dropped the words "Soviet Socialist" from the title, adopted the Declaration of Sovereignty of the Azerbaijan Republic and restored the modified flag of the Azerbaijan Democratic Republic as a state flag. On 8 September 1991, Ayaz Mütallibov was elected president in nationwide elections in which he was the only candidate. On 18 October 1991, the Supreme Council of Azerbaijan adopted a Declaration of Independence which was affirmed by a nationwide referendum in December 1991, when the Soviet Union was officially dissolved. The early years of independence were overshadowed by the Nagorno-Karabakh War with neighboring Armenia. By the end of hostilities in 1994, Azerbaijan lost control of up to 16% of its territory, including Nagorno-Karabakh itself. An estimated 30,000 people had been killed and more than a million had been displaced. Four United Nations Security Council Resolutions (822, 853, 874, and 884) called for "the withdrawal of occupying forces from occupied areas of the Azerbaijani Republic". In 1993, democratically elected president Abülfaz Elçibay was overthrown by a military insurrection led by Colonel Surat Huseynov, which resulted in the rise to power of the former leader of Soviet Azerbaijan, Heydar Aliyev. In 1994, Surat Huseynov, by that time a prime minister, attempted another military coup against Heydar Aliyev, but Huseynov was arrested and charged with treason. In 1995, another coup attempt against Aliyev, by the commander of the OMON Militsiya special unit, Rovshan Javadov, was averted, resulting in the killing of the latter and disbanding of Azerbaijan's OMON units. Although during his presidency Aliyev managed to reduce the country's unemployment, reined in criminal groups, established the fundamental institutions of independent statehood, and brought stability, peace and major foreign investment, the country was tainted by rampant corruption in the governing bureaucracy. In October 1998, Aliyev was reelected for a second term. Despite the much improved economy, particularly with the exploitations of Azeri-Chirag-Guneshli oil field and Shah Deniz gas field, Aliyev's presidency became unpopular due to vote fraud, widespread corruption and objection to his autocratic regime. The same harsh criticism followed the elections of former Prime Minister Ilham Aliyev, the second leader of New Azerbaijan Party after the death of his father Heydar.
Azerbaijan is in the South Caucasus region of Eurasia, straddling Western Asia and Eastern Europe. Three physical features dominate Azerbaijan: the Caspian Sea, whose shoreline forms a natural boundary to the east; the Greater Caucasus mountain range to the north; and the extensive flatlands at the country's center. The total length of Azerbaijan's land borders is, of which 1007 are with Armenia, 756 with Iran, 480 with Georgia, 390 with Russia and 15 with Turkey. The coastline stretches for, and the length of the widest area of the Azerbaijani section of the Caspian Sea is. The territory of Azerbaijan extends from north to south, and from west to east. The three mountain ranges are the Greater and Lesser Caucasus, and the Talysh Mountains, together covering approximately 40% of the country. The highest peak of Azerbaijan is mount Bazardüzü (4,466 m), while the lowest point lies in the Caspian Sea (−28 m). Nearly half of all the mud volcanoes on Earth are concentrated in Azerbaijan. The main water sources are the surface waters. However, only 24 of the 8,350 rivers are greater than in length. All the rivers drain into the Caspian Sea in the east of the country. The largest lake is Sarysu (67 km²), and the longest river is Kur (1,515 km), which is transboundary. Azerbaijan's four main islands in the Caspian Sea have a combined area of over thirty square kilometres.
Azerbaijan is home to a vast variety of landscapes. Over half of Azerbaijan's land mass consists of mountain ridges, crests, yailas and plateaus which rise up to hypsometric levels of 400–1000 meters (including the Middle and Lower lowlands), in some places (Talis, Jeyranchol-Ajinohur and Langabiz-Alat foreranges) up to 100–120 metres, and others from 0 – 50 meters and up (Qobustan, Absheron). The rest of Azerbaijan's terrain consist of plains and lowlands. Hypsometric marks within the Caucasus region vary from about −28 metres at the Caspian Sea shoreline up to 4466 metres, (Bazardüzü peak).
The formation of climate in Azerbaijan is influenced particularly by cold arctic air masses of Scandinavian anticyclone, temperate of Siberian anticyclone, and Central Asian anticyclone. Azerbaijan's diverse landscape affects the ways air masses enter the country. The Greater Caucasus protects the country from direct influences of cold air masses coming from the north. That leads to the formation of subtropical climate on most foothills and plains of the country. Meanwhile, plains and foothills are characterized by high solar radiation rates. Nine out of eleven existing climate zones are present in Azerbaijan. Both the absolute minimum temperature () and the absolute maximum temperature () were observed in Julfa and Ordubad. The maximum annual precipitation falls in Lankaran (1,600 to 1,800 mm) and the minimum in Absheron (200 to 350 mm).
The first reports on the richness and diversity of animal life in Azerbaijan can be found in travel notes of Eastern travelers. Animal carvings on architectural monuments, ancient rocks and stones survived up to the present times. The first information on the animal kingdom of Azerbaijan was collected during the visits of naturalists to Azerbaijan in 17th century. Unlike fauna, the concept of animal kingdom covers not only the types of animals, but also the number of individual species. There are 106 species of mammals, 97 species of fish, 363 species of birds, 10 species of amphibians and 52 species of reptiles which have been recorded and classified in Azerbaijan. The symbol of Fauna in Azerbaijan is the Karabakh horse which is a mountain-steppe racing and riding horse which can only be found in Azerbaijan. The Karabakh horse has a reputation for its good temper, speed, elegance and intelligence. It is one of the oldest breeds, with ancestry dating to the ancient world. The horse was originally developed in the Azerbaijani Karabakh region in the 5th century and is named after it.
Rivers and lakes form the principal part of the water systems of Azerbaijan, they were formed over a long geological timeframe and changed significantly throughout that period. This is particularly evidenced by remnants of ancient rivers found throughout the country. The country's water systems are continually changing under the influence of natural forces and human introduced industrial activities. Artificial rivers (canals) and ponds are a part of Azerbaijan's water systems. There are 8,359 rivers of various lengths within Azerbaijan. Of them 8,188 rivers are less than 25 kilometers in length. Only 24 rivers are over 100 kilometers long. The Kura and Aras are the most popular rivers in Azerbaijan, they run through the Kura-Aras Lowland. The rivers that directly flow into the Caspian Sea, originate mainly from the north-eastern slope of the Major Caucasus and Talysh Mountains and run along the Samur-Devechi and Lenkeran lowlands. From the water supply point, Azerbaijan is below the average in the world with approximately 100,000 m³/year of water per km². All big water reservoirs are built on Kur. The hydrography of Azerbaijan basically belongs to the Caspian Sea basin.
Since the independence of Azerbaijan in 1991, the Azerbaijani government has taken drastic measures to preserve the environment of Azerbaijan. But national protection of the environment started to truly improve after 2001 when the state budget increased due to new revenues provided by the Baku-Tbilisi-Ceyhan pipeline. Within four years protected areas doubled and now make up eight percent of the country's territory. Since 2001 the government has set up seven large reserves and almost doubled the sector of the budget earmarked for environmental protection.
Azerbaijan is divided into 59 rayons ("rayonlar", singular "rayon"), 11 city districts ("şəhərlər", singular "şəhər"), and one autonomous republic ("muxtar respublika") of Nakhchivan, which subdivides into 7 rayons and a city. The President of Azerbaijan appoints the governors of these units, while the government of Nakhchivan is elected and approved by the parliament of Nakhchivan Autonomous Republic.
The structural formation of Azerbaijan's political system was completed by the adoption of the new Constitution on 12 November 1995. According to the Article 23 of Constitution, the state symbols of the Azerbaijan Republic are the flag, the coat of arms and the national anthem. The state power in Azerbaijan is limited only by law for internal issues, but for international affairs is additionally limited by the provisions of international agreements. The government of Azerbaijan is based on the separation of powers among the legislative, executive and judicial branches. The legislative power is held by the unicameral National Assembly and the Supreme National Assembly in the Nakhchivan Autonomous Republic. Parliamentary elections are held every five years, on the first Sunday of November. The accuracy of the election results is checked and confirmed by the Constitutional Court. The laws enacted by the National Assembly, unless specified otherwise, go into effect on the day of their publication. The executive power is held by the President, who is elected for a 5-year term by direct elections. The president is authorized to form the Cabinet, an inferior executive body, subordinated to him. The Cabinet of Azerbaijan consists primarily of the Prime Minister, his Deputies and Ministers. The president does not have the right to dissolve the National Assembly, but he has the right to veto its decisions. To override the presidential veto, the parliament must have a majority of 95 votes. The judicial power is vested in the Constitutional Court, Supreme Court and the Economic Court. The President nominates the judges in these courts. The Security Council is the deliberative body under the president, and he organizes it according to the Constitution. It was established on 10 April 1997. The administrative department is not a part of the president's office but manages the financial, technical and pecuniary activities of both the president and his office. Although Azerbaijan has held several elections since regaining its independence and it has many of the formal institutions of democracy, it remains classified as "not free" (on border with "partly free") in Freedom House's Freedom in the World 2009 survey.
The short-lived Azerbaijan Democratic Republic succeeded in establishing diplomatic relations with six countries, sending diplomatic representatives to Germany and Finland. The process of international recognition of Azerbaijan's independence from the collapsing Soviet Union lasted roughly one year. The most recent country to recognize Azerbaijan was Bahrain, on 6 November 1996. Full diplomatic relations, including mutual exchanges of missions, were first established with Turkey, Pakistan, the United States, Iran and Israel. Azerbaijan has diplomatic relations with 158 countries so far and holds membership in 38 international organizations. It holds observer status in the Non-Aligned Movement and World Trade Organization and is a correspondent at the International Telecommunication Union. The Azerbaijani diaspora is found in 36 countries, and in turn there are dozens of centers for ethnic minorities inside Azerbaijan, including the (German cultural society "Karelhaus", Slavic cultural center, Azerbaijani-Israeli community, Kurdish cultural center, International Talysh Association, Lezgin national center "Samur", Azerbaijani-Tatar community, Crimean Tatars society, etc.). On 9 May 2006 Azerbaijan was elected to membership in the newly established Human Rights Council by the United Nations General Assembly. The term of office began on 19 June 2006. Foreign policy priorities of Azerbaijan include: first of all, the restoration of its territorial integrity; elimination of the consequences of the loss of Nagorno-Karabakh and seven other regions of Azerbaijan; development of good-neighbourly and mutually advantageous relations with neighbouring countries; promotion of security and stability in the region; integration into European and Transatlantic security and cooperation structures; and promotion of transregional economic, energy and transportation projects. The Azeri Government, in late 2007, stated that the long-standing dispute over the Armenian-occupied territory of Nagorno-Karabakh is almost certain to spark a new war if it remains unresolved. The Government is in the process of increasing its military budget, as its oil and gas revenues bring a torrent of cash into its coffers. Furthermore, economic sanctions by Turkey to the west and by Azerbaijan itself to the east have combined to greatly erode Armenia's economy, leading to steep increases in prices for basic commodities and a great decline in the Armenian state revenues. Azerbaijan is an active member of international coalitions fighting international terrorism. The country is contributing to peacekeeping efforts in Kosovo, Afghanistan and Iraq. Azerbaijan is an active member of NATO's “Partnership for Peace” program. It also maintains good relations with the European Union and could potentially one day apply for membership.
The history of the modern Azerbaijan army dates back to Azerbaijan Democratic Republic in 1918, when the National Army of the newly formed Azerbaijan Democratic Republic was created on 26 June 1918. When Azerbaijan gained independence after the collapse of the Soviet Union, the Armed Forces of the Republic of Azerbaijan were created according to the Law on the Armed Forces of 9 October 1991. The original date of the establishment of the short-lived National Army is celebrated as Army Day (26 June) in today's Azerbaijan. Initially, the equipment and facilities of Azerbaijan's army were those of the Soviet 4th Army. The Armed Forces have three branches, according to the CIA World Fact Book: Land Forces, Air Force and Air Defence Force (a united branch), Navy. Besides the Armed Forces there are several military sub-groups that can be involved in state defence when needed. These are the Internal Troops of the Ministry of Internal Affairs and forces of the State Border Service, which includes the Coast Guard as well. The Azerbaijan National Guard is a further paramilitary force. It operates as a semi-independent entity of the Special State Protection Service, an agency subordinate to the President. Azerbaijan adheres to the Treaty on Conventional Armed Forces in Europe and has signed all major international arms and weapons treaties. Azerbaijan closely cooperates with NATO in programs such as Partnership for Peace and Individual Partnership Action Plan. Azerbaijan has deployed 151 of its Peacekeeping Forces in Iraq and another 184 in Afghanistan. The military expenditures of Azerbaijan for 2009 are set at $2.46 billion USD. Azerbaijan has its own Defense Industry, which manufactures small arms, artillery systems, tanks, armors and noctovision devices, aviation bombs, pilotless vehicles, various military vehicles and military planes and helicopters. Azerbaijan's Armed Forces have a training cooperation partnership with the Oklahoma Army National Guard.
After gaining independence in 1991, Azerbaijan became a member of the International Monetary Fund, the World Bank, the European Bank for Reconstruction and Development, the Islamic Development Bank and the Asian Development Bank. The banking system of Azerbaijan consists of the Central Bank of Azerbaijan, commercial banks and non-banking credit organizations. The National (now Central) Bank was created in 1992 based on the Azerbaijan State Savings Bank, an affiliate of the former State Savings Bank of the USSR. The Central Bank serves as Azerbaijan's central bank, empowered to issue the national currency, the Azerbaijani manat, and to supervise all commercial banks. Two major commercial banks are the state-owned International Bank of Azerbaijan and the United Universal Joint-Stock Bank. Pushed up by spending and demand growth, the 2007 Q1 inflation rate reached 16.6%. Nominal incomes and monthly wages climbed 29% and 25% respectively against this figure, but price increases in non-oil industry encouraged inflation in the country. Azerbaijan shows some signs of the so-called "Dutch disease" because of the fast growing energy sector, which causes inflation and makes non-energy exports more expensive. Two thirds of Azerbaijan is rich in oil and natural gas. The region of the Lesser Caucasus accounts for most of the country's gold, silver, iron, copper, titanium, chromium, manganese, cobalt, molybdenum, complex ore and antimony. In September 1994, a 30-year contract was signed between the State Oil Company of Azerbaijan Republic (SOCAR) and 13 oil companies, among them Amoco, BP, Exxon, LUKoil and Statoil. As Western oil companies are able to tap deepwater oilfields untouched by the Soviet exploitation, Azerbaijan is considered one of the most important spots in the world for oil exploration and development. Meanwhile the State Oil Fund was established as an extra-budgetary fund to ensure the macroeconomic stability, transparency in the management of oil revenue, and the safeguarding of resources for future generations. At the beginning of 2007 there were 4,755,100 hectares of utilized agricultural area. In the same year the total wood resources counted 136 million m³. Azerbaijan's agricultural scientific research institutes are focused on meadows and pastures, horticulture and subtropical crops, green vegetables, viticulture and wine-making, cotton growing and medicinal plants. In some lands it is profitable to grow grain, potatoes, sugar beets, cotton and tobacco. The Caspian fishing industry is concentrated on the dwindling stocks of sturgeon and beluga. In 2002 the Azerbaijani merchant marine had 54 ships. Some portions of most products that were previously imported from abroad have begun to be produced locally (among them are Coca Cola by Coca Cola Bottlers LTD, beer by Baki-Kastel, parquet by Nehir and oil pipes by EUPEC Pipe Coating Azerbaijan). Azerbaijan is also an important economic hub in the transportation of raw materials. The Baku-Tbilisi-Ceyhan pipeline (BTC) became operational in May 2006 and extends more than 1,774 kilometers through the territories of Azerbaijan (440 km), Georgia (260 km) and Turkey (1114 km). The BTC is designed to transport up to 50 million tons of crude oil annually and carries oil from the Caspian Sea oilfields to global markets. The South Caucasus Pipeline, also stretching through the territory of Azerbaijan, Georgia and Turkey, became operational at the end of 2006 and offers additional gas supplies to the European market from the Shah Deniz gas field. It is expected to produce up to 296 billion cubic metres of natural gas per year. Azerbaijan also plays a major role in the EU-sponsored Silk Road Project. Azeriqaz, a sub-company of the State Oil Company of Azerbaijan, intends to ensure full gasification of the country by 2021.
Tourism is an important part of the economy of Azerbaijan. The country's large abundance of natural and cultural attractions make it an attractive destination of visitors. The country was a well-known tourist spot in the 1980s, yet, the Nagorno-Karabakh War during the 1990s crippled the tourist industry and negatively impacted the image of Azerbaijan as a tourist destination. It was not until 2000s that the tourism industry began to recover, and the country has since experienced a high rate of growth in the number of tourist visits and overnight stays. The Government of Azerbaijan has set the development of Azerbaijan as an elite tourist destination a top priority. It is a national strategy to make tourism a major, if not the single largest, contributor to the Azerbaijani economy.
The convenient location of Azerbaijan on the crossroad of major international traffic arteries, such as the Silk Road and the South-North corridor, highlights the strategic importance of transportation sector for the country’s economy. In 2002 the Azerbaijani government established the Ministry of Transport with a broad range of policy and regulatory functions. In the same year, the country became a member of the Vienna Convention on Road Traffic. The highest priority being; upgrading the transport network and transforming transportation services into one of the key comparative advantages of the country, as this would be highly conducive to the development of other sectors of the economy. Broad gauge railways in 2005 stretched for and electrified railways numbered. By 2006, there were 36 airports and one heliport. The transport sector in Azerbaijan includes roads, railways, aviation, and maritime transport. The economy of Azerbaijan has been markedly stronger in recent years and, not surprisingly, the country has been making progress in developing its telecoms sector. Nonetheless, it still faces problems. These include poor infrastructure and an immature telecom regulatory regime. The Ministry of Communications & Information Technologies (MCIT), as well as being an operator through its role in Aztelekom, is both a policy-maker and regulator. A boom in oil and gas exports has boosted the economy, reducing the country’s dependence on international aid. In 2002 Azerbaijan led the way in per capita mobile phone use within the CIS. Public pay phones are available for local calls and require the purchase of a token from the telephone exchange or some shops and kiosks. Tokens allow a call of indefinite duration. As of 2005, there were 1,091,400 main telephone lines and 1,036,000 internet users. There are three GSM: Azerfon (Nar Mobile), Bakcell and Azercell mobile network operators and one CDMA.
From the total population of about 8 million people as of April 2006, there were 4,380,000 (nearly 51%) city dwellers and a rural population of 4,060,000 (49%). 51% of the total population were female. The sex ratio for total population in that year was therefore 0.94 males per female. The 2006 population growth rate was 0.66%, compared to 1.14% worldwide. A significant factor restricting the population growth is rather a high level of migration. As many as 3 million Azeris, many of them guest workers, live in Russia. In 2006 Azerbaijan saw migration of −4.38/1,000 persons. The highest morbidity in 2005 was from respiratory diseases (806.9 diseases per 10,000 of total population). In 2005, the highest morbidity for infectious and parasitic diseases was noted among influenza and acute respiratory infections (4168,2 per 100,000 population). 2007 estimate for total life expectancy is 66 years, 70.7 years for women and 61.9 for men. With 800,000 refugees and IDPs, Azerbaijan has the largest internally displaced population in the region, and, as of 2006, had the highest per capita IDP population in the world. The ethnic composition of the population according to the 1999 population census: 90.6% Azeris, 2.2% Lezgins, 1.8% Russians, 1.5% Armenians (Almost all live in the break-away region of Nagorno-Karabakh), 1.0% Talysh (disputed as too low by Talysh nationalists), 0.6% Avars, 0.5% Turks, 0.4% Tatars, 0.4% Ukrainians, 0.2% Tsakhur, 0.2% Georgians, 0.13% Kurds, 0.13% Tats, 0.1% Jews, 0.05% Udins, other 0.2%. Many Russians left Azerbaijan during the 1990s. According to the 1989 census, there were 392,000 ethnic Russians in Azerbaijan, or 5.6% of the population. According to the statistics, about 390,000 Armenians lived in Azerbaijan in 1989. Although Azerbaijani (also called "Azeri") is the most widely spoken language in the country and is spoken by about a quarter of the population of Iran, there are 13 other languages spoken natively in the country. Some of these languages are very small communities, others are more vital. Azerbaijani is a Turkic language which belongs to the Altaic family and is mutually intelligible with Turkish. The language is written with a modified Latin alphabet today, but was earlier written in the Arabic alphabet (until 1929), in the Uniform Turkic Alphabet (1929–1939), and in the Cyrillic alphabet (1939–1991). The changes in alphabet have been largely molded by religious and political forces. Iranian Azeris are the largest minority in Iran. The CIA World Factbook estimates Iranian Azeris as comprising nearly 16 million, or 24% of Iran's population.
Approximately 95% of the population of Azerbaijan is Muslim. There are many other faiths practiced among the different ethnic groups within the country. By article 48 of its Constitution, Azerbaijan is a secular state and ensures religious freedom. Of the nation's religious minorites, Christians comprise 3% to 4% of the population, of whom most are Russian, Georgian and Armenian Orthodox (Almost all Armenians live in the break-away region of Nagorno-Karabakh). In 2003 there were 250 Roman Catholics. Other Christian denominations as of 2002 include Lutherans, Baptists and Molokans. There are also Jewish, Bahá'í, Hare Krishna and Jehovah's Witnesses communities, as well as adherents of the Nehemiah Church, Star in the East Church and the Cathedral of Praise Church. Zoroastrianism had a long history in Azerbaijan, evident in sites such as the Fire Temple of Baku, and along with Manichean. It is estimated that the Zoroastrian community of Azerbaijan numbers around 2,000. According to the recent Gallup Poll Azerbaijan is one of the most irreligious countries in the world with about 50% of respondents indicating the importance of religion in their life as little or none. Even so, religious tolerance has been threatened in Azerbaijan, though it continues a signatory to the Convention for the Protection of Human Rights and Fundamental Freedoms. A number of nationals who are Jehovah's Witnesses have been harassed, detained, jailed and in some cases physically assaulted by police because of their religious activity. Jehovah's Witnesses are entitled to protection of freedom of religion under Articles 9, 10, and 11 of the aforementioned Convention. In some cases the defendants have been cleared of all charges.
Azerbaijani culture has developed as a result of many influences. Today, Western influences, including globalized consumer culture, are strong. Azerbaijan folk consists of Azerbaijanis, the representative part of society, as well as of nations and ethnic groups, compactly living in various areas of the country. Azerbaijani national and traditional dresses are the Chokha and Papakhi. There are radio broadcasts in Russian, Armenian, Georgian, Kurdish, Lezgin and Talysh languages, which are financed from the state budget. Some local radio stations in Balakən and Xaçmaz organize broadcasts in Avar and Tat. In Baku several newspapers are published in Russian, Kurdish ("Dengi Kurd"), Lezgin ("Samur") and Talysh languages. Jewish society "Sokhnut" publishes the newspaper "Aziz".
Azerbaijani architecture typically combines elements of East and West. Many ancient architectural treasures such as the Maiden Tower and Palace of the Shirvanshahs in the Walled City of Baku survive in modern Azerbaijan. Entries submitted on the UNESCO World Heritage tentative list include the Gobustan State Reserve, the Fire Temple of Baku, the Momine Khatun Mausoleum and the Palace of Shaki Khans in Shaki. Among other medieval architectural treasures reflecting the influence of several schools are the Shirvan Shahs' palace in Baku, the palace of the Shaki Khan's in the town of Shaki in north-central Azerbaijan, the Surakhany Temple on the Absheron Peninsula, a number of bridges spanning the Aras River, and several mausoleums. In the nineteenth and early twentieth centuries, little monumental architecture was created, but distinctive residences were built in Baku and elsewhere. Among the most recent architectural monuments, the Baku subways are noted for their lavish decor.
The film industry in Azerbaijan dates back to 1898. In fact, Azerbaijan was among the first countries involved in cinematography. When the Lumière brothers of France premiered their first motion picture footage in Paris on December 28, 1895, little did they know how rapidly it would ignite a new age of photographic documentation. These ingenuous brothers invented an apparatus, patented in February 1895, which they called the "Cinématographe" (from which the word "cinematography" is derived). It's not surprising that this apparatus soon showed up in Baku – at the turn of the 19th century, this bay town on the Caspian was producing more than 50 percent of the world's supply of oil. Just like today, the oil industry attracted foreigners eager to invest and to work. In 1919, during the Azerbaijan Democratic Republic, a documentary called The Celebration of the Anniversary of Azerbaijani Independence was filmed on Azerbaijan's independence day, May 28, and premiered in June 1919 at several theatres in Baku. After the Soviet power was established in 1920, Nariman Narimanov, Chairman of the Revolutionary Committee of Azerbaijan, signed a decree nationalizing Azerbaijan's cinema. In 1991, after Azerbaijan gained its independence from the Soviet Union, first Baku International Film Festival East-West was held in Baku.
Azerbaijani cuisine, throughout the centuries, has been influenced by the foods of different cultures due to political and economic processes in Azerbaijan. Still, today's Azerbaijani cuisine has distinctive and unique features. Many foods that are indigenous to the country can now be seen in the cuisines of other cultures. For the Azerbaijanis, food is an important part of the country's culture and is deeply rooted in the history, traditions and values of the nation. Azerbaijani cuisine is an important part of the country's culture. Climatic diversity and fertility of the land are reflected in the national dishes, which are based on fish from the Caspian Sea, local meat (mainly mutton and beef), and an abundance of seasonal vegetables and greens. Saffron-rice plov is the flagship food in Azerbaijan and black tea is the national beverage.
There are a number of Azerbaijani dances, these folk dances of the Azerbaijani people are old and extremely melodious. It is performed at formal celebrations and the dancers wear festival clothes or Chokha cloaks. It has a very fast rhythm, so the dancer must have inherent skill. Azerbaijan’s national dance shows the characteristics of the Azerbaijani nation. These dances differ from other dances with its quick temp and optimism. And this talks about nation’s braveness. The national clothes of Azerbaijan are well preserved within the national dances. Azerbaijan is a country where national traditions are well preserved. In Azerbaijan where are a lot of traditions. Novruz holiday (novruz is translated as "a new day") is the most ancient and cherished holiday of a New Year and spring. It is celebrated on the day of vernal equinox – March 21–22. Novruz is the symbol of nature renewal and fertility. Agrarian peoples of Middle East have been celebrating Novruz since ancient times.
The Azeris have a rich and distinctive culture, a major part of which is decorative and applied art. This form of art is represented by a wide range of handicrafts, such as chasing, jeweler, engraving in metal, carving in wood, stone and bone, carpet-making, lasing, pattern weaving and printing, knitting and embroidery. Each of these types of decorative art, evidence of the and endowments of the Azerbaijan nation, is very much in favor here. Many interesting facts pertaining to the development of arts and crafts in Azerbaijan were reported by numerous merchants, travelers and diplomats who had visited these places at different times.
Music of Azerbaijan builds on folk traditions that reach back nearly 1,000 years. For centuries Azerbaijani music has evolved under the badge of monody, producing rhythmically diverse melodies. Azerbaijani music has a branchy mode system, where chromatisation of major and minor scales is of great importance. According to The Grove Dictionary of Music and Musicians "In terms of ethnicity, culture and religion the Azeri are musically much closer to Iran than Turkey." "Mugham", "Meykhana" and "Ashik art" are one of the many musical traditions of Azerbaijan. Mugham is usually a suite with poetry and instrumental interludes. When performing Mugam, the singers have to transform their emotions into singing and music. Mugham singer Alim Qasimov is revered as one of the five best singers of all time. In contrast to the "mugam" traditions of Central Asian countries, Azeri "mugam" is more free-form and less rigid; it is often compared to the improvised field of jazz. UNESCO proclaimed the Azerbaijani "mugam" tradition a Masterpiece of the Oral and Intangible Heritage of Humanity on 7 November 2003. Meykhana is a kind of traditional Azeri distinctive folk unaccompanied song, usually performed by several people improvising on a particular subject. Among national musical instruments there are fourteen string instruments, eight percussion instruments and six wind instruments. Ashik is a mystic troubadour or traveling bard who sings and plays the saz. This tradition has its origin in the Shamanistic beliefs of ancient Turkic peoples. Ashiks' songs are semi-improvised around common bases. Azerbaijan’s ashik art was included in the list of Intangible Cultural Heritage by the UNESCO on September 30, 2009. Azerbaijan made its debut appearance at the Eurovision Song Contest 2008, and placed 8th among 43 contestants. The country's entry in the Eurovision Song Contest 2009 by AySel and Arash won the 3rd place.
Sport in Azerbaijan has ancient roots, and even now, both traditional and modern sports are still practiced. Freestyle wrestling has been traditionally regarded as Azerbaijan's national sport, however today, the most popular sports in Azerbaijan are football (soccer) and chess. Backgammon, a game that has ancient roots in Persian Empire, plays a major role in Azerbaijani culture. This game is very popular in Azerbaijan and is widely played among the local public. There are also different variations of backgammon developed and analysed by Azerbaijani experts. Azerbaijan is known as one of the chess superpowers; despite the collapse of the Soviet Union, chess is still extremely popular. Notable Azerbaijani chess players include Teimour Radjabov, Shahriyar Mammadyarov, Vladimir Makogonov, Gary Kasparov, Vugar Gashimov and Zeinab Mamedyarova. Azerbaijan has also hosted many international chess tournaments and competitions and became European Team Chess Championship winners in 2009. Futsal is another popular sport in Azerbaijan. Azerbaijan national futsal team reached the fourth place in 2010 UEFA Futsal Championship, while domestic club Araz Naxçivan also advanced to the semi-finals of UEFA Futsal Cup in 2010. Other Azerbaijani well-known athletes are Namig Abdullayev, Rovshan Bayramov, Mariya Stadnik and Farid Mansurov in wrestling, Ramil Guliyev in athletics, Elnur Mammadli and Movlud Miraliyev in judo, Rafael Aghayev in karate, Valeriya Korotenko and Natalya Mammadova in volleyball and K-1 fighter Zabit Samedov.
Amateur astronomy, also called backyard astronomy, is a hobby whose participants enjoy watching the night sky (and the day sky too, for sunspots, eclipses, etc.), and the plethora of objects found in it, mainly with portable telescopes and binoculars. Even though scientific research is not their main goal, many amateur astronomers make a contribution to astronomy by monitoring variable stars, tracking asteroids and discovering transient objects, such as comets. Such efforts are one of the relatively few ways interested amateurs can still make useful contributions to scientific knowledge.
The typical amateur astronomer is one who does not depend on the field of astronomy as a primary source of income or support, and does not have a professional degree or advanced academic training in the subject. Many amateurs are beginners, while others have a high degree of experience in astronomy and often assist and work alongside professional astronomers. Amateur astronomy is usually associated with viewing the night sky when most celestial objects and events are visible, but sometimes amateur astronomers also operate during the day for events such as sunspots and solar eclipses. Amateur astronomers often look at the sky using nothing more than their eyes, but common tools for amateur astronomy include portable telescopes and binoculars. People have studied the sky throughout history in an amateur framework, without any formal method of funding. It is only within about the past century, however, that amateur astronomy has become an activity clearly distinguished from professional astronomy, and other related activities.
Collectively, amateur astronomers observe a variety of celestial objects and phenomena. Common targets of amateur astronomers include the Moon, planets, stars, comets, meteor showers, and a variety of deep sky objects such as star clusters, galaxies, and nebulae. Many amateurs like to specialise in observing particular objects, types of objects, or types of events which interest them. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Astrophotography has become more popular for amateurs in recent times, as relatively sophisticated equipment, such as high quality CCD cameras, has become more affordable. Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. The pioneer of amateur radio astronomy was Karl Jansky who started observing the sky at radio wavelengths in the 1930s, and interest has increased over time. Non-visual amateur astronomy includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. Some amateur astronomers use home-made radio telescopes, while others use radio telescopes that were originally built for astronomy research but have since been made available for use by amateurs. The One-Mile Telescope is one such example.
Amateur astronomers use a range of instruments to study the sky, depending on a combination of their interests and resources. Methods include simply looking at the night sky with the naked eye, using binoculars, and using a variety of optical telescopes of varying power and quality, as well as additional sophisticated equipment, such as cameras, to study light from the sky in both the visual and non-visual parts of the spectrum. Commercial telescopes are available new and used, but in some places it is also common for amateur astronomers to build (or commission the building of) their own custom telescope. Some people even focus on amateur telescope making as their primary interest within the hobby of amateur astronomy. Although specialized and experienced amateur astronomers tend to acquire more specialized and more powerful equipment over time, relatively simple equipment is often preferred for certain tasks. Binoculars, for instance, although generally of lower power than the majority of telescopes, also tend to provide a wider field of view, which is preferable for looking at some objects in the night sky. Amateur astronomers also use star charts that, depending on experience and intentions, may range from simple planispheres through to detailed charts of very specific areas of the night sky. A range of astronomy software is available and used by amateur astronomers, including software that generates maps of the sky, software to assist with astrophotography, observation scheduling software, and software to perform various calculations pertaining to astronomical phenomena. Amateur astronomers often like to keep records of their observations, which usually takes the form of an observing log. Observing logs typically record details about which objects were observed and when, as well as describing the details that were seen. Sketching is sometimes used within logs, and photographic records of observations have also been used in recent times. The Internet is an essential tool of amateur astronomers. Almost all astronomy clubs, even those with very few members, have a web site. The popularity of CCD imaging among amateurs has led to large numbers of web sites being written by individuals about their images and equipment. Much of the social interaction of amateur astronomy occurs on mailing lists or discussion groups. Discussion group servers host numerous astronomy lists. A great deal of the commerce of amateur astronomy, the buying and selling of equipment, occurs online. Many amateurs use online tools to plan their nightly observing sessions using tools such as the Clear Sky Chart.
Star hopping is a method often used by amateur astronomers with low-tech equipment such as binoculars or a manually driven telescope. It involves the use of maps (or memory) to locate known landmark stars, and "hopping" between them, often with the aid of a finderscope. Because of its simplicity, star hopping is a very common method for finding objects that are close to naked-eye stars. More advanced methods of locating objects in the sky include telescope mounts with "setting circles", which assist with pointing telescopes to positions in the sky that are known to contain objects of interest, and "GOTO telescopes", which are fully automated telescopes that are capable of locating objects on demand (having first been calibrated).
Setting circles are angular measurement scales that can be placed on the two main rotation axes of some telescopes. Since the widespread adoption of digital setting circles, any classical engraved setting circle is now specifically identified as an "analog setting circle" (ASC). By knowing the coordinates of an object (usually given in equatorial coordinates), the telescope user can use the setting circle to align the telescope in the appropriate direction before looking through its eyepiece. A computerized setting circle is called a "digital setting circle" (DSC). Although digital setting circles can be used to display a telescope's RA and Dec coordinates, they are not simply a digital read-out of what can be seen on the telescope's analog setting circles. As with go-to telescopes, digital setting circle computers (commercial names include Argo Navis, Sky Commander, and NGC Max) contain databases of tens of thousands of celestial objects and projections of planet positions. To find an object, such as globular cluster NGC 6712, one does not need to look up the RA and Dec coordinates in a book, and then move the telescope to those numerical readings. Rather, the object is chosen from the database and arrow markers appear in the display which indicate the direction to move the telescope. The telescope is moved until the distance value reaches zero. When both the RA and Dec axes are thus "zeroed out", the object should be in the eyepiece. The user therefore does not have to go back and forth from some other database (such as a book or laptop) to match the desired object's listed coordinates to the coordinates on the telescope. However, many DSCs, and also go-to systems, can work in conjunction with laptop sky programs. Computerized systems provide the further advantage of computing coordinate precession. Traditional printed sources are subtitled by the "epoch" year, which refers to the positions of celestial objects at a given time to the nearest year (e.g., J2005, J2007). Most such printed sources have been updated for intervals of only about every fifty years (e.g., J1900, J1950, J2000). Computerized sources, on the other hand, are able to calculate the right ascension and declination of the "epoch of date" to the exact instant of observation.
GOTO telescopes have become more popular since the 1980s as technology has improved and prices have been reduced. With these computer-driven telescopes, the user typically enters the name of the item of interest and the mechanics of the telescope point the telescope towards that item automatically. They have several notable advantages for amateur astronomers intent on research. For example, GOTO telescopes tend to be faster for locating items of interest than star hopping, allowing more time for studying of the object. GOTO also allows manufacturers to add equatorial tracking to mechanically simpler alt-azimuth telescope mounts, allowing them to produce an over all less expensive product.
Scientific research is most often not the "main" goal for many amateur astronomers, unlike professional astronomy. Work of scientific merit is possible, however, and many amateurs successfully contribute to the knowledge base of professional astronomers. Astronomy is sometimes promoted as one of the few remaining sciences for which amateurs can still contribute useful data. To recognize this, the Astronomical Society of the Pacific annually gives Amateur Achievement Awards for significant contributions to astronomy by amateurs. The majority of scientific contributions by amateur astronomers are in the area of data collection. In particular, this applies where large numbers of amateur astronomers with small telescopes are more effective than the relatively small number of large telescopes that are available to professional astronomers. Several organizations, such as the, exist to help coordinate these contributions. Amateur astronomers often contribute toward activities such as monitoring the changes in brightness of variable stars, helping to track asteroids, and observing occultations to determine both the shape of asteroids and the shape of the terrain on the apparent edge of the Moon as seen from Earth. With more advanced equipment, but still cheap in comparison to professional setups, amateur astronomers can measure the light spectrum emitted from astronomical objects, which can yield high-quality scientific data if the measurements are performed with due care. A relatively recent role for amateur astronomers is searching for overlooked phenomena (e.g., Kreutz Sungrazers) in the vast libraries of digital images and other data captured by Earth and space based observatories, much of which is available over the Internet. In the past and present, amateur astronomers have played a major role in discovering new comets. Recently however, funding of projects such as the Lincoln Near-Earth Asteroid Research and Near Earth Asteroid Tracking projects has meant that "most" comets are now discovered by automated systems, long before it is possible for amateurs to see them.
There is a large number of amateur astronomical societies around the world that serve as a meeting point for those interested in amateur astronomy, whether they be people who are actively interested in observing or "armchair astronomers" who may be simply interested in the topic. Societies range widely in their goals, depending on a variety of factors such as geographic spread, local circumstances, size, and membership. For instance, a local society in the middle of a large city may have regular meetings with speakers, focusing less on observing the night sky if the membership is less able to observe due to factors such as light pollution. It is common for local societies to hold regular meetings, which may include activities such as star parties or presentations. Societies are also a meeting point for people with particular interests, such as amateur telescope making.
is a Japanese martial art developed by Morihei Ueshiba as a synthesis of his martial studies, philosophy, and religious beliefs. Aikido is often translated as "the Way of unifying (with) life energy" or as "the Way of harmonious spirit." Ueshiba's goal was to create an art that practitioners could use to defend themselves while also protecting their attacker from injury. Aikido is performed by blending with the motion of the attacker and redirecting the force of the attack rather than opposing it head-on. This requires very little physical energy, as the "aikidōka" (aikido practitioner) "leads" the attacker's momentum using entering and turning movements. The techniques are completed with various throws or joint locks. Aikido can be categorized under the general umbrella of grappling arts. Aikido derives mainly from the martial art of Daitō-ryū Aiki-jūjutsu, but began to diverge from it in the late 1920s, partly due to Ueshiba's involvement with the Ōmoto-kyō religion. Ueshiba's early students' documents bear the term "aiki-jūjutsu". Many of Ueshiba's senior students have different approaches to aikido, depending on when they studied with him. Today aikido is found all over the world in a number of styles, with broad ranges of interpretation and emphasis. However, they all share techniques learned from Ueshiba and most have concern for the well-being of the attacker.
The term 'aiki' does not readily appear in the Japanese language outside the scope of Budo. This has led to many possible interpretations of the word. 合　is mainly used in compounds to mean 'combine, unite, join together, meet' examples being 合同(combined/united)　合成(composition)　結合(unite/combine/join together)　連合(union/alliance/association)　統合(combine/unify)　合意(mutual agreement). As well as an idea of reciprocalality,　知り合う(to get to know one another)　話し合い(talk/discussion/negotiation) 待ち合わせる(meet by appointment). 気　is often used as a feeling as in 気がする('I feel', as in terms of thinking but with less cognitive reasoning) 気持ち(feeling/sensation) 気分(mood/morale). Also Energy or force.　電気(electricity) 磁気 (magnetism). The term connects the practice of aikido with the philosophical concept of "Tao", which can be found in martial arts such as judo and kendo, and in the more peaceful arts such as Japanese calligraphy (), flower arranging () and tea ceremony (). Therefore from a purely linguistic point of view, we could say Aikido is 'Way of combining forces'. The term refers to the martial arts principle or tactic of blending with an attacker's movements for the purpose of controlling their actions with minimal effort. One applies by understanding the rhythm and intent of the attacker to find the optimal position and timing to apply a counter-technique. This then is very similar to the principles expressed by Kano Jigoro, when he founded Judo. Of an interesting note, these kanji are identical to the Korean versions of the characters that form the word hapkido, a Korean martial art. Although there are no known direct connections between the two arts, it is suspected that the founders of both arts trained in Daitō-ryū Aiki-jūjutsu.
Aikido was created by Morihei Ueshiba (, 14 December 1883–26 April 1969), referred to by some aikido practitioners as ("Great Teacher"). Ueshiba envisioned aikido not only as the synthesis of his martial training, but also an expression of his personal philosophy of universal peace and reconciliation. During Ueshiba's lifetime and continuing today, aikido has evolved from the "koryū" (old-style martial arts) that Ueshiba studied into a wide variety of expressions by martial artists throughout the world.
Ueshiba developed aikido primarily during the late 1920s through the 1930s through the synthesis of the older martial arts that he had studied. The core martial art from which aikido derives is Daitō-ryū aiki-jūjutsu, which Ueshiba studied directly with Takeda Sokaku, the reviver of that art. Additionally, Ueshiba is known to have studied Tenjin Shin'yō-ryū with Tozawa Tokusaburō in Tokyo in 1901, Gotōha Yagyū Shingan-ryū under Nakai Masakatsu in Sakai from 1903 to 1908, and judo with Kiyoichi Takagi (, 1894–1972) in Tanabe in 1911. The art of Daitō-ryū is the primary technical influence on aikido. Along with empty-handed throwing and joint-locking techniques, Ueshiba incorporated training movements with weapons, such as those for the spear (), short staff (), and perhaps the. However, aikido derives much of its technical structure from the art of swordsmanship (). Ueshiba moved to Hokkaidō in 1912, and began studying under Takeda Sokaku in 1915. His official association with Daitō-ryū continued until 1937. However, during the latter part of that period, Ueshiba had already begun to distance himself from Takeda and the Daitō-ryū. At that time Ueshiba was referring to his martial art as "Aiki Budō". It is unclear exactly when Ueshiba began using the name "aikido", but it became the official name of the art in 1942 when the Greater Japan Martial Virtue Society () was engaged in a government sponsored reorganization and centralization of Japanese martial arts.
After Ueshiba left Hokkaidō in 1919, he met and was profoundly influenced by Onisaburo Deguchi, the spiritual leader of the Ōmoto-kyō religion (a neo-Shinto movement) in Ayabe. One of the primary features of Ōmoto-kyō is its emphasis on the attainment of utopia during one's life. This was a great influence on Ueshiba's martial arts philosophy of extending love and compassion especially to those who seek to harm others. Aikido demonstrates this philosophy in its emphasis on mastering martial arts so that one may receive an attack and harmlessly redirect it. In an ideal resolution, not only is the receiver unharmed, but so is the attacker. In addition to the effect on his spiritual growth, the connection with Deguchi gave Ueshiba entry to elite political and military circles as a martial artist. As a result of this exposure, he was able to attract not only financial backing but also gifted students. Several of these students would found their own styles of aikido.
Aikido was first brought to the rest of the world in 1951 by Minoru Mochizuki with a visit to France where he introduced aikido techniques to judo students. He was followed by Tadashi Abe in 1952 who came as the official Aikikai Hombu representative, remaining in France for seven years. Kenji Tomiki toured with a delegation of various martial arts through fifteen continental states of the United States in 1953. Later in that year, Koichi Tohei was sent by Aikikai Hombu to Hawaii, for a full year, where he set up several dojo. This was followed up by several further visits and is considered the formal introduction of aikido to the United States. The United Kingdom followed in 1955; Italy in 1964; Germany and Australia in 1965. Designated "Official Delegate for Europe and Africa" by Morihei Ueshiba, Masamichi Noro arrived in France in September 1961. Today there are aikido dojo available throughout the world.
The biggest aikido organisation is the Aikikai Foundation which remains under the control of the Ueshiba family. However, aikido has many styles, mostly formed by Morihei Ueshiba's major students. The earliest independent styles to emerge were Yoseikan Aikido, begun by Minoru Mochizuki in 1931, Yoshinkan Aikido founded by Gozo Shioda in 1955, and Shodokan Aikido, founded by Kenji Tomiki in 1967. The emergence of these styles pre-dated Ueshiba's death and did not cause any major upheavals when they were formalized. Shodokan Aikido, however, was controversial, since it introduced a unique rule-based competition that some felt was contrary to the spirit of aikido. After Ueshiba's death in 1969, two more major styles emerged. Significant controversy arose with the departure of the Aikikai Hombu Dojo's chief instructor Koichi Tohei, in 1974. Tohei left as a result of a disagreement with the son of the founder, Kisshomaru Ueshiba, who at that time headed the Aikikai Foundation. The disagreement was over the proper role of "ki" development in regular aikido training. After Tohei left, he formed his own style, called Shin Shin Toitsu Aikido, and the organization which governs it, the Ki Society ("Ki no Kenkyūkai"). A final major style evolved from Ueshiba's retirement in Iwama, Ibaraki, and the teaching methodology of long term student Morihiro Saito. It is unofficially referred to as the "Iwama style", and at one point a number of its followers formed a loose network of schools they called Iwama Ryu. Although Iwama style practitioners remained part of the Aikikai until Saito's death in 2002, followers of Saito subsequently split into two groups; one remaining with the Aikikai and the other forming the independent organization the Shinshin Aikishuren Kai, in 2004 around Saito's son Hitohiro Saito. Today, the major styles of aikido are each run by a separate governing organization, have their own in Japan, and have an international breadth.
In aikido, as in virtually all Japanese martial arts, there are both physical and mental aspects of training. The physical training in aikido is diverse, covering both general physical fitness and conditioning, as well as specific techniques. Because a substantial portion of any aikido curriculum consists of throws, the first thing most students learn is how to safely fall or roll. The specific techniques for attack include both strikes and grabs; the techniques for defense consist of throws and pins. After basic techniques are learned, students study freestyle defense against multiple opponents, and in certain styles, techniques with weapons.
Physical training goals pursued in conjunction with aikido include controlled relaxation, flexibility, and endurance, with less emphasis on strength training. In aikido, pushing or extending movements are much more common than pulling or contracting movements. This distinction can be applied to general fitness goals for the aikido practitioner. Certain anaerobic fitness activities, such as weight training, emphasize contracting movements. In aikido, specific muscles or muscle groups are not isolated and worked to improve tone, mass, and power. Aikido-related training emphasizes the use of coordinated whole-body movement and balance similar to yoga or pilates. For example, many dojo begin each class with, which may include stretching and break falls. Roles of "uke" and "tori". Aikido training is based primarily on two partners practicing pre-arranged forms ("kata") rather than freestyle practice. The basic pattern is for the receiver of the technique ("uke") to initiate an attack against the person who applies the technique - the 取り "tori", or "shite", (depending on aikido style) also referred to as ("nage" (when applying a throwing technique), who neutralises this attack with an aikido technique. Both halves of the technique, that of "uke" and that of "nage", are considered essential to aikido training. Both are studying aikido principles of blending and adaptation. "Nage" learns to blend with and control attacking energy, while "uke" learns to become calm and flexible in the disadvantageous, off-balance positions in which "nage" places them. This "receiving" of the technique is called "ukemi". "Uke" continuously seeks to regain balance and cover vulnerabilities (e.g., an exposed side), while "nage" uses position and timing to keep "uke" off-balance and vulnerable. In more advanced training, "uke" will sometimes apply to regain balance and pin or throw "nage". refers to the act of receiving a technique. Good "ukemi" involves a parry or breakfall that is used to avoid pain or injury, such as joint dislocations.
Aikido makes use of body movement ("tai sabaki") to blend with "uke". For example, an "entering" ("irimi") technique consists of movements inward towards "uke", while a technique uses a pivoting motion. Additionally, an technique takes place in front of "uke", whereas an technique takes place to his side; a technique is applied with motion to the front of "uke", and a version is applied with motion towards the rear of "uke", usually by incorporating a turning or pivoting motion. Finally, most techniques can be performed while in a seated posture ("seiza"). Techniques where both "uke" and "nage" are sitting are called "suwari-waza", and techniques performed with "uke" standing and "nage" sitting are called "hanmi handachi". Thus, from fewer than twenty basic techniques, there are thousands of possible implementations. For instance, "ikkyō" can be applied to an opponent moving forward with a strike (perhaps with an "ura" type of movement to redirect the incoming force), or to an opponent who has already struck and is now moving back to reestablish distance (perhaps an "omote-waza" version). Specific aikido "kata" are typically referred to with the formula "attack-technique(-modifier)". For instance, "katate-dori ikkyō" refers to any "ikkyō" technique executed when "uke" is holding one wrist. This could be further specified as "katate-dori ikkyō omote", referring to any forward-moving "ikkyō" technique from that grab. "Atemi" () are strikes (or feints) employed during an aikido technique. Some view "atemi" as attacks against "vital points" meant to cause damage in and of themselves. For instance, Gōzō Shioda described using "atemi" in a brawl to quickly down a gang's leader. Others consider "atemi", especially to the face, to be methods of distraction meant to enable other techniques. A strike, whether or not it is blocked, can startle the target and break his or her concentration. The target may also become unbalanced in attempting to avoid the blow, for example by jerking the head back, which may allow for an easier throw. Many sayings about "atemi" are attributed to Morihei Ueshiba, who considered them an essential element of technique.
Weapons training in aikido traditionally includes the short staff ("jō"), wooden sword ("bokken"), and knife ("tantō"). Today, some schools also incorporate firearms-disarming techniques. Both weapon-taking and weapon-retention are sometimes taught, to integrate armed and unarmed aspects, although some schools of aikido do not train with weapons at all. Others, such as the Iwama style of Morihiro Saito, usually spend substantial time with "bokken" and "jō", practised under the names "aiki-ken", and "aiki-jō", respectively. The founder developed much of empty handed aikido from traditional sword and spear movements, so the practice of these movements is generally for the purpose of giving insight into the origin of techniques and movements, as well as vital practice of these basic building blocks.
One feature of aikido is training to defend against multiple attackers, often called "taninzudori", or "taninzugake". Freestyle ("randori", or "jiyūwaza") practice with multiple attackers is a key part of most curricula and is required for the higher level ranks. "Randori" exercises a person's ability to intuitively perform techniques in an unstructured environment. Strategic choice of techniques, based on how they reposition the student relative to other attackers, is important in "randori" training. For instance, an "ura" technique might be used to neutralise the current attacker while turning to face attackers approaching from behind. In Shodokan Aikido, "randori" differs in that it is not performed with multiple persons with defined roles of defender and attacker, but between two people, where both participants attack, defend, and counter at will. In this respect it resembles judo "randori".
In applying a technique during training, it is the responsibility of "nage" to prevent injury to "uke" by employing a speed and force of application that is commensurate with their partner's proficiency in "ukemi". Injuries (especially those to the joints), when they do occur in aikido, are often the result of "nage" misjudging the ability of "uke" to receive the throw or pin. A study of injuries in the martial arts showed that while the type of injuries varied considerably from one art to the other, the differences in overall rates of injury were much less pronounced. Soft tissue injuries are one of the most common types of injuries found within aikido although a few deaths from repetitive "shihōnage" have been reported.
Aikido training is mental as well as physical, emphasizing the ability to relax the mind and body even under the stress of dangerous situations. This is necessary to enable the practitioner to perform the bold enter-and-blend movements that underlie aikido techniques, wherein an attack is met with confidence and directness. Morihei Ueshiba once remarked that one "must be willing to receive 99% of an opponent's attack and stare death in the face" in order to execute techniques without hesitation. As a martial art concerned not only with fighting proficiency but also with the betterment of daily life, this mental aspect is of key importance to aikido practitioners.
The most common criticism of aikido is that it suffers from a lack of realism in training. The attacks initiated by "uke" (and which "nage" must defend against) have been criticized as being "weak," "sloppy," and "little more than caricatures of an attack." Weak attacks from "uke" cause a conditioned response from "nage", and result in underdevelopment of the strength and conditioning needed for the safe and effective practice of both partners. To counteract this, some styles allow students to become less compliant over time but, in keeping with the core philosophies, this is after having demonstrated proficiency in being able to protect themselves and their training partners. Shodokan Aikido addresses the issue by practising in a competitive format. Such adaptations are debated between styles, with some maintaining that there is no need to adjust their methods because either the criticisms are unjustified, or that they are not training for self-defence or combat effectiveness, but spiritual, fitness or other reasons. Another criticism is that after the end of Ueshiba's seclusion in Iwama from 1942 to the mid 1950s, he increasingly emphasized the spiritual and philosophical aspects of aikido. As a result, strikes to vital points by "nage", entering ("irimi") and initiation of techniques by "nage", the distinction between "omote" (front side) and "ura" (back side) techniques, and the practice of weapons, were all deemphasized or eliminated from practice. Lack of training in these areas is thought to lead to an overall loss of effectiveness by some aikido practitioners. Alternately, there are some who criticize aikido practitioners for not placing enough importance on the spiritual practices emphasized by Ueshiba. The premise of this criticism is that "O-Sensei’s aikido was not a continuation and extension of the old and has a distinct discontinuity with past martial and philosophical concepts." That is, that aikido practitioners who focus on aikido's roots in traditional jujutsu or "kenjutsu" are diverging from what Ueshiba taught. Such critics urge practitioners to embrace the assertion that "[Ueshiba's] transcendence to the spiritual and universal reality was the fundamentals ["sic"] of the paradigm that he demonstrated."
The study of "ki" is a critical component of aikido, and its study defies categorization as either "physical" or "mental" training, as it encompasses both. The original "kanji" for "ki" was (shown right), and is a symbolic representation of a lid covering a pot full of rice; the "nourishing vapors" contained within are "ki". The character for "ki" is used in everyday Japanese terms, such as, or. "Ki" is most often understood as unified physical and mental intention, however in traditional martial arts it is often discussed as "life energy". Gōzō Shioda's Yoshinkan Aikido, considered one of the "hard styles," largely follows Ueshiba's teachings from before World War II, and surmises that the secret to "ki" lies in timing and the application of the whole body's strength to a single point. In later years, Ueshiba's application of "ki" in aikido took on a softer, more gentle feel. This was his Takemusu Aiki and many of his later students teach about "ki" from this perspective. Koichi Tohei's Ki Society centers almost exclusively around the study of the empirical (albeit subjective) experience of "ki" with students ranked separately in aikido techniques and "ki" development.
Aikido practitioners (commonly called "aikidōka" outside of Japan) generally progress by promotion through a series of "grades" ("kyū"), followed by a series of "degrees" ("dan"), pursuant to formal testing procedures. Most aikido organisations use only white and black belts to distinguish rank, but some use various belt colors. Testing requirements vary, so a particular rank in one organization is not always comparable or interchangeable with the rank of another. Some dojos do not allow students to take the test to obtain a dan unless they are 16 or older. The uniform worn for practicing aikido ("aikidōgi") is similar to the training uniform ("keikogi") used in most other modern martial arts; simple trousers and a wraparound jacket, usually white. Both thick ("judo-style"), and thin ("karate-style") cotton tops are used. Aikido-specific tops are also available with shorter sleeves which reach to just below the elbow. Most aikido systems also add a pair of wide pleated black or indigo trousers called a "hakama". In many styles its use is reserved for practitioners with black belt ("dan") ranks or for instructors, while others allow all practitioners or female practitioners to wear a "hakama" regardless of rank.
Art is the process or product of deliberately arranging elements in a way to affect the senses or emotions. It encompasses a diverse range of human activities, creations, and modes of expression, including music, literature, film, sculpture, and paintings. The meaning of art is explored in a branch of philosophy known as aesthetics. The definition and evaluation of art has become especially problematic since the early 20th century. Richard Wollheim distinguishes three approaches: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans. An object may be characterized by the intentions, or lack thereof, of its creator, regardless of its apparent purpose. A cup, which ostensibly can be used as a container, may be considered art if intended solely as an ornament, while a painting may be deemed craft if mass-produced. Traditionally, the term "art" was used to refer to any skill or mastery. This conception changed during the Romantic period, when art came to be seen as "a special faculty of the human mind to be classified with religion and science". Generally, art is made with the intention of stimulating thoughts and emotions. The nature of art has been described by Richard Wollheim as "one of the most elusive of the traditional problems of human culture". It has been defined as a vehicle for the expression or communication of emotions and ideas, a means for exploring and appreciating formal elements for their own sake, and as "mimesis" or representation. Leo Tolstoy identified art as a use of indirect means to communicate from one person to another. Benedetto Croce and R.G. Collingwood advanced the idealist view that art expresses emotions, and that the work of art therefore essentially exists in the mind of the creator. The theory of art as form has its roots in the philosophy of Immanuel Kant, and was developed in the early twentieth century by Roger Fry and Clive Bell. Art as "mimesis" or representation has deep roots in the philosophy of Aristotle. More recently, thinkers influenced by Martin Heidegger have interpreted art as the means by which a community develops for itself a medium for self-expression and interpretation.
Britannica Online defines "art" as "the use of skill and imagination in the creation of aesthetic objects, environments, or experiences that can be shared with others." By this definition of the word, artistic works have existed for almost as long as humankind: from early pre-historic art to contemporary art; however, some theories restrict the concept to modern Western societies. Adorno said in 1970, "It is now taken for granted that nothing which concerns art can be taken for granted any more: neither art itself, nor art in relationship to the whole, nor even the right of art to exist." The first and broadest sense of "art" is the one that has remained closest to the older Latin meaning, which roughly translates to "skill" or "craft." A few examples where this meaning proves very broad include "artifact", "artificial", "artifice", "medical arts", and "military arts". However, there are many other colloquial uses of the word, all with some relation to its etymology. The second and more recent sense of the word "art" is as an abbreviation for "creative art" or "fine art". Fine art means that a skill is being used to express the artist's creativity, or to engage the audience's aesthetic sensibilities, or to draw the audience towards consideration of the "finer" things. Often, if the skill is being used in a common or practical way, people will consider it a craft instead of art. Likewise, if the skill is being used in a commercial or industrial way, it will be considered Commercial art instead of fine art. On the other hand, crafts and design are sometimes considered applied art. Some art followers have argued that the difference between fine art and applied art has more to do with value judgments made about the art than any clear definitional difference. However, even fine art often has goals beyond pure creativity and self-expression. The purpose of works of art may be to communicate ideas, such as in politically-, spiritually-, or philosophically-motivated art; to create a sense of beauty (see aesthetics); to explore the nature of perception; for pleasure; or to generate strong emotions. The purpose may also be seemingly nonexistent. Art can describe several things: a study of creative skill, a process of using the creative skill, a product of the creative skill, or the audience's experience with the creative skill. The creative arts ("art" as discipline) are a collection of disciplines ("arts") that produce "artworks" ("art" as objects) that are compelled by a personal drive (art as activity) and echo or reflect a message, mood, or symbolism for the viewer to interpret (art as experience). Artworks can be defined by purposeful, creative interpretations of limitless concepts or ideas in order to communicate something to another person. Artworks can be explicitly made for this purpose or interpreted based on images or objects. Art is something that stimulates an individual's thoughts, emotions, beliefs, or ideas through the senses. It is also an expression of an idea and it can take many different forms and serve many different purposes. Although the application of scientific knowledge to derive a new scientific theory involves skill and results in the "creation" of something new, this represents science only and is not categorized as art.
Sculptures, cave paintings, rock paintings, and petroglyphs from the Upper Paleolithic dating to roughly 40,000 years ago have been found, but the precise meaning of such art is often disputed because so little is known about the cultures that produced them. The oldest art objects in the world—a series of tiny, drilled snail shells about 75,000 years old—were discovered in a South African cave. Many great traditions in art have a foundation in the art of one of the great ancient civilizations: Ancient Egypt, Mesopotamia, Persia, India, China, Ancient Greece, Rome, as well as Inca, Maya, and Olmec. Each of these centers of early civilization developed a unique and characteristic style in their art. Because of the size and duration these civilizations, more of their art works have survived and more of their influence has been transmitted to other cultures and later times. Some also have provided the first records of how artists worked. For example, this period of Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions. In Byzantine and Medieval art of the Western Middle Ages, much art focused on the expression of Biblical and not material truths, and used styles which showed the higher unseen glory of a heavenly world, such as the use of gold in the background of paintings, or glass in mosaics or windows, which also presented figures in idealized, patterned (flat) forms. Nevertheless a classical realist tradition persisted in small Byzantine works, and realism steadily grew in the art of Catholic Europe. Renaissance art had a greatly increased emphasis on the realistic depiction of the material world, and the place of humans in it, reflected in the corporeality of the human body, and development of a systematic method of graphical perspective to depict recession in a three dimensional picture space. In the east, Islamic art's rejection of iconography led to emphasis on geometric patterns, calligraphy, and architecture. Further east, religion dominated artistic styles and forms too. India and Tibet saw emphasis on painted sculptures and dance with religious painting borrowing many conventions from sculpture and tending to bright contrasting colors with emphasis on outlines. China saw many art forms flourish, jade carving, bronzework, pottery (including the stunning terracotta army of Emperor Qin), poetry, calligraphy, music, painting, drama, fiction, etc. Chinese styles vary greatly from era to era and are traditionally named after the ruling dynasty. So, for example, Tang Dynasty paintings are monochromatic and sparse, emphasizing idealized landscapes, but Ming Dynasty paintings are busy, colorful, and focus on telling stories via setting and composition. Japan names its styles after imperial dynasties too, and also saw much interplay between the styles of calligraphy and painting. Woodblock printing became important in Japan after the 17th century. The western Age of Enlightenment in the 18th century saw artistic depictions of physical and rational certainties of the clockwork universe, as well as politically revolutionary visions of a post-monarchist world, such as Blake's portrayal of Newton as a divine geometer, or David's propagandistic paintings. This led to Romantic rejections of this in favor of pictures of the emotional side and individuality of humans, exemplified in the novels of Goethe. The late 19th century then saw a host of artistic movements, such as academic art, Symbolism, impressionism and fauvism among others. The history of twentieth century art is a narrative of endless possibilities and the search for new standards, each being torn down in succession by the next. Thus the parameters of Impressionism, Expressionism, Fauvism, Cubism, Dadaism, Surrealism, etc cannot be maintained very much beyond the time of their invention. Increasing global interaction during this time saw an equivalent influence of other cultures into Western art, such as Pablo Picasso being influenced by African sculpture. Japanese woodblock prints (which had themselves been influenced by Western Renaissance draftsmanship) had an immense influence on Impressionism and subsequent development. Later, African sculptures were taken up by Picasso and to some extent by Matisse. Similarly, the west has had huge impacts on Eastern art in 19th and 20th century, with originally western ideas like Communism and Post-Modernism exerting powerful influence on artistic styles. Modernism, the idealistic search for truth, gave way in the latter half of the 20th century to a realization of its unattainability. Relativism was accepted as an unavoidable truth, which led to the period of contemporary art and postmodern criticism, where cultures of the world and of history are seen as changing forms, which can be appreciated and drawn from only with irony. Furthermore the separation of cultures is increasingly blurred and some argue it is now more appropriate to think in terms of a global culture, rather than regional cultures.
Art tends to facilitate intuitive rather than rational understanding, and is usually consciously created with this intention. Fine art intentionally serves no other purpose. As a result of this impetus, works of art are elusive, refractive to attempts at classification, because they can be appreciated in more than one way, and are often susceptible to many different interpretations. In the case of Géricault's "Raft of the Medusa", special knowledge concerning the shipwreck that the painting depicts is not a prerequisite to appreciating it, but allows the appreciation of Géricault's political intentions in the piece. Even art that superficially depicts a mundane event or object, may invite reflection upon elevated themes. Traditionally, the highest achievements of art demonstrate a high level of ability or fluency within a medium. This characteristic might be considered a point of contention, since many modern artists (most notably, conceptual artists) do not themselves create the works they conceive, or do not even create the work in a conventional, demonstrative sense. Art has a transformative capacity: confers particularly appealing or aesthetically satisfying structures or forms upon an original set of unrelated, passive constituents. Forms, genres, media, and styles. The creative arts are often divided into more specific categories that are related to their technique, or medium, such as decorative arts, plastic arts, performing arts, or literature. Unlike scientific fields, art is one of the few subjects that is academically organized according to technique. An artistic medium is the substance or material the artistic work is made from, and may also refers to the technique used. For example, paint is the medium used in painting, paper is a medium used in drawing. An "art form" is the specific "shape", or quality an artistic expression takes. The media used often influences the form. For example, the form of a sculpture must exist in space in three-dimensions, and respond to gravity. The constraints and limitations of a particular medium are thus called its "formal qualities". To give another example, the formal qualities of painting are the canvas texture, color, and brush texture. The formal qualities of video games are non-linearity, interactivity and virtual presence. The "form" of a particular work of art is determined by both the formal qualities of the media, and the intentions of the artist. A "genre" is a set of conventions and styles within a particular media. For instance, well recognized genres in film are western, horror and romantic comedy. Genres in music include death metal and trip hop. Genres in painting include still life, and pastoral landscape. A particular work of art may bend or combine genres but each genre has a recognizable group of conventions, clichés and tropes. (One note: the word genre has a second older meaning within painting; genre painting was a phrase used in the 17th to 19th century to refer specifically to paintings of scenes of everyday life and can still be used in this way.) An artwork, artist's, or movement's "style" is the distinctive method and form that art takes. Any loose brushy, dripped or poured abstract painting is called "expressionistic". Often these styles are linked with a particular historical period, set of ideas, and particular artistic movement. So Jackson Pollock is called an Abstract Expressionist. Because a particular style may have a specific cultural meanings, it is important to be sensitive to differences in technique. Roy Lichtenstein's (1923–1997) paintings are not pointillist, despite his uses of dots, because they are not aligned with the original proponents of Pointillism. Lichtenstein used Ben-Day dots: they are evenly-spaced and create flat areas of color. These types of dots, used in halftone printing, were originally used in comic strips and newspapers to reproduce color. Lichtenstein thus uses the dots as a style to question the "high" art of painting with the "low" art of comics - to comment on class distinctions in culture. Lichtenstein is thus associated with the American Pop art movement (1960s). Pointillism is a technique in late Impressionism (1880s), developed especially by the artist Georges Seurat, that employs dots that are spaced in a way to create variation in color and depth in an attempt to paint images that were closer to the way people really see color. Both artists use dots, but the particular style and technique relates to the artistic movement these artists were a part of. These are all ways of beginning to define a work of art, to narrow it down. "Imagine you are an art critic whose mission is to compare the meanings you find in a wide range of individual artworks. How would you proceed with your task? One way to begin is to examine the materials each artist selected in making an object, image video, or event. The decision to cast a sculpture in bronze, for instance, inevitably effects its meaning; the work becomes something different than if it had been cast in gold or plastic or chocolate, even if everything else about the artwork remained the same. Next, you might examine how the materials in each artwork have become an arrangement of shapes, colors, textures, and lines. These, in turn, are organized into various patterns and compositional structures. In your interpretation, you would comment on how salient features of the form contribute to the overall meaning of the finished artwork. [But in the end] the meaning of most artworks... is not exhausted by a discussion of materials, techniques, and form. Most interpretations also include a discussion of the ideas and feelings the artwork engenders."
Art can connote a sense of trained ability or mastery of a medium. Art can also simply refer to the developed and efficient use of a language to convey meaning with immediacy and or depth. Art is an act of expressing feelings, thoughts, and observations. There is an understanding that is reached with the material as a result of handling it, which facilitates one's thought processes. A common view is that the epithet "art", particular in its elevated sense, requires a certain level of creative expertise by the artist, whether this be a demonstration of technical ability or an originality in stylistic approach such as in the plays of Shakespeare, or a combination of these two. Traditionally skill of execution was viewed as a quality inseparable from art and thus necessary for its success; for Leonardo da Vinci, art, neither more nor less than his other endeavors, was a manifestation of skill. Rembrandt's work, now praised for its ephemeral virtues, was most admired by his contemporaries for its virtuosity. At the turn of the 20th century, the adroit performances of John Singer Sargent were alternately admired and viewed with skepticism for their manual fluency, yet at nearly the same time the artist who would become the era's most recognized and peripatetic iconoclast, Pablo Picasso, was completing a traditional academic training at which he excelled. A common contemporary criticism of some modern art occurs along the lines of objecting to the apparent lack of skill or ability required in the production of the artistic object. In conceptual art, Marcel Duchamp's "Fountain" is among the first examples of pieces wherein the artist used found objects ("ready-made") and exercised no traditionally recognised set of skills. Tracey Emin's "My Bed", or Damien Hirst's "The Physical Impossibility of Death in the Mind of Someone Living" follow this example and also manipulate the mass media. Emin slept (and engaged in other activities) in her bed before placing the result in a gallery as work of art. Hirst came up with the conceptual design for the artwork but has left most of the eventual creation of many works to employed artisans. Hirst's celebrity is founded entirely on his ability to produce shocking concepts. The actual production in many conceptual and contemporary works of art is a matter of assembly of found objects. However there are many modernist and contemporary artists who continue to excel in the skills of drawing and painting and in creating "hands on" works of art.
Somewhat in relation to the above, the word "art" is also used to apply judgments of value, as in such expressions like "that meal was a work of art" (the cook is an artist), or "the art of deception", (the highly attained level of skill of the deceiver is praised). It is this use of the word as a measure of high quality and high value that gives the term its flavor of subjectivity. Making judgments of value requires a basis for criticism. At the simplest level, a way to determine whether the impact of the object on the senses meets the criteria to be considered "art" is whether it is perceived to be attractive or repulsive. Though perception is always colored by experience, and is necessarily subjective, it is commonly taken that - that which is not aesthetically satisfying in some fashion cannot be art. However, "good" art is not always or even regularly aesthetically appealing to a majority of viewers. In other words, an artist's prime motivation need not be the pursuit of the aesthetic. Also, art often depicts terrible images made for social, moral, or thought-provoking reasons. For example, Francisco Goya's painting depicting the Spanish shootings of 3rd of May 1808 is a graphic depiction of a firing squad executing several pleading civilians. Yet at the same time, the horrific imagery demonstrates Goya's keen artistic ability in composition and execution and produces fitting social and political outrage. Thus, the debate continues as to what mode of aesthetic satisfaction, if any, is required to define 'art'. The assumption of new values or the rebellion against accepted notions of what is aesthetically superior need not occur concurrently with a complete abandonment of the pursuit of that which is aesthetically appealing. Indeed, the reverse is often true, that the revision of what is popularly conceived of as being aesthetically appealing allows for a re-invigoration of aesthetic sensibility, and a new appreciation for the standards of art itself. Countless schools have proposed their own ways to define quality, yet they all seem to agree in at least one point: once their aesthetic choices are accepted, the value of the work of art is determined by its capacity to transcend the limits of its chosen medium in order to strike some universal chord by the rarity of the skill of the artist or in its accurate reflection in what is termed the "zeitgeist".
Art is often intended to appeal and connect with human emotion. It can arouse aesthetic or moral feelings, and can be understood as a way of communicating these feelings. Artists express something so that their audience is aroused to some extent, but they do not have to do so consciously. Art explores what is commonly termed as "the human condition"; that is, essentially what it is to be human. Effective art often brings about some new insight concerning the human condition either singly or en-mass, which is not necessarily always positive, or necessarily widens the boundaries of collective human ability. The degree of skill that the artist has, will affect their ability to trigger an emotional response and thereby provide new insights, the ability to manipulate them at will shows exemplary skill and determination.
Art has had a great number of different functions throughout its history, making its purpose difficult to abstract or quantify to any single concept. This does not imply that the purpose of Art is "vague", but that it has had many unique, different, reasons for being created. Some of these functions of Art are provided in the following outline. The different purposes of art may be grouped according to those which are non-motivated, and those which are motivated (Levi-Strauss).
The purposes of art which are motivated refer to intentional, conscious actions on the part of the artists or creator. These may be to bring about political change, to comment on an aspect of society, to convey a specific emotion or mood, to address personal psychology, to illustrate another discipline, to (with commercial arts) to sell a product, or simply as a form of communication. The functions of art described above are not mutually exclusive, as many of them may overlap. For example, art for the purpose of entertainment may also seek to sell a product, i.e. the movie or video game.
Théodore Géricault's "Raft of the Medusa (c. 1820), was a social commentary on a current event, unprecedented at the time. Édouard Manet's "Le Déjeuner sur l'Herbe" (1863), was considered scandalous not because of the nude woman, but because she is seated next to men fully dressed in the clothing of the time, rather than in robes of the antique world. John Singer Sargent's "Madame Pierre Gautreau (Madam X)" (1884), caused a huge uproar over the reddish pink used to color the woman's ear lobe, considered far too suggestive and supposedly ruining the high-society model's reputation. In the twentieth century, Pablo Picasso's "Guernica" (1937) used arresting cubist techniques and stark monochromatic oils, to depict the harrowing consequences of a contemporary bombing of a small, ancient Basque town. Leon Golub's "Interrogation III" (1981), depicts a female nude, hooded detainee strapped to a chair, her legs open to reveal her sexual organs, surrounded by two tormentors dressed in everyday clothing. Andres Serrano's "Piss Christ" (1989) is a photograph of a crucifix, sacred to the Christian religion and representing Christ's sacrifice and final suffering, submerged in a glass of the artist's own urine. The resulting uproar led to comments in the United States Senate about public funding of the arts.
In the nineteenth century, artists were primarily concerned with ideas of "truth" and "beauty". The aesthetic theorist John Ruskin, who championed what he saw as the naturalism of J. M. W. Turner, saw art's role as the communication by artifice of an essential truth that could only be found in nature. The definition and evaluation of art has become especially problematic since the 20th century. Richard Wollheim distinguishes three approaches: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans. After Greenberg, several important art theorists emerged, such as Michael Fried, T. J. Clark, Rosalind Krauss, Linda Nochlin and Griselda Pollock among others. Though only originally intended as a way of understanding a specific set of artists, Greenberg's definition of modern art is important to many of the ideas of art within the various art movements of the 20th century and early 21st century. Pop artists like Andy Warhol became both noteworthy and influential through work including and possibly critiquing popular culture, as well as the art world. Artists of the 1980s, 1990s, and 2000s expanded this technique of self-criticism beyond "high art" to all cultural image-making, including fashion images, comics, billboards and pornography.
Disputes as to whether or not to classify something as a work of art are referred to as classificatory disputes about art. Classificatory disputes in the 20th century have included cubist and impressionist paintings, Duchamp's "Fountain", the movies, superlative imitations of banknotes, Conceptual art, and Video games. Philosopher David Novitz has argued that disagreement about the definition of art are rarely the heart of the problem. Rather, "the passionate concerns and interests that humans vest in their social life" are "so much a part of all classificatory disputes about art" (Novitz, 1996). According to Novitz, classificatory disputes are more often disputes about societal values and where society is trying to go than they are about theory proper. For example, when the Daily Mail criticized Hirst's and Emin's work by arguing "For 1,000 years art has been one of our great civilising forces. Today, pickled sheep and soiled beds threaten to make barbarians of us all" they are not advancing a definition or theory about art, but questioning the value of Hirst's and Emin's work. In 1998, Arthur Danto, suggested a thought experiment showing that "the status of an artifact as work of art results from the ideas a culture applies to it, rather than its inherent physical or perceptible qualities. Cultural interpretation (an art theory of some kind) is therefore constitutive of an object's arthood." Anti-art is a label for art which intentionally challenges the established parameters and values of art; it is term associated with Dadaism and attributed to Marcel Duchamp just before World War I, when he was making art from found objects. One of these, "Fountain" (1917), an ordinary urinal, has achieved considerable prominence and influence on art. Anti-art is a feature of work by Situationist International, the lo-fi Mail art movement, and the Young British Artists, though it is a form still rejected by the Stuckists, who describe themselves as anti-anti-art.
Art has been perceived by some as belonging to some social classes and often excluding others. In this context, art is seen as an upper-class activity associated with wealth, the ability to purchase art, and the leisure required to pursue or enjoy it. For example, the palaces of Versailles or the Hermitage in St. Petersburg with their vast collections of art, amassed by the fabulously wealthy royalty of Europe exemplify this view. Collecting such art is the preserve of the rich, or of governments and institutions. "Fine" and expensive goods have been popular markers of status in many cultures, and they continue to be so today. There has been a cultural push in the other direction since at least 1793, when the Louvre, which had been a private palace of the Kings of France, was opened to the public as an art museum during the French Revolution. Most modern public museums and art education programs for children in schools can be traced back to this impulse to have art available to everyone. Museums in the United States tend to be gifts from the very rich to the masses (The Metropolitan Museum of Art in New York City, for example, was created by John Taylor Johnston, a railroad executive whose personal art collection seeded the museum.) But despite all this, at least one of the important functions of art in the 21st century remains as a marker of wealth and social status. There have been attempts by artists to create art that can not be bought by the wealthy as a status object. One of the prime original motivators of much of the art of the late 1960s and 1970s was to create art that could not be bought and sold. It is "necessary to present something more than mere objects" said the major post war German artist Joseph Beuys. This time period saw the rise of such things as performance art, video art, and conceptual art. The idea was that if the artwork was a performance that would leave nothing behind, or was simply an idea, it could not be bought and sold. "Democratic precepts revolving around the idea that a work of art is a commodity impelled the aesthetic innovation which germinated in the mid-1960s and was reaped throughout the 1970s. Artists broadly identified under the heading of Conceptual art... substituting performance and publishing activities for engagement with both the material and materialistic concerns of painted or sculptural form... [have] endeavored to undermine the art object qua object." In the decades since, these ideas have been somewhat lost as the art market has learned to sell limited edition DVDs of video works, invitations to exclusive performance art pieces, and the objects left over from conceptual pieces. Many of these performances create works that are only understood by the elite who have been educated as to why an idea or video or piece of apparent garbage may be considered art. The marker of status becomes understanding the work instead of necessarily owning it, and the artwork remains an upper-class activity. "With the widespread use of DVD recording technology in the early 2000s, artists, and the gallery system that derives its profits from the sale of artworks, gained an important means of controlling the sale of video and computer artworks in limited editions to collectors."
Agnostida is an order of trilobite which first developed near the end of the Early Cambrian period and thrived during the Middle Cambrian. They are present in the lower Cambrian fossil record along with trilobites from the Redlichiida, Corynexochida, and Ptychopariida orders. The last agnostids went extinct in the Late Ordovician. The Agnostida are divided into two suborders — Agnostina and Eodiscina — that are then divided into a number of families. As a group, agnostids have "pygidia" (tails) that are so similar in size and shape to their "cephalons" (heads) that it is difficult to distinguish which end is which. Most agnostid species were eyeless. The systematic position of Order Agnostida within Class Trilobita remains uncertain, and there has been continuing debate whether they are trilobites or a stem group. The challenge to the status has focused on the Agnostina partly because juveniles of one genus have been found with legs greatly different from those of adult trilobites, suggesting they are separately descended from Crustaceans. Other researchers have suggested, based on cladistic analyses, that Eodiscina and Agnostida are closely united, and that the Eodiscina descended from the trilobite Order Ptychopariida. Scientists have long debated whether the agnostids lived a pelagic or a benthic lifestyle. Their lack of eyes, a morphology not well-suited for swimming, and their fossils found in association with other benthic trilobites all suggest a benthic (bottom-dwelling) mode of life. They likely lived on areas of the ocean floor that received little or no light and fed on detritus that descended from upper layers of the sea to the bottom. In contrast, their wide geographic dispersion in the fossil record is uncharacteristic of benthic animals, suggesting a pelagic existence. The thoracic segment appears to form a hinge between the head and pygidium allowing for a bivalved ostracodan-type lifestyle. Furthermore, the orientation of the thoracic appendages appears ill suited for benthic living. Agnostina are generally referred to simply as "agnostids" even though they probably should be called "agnostines".
Abortion is the termination of a pregnancy by the removal or expulsion from the uterus of a fetus or embryo, resulting in or caused by its death. An abortion can occur spontaneously due to complications during pregnancy or can be induced, in humans and other species. In the context of human pregnancies, an abortion induced to preserve the health of the gravida (pregnant female) is termed a "therapeutic abortion", while an abortion induced for any other reason is termed an "elective abortion". The term "abortion" most commonly refers to the induced abortion of a human pregnancy, while spontaneous abortions are usually termed miscarriages. Abortion has a long history and has been induced by various methods including herbal abortifacients, the use of sharpened tools, physical trauma and other traditional methods. Contemporary medicine utilizes medications and surgical procedures to induce abortion. The legality, prevalence, and cultural views on abortion vary substantially around the world. In many parts of the world there is prominent and divisive public controversy over the ethical and legal issues of abortion. Abortion and abortion-related issues feature prominently in the national politics in many nations, often involving the opposing "pro-life" and "pro-choice" worldwide social movements. Incidence of abortion has declined worldwide, as access to family planning education and contraceptive services has increased. Abortion incidence in the United States declined 8% from 1996 to 2003.
Spontaneous abortion (also known as miscarriage) is the expulsion of an embryo or fetus due to accidental trauma or natural causes before approximately the 22nd week of gestation; the definition by gestational age varies by country. Most miscarriages are due to incorrect replication of chromosomes; they can also be caused by environmental factors. A pregnancy that ends before 37 weeks of gestation resulting in a live-born infant is known as a "premature birth". When a fetus dies in utero after about 22 weeks, or during delivery, it is usually termed "stillborn". Premature births and stillbirths are generally not considered to be miscarriages although usage of these terms can sometimes overlap. Between 10% and 50% of pregnancies end in clinically apparent miscarriage, depending upon the age and health of the pregnant woman. Most miscarriages occur very early in pregnancy, in most cases, they occur so early in the pregnancy that the woman is not even aware that she was pregnant. One study testing hormones for ovulation and pregnancy found that 61.9% of conceptuses were lost prior to 12 weeks, and 91.7% of these losses occurred subclinically, without the knowledge of the once pregnant woman. The risk of spontaneous abortion decreases sharply after the 10th week from the last menstrual period (LMP). One study of 232 pregnant women showed "virtually complete [pregnancy loss] by the end of the embryonic period" (10 weeks LMP) with a pregnancy loss rate of only 2 percent after 8.5 weeks LMP. The most common cause of spontaneous abortion during the first trimester is chromosomal abnormalities of the embryo/fetus, accounting for at least 50% of sampled early pregnancy losses. Other causes include vascular disease (such as lupus), diabetes, other hormonal problems, infection, and abnormalities of the uterus. Advancing maternal age and a patient history of previous spontaneous abortions are the two leading factors associated with a greater risk of spontaneous abortion. A spontaneous abortion can also be caused by accidental trauma; intentional trauma or stress to cause miscarriage is considered induced abortion or feticide.
"Medical abortions" are non-surgical abortions that use pharmaceutical drugs, and are only effective in the first trimester of pregnancy. Medical abortions comprise 10% of all abortions in the United States and Europe. Combined regimens include methotrexate or mifepristone, followed by a prostaglandin (either misoprostol or gemeprost: misoprostol is used in the U.S.; gemeprost is used in the UK and Sweden.) When used within 49 days gestation, approximately 92% of women undergoing medical abortion with a combined regimen completed it without surgical intervention. Misoprostol can be used alone, but has a lower efficacy rate than combined regimens. In cases of failure of medical abortion, vacuum or manual aspiration is used to complete the abortion surgically.
In the first 12 weeks, suction-aspiration or vacuum abortion is the most common method. "Manual Vacuum aspiration" (MVA) abortion consists of removing the fetus or embryo, placenta and membranes by suction using a manual syringe, while "electric vacuum aspiration" (EVA) abortion uses an electric pump. These techniques are comparable, and differ in the mechanism used to apply suction, how early in pregnancy they can be used, and whether cervical dilation is necessary. MVA, also known as "mini-suction" and "menstrual extraction", can be used in very early pregnancy, and does not require cervical dilation. Surgical techniques are sometimes referred to as 'Suction (or surgical) Termination Of Pregnancy' (STOP). From the 15th week until approximately the 26th, dilation and evacuation (D&E) is used. D&E consists of opening the cervix of the uterus and emptying it using surgical instruments and suction. "Dilation and curettage" (D&C), the second most common method of abortion, is a standard gynecological procedure performed for a variety of reasons, including examination of the uterine lining for possible malignancy, investigation of abnormal bleeding, and abortion. "Curettage" refers to cleaning the walls of the uterus with a curette. The World Health Organization recommends this procedure, also called "sharp curettage," only when MVA is unavailable. The term "D and C", or sometimes "suction curette", is used as a euphemism for the first trimester abortion procedure, whichever the method used. Other techniques must be used to induce abortion in the second trimester. Premature delivery can be induced with prostaglandin; this can be coupled with injecting the amniotic fluid with hypertonic solutions containing saline or urea. After the 16th week of gestation, abortions can be induced by intact dilation and extraction (IDX) (also called intrauterine cranial decompression), which requires surgical decompression of the fetus' head before evacuation. IDX is sometimes called "partial-birth abortion," which has been federally banned in the United States. A hysterotomy abortion is a procedure similar to a caesarean section and is performed under general anesthesia. It requires a smaller incision than a caesarean section and is used during later stages of pregnancy. From the 20th to 23rd week of gestation, an injection to stop the fetal heart can be used as the first phase of the surgical abortion procedure to ensure that the fetus is not born alive.
Historically, a number of herbs reputed to possess abortifacient properties have been used in folk medicine: tansy, pennyroyal, black cohosh, and the now-extinct silphium (see history of abortion). The use of herbs in such a manner can cause serious—even lethal—side effects, such as multiple organ failure, and is not recommended by physicians. Abortion is sometimes attempted by causing trauma to the abdomen. The degree of force, if severe, can cause serious internal injuries without necessarily succeeding in inducing miscarriage. Both accidental and deliberate abortions of this kind can be subject to criminal liability in many countries. In Southeast Asia, there is an ancient tradition of attempting abortion through forceful abdominal massage. One of the bas reliefs decorating the temple of Angkor Wat in Cambodia depicts a demon performing such an abortion upon a woman who has been sent to the underworld. Reported methods of unsafe, self-induced abortion include misuse of misoprostol, and insertion of non-surgical implements such as knitting needles and clothes hangers into the uterus. These methods are rarely seen in developed countries where surgical abortion is legal and available.
Early-term surgical abortion is a simple procedure which is safer than childbirth when performed before the 21st week. Abortion methods, like most minimally invasive procedures, carry a small potential for serious complications. The risk of complications can increase depending on how far pregnancy has progressed. Women typically experience minor pain during first-trimester abortion procedures. In a 1979 study of 2,299 patients, 97% reported experiencing some degree of pain. Patients rated the pain as being less than earache or toothache, but more than headache or backache. Local and general anesthetics are used during surgical procedures.
The relationship between induced abortion and mental health is an area of controversy. No scientific research has demonstrated a direct causal relationship between abortion and poor mental health, though some studies have noted that there may be a statistical correlation. Pre-existing factors in a woman's life, such as emotional attachment to the pregnancy, lack of social support, pre-existing psychiatric illness, and conservative views on abortion increase the likelihood of experiencing negative feelings after an abortion. In a 1990 review, the American Psychological Association (APA) found that "severe negative reactions [after abortion] are rare and are in line with those following other normal life stresses." The APA revised and updated its findings in August 2008 to account for the accumulation of new evidence, and again concluded that induced abortion did not lead to increased mental health problems. A 2008 review by a group from the Johns Hopkins Bloomberg School of Public Health concluded that the highest quality studies found few, if any, mental health differences between women who had abortions and their comparison groups, whereas studies with the most flaws reported negative mental health consequences of abortion. As of August 2008, the United Kingdom Royal College of Psychiatrists is also performing a systematic review of the medical literature to update their position statement on the subject. Some proposed negative psychological effects of abortion have been referred to by pro-life advocates as a separate condition called "post-abortion syndrome." However, the existence of "post-abortion syndrome" is not recognized by any medical or psychological organization, and some physicians and pro-choice advocates have argued that the effort to popularize the idea of a "post-abortion syndrome" is a tactic used by pro-life advocates for political purposes.
The incidence and reasons for induced abortion vary regionally. It has been estimated that approximately 46 million abortions are performed worldwide every year. Of these, 26 million are said to occur in places where abortion is legal; the other 20 million happen where the procedure is illegal. Some countries, such as Belgium (11.2 per 100 known pregnancies) and the Netherlands (10.6 per 100), have a low rate of induced abortion, while others like Russia (62.6 per 100) and Vietnam (43.7 per 100) have a comparatively high rate. The world ratio is 26 induced abortions per 100 known pregnancies. By gestational age and method. Abortion rates also vary depending on the stage of pregnancy and the method practiced. In 2003, from data collected in those areas of the United States that sufficiently reported gestational age, it was found that 88.2% of abortions were conducted at or prior to 12 weeks, 10.4% from 13 to 20 weeks, and 1.4% at or after 21 weeks. 90.9% of these were classified as having been done by "curettage" (suction-aspiration, Dilation and curettage, Dilation and evacuation), 7.7% by "medical" means (mifepristone), 0.4% by "intrauterine instillation" (saline or prostaglandin), and 1.0% by "other" (including hysterotomy and hysterectomy). The Guttmacher Institute estimated there were 2,200 intact dilation and extraction procedures in the U.S. during 2000; this accounts for 0.17% of the total number of abortions performed that year. Similarly, in England and Wales in 2006, 89% of terminations occurred at or under 12 weeks, 9% between 13 to 19 weeks, and 1.5% at or over 20 weeks. 64% of those reported were by vacuum aspiration, 6% by D&E, and 30% were medical. Later abortions are more common in China, India, and other developing countries than in developed countries. By personal and social factors. A 1998 aggregated study, from 27 countries, on the reasons women seek to terminate their pregnancies concluded that common factors cited to have influenced the abortion decision were: desire to delay or end childbearing, concern over the interruption of work or education, issues of financial or relationship stability, and perceived immaturity. A 2004 study in which American women at clinics answered a questionnaire yielded similar results. In Finland and the United States, concern for the health risks posed by pregnancy in individual cases was not a factor commonly given; however, in Bangladesh, India, and Kenya health concerns were cited by women more frequently as reasons for having an abortion. 1% of women in the 2004 survey-based U.S. study became pregnant as a result of rape and 0.5% as a result of incest. Another American study in 2002 concluded that 54% of women who had an abortion were using a form of contraception at the time of becoming pregnant while 46% were not. Inconsistent use was reported by 49% of those using condoms and 76% of those using the combined oral contraceptive pill; 42% of those using condoms reported failure through slipping or breakage. The Guttmacher Institute estimated that "most abortions in the United States are obtained by minority women" because minority women "have much higher rates of unintended pregnancy." Some abortions are undergone as the result of societal pressures. These might include the stigmatization of disabled persons, preference for children of a specific sex, disapproval of single motherhood, insufficient economic support for families, lack of access to or rejection of contraceptive methods, or efforts toward population control (such as China's one-child policy). These factors can sometimes result in compulsory abortion or sex-selective abortion.
Induced abortion can be traced to ancient times. There is evidence to suggest that, historically, pregnancies were terminated through a number of methods, including the administration of abortifacient herbs, the use of sharpened implements, the application of abdominal pressure, and other techniques. The Hippocratic Oath, the chief statement of medical ethics for Hippocratic physicians in Ancient Greece, forbade doctors from helping to procure an abortion by pessary. Soranus, a second-century Greek physician, suggested in his work "Gynaecology" that women wishing to abort their pregnancies should engage in energetic exercise, energetic jumping, carrying heavy objects, and riding animals. He also prescribed a number of recipes for herbal baths, pessaries, and bloodletting, but advised against the use of sharp instruments to induce miscarriage due to the risk of organ perforation. It is also believed that, in addition to using it as a contraceptive, the ancient Greeks relied upon silphium as an abortifacient. Such folk remedies, however, varied in effectiveness and were not without risk. Tansy and pennyroyal, for example, are two poisonous herbs with serious side effects that have at times been used to terminate pregnancy. During the medieval period, physicians in the Islamic world documented detailed and extensive lists of birth control practices, including the use of abortifacients, commenting on their effectiveness and prevalence. They listed many different birth control substances in their medical encyclopedias, such as Avicenna listing 20 in "The Canon of Medicine" (1025) and Muhammad ibn Zakariya ar-Razi listing 176 in his "Hawi" (10th century). This was unparalleled in European medicine until the 19th century. During the Middle Ages, abortion was tolerated because there were no laws against it. A medieval female physician, Trotula of Salerno, administered a number of remedies for the “retention of menstrua,” which was sometimes a code for early abortifacients. Pope Sixtus V (1585–1590) is noted as the first Pope to declare that abortion is homicide regardless of the stage of pregnancy. Abortion in the 19th century continued, despite bans in both the United Kingdom and the United States, as the disguised, but nonetheless open, advertisement of services in the Victorian era suggests. In the 20th century the Soviet Union (1919), Iceland (1935) and Sweden (1938) were among the first countries to legalize certain or all forms of abortion. In 1935 Nazi Germany, a law was passed permitting abortions for those deemed "hereditarily ill," while women considered of German stock were specifically prohibited from having abortions. Sex-selective abortion and female infanticide. Sonography and amniocentesis allow parents to determine sex before birth. The development of this technology has led to sex-selective abortion, or the targeted termination of female fetuses. It is suggested that sex-selective abortion might be partially responsible for the noticeable disparities between the birth rates of male and female children in some places. The preference for male children is reported in many areas of Asia, and abortion used to limit female births has been reported in Mainland China, Taiwan, South Korea, and India. In India, the economic role of men, the costs associated with dowries, and a Hindu tradition which dictates that funeral rites must be performed by a male relative have led to a cultural preference for sons. The widespread availability of diagnostic testing, during the 1970s and '80s, led to advertisements for services which read, "Invest 500 rupees [for a sex test] now, save 50,000 rupees [for a dowry] later." In 1991, the male-to-female sex ratio in India was skewed from its biological norm of 105 to 100, to an average of 108 to 100. Researchers have asserted that between 1985 and 2005 as many as 10 million female fetuses may have been selectively aborted. The Indian government passed an official ban of pre-natal sex screening in 1994 and moved to pass a complete ban of sex-selective abortion in 2002. In the People's Republic of China, there is also a historic son preference. The implementation of the one-child policy in 1979, in response to population concerns, led to an increased disparity in the sex ratio as parents attempted to circumvent the law through sex-selective abortion or the abandonment of unwanted daughters. Sex-selective abortion might be an influence on the shift from the baseline male-to-female birth rate to an elevated national rate of 117:100 reported in 2002. The trend was more pronounced in rural regions: as high as 130:100 in Guangdong and 135:100 in Hainan. A ban upon the practice of sex-selective abortion was enacted in 2003.
Women seeking to terminate their pregnancies sometimes resort to unsafe methods, particularly where and when access to legal abortion is being barred. The World Health Organization (WHO) defines an unsafe abortion as being "a procedure... carried out by persons lacking the necessary skills or in an environment that does not conform to minimal medical standards, or both." Unsafe abortions are sometimes known colloquially as "back-alley" abortions. This can include a person without medical training, a professional health provider operating in sub-standard conditions, or the woman herself. Unsafe abortion remains a public health concern today due to the higher incidence and severity of its associated complications, such as incomplete abortion, sepsis, hemorrhage, and damage to internal organs. WHO estimates that 19 million unsafe abortions occur around the world annually and that 68,000 of these result in the woman's death. Complications of unsafe abortion are said to account, globally, for approximately 13% of all maternal mortalities, with regional estimates including 12% in Asia, 25% in Latin America, and 13% in sub-Saharan Africa. A 2007 study published in the "The Lancet" found that, although the global rate of abortion declined from 45.6 million in 1995 to 41.6 million in 2003, unsafe procedures still accounted for 48% of all abortions performed in 2003. Health education, access to family planning, and improvements in health care during and after abortion have been proposed to address this phenomenon.
In the history of abortion, induced abortion has been the source of considerable debate, controversy, and activism. An individual's position on the complex ethical, moral, philosophical, biological, and legal issues is often related to his or her value system. The main positions are the pro-choice position, which argues in favor of access to abortion, and the pro-life position, which argues against access to abortion. Opinions of abortion may be described as being a combination of beliefs on its morality, and beliefs on the responsibility, ethical scope, and proper extent of governmental authorities in public policy. Religious ethics also has an influence upon both personal opinion and the greater debate over abortion (see religion and abortion). Abortion debates, especially pertaining to abortion laws, are often spearheaded by groups advocating one of these two positions. In the United States, those in favor of greater legal restrictions on, or even complete prohibition of abortion, most often describe themselves as pro-life while those against legal restrictions on abortion describe themselves as pro-choice. Generally, the pro-life position argues that a human fetus is a human being with the right to live making abortion tantamount to murder. The pro-choice position argues that a woman has certain reproductive rights, especially the choice whether or not to carry a pregnancy to term. In both public and private debate, arguments presented in favor of or against abortion focus on either the moral permissibility of an induced abortion, or justification of laws permitting or restricting abortion. Debate also focuses on whether the pregnant woman should have to notify and/or have the consent of others in distinct cases: a minor, her parents; a legally married or common-law wife, her husband; or a pregnant woman, the biological father. In a 2003 Gallup poll in the United States, 79% of male and 67% of female respondents were in favor of legalized mandatory spousal notification; overall support was 72% with 26% opposed.
A number of opinion polls around the world have explored public opinion regarding the issue of abortion. Results have varied from poll to poll, country to country, and region to region, while varying with regard to different aspects of the issue. A May 2005 survey examined attitudes toward abortion in 10 European countries, asking polltakers whether they agreed with the statement, "If a woman doesn't want children, she should be allowed to have an abortion". The highest level of approval was 81% (in the Czech Republic); the lowest was 47% (in Poland). In North America, a December 2001 poll surveyed Canadian opinion on abortion, asking Canadians in what circumstances they believe abortion should be permitted; 32% responded that they believe abortion should be legal in all circumstances, 52% that it should be legal in certain circumstances, and 14% that it should be legal in no circumstances. A similar poll in April 2009 surveyed people in the United States about U.S. opinion on abortion; 18% said that abortion should be "legal in all cases", 28% said that abortion should be "legal in most cases", 28% said abortion should be "illegal in most cases" and 16% said abortion should be "illegal in all cases". A November 2005 poll in Mexico found that 73.4% think abortion should not be legalized while 11.2% think it should. Of attitudes in South America, a December 2003 survey found that 30% of Argentines thought that abortion in Argentina should be allowed "regardless of situation", 47% that it should be allowed "under some circumstances", and 23% that it should not be allowed "regardless of situation". A March 2007 poll regarding the abortion law in Brazil found that 65% of Brazilians believe that it "should not be modified", 16% that it should be expanded "to allow abortion in other cases", 10% that abortion should be "decriminalized", and 5% were "not sure". A July 2005 poll in Colombia found that 65.6% said they thought that abortion should remain illegal, 26.9% that it should be made legal, and 7.5% that they were unsure.
The abortion-breast cancer hypothesis posits that induced abortion increases the risk of developing breast cancer. This position contrasts with the scientific consensus that abortion does "not" cause breast cancer. In early pregnancy, levels of estrogen increase, leading to breast growth in preparation for lactation. The hypothesis proposes that if this process is interrupted by an abortion before full maturity in the third trimester then more relatively vulnerable immature cells could be left than there were prior to the pregnancy, resulting in a greater potential risk of breast cancer. The hypothesis mechanism was first proposed and explored in rat studies conducted in the 1980s.
Fetal pain, its existence, and its implications are part of a larger debate about abortion. Many researchers in the area of fetal development believe that a fetus is unlikely to feel pain until after the seventh month of pregnancy. Others disagree. However, legislation has been proposed by pro-life advocates requiring abortion providers to tell a woman that the fetus may feel pain during an abortion procedure. A review by researchers from the University of California, San Francisco in "JAMA" concluded that data from dozens of medical reports and studies indicate that fetuses are unlikely to feel pain until the third trimester of pregnancy. However a number of medical critics have since disputed these conclusions. At the end of the 20th century there was an emerging consensus among developmental neurobiologists that the establishment of thalamocortical connections (at about 26 weeks) is a critical event with regard to fetal perception of pain. Other researchers such as Anand and Fisk have challenged this late date, positing that pain can be felt around 20 weeks. Because pain can involve sensory, emotional and cognitive factors, it may be "impossible to know" when painful experiences are perceived, even if it is known when thalamocortical connections are established. In any case, one of the first steps in second-trimester and third-trimester abortions is to anesthetize the fetus or stop its heart to prevent fetal pain.
A theory attempts to draw a correlation between the United States' unprecedented nationwide decline of the overall crime rate during the 1990s and the decriminalization of abortion 20 years prior. The suggestion was brought to widespread attention by a 1999 academic paper, "The Impact of Legalized Abortion on Crime", authored by the economists Steven D. Levitt and John Donohue. They attributed the drop in crime to a reduction in individuals said to have a higher statistical probability of committing crimes: unwanted children, especially those born to mothers who are African-American, impoverished, adolescent, uneducated, and single. The change coincided with what would have been the adolescence, or peak years of potential criminality, of those who had not been born as a result of "Roe v. Wade" and similar cases. Donohue and Levitt's study also noted that states which legalized abortion before the rest of the nation experienced the lowering crime rate pattern earlier, and those with higher abortion rates had more pronounced reductions. Fellow economists Christopher Foote and Christopher Goetz criticized the methodology in the Donohue-Levitt study, noting a lack of accommodation for statewide yearly variations such as cocaine use, and recalculating based on incidence of crime per capita; they found no statistically significant results. Levitt and Donohue responded to this by presenting an adjusted data set which took into account these concerns and reported that the data maintained the statistical significance of their initial paper. Such research has been criticized by some as being utilitarian, discriminatory as to race and socioeconomic class, and as promoting eugenics as a solution to crime. Levitt states in his book "Freakonomics" that they are neither promoting nor negating any course of action—merely reporting data as economists.
The Mexico City policy, also known as the "Global Gag Rule" required any non-governmental organization receiving US Government funding to refrain from performing or promoting abortion services in other countries. This had a significant effect on the health policies of many nations across the globe. The Mexico City Policy was instituted under President Reagan, suspended under President Clinton, reinstated by President George W. Bush, and suspended again by President Barack Obama on January 24, 2009.
Before the scientific discovery in the nineteenth century that human development begins at fertilization, English common law forbade abortions after "quickening”, that is, after “an infant is able to stir in the mother's womb.” There was also an earlier period in England when abortion was prohibited "if the foetus is already formed" but not yet quickened. Both pre- and post-quickening abortions were criminalized by "Lord Ellenborough's Act" in 1803. In 1861, the Parliament of the United Kingdom passed the "Offences against the Person Act 1861", which continued to outlaw abortion and served as a model for similar prohibitions in some other nations. The Soviet Union, with legislation in 1920, and Iceland, with legislation in 1935, were two of the first countries to generally allow abortion. The second half of the 20th century saw the liberalization of abortion laws in other countries. The "Abortion Act 1967" allowed abortion for limited reasons in the United Kingdom (except Northern Ireland). In the 1973 case, "Roe v. Wade", the United States Supreme Court struck down state laws banning abortion, ruling that such laws violated an implied right to privacy in the United States Constitution. The Supreme Court of Canada, similarly, in the case of "R. v. Morgentaler", discarded its criminal code regarding abortion in 1988, after ruling that such restrictions violated the security of person guaranteed to women under the "Canadian Charter of Rights and Freedoms". Canada later struck down provincial regulations of abortion in the case of "R. v. Morgentaler (1993)." By contrast, abortion in Ireland was affected by the addition of an amendment to the Irish Constitution in 1983 by popular referendum, recognizing "the right to life of the unborn". Other countries, in which abortion is normally illegal, will allow one to be performed in the case of rape, incest, or danger to the pregnant woman's life or health. In places where abortion is illegal or carries heavy social stigma, pregnant women may engage in medical tourism and travel to countries where they can terminate their pregnancy. Women without the means to travel can resort to providers of illegal abortions or try to do it themselves. In the USA, about 8% of abortions are performed on women who travel from another state. However, that is driven at least partly by differing limits on abortion according to gestational age or the scarcity of doctors trained and willing to do later abortions.
Spontaneous abortion occurs in various animals. For example, in sheep, it may be caused by crowding through doors, or being chased by dogs. In cows, abortion may be caused by contagious disease, such as Brucellosis or Campylobacter, but can often be controlled by vaccination. Additionally, many other diseases are known to increase the risk of miscarriage in humans and other animals. Abortion may also be induced in animals, in the context of animal husbandry. For example, abortion may be induced in mares that have been mated improperly, or that have been purchased by owners who did not realize the mares were pregnant, or that are pregnant with twin foals. Feticide can occur in horses and zebras due to male harassment of pregnant mares or forced copulation, although the frequency in the wild has been questioned. Male Gray langur monkeys may attack females following male takeover, causing miscarriage.
The Abstract of Title, used in real estate transactions, is the more common form of abstract. An abstract of title lists all the owners of a piece of land, a house, or a building before it came into possession of the present owner. The abstract also records all deeds, wills, mortgages, and other documents that affect ownership of the property. An abstract describes a chain of transfers from owner to owner and any agreements by former owners that are binding on later owners.
A Clear Title to property is one that clearly states any obligation in the deed to the property. It reveals no breaks in the chain of legal ownership. After the records of the property have been traced and the title has been found clear, it is sometimes guaranteed, or insured. In a few states, a different system of insuring title of real properties provides for registration of a clear title with public authorities. After this is accomplished, no abstract of title is necessary.
The American Revolutionary War (1775–1783) or American War of Independence began as a war between the Kingdom of Great Britain and thirteen former British colonies in North America, and concluded in a global war between several European great powers. The war was the culmination of the political American Revolution, whereby many of the colonists rejected the legitimacy of the Parliament of Great Britain to govern them without representation, claiming that this violated the Rights of Englishmen. The First Continental Congress met in 1774 to coordinate relations with Great Britain and the thirteen now self-governing and individual provinces, petitioning George III for intervention with Parliament, organizing a boycott of British goods, while affirming loyalty to the British Crown. Their pleas ignored, and with British combat troops billeted in Boston, Massachusetts, by 1775 the Provincial Congresses formed the Second Continental Congress and authorized a Continental Army. Additional petitions to the king to intervene with Parliament resulted in the following year with Congress being declared traitors and the states to be in rebellion. The Americans responded in 1776 by formally declaring their independence as one new nation — the United States of America — claiming their own sovereignty and rejecting any allegiance to the British monarchy. France provided supplies, ammunition and weapons to the rebels from 1776, and the Continentals' capture of a British army in 1777 led France to enter the war in early 1778, which evened the military strength with Britain. Spain and the Dutch Republic – French allies – also went to war with Britain over the next two years, threatening an invasion of England and severely testing British military strength with campaigns in Europe — including attacks on Minorca and Gibraltar — and an escalating global naval war. Spain's involvement culminated in the expulsion of British armies from West Florida, securing the American colonies' southern flank. Throughout the war, the British were able to use their naval superiority to capture and occupy American coastal cities, but control of the countryside (where 90% of the population lived) largely eluded them because of the relatively small size of their land army. French involvement proved decisive, with a French naval victory in the Chesapeake leading at Yorktown in 1781 to the surrender of a second British army. In 1783, the Treaty of Paris ended the war and recognized the sovereignty of the United States over the territory bounded by what is now Canada to the north, Florida to the south, and the Mississippi River to the west.
At the outset of the war, the thirteen colonies lacked a professional army or navy. Each colony provided for its own defenses with local militia. Militiamen were lightly armed, slightly trained, and usually did not have uniforms. Their units served for only a few weeks or months at a time, were reluctant to go very far from home, and were thus generally unavailable for extended operations. Militia lacked the training and discipline of soldiers with more experience, but were more numerous and could overwhelm regular troops as at the battles of Concord, Bennington and Saratoga, and the siege of Boston. Both sides used partisan warfare but the Americans were particularly effective at suppressing Loyalist activity when British regulars were not in the area. Seeking to coordinate military efforts, the Continental Congress established (on paper) a regular army in June 1775, and appointed George Washington as commander-in-chief. The development of the Continental Army was always a work in progress, and Washington used both his regulars and state militia throughout the war. The United States Marine Corps traces its institutional roots to the Continental Marines of the war, formed at Tun Tavern in Philadelphia, by a resolution of the Continental Congress on November 10, 1775, a date regarded and celebrated as the birthday of the Marine Corps. At the beginning of 1776, Washington's army had 20,000 men, with two-thirds enlisted in the Continental Army and the other third in the various state militias. At the end of the American Revolution in 1783, both the Continental Navy and Continental Marines were disbanded. About 250,000 men served as regulars or as militiamen for the Revolutionary cause in the eight years of the war, but there were never more than 90,000 total men under arms at one time. Armies were small by European standards of the era, largely attributable to limitations such as lack of powder and other logistical capabilities on the American side. By comparison, Duffy notes that Frederick the Great usually commanded from 23,000 to 50,000 in battle. Both figures pale in comparison to the armies that would be fielded in the early nineteenth century, where troop formations approached or exceeded 100,000 men.
Historians have estimated that approximately 40–45% of the colonists actively supported the rebellion while 15–20% of the population of the thirteen colonies remained loyal to the British Crown. The remaining 35–45% attempted to remain neutral. At least 25,000 Loyalists fought on the side of the British. Thousands served in the Royal Navy. On land, Loyalist forces fought alongside the British in most battles in North America. Many Loyalists fought in partisan units, especially in the Southern theater. The British military met with many difficulties in maximizing the use of Loyalist factions. British historian Jeremy Black wrote, “In the American war it was clear to both royal generals and revolutionaries that organized and significant Loyalist activity would require the presence of British forces.” In the South, the use of Loyalists presented the British with “major problems of strategic choice” since while it was necessary to widely disperse troops in order to defend Loyalist areas, it was also recognized that there was a need for “the maintenance of large concentrated forces able” to counter major attacks from the American forces. In addition, the British were forced to ensure that their military actions would not “offend Loyalist opinion”, eliminating such options as attempting to “live off the country’, destroying property for intimidation purposes, or coercing payments from colonists (“laying them under contribution”).
Early in 1775, the British Army consisted of about 36,000 men worldwide, but wartime recruitment steadily increased this number. Great Britain had a difficult time appointing general officers, however. General Thomas Gage, in command of British forces in North America when the rebellion started, was criticized for being too lenient (perhaps influenced by his American wife). General Jeffrey Amherst, 1st Baron Amherst turned down an appointment as commander in chief due to an unwillingness to take sides in the conflict. Similarly, Admiral Augustus Keppel turned down a command, saying "I cannot draw the sword in such a cause." The Earl of Effingham very publicly resigned his commission when his 22nd Regiment of foot was posted to America, and William Howe and John Burgoyne were both members of parliament who opposed military solutions to the American rebellion. Howe and Henry Clinton both made statements that they were not willing participants in the war, but were following orders. Over the course of the war, Great Britain signed treaties with various German states, which supplied about 30,000 soldiers. Germans made up about one-third of the British troop strength in North America. Hesse-Kassel contributed more soldiers than any other state, and German soldiers became known as "Hessians" to the Americans. Revolutionary speakers called German soldiers "foreign mercenaries," and they are scorned as such in the Declaration of Independence. By 1779, the number of British and German troops stationed in North America was over 60,000, although these were spread from Canada to Florida. About 10,000 Loyalist Americans under arms for the British are included in these figures.
African Americans—slave and free—served on both sides during the war. The British actively recruited slaves belonging to Patriot masters. Because of manpower shortages, George Washington lifted the ban on black enlistment in the Continental Army in January 1776. Small all-black units were formed in Rhode Island and Massachusetts; many slaves were promised freedom for serving. Another all-black unit came from Haiti with French forces. At least 5,000 black soldiers fought for the Revolutionary cause and almost 1,000 black soldiers fought on the British side (although more than 20,000 blacks were with the British at war's end).
Most Native Americans east of the Mississippi River were affected by the war, and many communities were divided over the question of how to respond to the conflict. Though a few tribes were on friendly terms with the Americans, most Native Americans opposed the United States as a potential threat to their territory. Approximately 13,000 Native Americans fought on the British side, with the largest group coming from the Iroquois tribes, who fielded around 1,500 men. The powerful Iroquois Confederacy was shattered as a result of the conflict; the Mohawk, Seneca, Onondaga, and Cayuga sided with the British, while many Tuscarora and Oneida sided with the colonists. The Continental Army sent the Sullivan Expedition to cripple the Iroquois tribes which had sided with the British. Both during and after the war friction between the Mohawks Joseph Louis Cook and Joseph Brant, who had sided with the Americans and the British respectively, further exacerbated the split.
Before the war, Boston had been the scene of much revolutionary activity, leading to the Massachusetts Government Act that ended home rule as a punishment in 1774. Popular resistance to these measures, however, compelled the newly appointed royal officials in Massachusetts to resign or to seek refuge in Boston. Lieutenant General Thomas Gage, the British North American commander-in chief, commanded four regiments of British regulars (about 4,000 men) from his headquarters in Boston, but the countryside was in the hands of the Revolutionaries. On the night of April 18, 1775, General Gage sent 700 men to seize munitions stored by the colonial militia at Concord, Massachusetts. Riders including Paul Revere alerted the countryside, and when British troops entered Lexington on the morning of April 19, they found 77 minutemen formed up on the village green. Shots were exchanged, killing several minutemen. The British moved on to Concord, where a detachment of three companies was engaged and routed at the North Bridge by a force of 500 minutemen. As the British retreated back to Boston, thousands of militiamen attacked them along the roads, inflicting great damage before timely British reinforcements prevented a total disaster. With the Battles of Lexington and Concord, the war had begun. The militia converged on Boston, bottling up the British in the city. About 4,500 more British soldiers arrived by sea, and on June 17, 1775, British forces under General William Howe seized the Charlestown peninsula at the Battle of Bunker Hill. The Americans fell back, but British losses were so heavy that the attack was not followed up. The siege was not broken, and Gage was soon replaced by Howe as the British commander-in-chief. In July 1775, newly appointed General Washington arrived outside Boston to take charge of the colonial forces and to organize the Continental Army. Realizing his army's desperate shortage of gunpowder, Washington asked for new sources. Arsenals were raided and some manufacturing was attempted; 90% of the supply (2 million pounds) was imported by the end of 1776, mostly from France. The standoff continued throughout the fall and winter. In early March 1776, heavy cannons that the patriots had captured at Fort Ticonderoga were brought to Boston by Colonel Henry Knox, and placed on Dorchester Heights. Since the artillery now overlooked the British positions, Howe's situation was untenable, and the British fled on March 17, 1776, sailing to their naval base at Halifax, Nova Scotia. Washington then moved most of the Continental Army to fortify New York City.
Three weeks after the siege of Boston began, a troop of militia volunteers led by Ethan Allen and Benedict Arnold captured Fort Ticonderoga, a strategically important point on Lake Champlain between New York and the Province of Quebec. After that action they also raided Fort St. John's, not far from Montreal, which alarmed the population and the authorities there. In response, Quebec's governor Guy Carleton began fortifying St. John's, and opened negotiations with the Iroquois and other Native American tribes for their support. These actions, combined with lobbying by both Allen and Arnold and the fear of a British attack from the north, eventually persuaded the Congress to authorize an invasion of Quebec, with the goal of driving the British military from that province. (Quebec was then frequently referred to as "Canada", as most of its territory included the former French Province of Canada.) Two Quebec-bound expeditions were undertaken. On September 28, 1775, Brigadier General Richard Montgomery marched north from Fort Ticonderoga with about 1,700 militiamen, besieging and capturing Fort St. Jean on November 2 and then Montreal on November 13. General Carleton escaped to Quebec City and began preparing that city for an attack. The second expedition, led by Colonel Arnold, went through the wilderness of what is now northern Maine. It was a logistical nightmare, with 300 men turning back, and another 200 perishing due to the difficult conditions. By the time Arnold reached Quebec City in early November, he had but 600 of his original 1,100 men. Montgomery's force joined Arnold's, and they attacked Quebec City on December 31, but were defeated by Carleton in a battle that ended with Montgomery dead, Arnold wounded, and over 400 Americans taken prisoner. The remaining Americans held on outside Quebec City until the spring of 1776, suffering from poor camp conditions and smallpox, and then withdrew when a squadron of British ships under Captain Charles Douglas arrived to relieve the siege. Another attempt was made by the Americans to push back towards Quebec, but they failed at Trois-Rivières on June 8, 1776. Carleton then launched his own invasion and defeated Arnold at the Battle of Valcour Island in October. Arnold fell back to Fort Ticonderoga, where the invasion had begun. While the invasion ended as a disaster for the Americans, Arnold's efforts in 1776 delayed a full-scale British counteroffensive until the Saratoga campaign of 1777. The invasion cost the Americans their base of support in British public opinion, "So that the violent measures towards America are freely adopted and countenanced by a majority of individuals of all ranks, professions, or occupations, in this country." It gained them at best limited support in the population of Quebec, which, while somewhat supportive early in the invasion, became less so later during the occupation, when American policies against suspected Loyalists became harsher, and the army's hard currency ran out. Two small regiments of Canadiens were recruited during the operation, and they were with the army on its retreat back to Ticonderoga. New York and New Jersey. Having withdrawn his army from Boston, General Howe now focused on capturing New York City. To defend the city, General Washington divided his 20,000 soldiers between Long Island and Manhattan. While British troops were assembling on Staten Island for the campaign, Washington had the newly issued Declaration of American Independence read to his men. No longer was there any possibility of compromise. On August 27, 1776, after landing about 22,000 men on Long Island, the British drove the Americans back to Brooklyn Heights, securing a decisive British victory in the largest battle of the entire Revolution. Howe then laid siege to fortifications there. In a feat considered by many historians to be one of his most impressive actions as Commander in Chief, Washington personally directed the withdrawal of his entire remaining army and all their supplies across the East River in one night without discovery by the British or the loss of a single man or any materiel. On September 15, Howe landed about 12,000 men on lower Manhattan, quickly taking control of New York City. The Americans withdrew to Harlem Heights, where they skirmished the next day but held their ground. When Howe moved to encircle Washington's army in October, the Americans again fell back, and a battle at White Plains was fought on October 28. Again Washington retreated, and Howe returned to Manhattan and captured Fort Washington in mid November, taking about 2,000 prisoners (with an additional 1,000 having been captured during the battle for Long Island). Thus began the infamous "prison ships" system the British maintained in New York for the remainder of the war, in which more American soldiers and sailors died of neglect than died in every battle of the entire war, combined. General Lord Cornwallis continued to chase Washington's army through New Jersey, until the Americans withdrew across the Delaware River into Pennsylvania in early December. With the campaign at an apparent conclusion for the season, the British entered winter quarters. Although Howe had missed several opportunities to crush the diminishing American army, he had killed or captured over 5,000 Americans. The outlook of the Continental Army was bleak. "These are the times that try men's souls," wrote Thomas Paine, who was with the army on the retreat. The army had dwindled to fewer than 5,000 men fit for duty, and would be reduced to 1,400 after enlistments expired at the end of the year. Congress had abandoned Philadelphia in despair, although popular resistance to British occupation was growing in the countryside. Washington decided to take the offensive, stealthily crossing the Delaware on Christmas night and capturing nearly 1,000 Hessians at the Battle of Trenton on December 26, 1776. Cornwallis marched to retake Trenton but was outmaneuvered by Washington, who successfully attacked the British rearguard at Princeton on January 3, 1777. Washington then entered winter quarters at Morristown, New Jersey, having given a morale boost to the American cause. New Jersey militia continued to harass British and Hessian forces throughout the winter, forcing the British to retreat to their base in and around New York City. At every stage the British strategy assumed a large base of Loyalist supporters would rally to the King given some military support. In February 1776 Clinton took 2,000 men and a naval squadron to invade North Carolina, which he called off when he learned the Loyalists had been crushed at the Battle of Moore's Creek Bridge. In June he tried to seize Charleston, South Carolina, the leading port in the South, hoping for a simultaneous rising in South Carolina. It seemed a cheap way of waging the war but it failed as the naval force was defeated by the forts and because no local Loyalists attacked the town from behind. The loyalists were too poorly organized to be effective, but as late as 1781 senior officials in London, misled by Loyalist exiles, placed their confidence in their rising.
When the British began to plan operations for 1777, they had two main armies in North America: Carleton's army in Quebec, and Howe's army in New York. In London, Lord George Germain approved campaigns for these armies which, because of miscommunication, poor planning, and rivalries between commanders, did not work in conjunction. Although Howe successfully captured Philadelphia, the northern army was lost in a disastrous surrender at Saratoga. Both Carleton and Howe resigned after the 1777 campaign.
The first of the 1777 campaigns was an expedition from Quebec led by General John Burgoyne. The goal was to seize the Lake Champlain and Hudson River corridor, effectively isolating New England from the rest of the American colonies. Burgoyne's invasion had two components: he would lead about 10,000 men along Lake Champlain towards Albany, New York, while a second column of about 2,000 men, led by Barry St. Leger, would move down the Mohawk River Valley and link up with Burgoyne in Albany, New York. Burgoyne set off in June, and recaptured Fort Ticonderoga in early July. Thereafter, his march was slowed by Americans who literally knocked down trees in his path. A detachment was sent out to seize supplies but was decisively defeated in the Battle of Bennington by American militia in August, depriving Burgoyne of nearly 1,000 men. Meanwhile, St. Leger—half of his force Native Americans led by Sayenqueraghta—had laid siege to Fort Stanwix. American militiamen and their Native American allies marched to relieve the siege but were ambushed and scattered at the Battle of Oriskany. When a second relief expedition approached, this time led by Benedict Arnold, St. Leger's Indian support abandoned him, forcing him to break off the siege and return to Quebec. Burgoyne's army had been reduced to about 6,000 men by the loss at Bennington and the need to garrison Ticonderoga, and he was running short on supplies. Despite these setbacks, he determined to push on towards Albany. An American army of 8,000 men, commanded by the General Horatio Gates, had entrenched about 10 miles (16 km) south of Saratoga, New York. Burgoyne tried to outflank the Americans but was checked at the first battle of Saratoga in September. Burgoyne's situation was desperate, but he now hoped that help from Howe's army in New York City might be on the way. It was not: Howe had instead sailed away on his expedition to capture Philadelphia. American militiamen flocked to Gates' army, swelling his force to 11,000 by the beginning of October. After being badly beaten at the second battle of Saratoga, Burgoyne surrendered on October 17. Saratoga was the turning point of the war. Revolutionary confidence and determination, suffering from Howe's successful occupation of Philadelphia, was renewed. What is more important, the victory encouraged France to make an open alliance with the Americans, after two years of semi-secret support. For the British, the war had now become much more complicated.
Having secured New York City in 1776, General Howe concentrated on capturing Philadelphia, the seat of the Revolutionary government, in 1777. He moved slowly, landing 15,000 troops in late August at the northern end of Chesapeake Bay. Washington positioned his 11,000 men between Howe and Philadelphia but was driven back at the Battle of Brandywine on September 11, 1777. The Continental Congress again abandoned Philadelphia, and on September 26, Howe finally outmaneuvered Washington and marched into the city unopposed. Washington unsuccessfully attacked the British encampment in nearby Germantown in early October and then retreated to watch and wait. After repelling a British attack at White Marsh, Washington and his army encamped at Valley Forge in December 1777, about 20 miles (32 km) from Philadelphia, where they stayed for the next six months. Over the winter, 2,500 men (out of 10,000) died from disease and exposure. The next spring, however, the army emerged from Valley Forge in good order, thanks in part to a training program supervised by Baron von Steuben, who introduced the most modern Prussian methods of organization and tactics. General Clinton replaced Howe as British commander-in-chief. French entry into the war had changed British strategy, and Clinton abandoned Philadelphia to reinforce New York City, now vulnerable to French naval power. Washington shadowed Clinton on his withdrawal and forced a strategic victory at the battle at Monmouth on June 28, 1778, the last major battle in the north. Clinton's army went to New York City in July, arriving just before a French fleet under Admiral d'Estaing arrived off the American coast. Washington's army returned to White Plains, New York, north of the city. Although both armies were back where they had been two years earlier, the nature of the war had now changed.
In 1778, the war over the rebellion in North America became international, spreading not only to Europe but to European colonies in the West Indies and in India. From 1776 France had informally been involved, with French admiral Latouche Tréville having provided supplies, ammunition and guns from France to the United States after Thomas Jefferson had encouraged a French alliance, and guns such as de Valliere type were used, playing an important role in such battles as the Battle of Saratoga. George Washington wrote about the French supplies and guns in a letter to General Heath on 2 May 1777. After learning of the American victory at Saratoga, France signed the Treaty of Alliance with the United States on February 6, 1778, formalizing the Franco-American alliance negotiated by Benjamin Franklin. Spain entered the war as an ally of France in June 1779, a renewal of the Bourbon Family Compact. Unlike France, Spain initially refused to recognize the independence of the United States, because Spain was not keen on encouraging similar anti-colonial rebellions in the Spanish Empire. Both countries had quietly provided assistance to the Americans since the beginning of the war, hoping to dilute British power. So too had the Dutch Republic, which was formally brought into the war at the end of 1780. In London King George III gave up hope of subduing America by more armies while Britain had a European war to fight. "It was a joke," he said, "to think of keeping Pennsylvania." There was no hope of recovering New England. But the King was determined "never to acknowledge the independence of the Americans, and to punish their contumacy by the indefinite prolongation of a war which promised to be eternal." His plan was to keep the 30,000 men garrisoned in New York, Rhode Island, Quebec, and Florida; other forces would attack the French and Spanish in the West Indies. To punish the Americans the King planned to destroy their coasting-trade, bombard their ports; sack and burn towns along the coast (as Benedict Arnold did to New London, Connecticut in 1781), and turn loose the Native Americans to attack civilians in frontier settlements. These operations, the King felt, would inspire the Loyalists; would splinter the Congress; and "would keep the rebels harassed, anxious, and poor, until the day when, by a natural and inevitable process, discontent and disappointment were converted into penitence and remorse" and they would beg to return to his authority. The plan meant destruction for the Loyalists and loyal Native Americans, an indefinite prolongation of a costly war, and the risk of disaster as the French and Spanish assembled an armada to invade the British Isles. The British planned to re-subjugate the rebellious colonies after dealing with the Americans' European allies. Widening of the naval war. When the war began, the British had overwhelming naval superiority over the American colonists. The Royal Navy had over 100 ships of the line and many frigates and smaller craft, although this fleet was old and in poor condition, a situation which would be blamed on Lord Sandwich, the First Lord of the Admiralty. During the first three years of the war, the Royal Navy was primarily used to transport troops for land operations and to protect commercial shipping. The American colonists had no ships of the line, and relied extensively on privateering to harass British shipping. The privateers caused worry disproportionate to their material success, although those operating out of French channel ports before and after France joined the war caused significant embarrassment to the Royal Navy and inflamed Anglo-French relations. About 55,000 American sailors served aboard the privateers during the war. The American privateers had almost 1,700 ships, and they captured 2,283 enemy ships. The Continental Congress authorized the creation of a small Continental Navy in October 1775, which was primarily used for commerce raiding. John Paul Jones became the first great American naval hero, capturing HMS "Drake" on April 24, 1778, the first victory for any American military vessel in British waters. France's formal entry into the war meant that British naval superiority was now contested. The Franco-American alliance began poorly, however, with failed operations at Rhode Island in 1778 and Savannah, Georgia, in 1779. Part of the problem was that France and the United States had different military priorities: France hoped to capture British possessions in the West Indies before helping to secure American independence. While French financial assistance to the American war effort was already of critical importance, French military aid to the Americans would not show positive results until the arrival in July 1780 of a large force of soldiers led by the Comte de Rochambeau. Spain entered the war as a French ally with the goal of recapturing Gibraltar and Minorca, which it had lost to the British in 1704. Gibraltar was besieged for more than three years, but the British garrison stubbornly resisted and was resupplied twice: once after Admiral Rodney's victory over Juan de Lángara in the 1780 "Moonlight Battle", and again after Admiral Richard Howe fought Luis de Córdova y Córdova to a draw in the Battle of Cape Spartel. Further Franco-Spanish efforts to capture Gibraltar were unsuccessful. One notable success took place on February 5, 1782, when Spanish and French forces captured Minorca, which Spain retained after the war. Ambitious plans for an invasion of England in 1779 had to be abandoned. West Indies and Gulf Coast. There was much action in the West Indies, with several islands changing hands, especially in the Lesser Antilles. At the Battle of the Saintes in April 1782, a victory by Rodney's fleet over the French Admiral de Grasse frustrated the hopes of France and Spain to take Jamaica and other colonies from the British. On May 8, 1782, Count Bernardo de Gálvez, the Spanish governor of Louisiana, captured the British naval base at New Providence in the Bahamas. On the Gulf Coast, Gálvez quickly removed the British from their outposts on the lower Mississippi River in 1779 in actions at Manchac and Baton Rouge in British West Florida. Gálvez then captured Mobile in 1780 and stormed and captured the British citadel and capital of Pensacola in 1781. His actions led to the Spanish acquisition East and West Florida in the peace settlement. Gálvez' success denied the British the opportunity of encircling the American rebels from the south, and kept open a vital conduit for supplies. George Washington took him to his right in the parade of July 4 and the American Congress cited Gálvez for his aid during the Revolution. Central America was also subject to conflict between Britain and Spain, as Britain sought to expand its influence beyond coastal logging and fishing communities in present-day Belize, Honduras, and Nicaragua. Expeditions against San Fernando de Omoa in 1779 and San Juan in 1780 (the latter famously led by a young Horatio Nelson) met with only temporary success before being abandoned due to disease. The Spanish colonial leaders, in turn, were unable to completely eliminate British influences along the Mosquito Coast. Except for the French acquisition of Tobago, sovereignty in the West Indies was returned to the "status quo ante bellum" in the peace of 1783.
When word reached India in 1778 that France had entered the war, British military forces moved quickly to capture French colonial outposts there, capturing Pondicherry after two months of siege. The capture of the French-controlled port of Mahé on India's west coast motivated Mysore's ruler Hyder Ali (who was already upset at other British actions, and benefited from trade through the port) to open the Second Anglo-Mysore War in 1780. Ali, and later his son Tipu Sultan, almost drove the British from southern India but was frustrated by weak French support, and the war ended "status quo ante bellum" with the 1784 Treaty of Mangalore. French opposition was led in 1782 and 1783 by Admiral the Baillie de Suffren, who recaptured Trincomalee from the British and fought five celebrated, but largely inconclusive, naval engagements against British Admiral Sir Edward Hughes. France's Indian colonies were returned after the war. The Dutch Republic, nominally neutral, had been trading with the Americans, exchanging Dutch arms and munitions for American colonial wares (in contravention of the British "Navigation Acts"), primarily through activity based in St. Eustatius, before the French formally entered the war. The British considered this trade to include contraband military supplies and had attempted to stop it, at first diplomatically by appealing to previous treaty obligations, interpretation of whose terms the two nations disagreed on, and then by searching and seizing Dutch merchant ships. The situation escalated when the British seized a Dutch merchant convoy sailing under Dutch naval escort in December 1779, prompting the Dutch to join the League of Armed Neutrality. Britain responded to this decision by declaring war on the Dutch in December 1780, sparking the Fourth Anglo-Dutch War. The war was a military and economic disaster for the Dutch Republic. Paralyzed by internal political divisions, it was unable to effectively respond to British blockades of its coast and the capture of many of its colonies. In the 1784 peace treaty between the two nations, the Dutch lost the Indian port of Negapatam and were forced to make trade concessions. The Dutch Republic signed a friendship and trade agreement with the United States in 1782, and was the second country (after France) to formally recognize the United States.
During the first three years of the American Revolutionary War, the primary military encounters were in the north, although some attempts to organize Loyalists were defeated, a British attempt at Charleston, South Carolina failed, and a variety of efforts to attack British forces in East Florida failed. After French entry into the war, the British turned their attention to the southern colonies, where they hoped to regain control by recruiting Loyalists. This southern strategy also had the advantage of keeping the Royal Navy closer to the Caribbean, where the British needed to defend economically important possessions against the French and Spanish. On December 29, 1778, an expeditionary corps from Clinton's army in New York captured Savannah, Georgia. An attempt by French and American forces to retake Savannah failed on October 9, 1779. Clinton then besieged Charleston, capturing it and most of the southern Continental Army on May 12, 1780. With relatively few casualties, Clinton had seized the South's biggest city and seaport, paving the way for what seemed like certain conquest of the South. The remnants of the southern Continental Army began to withdraw to North Carolina but were pursued by Lt. Colonel Banastre Tarleton, who defeated them at the Waxhaws on May 29, 1780. With these events, organized American military activity in the region collapsed, though the war was carried on by partisans such as Francis Marion. Cornwallis took over British operations, while Horatio Gates arrived to command the American effort. On August 16, 1780, Gates was defeated at the Battle of Camden, setting the stage for Cornwallis to invade North Carolina. Cornwallis' victories quickly turned, however. One wing of his army was utterly defeated at the Battle of Kings Mountain on October 7, 1780, and Tarleton was decisively defeated by Daniel Morgan at the Battle of Cowpens on January 17, 1781. General Nathanael Greene, who replaced General Gates, proceeded to wear down the British in a series of battles, each of them tactically a victory for the British but giving no strategic advantage to the victors. Greene summed up his approach in a motto that would become famous: "We fight, get beat, rise, and fight again." By March, Greene's army had grown to the point were he felt that he could face Cornwallis directly. In the key Battle of Guilford Court House, Cornwallis defeated Greene, but at tremendous cost, and without breaking Greene's army. He retreated to Wilmington, North Carolina for resupply and reinforcement, after which he moved north into Virginia, leaving the Carolinas and Georgia open to Greene. In March 1781, General Washington dispatched General Lafayette to defend Virginia, and in April, a British force under the recently-turned Benedict Arnold landed there. Arnold moved through the Virginia countryside, destroying supply depots, mills, and other economic targets, before joining his army with that of Cornwallis. Lafayette skirmished with Cornwallis, avoiding a decisive battle while gathering reinforcements. Cornwallis was unable to trap Lafayette, and so he moved his forces to Yorktown, Virginia, in July so the Royal Navy could return his army to New York.
West of the Appalachian Mountains and along the border with Quebec, the American Revolutionary War was an "Indian War". Most Native Americans supported the British. Like the Iroquois Confederacy, tribes such as the Cherokees and the Shawnees split into factions. The British supplied their native allies with muskets and gunpowder and advised raids against civilian settlements, especially in New York, Kentucky, and Pennsylvania. Joint Iroquois-Loyalist attacks in the Wyoming Valley and at Cherry Valley in 1778 provoked Washington to send the Sullivan Expedition into western New York during the summer of 1779. There was little fighting as Sullivan systematically destroyed the Native American winter food supplies, forcing them to flee permanently to British bases in Quebec and the Niagara Falls area. In the Ohio Country and the Illinois Country, the Virginia frontiersman George Rogers Clark attempted to neutralize British influence among the Ohio tribes by capturing the outposts of Kaskaskia and Cahokia and Vincennes in the summer of 1778. When General Henry Hamilton, the British commander at Detroit, retook Vincennes, Clark returned in a surprise march in February 1779 and captured Hamilton himself. In March 1782, Pennsylvania militiamen killed about a hundred neutral Native Americans in the Gnadenhütten massacre. In one of the last major encounters of the war, a force of 200 Kentucky militia was defeated at the Battle of Blue Licks in August 1782. Yorktown and the Surrender of Cornwallis. The northern, southern, and naval theaters of the war converged in 1781 at Yorktown, Virginia. In early September, French naval forces defeated a British fleet at the Battle of the Chesapeake, cutting off Cornwallis' escape. Washington hurriedly moved American and French troops from New York, and a combined Franco-American force of 17,000 men commenced the Siege of Yorktown in early October. For several days, the French and Americans bombarded the British defenses. Cornwallis' position quickly became untenable, and he surrendered his entire army of 7,000 men on October 19, 1781. With the surrender at Yorktown, King George lost control of Parliament to the peace party, and there were no further major military activities on land. The British had 30,000 garrison troops occupying New York City, Charleston, and Savannah. The war continued at sea between the British and the French fleets in the West Indies.
In London, as political support for the war plummeted after Yorktown, Prime Minister Lord North resigned in March 1782. In April 1782, the Commons voted to end the war in America. Preliminary peace articles were signed in Paris at the end of November, 1782; the formal end of the war did not occur until the Treaty of Paris and Treaties of Versailles were signed on September 3, 1783. The last British troops left New York City on November 25, 1783, and the United States Congress of the Confederation ratified the Paris treaty on January 14, 1784. Britain negotiated the Paris peace treaty without consulting her Native American allies and ceded all Native American territory between the Appalachian Mountains and the Mississippi River to the United States. Full of resentment, Native Americans reluctantly confirmed these land cessions with the United States in a series of treaties, but the fighting would be renewed in conflicts along the frontier in the coming years, the largest being the Northwest Indian War. Advantages/disadvantages of the opposing sides. During the war the Americans benefited greatly from international assistance. In addition, Britain had significant military disadvantages. Distance was a major problem: most troops and supplies had to be shipped across the Atlantic Ocean. The British usually had logistical problems whenever they operated away from port cities, while the Americans had local sources of manpower and food and were more familiar with (and accustomed to) the territory. Additionally, ocean travel meant that British communications were always about two months out of date: by the time British generals in America received their orders from London, the military situation had usually changed. Suppressing a rebellion in America also posed other problems. Since the colonies covered a large area and had not been united before the war, there was no central area of strategic importance. In Europe, the capture of a capital often meant the end of a war; in America, when the British seized cities such as New York and Philadelphia, the war continued unabated. Furthermore, the large size of the colonies meant that the British lacked the manpower to control them by force. Once any area had been occupied, troops had to be kept there or the Revolutionaries would regain control, and these troops were thus unavailable for further offensive operations. The British had sufficient troops to defeat the Americans on the battlefield but not enough to simultaneously occupy the colonies. This manpower shortage became critical after French and Spanish entry into the war, because British troops had to be dispersed in several theaters, where previously they had been concentrated in America. The British also had the difficult task of fighting the war while simultaneously retaining the allegiance of Loyalists. Loyalist support was important, since the goal of the war was to keep the colonies in the British Empire, but this imposed numerous military limitations. Early in the war, the Howe brothers served as peace commissioners while simultaneously conducting the war effort, a dual role which may have limited their effectiveness. Additionally, the British could have recruited more slaves and Native Americans to fight the war, but this would have alienated many Loyalists, even more so than the controversial hiring of German mercenaries. The need to retain Loyalist allegiance also meant that the British were unable to use the harsh methods of suppressing rebellion they employed in Ireland and Scotland. Even with these limitations, many potentially neutral colonists were nonetheless driven into the ranks of the Revolutionaries because of the war. This combination of factors led ultimately to the downfall of British rule in America and the rise of the revolutionaries' own independent nation, the United States of America.
The total loss of life resulting from the American Revolutionary War is unknown. As was typical in the wars of the era, disease claimed more lives than battle. Between 1775 and 1782, a smallpox epidemic raged across much of North America, killing more than 130,000 people. Historian Joseph Ellis suggests that Washington's decision to have his troops inoculated against the smallpox epidemic was one of his most important decisions. Approximately 25,000 American Revolutionaries died during active military service. About 8,000 of these deaths were in battle; the other 17,000 deaths were from disease, including about 8,000 – 12,000 who died while prisoners of war, most in rotting prison ships in New York. The number of Revolutionaries seriously wounded or disabled by the war has been estimated from 8,500 to 25,000. The total American military casualty figure was therefore as high as 50,000. About 171,000 sailors served for the British during the war; about 25 to 50 percent of them had been pressed into service. About 1,240 were killed in battle, while 18,500 died from disease. The greatest killer was scurvy, a disease known at the time to be easily preventable by issuing lemon juice to sailors. About 42,000 British sailors deserted during the war. Approximately 1,200 Germans were killed in action and 6,354 died from illness or accident. About 16,000 of the remaining German troops returned home, but roughly 5,500 remained in the United States after the war for various reasons, many eventually becoming American citizens. No reliable statistics exist for the number of casualties among other groups, including Loyalists, British regulars, Native Americans, French and Spanish troops, and civilians.
The British spent about £80 million and ended with a national debt of £250 million, which it easily financed at about £9.5 million a year in interest. The French spent 1.3 billion livres (about £56 million). Their total national debt was £187 million, which they could not easily finance; over half the French national revenue went to debt service in the 1780s. The debt crisis became a major enabling factor of the French Revolution as the government was unable to raise taxes without public approval. The United States spent $37 million at the national level plus $114 million by the states. This was mostly covered by loans from France and the Netherlands, loans from Americans, and issuance of an increasing amount of paper money (which became "not worth a continental.") The U.S. finally solved its debt and currency problems in the 1790s when Alexander Hamilton spearheaded the establishment of the First Bank of the United States.
The ampere (symbol: A) is the SI unit of electric current and is one of the seven SI base units. It is named after André-Marie Ampère (1775–1836), French mathematician and physicist, considered the father of electrodynamics. In practice, its name is often shortened to amp. In practical terms, the ampere is a measure of the amount of electric charge passing a point per unit time. Around 6.242 × 1018 electrons passing a given point each second constitutes one ampere. (Since electrons have negative charge, they flow in the opposite direction to the conventional current.)
Ampère's force law states that there is an attractive force between two parallel wires carrying an electric current. This force is used in the formal definition of the ampere which states that it is "the constant current which will produce an attractive force of 2 × 10–7 newtons per metre of length between two straight, parallel conductors of infinite length and negligible circular cross section placed one metre apart in a vacuum". In terms of Ampère's force law, The SI unit of charge, the coulomb, "is the quantity of electricity carried That is, in general, charge "Q" is determined by steady current "I" flowing for a time "t" as "Q" = "It".
The ampere was originally defined as one tenth of the CGS system electromagnetic unit of current (now known as the abampere), the amount of current which generates a force of two dynes per centimetre of length between two wires one centimetre apart. The size of the unit was chosen so that the units derived from it in the MKSA system would be conveniently sized. The "international ampere" was an early realisation of the ampere, defined as the current that would deposit grams of silver per second from a silver nitrate solution. Later, more accurate measurements revealed that this current is 0.99985 A.
The ampere is most accurately realised using a watt balance, but is in practice maintained via Ohm's Law from the units of electromotive force and resistance, the volt and the ohm, since the latter two can be tied to physical phenomena that are relatively easy to reproduce, the Josephson junction and the quantum Hall effect, respectively. At present, techniques to establish the realisation of an ampere have a relative uncertainty of approximately a few parts in 107, and involve realisations of the watt, the ohm and the volt.
Rather than a definition in terms of the force between two current-carrying wires, it has been proposed to define the ampere in terms of the rate of flow of elementary charges. Since a coulomb is approximately equal to elementary charges, one ampere is approximately equivalent to elementary charges, such as electrons, moving past a boundary in one second. The proposed change would define 1 A as being the current in the direction of flow of a particular number of elementary charges per second. In 2005, the International Committee for Weights and Measures (CIPM) agreed to study the proposed change, and, depending on the outcome of experiments over the next few years, to formally propose the change at the 24th General Conference on Weights and Measures (CGPM) in 2011.
In mathematics, computer science, and related subjects, an algorithm is an effective method for solving a problem using a finite sequence of instructions. Algorithms are used for calculation, data processing, and many other fields. Each algorithm is a list of well-defined instructions for completing a task. Starting from an initial state, the instructions describe a computation that proceeds through a well-defined series of successive states, eventually terminating in a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate randomness. A partial formalization of the concept began with attempts to solve the Entscheidungsproblem (the "decision problem") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define "effective calculability" or "effective method"; those formalizations included the Gödel-Herbrand-Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's "Formulation 1" of 1936, and Alan Turing's Turing machines of 1936–7 and 1939.
Al-Khwārizmī, Persian astronomer and mathematician, wrote a treatise in the arabic language in 825 AD, "On Calculation with Hindu–Arabic numeral system". (See algorism). It was translated from arabic into Latin in the 12th century as "Algoritmi de numero Indorum" (al-Daffa 1977), whose title is supposedly likely intended to mean "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's rendition of the author's name; but people misunderstanding the title treated "Algoritmi" as a Latin plural and this led to the word "algorithm" (Latin "algorismus") coming to mean "calculation method". The intrusive "th" is most likely due to a false cognate with the Greek ("arithmos") meaning "numbers". Why algorithms are necessary: an informal definition. While there is no generally accepted "formal" definition of "algorithm," an informal definition could be "a process that performs some sequence of operations." For some people, a program is only an algorithm if it stops eventually. For others, a program is only an algorithm if it stops before a given number of calculation steps. A prototypical example of an algorithm is Euclid's algorithm to determine the maximum common divisor of two integers. No human being can write fast enough, or long enough, or small enough† (†"smaller and smaller without limit...you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the "n"th member of the set, for arbitrary finite "n". Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols The concept of "algorithm" is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of "algorithm" that suits both concrete (in some sense) and abstract usage of the term.
Minsky: "But we will also maintain, with Turing... that any procedure which could "naturally" be called effective, can in fact be realized by a (simple) machine. Although this may seem extreme, the arguments... in its favor are hard to refute". Gurevich: "...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine... according to Savage [1987], an algorithm is a computational process defined by a Turing machine". Typically, when an algorithm is associated with processing information, data is read from an input source, written to an output device, and/or stored for further processing. Stored data is regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures. For any such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable). Because an algorithm is a precise list of precise steps, the order of computation will always be critical to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by "flow of control". So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of "memory" as a scratchpad. There is an example below of such an assignment. For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.
Some writers restrict the definition of "algorithm" to procedures that eventually finish. In such a category Kleene places the "decision procedure" or "decision method" or "algorithm" for the question". Others, including Kleene, include procedures that could run forever without stopping; such a procedure has been called a "computational method" or "calculation procedure" or "algorithm" (and hence a "calculation problem") in relation to a general question which requires for an answer, not yes or no, but the exhibiting of some object". But if the length of the process isn't known in advance, then "trying" it may not be decisive, because if the process does go on forever — then at no time will we ever be sure of the answer. As it happens, no other method can do any better, as was shown by Alan Turing with his celebrated result on the undecidability of the so-called halting problem. There is no algorithmic procedure for determining of arbitrary algorithms whether or not they terminate from given starting states. The analysis of algorithms for their likelihood of termination is called termination analysis. We normally require auxiliary evidence for this [that the algorithm correctly defines a mu recursive function], e.g., in the form of an inductive proof that, for each argument value, the computation terminates with a unique value.
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements, while remaining independent of a particular implementation language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define or document algorithms. There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite state machine and state transition table), as flowcharts (see more at state diagram), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see more at Turing machine). Sometimes it is helpful in the description of an algorithm to supplement small "flow charts" (state diagrams) with natural-language and/or arithmetic expressions written inside "block diagrams" to summarize what the "flow charts" are accomplishing.
Note that CASE 3 includes two possibilities: (i) the document is NOT located at 'D:/My Documents' AND there's paper in the printer OR (ii) the document is NOT located at 'D:/My Documents' AND there's NO paper in the printer. These examples' logic grants precedence to the instance of "NO document at 'D:/My Documents' ". Also observe that in a well-crafted CASE statement or sequence of IF-THEN-ELSE statements the number of distinct actions—4 in these examples: do nothing, print the document, display 'document not found', display 'out of paper' -- equals the number of cases. Given unlimited memory, a computational machine with the ability to execute either a set of CASE statements or a sequence of IF-THEN-ELSE statements is Turing complete. Therefore, anything that is computable can be computed by this machine. This form of algorithm is fundamental to computer programming in all its forms (see more at McCarthy formalism).
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the algorithm above has a time requirement of O("n"), using the big O notation with "n" as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore it is said to have a space requirement of "O(1)", if the space required to store the input numbers is not counted, or O("n") if it is counted. Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm will usually outperform a brute force sequential search when used for table lookups on sorted lists.
The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. Empirical testing is useful because it may uncover unexpected interactions that affect performance. For instance an algorithm that has no locality of reference may have much poorer performance than predicted because it 'thrashes the cache'. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, machine learning, cryptography, data compression algorithms and parsing techniques. Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.
Algorithms can be classified by the amount of time they need to complete compared to their input size. There is a wide variety: some algorithms complete in linear time relative to input size, some do so in an exponential amount of time or even worse, and some never halt. Additionally, some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.
Another way to classify algorithms is by computing power. This is typically done by considering some collection (class) of algorithms. A recursive class of algorithms is one that includes algorithms for all Turing computable functions. Looking at classes of algorithms allows for the possibility of restricting the available computational resources (time and memory) used in a computation. A subrecursive class of algorithms is one in which not all Turing computable functions can be obtained. For example, the algorithms that run in polynomial time suffice for many important types of computation but do not exhaust all Turing computable functions. The class of algorithms implemented by primitive recursive functions is another subrecursive class. Burgin (2005, p. 24) uses a generalized definition of algorithms that relaxes the common requirement that the output of the algorithm that computes a function must be determined after a finite number of steps. He defines a super-recursive class of algorithms as "a class of algorithms in which it is possible to compute functions not computable by any Turing machine" (Burgin 2005, p. 107). This is closely related to the study of methods of hypercomputation.
Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However, practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).
The word "algorithm" comes from the name of the 9th century Persian mathematician Muhammad ibn Mūsā al-Khwārizmī whose works introduced Indian numerals and algebraic concepts. He worked in Baghdad at the time when it was the centre of scientific studies and trade. The word "algorism" originally referred only to the rules of performing arithmetic using Arabic numerals but evolved via European Latin translation of al-Khwarizmi's name into "algorithm" by the 18th century. The word evolved to include all definite procedures for solving problems or performing tasks.
Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post-Turing machine computations. Mechanical contrivances with discrete states. The clock: Bolter credits the invention of the weight-driven clock as “The key invention [of Europe in the Middle Ages]", in particular the verge escapement that provides us with the tick and tock of a mechanical clock. “The accurate automatic machine” led immediately to "mechanical automata" beginning in the thirteenth century and finally to “computational machines" – the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace. Logical machines 1870 -- Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form similar to what are now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically... More recently however I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc]...". With this machine he could analyze a "syllogism or any other simple logical argument". This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 "Symbolic Logic", turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's "abacus"... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine... but I suppose that it could do very completely all that can be rationally expected of any logical machine". Jacquard loom, Hollerith punch cards, telegraphy and telephony — the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and “telephone switching technologies” were the roots of a tree leading to the development of the first computers. By the mid-1800s the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as “dots and dashes” a common sound. By the late 1800s the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the Teletype (ca. 1910) with its punched-paper use of Baudot code on tape. Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the “burdensome’ use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device". Mathematics during the 1800s up to the mid-1900s. Symbols and rules: In rapid succession the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's "The principles of arithmetic, presented by a new method" (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language". But Heijenoort gives Frege (1879) this kudos: Frege’s is "perhaps the most important single work ever written in logic.... in which we see a " 'formula language', that is a "lingua characterica", a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments... constructed from specific symbols that are manipulated according to definite rules". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913). The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel’s paper (1931) — he specifically cites the paradox of the liar — that completely reduces rules of recursion to numbers. Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely-honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine" -- in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine". S. C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I", and a few years later Kleene's renaming his Thesis "Church's Thesis" and proposing "Turing's Thesis". Emil Post (1936) and Alan Turing (1936-7, 1939). Here is a remarkable coincidence of two men not knowing each other but describing a process of men-as-computers working on computations — and they yield virtually identical definitions. Alan Turing’s work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing’s biographer believed that Turing’s use of a typewriter-like model derived from a youthful interest: “Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'". Given the prevalence of Morse code and telegraphy, ticker tape machines, and Teletypes we might conjecture that all were influences. Turing — his model of computation is now called a Turing machine — begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers. J. B. Rosser (1939) and S. C. Kleene (1943). Rosser's footnote #5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his "An Unsolvable Problem of Elementary Number Theory" (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper "On Formally Undecidable Propositions of Principia Mathematica and Related Systems I" (1931); and (3) Post (1936) and Turing (1936-7) in their mechanism-models of computation.
An annual plant is a plant that usually germinates, flowers, and dies in a year or season. True annuals will only live longer than a year if they are prevented from setting seed. Some seedless plants can also be considered annuals even though they do not grow a flower. In gardening, annual often refers to a plant grown outdoors in the spring and summer and surviving just for one growing season. Many food plants are, or are grown as, annuals, including most domesticated grains. Some perennials and biennials are grown in gardens as annuals for convenience, particularly if they are not considered cold hardy for the local climate. Carrot, celery and parsley are true biennials that are usually grown as annual crops for their edible roots, petioles and leaves, respectively. Tomato, sweet potato and bell pepper are tender perennials usually grown as annuals. Ornamental annualer perennials commonly grown as annuals are impatiens, wax begonia, snapdragon, "Pelargonium", coleus and petunia. Some biennials that can be grown as annuals are pansy and hollyhock. One seed-to-seed life cycle for an annual can occur in as little as a month in some species, though most last several months. Oilseed rapa can go from seed-to-seed in about five weeks under a bank of fluorescent lamps in a school classroom. Many desert annuals are therophytes, because their seed-to-seed life cycle is only weeks and they spend most of the year as seeds to survive dry conditions. Examples of true annuals include corn, lettuce, pea, cauliflower, watermelon, bean, zinnia and marigold.
Winter Annuals are plants that have an annual life span but tend to germinate in the fall or winter and bloom in late autumn/fall, winter or early spring. The plants grow and bloom during the cool season when most other plants are dormant or other annuals are in seed form waiting for warmer weather to germinate. Winter annuals die after flowering and setting seed, the seeds wait to germinate until the soil temperature is cool again in the fall or winter. Winter annuals typically grow low to the ground, where they are usually sheltered from the coldest nights by snow cover, and make use of warm periods in winter for growth when the snow melts. Some common winter annuals include henbit, deadnettle, chickweed, and winter cress. Winter annuals are important ecologically, as they provide vegetative cover that prevents soil erosion during winter and early spring when no other cover exists and they provide fresh vegetation for animals and birds that feed on them. Although they are often considered to be weeds in gardens, this viewpoint is not always necessary, as most of them die when the soil temperature warms up again in early to late spring when other plants are still dormant and have not yet leafed out. Even though they do not compete directly with cultivated plants, sometimes winter annuals are considered a pest in commercial agriculture, because they can be hosts for insect pests or fungal diseases (ovary smut - Microbotryum sp) which attack crops being cultivated. Ironically, the property that they prevent the soil from drying out can also be problematic for commercial agriculture.
The anthophytes were thought to be a clade comprising plants bearing flower-like structures. The group contained the angiosperms - the extant flowering plants - as well as the Gnetales and the extinct Bennettitales. Detailed morphological and molecular studies have shown that the group is not actually monophyletic. This makes it easier to reconcile molecular clock data that suggests that the angiosperms diverged from the gymnosperms around. Some more recent studies have used the word anthophyte to describe a group which includes the angiosperms and a variety of fossils (glossopterids, "Pentoxylon", Bennettitales, and "Caytonia"), but not the Gnetales.
Mouthwash or mouth rinse is a product used to enhance oral hygiene. Antiseptic and anti-plaque mouth rinse claims to kill the bacterial plaque causing caries, gingivitis, and bad breath. Anti-cavity mouth rinse uses fluoride to protect against tooth decay. It is, however, generally agreed that the use of mouthwash does not eliminate the need for both brushing and flossing. As per the American Dental Association, regular brushing and proper flossing are enough in most cases (in addition to regular dental check-ups) and mouthwash should only be used as a short-term solution. Mouthwash may also be used to help remove mucus and food particles deeper down in the throat. Alcoholic and strongly flavored mouthwash may cause coughing when used for this purpose.
The first known references to mouth rinsing is in Ayurveda and Chinese medicine, about 2700 BC, for treatment of gingivitis. Later, in the Greek and Roman periods, mouthrinsing following mechanical cleansing became common among the upper classes, and Hippocrates recommended a mixture of salt, alum, and vinegar. The Jewish Talmud, dating back about 1800 years, suggests a cure for gum ailments containing "dough water" and olive oil. Anton van Leeuwenhoek, the famous 17th century microscopist, discovered living organisms (living, because they were motile) in deposits on the teeth (what we now call dental plaque). He also found organisms in water from the canal next to his home in Delft. He experimented with samples by adding vinegar or brandy and found that this resulted in the immediate immobilization or killing of the organisms suspended in water. Next he tried rinsing the mouth of himself and somebody else with a rather foul mouthwash containing vinegar or brandy and found that living organisms remained in the dental plaque. He concluded—correctly—that the mouthwash either did not reach, or was not present long enough, to kill the plaque organisms. That remained the state of affairs until the late 1960s when Harald Loe (at the time a professor at the Royal Dental College in Aarhus, Denmark) demonstrated that a chlorhexidine compound could prevent the build-up of dental plaque. The reason for chlorhexidine effectiveness is that it strongly adheres to surfaces in the mouth and thus remains present in effective concentrations for many hours. Since then commercial interest in mouthwashes has been intense and several newer products claim effectiveness in reducing the build-up in dental plaque and the associated severity of gingivitis (inflammation of the gums), in addition to fighting bad breath. Many of these solutions aim to control the Volatile Sulfur Compound (VSC)-creating anaerobic bacteria that live in the mouth and excrete substances that lead to bad breath and unpleasant mouth taste.
Common use involves rinsing the mouth with about 20ml (2/3 fl oz) of mouthwash two times a day after brushing. The wash is typically swished or gargled for about half a minute and then spat out. In some brands, the expectorate is stained, so that one can see the bacteria and debris. It is probably advisable to use mouthwash at least an hour after brushing with toothpaste when the toothpaste contains sodium lauryl sulfate, since the anionic compounds in the SLS toothpaste can deactivate cationic agents present in the mouthrinse.
Active ingredients in commercial brands of mouthwash can include thymol, eucalyptol, hexetidine, methyl salicylate, menthol, chlorhexidine gluconate, benzalkonium chloride, cetylpyridinium chloride, methylparaben, hydrogen peroxide, domiphen bromide and sometimes fluoride, enzymes and calcium. Ingredients also include water, sweeteners such as sorbitol, sucralose, sodium saccharine, and xylitol (which doubles as a bacterial inhibitor). Sometimes a significant amount of alcohol (up to 27% vol) is added, as a carrier for the flavor, to provide "bite", and to contribute an antibacterial effect. Because of the alcohol content, it is possible to fail a breathalyzer test after rinsing; in addition, alcohol is a drying agent and may worsen chronic bad breath. Furthermore, it is possible for alcoholics to abuse mouthwash. Recently, the possibility that the alcohol used in mouthrinses acts as a carcinogen was raised, but there is to date no scientific consensus on the issue. Commercial mouthwashes usually contain a preservative such as sodium benzoate to preserve freshness once the container has been opened. Many newer brands are alcohol-free and contain odor-elimination agents such as oxidizers, as well as odor-preventing agents such as zinc ion to keep future bad breath from developing.
A salt mouthwash is a home treatment for mouth infections and/or injuries, or post extraction, and is made by dissolving a teaspoon of salt in a cup of warm water. Recently, the use of herbal mouthwashes such as persica is increasing, due to the perceived discoloration effects and unpleasant taste of Chlorhexidine. Research has also indicated that sesame and sunflower oils as cheap alternatives compared to chlorhexidine. Other products like hydrogen peroxide have been tried out as stand-alone and in combination with chlorhexidine, due to some inconsistent results regarding its usefulness. Another study has demonstrated that daily use of an alum-containing mouthrinse was safe and produced a significant effect on plaque that supplemented the benefits of daily toothbrushing. However, many studies acknowledge that Chlorhexidine remains the most effective mouthwash when used on an already clean tooth surface or immediately after surgery. As chlorhexidine has difficulty in penetrating plaque biofilm, other mouthwashes may be more effective where pre-existing plaque is present. One side-effect as noted on the label is the staining of the teeth will occur for prolong usage.
In January 2009 a report published in the "Dental Journal of Australia" concluded there is "sufficient evidence" that "alcohol-containing mouthwashes contribute to the increased risk of development of oral cancer". However, this claim has been disputed by Yinka Ebo of Cancer Research UK, concluding that "there is still not enough evidence to suggest that using mouthwash that contains alcohol will increase the risk of mouth cancer". More recent studies have shown that the risk of acquiring cancer rises almost five times in mouthwash users who neither smoke nor drink (with a higher rate of increase in those who do). The same study also highlighted side effects from several mainstream mouthwashes that included dental erosion and accidental poisoning of children.
Alexander III of Macedon (356–323 BC), popularly known as Alexander the Great (, "Mégas Aléxandros"), was a Greek king (basileus) of Macedon. He is the most celebrated member of the Argead Dynasty and created one of the largest empires in ancient history. Born in Pella in 356 BC, Alexander received a classical Greek education under the tutorship of famed philosopher Aristotle, succeeded his father Philip II of Macedon to the throne in 336 BC after the King was assassinated, and died thirteen years later at the age of 32. Although both Alexander's reign and empire were short-lived, the cultural impact of his conquests lasted for centuries. Alexander is one of the most famous figures of antiquity, and is remembered for his tactical ability, his conquests, and for spreading Greek culture into the East (marking the beginning of Hellenistic civilization). Philip had brought most of the city-states of mainland Greece under Macedonian hegemony, using both military and diplomatic means. Upon Philip's death, Alexander inherited a strong kingdom and an experienced army. He succeeded in being awarded the generalship of Greece and, with his authority firmly established, launched the military plans for expansion left by his father. He invaded Persian-ruled Asia Minor, and began a series of campaigns lasting ten years. Alexander repeatedly defeated the Persians in battle; marched through Syria, Egypt, Mesopotamia, Persia, and Bactria; and in the process he overthrew the Persian king Darius III and conquered the entirety of the Persian Empire. Following his desire to reach the "ends of the world and the Great Outer Sea", he invaded India, but was eventually forced to turn back by the near-mutiny of his troops, who were tired of war. Alexander died in Babylon in 323 BC, before realizing a series of planned campaigns that would have begun with an invasion of Arabia. In the years following Alexander's death, a series of civil wars tore his empire apart, which resulted in the formation of a number of states ruled by Macedonian aristocracy (the Diadochi). Remarkable though his conquests were, Alexander's lasting legacy was not his reign, but the cultural diffusion his conquests engendered. Alexander's importation of Greek colonists and culture to the East resulted in a new "Hellenistic" culture, aspects of which were still evident in the traditions of the Byzantine Empire until the mid-15th century. Alexander became legendary as a classical hero in the mould of Achilles, and features prominently in the history and myth of Greek and non-Greek cultures. He became the measure against which generals, even to this day, compare themselves, and military academies throughout the world still teach his tactical exploits.
Alexander was born on 20 (or 21) July 356 BC, in Pella, the capital of the Kingdom of Macedon. He was the son of Philip II, the King of Macedon. His mother was Philip's fourth wife Olympias, the daughter of Neoptolemus I, the king of the northern Greek state of Epirus. Although Philip had either seven or eight wives, Olympias was his principal wife for a time. As a member of the Argead dynasty, Alexander claimed patrilineal descent from Heracles through Caranus of Macedon. From his mother's side and the Aeacids, he claimed descent from Neoptolemus, son of Achilles; Alexander was a second cousin of the celebrated general Pyrrhus of Epirus, who was ranked by Hannibal as, depending on the source, either the best or second-best (after Alexander) commander the world had ever seen. According to the ancient Greek historian Plutarch, Olympias, on the eve of the consummation of her marriage to Philip, dreamed that her womb was struck by a thunder bolt, causing a flame that spread "far and wide" before dying away. Some time after the wedding, Philip was said to have seen himself, in a dream, sealing up his wife's womb with a seal upon which was engraved the image of a lion. Plutarch offers a variety of interpretations of these dreams: that Olympia was pregnant before her marriage, indicated by the sealing of her womb; or that Alexander's father was Zeus. Ancient commentators were divided as to whether the ambitious Olympias promulgated the story of Alexander's divine parentage, some claiming she told Alexander, others that she dismissed the suggestion as impious. On the day that Alexander was born, Philip was preparing himself for his siege on the city of Potidea on the peninsula of Chalkidiki. On the same day, Philip received news that his general Parmenion had defeated the combined Illyrian and Paeonian armies, and that his horses had won at the Olympic Games. It was also said that on this day, the Temple of Artemis in Ephesus—one of the Seven Wonders of the World—burnt down, leading Hegesias of Magnesia to say that it burnt down because Artemis was attending the birth of Alexander. In his early years, Alexander was raised by his nurse, Lanike, the sister of Alexander's future friend and general Cleitus the Black. Later on in his childhood, Alexander was tutored by the strict Leonidas, a relative of his mother, and by Lysimachus. When Alexander was ten years old, a horse trader from Thessaly brought Philip a horse, which he offered to sell for thirteen talents. The horse refused to be mounted by anyone, and Philip ordered it to be taken away. Alexander, however, detected the horse's fear of his own shadow and asked for a turn to tame the horse, which he eventually managed. According to Plutarch, Philip, overjoyed at this display of courage and ambition, kissed him tearfully, declaring: "My boy, you must find a kingdom big enough for your ambitions. Macedon is too small for you", and bought the horse for him. Alexander would name the horse Bucephalus, meaning 'ox-head'. Bucephalus would be Alexander's companion throughout his journeys as far as India. When Bucephalus died (due to old age, according to Plutarch, for he was already thirty), Alexander named a city after him (Bucephala).
When Alexander was thirteen years old, Philip decided that Alexander needed a higher education, and he began to search for a tutor. Many people were passed over including Isocrates and Speusippus, Plato's successor at the Academy, who offered to resign to take up the post. In the end, Philip offered the job to Aristotle, who accepted, and Philip gave them the Temple of the Nymphs at Mieza as their classroom. In return for teaching Alexander, Philip agreed to rebuild Aristotle's hometown of Stageira, which Philip had razed, and to repopulate it by buying and freeing the ex-citizens who were slaves, or pardoning those who were in exile. Mieza was like a boarding school for Alexander and the children of Macedonian nobles, such as Ptolemy, Hephaistion, and Cassander. Many of the pupils who learned by Alexander's side would become his friends and future generals, and are often referred to as the 'Companions'. At Mieza, Aristotle educated Alexander and his companions in medicine, philosophy, morals, religion, logic, and art. From Aristotle's teaching, Alexander developed a passion for the works of Homer, and in particular the Iliad; Aristotle gave him an annotated copy, which Alexander was to take on his campaigns. Regency and ascent of Macedon. When Alexander became sixteen years old, his tutorship under Aristotle came to an end. Philip, the king, departed to wage war against Byzantium, and Alexander was left in charge as regent of the kingdom. During Philip's absence, the Thracian Maedi revolted against Macedonian rule. Alexander responded quickly; he crushed the Maedi insurgence, driving them from their territory, colonised it with Greeks, and founded a city named Alexandropolis. After Philip's return from Byzantium, he dispatched Alexander with a small force to subdue certain revolts in southern Thrace. During another campaign against the Greek city of Perinthus, Alexander is reported to have saved his father's life. Meanwhile, the city of Amphissa began to work lands that were sacred to Apollo near Delphi, a sacrilege that gave Philip the opportunity to further intervene in the affairs of Greece. Still occupied in Thrace, Philip ordered Alexander to begin mustering an army for a campaign in Greece. Concerned with the possibility of other Greek states intervening, Alexander made it look as if he were preparing to attack Illyria instead. During this turmoil, the Illyrians took the opportunity to invade Macedonia, but Alexander repelled the invaders. Philip joined Alexander with his army in 338 BC, and they marched south through Thermopylae, which they took after a stubborn resistance from its Theban garrison. They went on to occupy the city of Elatea, a few days march from both Athens and Thebes. Meanwhile, the Athenians, led by Demosthenes, voted to seek an alliance with Thebes in the war against Macedonia. Both Athens and Philip sent embassies to try to win Thebes's favour, with the Athenians eventually succeeding. Philip marched on Amphissa (theoretically acting on the request of the Amphicytonic League), captured the mercenaries sent there by Demosthenes, and accepted the city's surrender. Philip then returned to Elatea and sent a final offer of peace to Athens and Thebes, which was rejected. As Philip marched south, he was blocked near Chaeronea, Boeotia by the forces of Athens and Thebes. During the ensuing Battle of Chaeronea, Philip commanded the right, and Alexander the left wing, accompanied by a group of Philip's trusted generals. According to the ancient sources, the two sides fought bitterly for a long time. Philip deliberately commanded the troops on his right wing to backstep, counting on the untested Athenian hoplites to follow, thus breaking their line. On the left, Alexander was the first to break into the Theban lines, followed by Philip's generals. Having achieved a breach in the enemy's cohesion, Philip ordered his troops to press forward and quickly routed his enemy. With the rout of the Athenians, the Thebans were left to fight alone; surrounded by the victorious enemy, they were crushed. After the victory at Chaeronea, Philip and Alexander marched unopposed into the Peloponnese welcomed by all cities; however, when they reached Sparta, they were refused, and they simply left. At Corinth, Philip established a "Hellenic Alliance" (modeled on the old anti-Persian alliance of the Greco-Persian Wars), with the exception of Sparta. Philip was then named "Hegemon" (often translated as 'Supreme Commander') of this league (known by modern historians as the League of Corinth). He then announced his plans for a war of revenge against the Persian Empire, which he would command.
After returning to Pella, Philip fell in love with and married Cleopatra Eurydice, the niece of one of his generals, Attalus. This marriage made Alexander's position as heir to the throne less secure, since if Cleopatra Eurydice bore Philip a son, there would be a fully Macedonian heir, while Alexander was only half Macedonian. During the wedding banquet, a drunken Attalus made a speech praying to the gods that the union would produce a legitimate heir to the Macedonian throne. Alexander shouted to Attalus, "What am I then, a bastard?" and he threw his goblet at him. Philip, who was also drunk, drew his sword and advanced towards Alexander before collapsing, leading Alexander to say, "See there, the man who makes preparations to pass out of Europe into Asia, overturned in passing from one seat to another." Alexander fled from Macedon taking his mother with him, whom he dropped off with her brother in Dodona, capital of Epirus. He carried on to Illyria, where he sought refuge with the Illyrian King and was treated as a guest by the Illyrians, despite having defeated them in battle a few years before. Alexander returned to Macedon after six months in exile due to the efforts of a family friend, Demaratus the Corinthian, who mediated between the two parties. The following year, the Persian satrap (governor) of Caria, Pixodarus, offered the hand of his eldest daughter to Alexander's half-brother, Philip Arrhidaeus. Olympias and several of Alexander's friends suggested to Alexander that this move showed that Philip intended to make Arrhidaeus his heir. Alexander reacted by sending an actor, Thessalus of Corinth, to tell Pixodarus that he should not offer his daughter's hand to an illegitimate son, but instead to Alexander. When Philip heard of this, he scolded Alexander for wishing to marry the daughter of a Carian. Philip had four of Alexander's friends, Harpalus, Nearchus, Ptolemy and Erigyius exiled, and had the Corinthians bring Thessalus to him in chains.
In 336 BC, whilst at Aegae, attending the wedding of his daughter by Olympias, Cleopatra, to Olympias's brother, Alexander I of Epirus, Philip was assassinated by the captain of his bodyguard, Pausanias. As Pausanias tried to escape, he tripped over a vine and was killed by his pursuers, including two of Alexander's companions, Perdiccas and Leonnatus. Alexander was proclaimed king by the Macedonian army and by the Macedonian noblemen at the age of 20.
Alexander began his reign by having his potential rivals to the throne murdered. He had his cousin, the former Amyntas IV, executed, as well as having two Macedonian princes from the region of Lyncestis killed, while a third, Alexander Lyncestes, was spared. Olympias had Cleopatra Eurydice and her daughter by Philip, Europa, burned alive. When Alexander found out about this, he was furious with his mother. Alexander also ordered the murder of Attalus, who was in command of the advance guard of the army in Asia Minor. Attalus was at the time in correspondence with Demosthenes, regarding the possibility of defecting to Athens. Regardless of whether Attalus actually intended to defect, he had already severely insulted Alexander, and having just had Attalus's daughter and grandchildren murdered, Alexander probably felt Attalus was too dangerous to leave alive. Alexander spared the life of Arrhidaeus, who was by all accounts mentally disabled, possibly as a result of poisoning by Olympias. News of Philip's death roused many states into revolt, including Thebes, Athens, Thessaly, and the Thracian tribes to the north of Macedon. When news of the revolts in Greece reached Alexander, he responded quickly. Though his advisors advised him to use diplomacy, Alexander mustered the Macedonian cavalry of 3,000 men and rode south towards Thessaly, Macedon's neighbor to the south. When he found the Thessalian army occupying the pass between Mount Olympus and Mount Ossa, he had the men ride over Mount Ossa. When the Thessalians awoke the next day, they found Alexander in their rear, and promptly surrendered, adding their cavalry to Alexander's force, as he rode down towards the Peloponnesus. Alexander stopped at Thermopylae, where he was recognized as the leader of the Amphictyonic League before heading south to Corinth. Athens sued for peace and Alexander received the envoy and pardoned anyone involved with the uprising. At Corinth, he was given the title "Hegemon", and like Philip, appointed commander of the forthcoming war against Persia. While at Corinth, he heard the news of the Thracian rising to the north.
Before crossing to Asia, Alexander wanted to safeguard his northern borders; and, in the spring of 335 BC, he advanced to suppress several apparent revolts. Starting from Amphipolis, he first went east into the country of the "Independent Thracians"; and at Mount Haemus, the Macedonian army attacked and defeated a Thracian army manning the heights. The Macedonians marched on into the country of the Triballi, and proceeded to defeat the Triballian army near the Lyginus river (a tributary of the Danube). Alexander then advanced for three days on to the Danube, encountering the Getae tribe on the opposite shore. Surprising the Getae by crossing the river at night, he forced the Getae army to retreat after the first cavalry skirmish, leaving their town to the Macedonian army. News then reached Alexander that Cleitus, King of Illyria, and King Glaukias of the Taulanti were in open revolt against Macedonian authority. Marching west into Illyria, Alexander defeated each in turn, forcing Cleitus and Glaukias to flee with their armies, leaving Alexander's northern frontier secure. While he was triumphantly campaigning north, the Thebans and Athenians rebelled once more. Alexander reacted immediately, but, while the other cities once again hesitated, Thebes decided to resist with the utmost vigor. However, the resistance was useless, as the city was razed to the ground amid great bloodshed, and its territory was divided between the other Boeotian cities. The end of Thebes cowed Athens into submission, leaving all of Greece at least outwardly at peace with Alexander.
Alexander's army crossed the Hellespont in 334 BC with approximately 42,000 soldiers from Macedon and various Greek city-states, mercenaries, and feudally-raised soldiers from Thrace, Paionia, and Illyria. After an initial victory against Persian forces at the Battle of the Granicus, Alexander accepted the surrender of the Persian provincial capital and treasury of Sardis and proceeded down the Ionian coast. At Halicarnassus, Alexander successfully waged the first of many sieges, eventually forcing his opponents, the mercenary captain Memnon of Rhodes and the Persian satrap of Caria, Orontobates, to withdraw by sea. Alexander left the government of Caria to Ada, who adopted Alexander as her son. From Halicarnassus, Alexander proceeded into mountainous Lycia and the Pamphylian plain, asserting control over all coastal cities. He did this to deny the Persians naval bases. Since Alexander had no reliable fleet of his own, defeating the Persian fleet required land control. From Pamphylia onward, the coast held no major ports and so Alexander moved inland. At Termessos, Alexander humbled but did not storm the Pisidian city. At the ancient Phrygian capital of Gordium, Alexander 'undid' the hitherto unsolvable Gordian Knot, a feat said to await the future "king of Asia". According to the most vivid story, Alexander proclaimed that it did not matter how the knot was undone, and he hacked it apart with his sword.
After spending the winter campaigning in Asia Minor, Alexander's army crossed the Cilician Gates in 333 BC, and defeated the main Persian army under the command of Darius III at the Battle of Issus in November. Darius was forced to flee the battle after his army broke, and in doing so left behind his wife, his two daughters, his mother Sisygambis, and a fabulous amount of treasure. He afterward offered a peace treaty to Alexander, the concession of the lands he had already conquered, and a ransom of 10,000 talents for his family. Alexander replied that since he was now king of Asia, it was he alone who decided territorial divisions. Alexander proceeded to take possession of Syria, and most of the coast of the Levant. However, the following year, 332 BC, he was forced to attack Tyre, which he eventually captured after a famous siege. After the capture of Tyre, Alexander crucified all the men of military age, and sold the women and children into slavery.
When Alexander destroyed Tyre, most of the towns on the route to Egypt quickly capitulated, with the exception of Gaza. The stronghold at Gaza was built on a hill and was heavily fortified. At the beginning of the Siege of Gaza, Alexander utilized the engines he had employed against Tyre. After three unsuccessful assaults, the stronghold was finally taken by force, but not before Alexander received a serious shoulder wound. When Gaza was taken, the male population was put to the sword and the women and children were sold into slavery. Jerusalem, on the other hand, opened its gates in surrender, and according to Josephus, Alexander was shown the book of Daniel's prophecy, presumably chapter 8, where a mighty Greek king would subdue and conquer the Persian Empire. Thereupon, Alexander spared Jerusalem and pushed south into Egypt. Alexander advanced on Egypt in later 332 BC, where he was regarded as a liberator. He was pronounced the new "master of the Universe" and son of the deity of Amun at the Oracle of Siwa Oasis in the Libyan desert. Henceforth, Alexander often referred to Zeus-Ammon as his true father, and subsequent currency depicted him adorned with ram horns as a symbol of his divinity. During his stay in Egypt, he founded Alexandria-by-Egypt, which would become the prosperous capital of the Ptolemaic kingdom after his death.
From Babylon, Alexander went to Susa, one of the Achaemenid capitals, and captured its legendary treasury. Sending the bulk of his army to the Persian ceremonial capital of Persepolis via the Royal Road, Alexander himself took selected troops on the direct route to the city. However, the pass of the Persian Gates (in the modern Zagros Mountains) had been blocked by a Persian army under Ariobarzanes, and Alexander had to storm the pass. Alexander then made a dash for Persepolis before its garrison could loot the treasury. At Persepolis, Alexander stared at the crumbled statue of Xerxes and decided to leave it on the ground. During their stay at the capital, a fire broke out in the eastern palace of Xerxes and spread to the rest of the city. Theories abound as to whether this was the result of a drunken accident, or a deliberate act of revenge for the burning of the Acropolis of Athens during the Second Persian War. Fall of the Empire and the East. Alexander then set off in pursuit of Darius again, first into Media, and then Parthia. The Persian king was no longer in control of his destiny, having been taken prisoner by Bessus, his Bactrian satrap and kinsman. As Alexander approached, Bessus had his men fatally stab the Great King and then declared himself Darius' successor as Artaxerxes V, before retreating into Central Asia to launch a guerrilla campaign against Alexander. Darius' remains were buried by Alexander next to his Achaemenid predecessors in a full regal funeral. Alexander claimed that, while dying, Darius had named him as his successor to the Achaemenid throne. The Achaemenid Empire is normally considered to have fallen with the death of Darius. Alexander, now considering himself the legitimate successor to Darius, viewed Bessus as a usurper to the Achaemenid throne, and set out to defeat him. This campaign, initially against Bessus, turned into a grand tour of central Asia, with Alexander founding a series of new cities, all called Alexandria, including modern Kandahar in Afghanistan, and Alexandria Eschate ("The Furthest") in modern Tajikistan. The campaign took Alexander through Media, Parthia, Aria (West Afghanistan), Drangiana, Arachosia (South and Central Afghanistan), Bactria (North and Central Afghanistan), and Scythia. Bessus was betrayed in 329 BC by Spitamenes, who held an undefined position in the satrapy of Sogdiana. Spitamenes handed over Bessus to Ptolemy, one of Alexander's trusted companions, and Bessus was executed. However, when, at some point later, Alexander was on the Jaxartes, Spitamenes raised Sogdiana in revolt. Alexander launched a campaign and defeated him in the Battle of Gabai; after the defeat, Spitamenes was killed by his own men, who then sued for peace.
During this time, Alexander took the Persian title "King of Kings" ("Shahanshah") and adopted some elements of Persian dress and customs at his court, notably the custom of "proskynesis", either a symbolic kissing of the hand, or prostration on the ground, that Persians paid to their social superiors. The Greeks regarded the gesture as the province of deities and believed that Alexander meant to deify himself by requiring it. This cost him much in the sympathies of many of his countrymen. A plot against his life was revealed, and one of his officers, Philotas, was executed for failing to bring the plot to his attention. The death of the son necessitated the death of the father, and thus Parmenion, who had been charged with guarding the treasury at Ecbatana, was assassinated by command of Alexander, so he might not make attempts at vengeance. Most infamously, Alexander personally slew the man who had saved his life at Granicus, Cleitus the Black, during a drunken argument at Maracanda. Later, in the Central Asian campaign, a second plot against his life was revealed, this one instigated by his own royal pages. His official historian, Callisthenes of Olynthus (who had fallen out of favor with the king by leading the opposition to his attempt to introduce "proskynesis"), was implicated in the plot; however, there has never been consensus among historians regarding his involvement in the conspiracy. Invasion of the Indian subcontinent. After the death of Spitamenes and his marriage to Roxana (Roshanak in Bactrian) to cement his relations with his new Central Asian satrapies, Alexander was finally free to turn his attention to the Indian subcontinent. Alexander invited all the chieftains of the former satrapy of Gandhara, in the north of what is now Pakistan, to come to him and submit to his authority. Omphis (whose actual name is Ambhi), ruler of Taxila, whose kingdom extended from the Indus to the Hydaspes, complied, but the chieftains of some hill clans, including the Aspasioi and Assakenoi sections of the Kambojas (known in Indian texts also as Ashvayanas and Ashvakayanas), refused to submit. In the winter of 327/326 BC, Alexander personally led a campaign against these clans; the Aspasioi of Kunar valleys, the Guraeans of the Guraeus valley, and the Assakenoi of the Swat and Buner valleys. A fierce contest ensued with the Aspasioi in which Alexander himself was wounded in the shoulder by a dart but eventually the Aspasioi lost the fight. Alexander then faced the Assakenoi, who fought bravely and offered stubborn resistance to Alexander in the strongholds of Massaga, Ora and Aornos. The fort of Massaga could only be reduced after several days of bloody fighting in which Alexander himself was wounded seriously in the ankle. According to Curtius, "Not only did Alexander slaughter the entire population of Massaga, but also did he reduce its buildings to rubbles". A similar slaughter then followed at Ora, another stronghold of the Assakenoi. In the aftermath of Massaga and Ora, numerous Assakenians fled to the fortress of Aornos. Alexander followed close behind their heels and captured the strategic hill-fort after the fourth day of a bloody fight. After Aornos, Alexander crossed the Indus and fought and won an epic battle against a local ruler Porus, who ruled a region in the Punjab, in the Battle of Hydaspes in 326 BC. Alexander was greatly impressed by Porus for his bravery in battle, and therefore made an alliance with him and appointed him as satrap of his own kingdom, even adding land he did not own before. Additional reasons were probably political since, to control lands so distant from Greece required local assistance and co-operation. Alexander named one of the two new cities that he founded, Bucephala, in honor of the horse that had brought him to India, and had died during the battle.
East of Porus' kingdom, near the Ganges River, was the powerful Nanda Empire of Magadha and Gangaridai Empire of Bengal. Fearing the prospects of facing other powerful Indian armies and exhausted by years of campaigning, his army mutinied at the Hyphasis River, refusing to march further east. This river thus marks the easternmost extent of Alexander's conquests. As for the Macedonians, however, their struggle with Porus blunted their courage and stayed their further advance into India. For having had all they could do to repulse an enemy who mustered only twenty thousand infantry and two thousand horse, they violently opposed Alexander when he insisted on crossing the river Ganges also, the width of which, as they learned, was thirty-two furlongs, its depth a hundred fathoms, while its banks on the further side were covered with multitudes of men-at-arms and horsemen and elephants. For they were told that the kings of the Ganderites and Praesii were awaiting them with eighty thousand horsemen, two hundred thousand footmen, eight thousand chariots, and six thousand war elephants. Alexander spoke to his army and tried to persuade them to march further into India but Coenus pleaded with him to change his opinion and return, the men, he said, "longed to again see their parents, their wives and children, their homeland". Alexander, seeing the unwillingness of his men, eventually agreed and turned south. Along the way his army conquered the Malli clans (in modern day Multan), and other Indian tribes.
Discovering that many of his satraps and military governors had misbehaved in his absence, Alexander executed a number of them as examples, on his way to Susa. As a gesture of thanks, he paid off the debts of his soldiers, and announced that he would send those over-aged and disabled veterans back to Macedon under Craterus. But, his troops misunderstood his intention and mutinied at the town of Opis, refusing to be sent away and bitterly criticizing his adoption of Persian customs and dress, and the introduction of Persian officers and soldiers into Macedonian units. Alexander executed the ringleaders of the mutiny, but forgave the rank and file. In an attempt to craft a lasting harmony between his Macedonian and Persian subjects, he held a mass marriage of his senior officers to Persian and other noblewomen at Susa, but few of those marriages seem to have lasted much beyond a year. Meanwhile, upon his return, Alexander learned some men had desecrated the tomb of Cyrus the Great, and swiftly executed them, because they were put in charge of guarding the tomb Alexander held in honor. After Alexander traveled to Ecbatana to retrieve the bulk of the Persian treasure, his closest friend and possibly lover Hephaestion died of an illness, or possibly of poisoning. According to Plutarch, Alexander, distraught over the death of his longtime companion, sacked a nearby town, and put all of its inhabitants to the sword, as a sacrifice to Hephaestion's ghost. Arrian finds great diversity and casts doubts on the accounts of Alexander's displays of grief, although he says that they all agree that Hephaestion's death devastated him, and that he ordered the preparation of an expensive funeral pyre in Babylon, as well as a decree for the observance of a public mourning. Back in Babylon, Alexander planned a series of new campaigns, beginning with an invasion of Arabia, but he would not have a chance to realize them.
On either 10 or 11 June 323 BC, Alexander died in the palace of Nebuchadnezzar II, in Babylon at the age of 32. Plutarch gives a lengthy account of the circumstances of his death, echoed (without firm dates) by Arrian. Roughly 14 days before his death, Alexander entertained his admiral Nearchus, and then, instead of going to bed, spent the night and next day drinking with Medius of Larissa. After this, and by 18 Daesius (a Macedonian month) he had developed a fever, which then grew steadily worse. By 25 Daesius, he was unable to speak. By 26 Daesius, the common soldiers had become anxious about his health, or thought he was already dead. They demanded to see him, and Alexander's generals acquiesced. The soldiers slowly filed past him, whilst Alexander raised his right hand in greeting, still unable to speak. Two days later, on 28 Daesius (although Aristobolus's account says it was 30 Daesius), Alexander was dead. Conversely, Diodorus recounts that Alexander was struck down with pain after downing a large bowl of unmixed wine in honour of Hercules, and (rather mysteriously) died after some agony, which is also mentioned as an alternative by Arrian, but Plutarch specifically refutes this claim.
Given the propensity of the Macedonian aristocracy to assassination, it is scarcely surprising that allegations of foul play have been made about the death of Alexander. Diodorus, Plutarch, Arrian and Justin all mention the theory that Alexander was poisoned. Plutarch dismisses it as a fabrication, but both Diodorus and Arrian say that they only mention it for the sake of completeness. The accounts are nevertheless fairly consistent in designating Antipater, recently removed from the position of Macedonian viceroy, and at odds with Olympias, as the head of the alleged plot. Perhaps taking his summons to Babylon as a death sentence in waiting, and having seen the fate of Parmenion and Philotas, Antipater arranged for Alexander to be poisoned by his son Iollas, who was Alexander's wine-pourer. There is even a suggestion that Aristotle may have had a hand in the plot. Conversely, the strongest argument against the poison theory is the fact that twelve days had passed between the start of his illness and his death; in the ancient world, such long-acting poisons were probably not available.
Several natural causes (diseases) have been suggested as the cause of Alexander's death; malaria or typhoid fever are obvious candidates. A 1998 article in the "New England Journal of Medicine" attributed his death to typhoid fever complicated by bowel perforation and ascending paralysis, whereas another recent analysis has suggested pyrogenic spondylitis or meningitis as the cause. Other illnesses could have also been the culprit, including acute pancreatitis or the West Nile virus. Natural-cause theories also tend to emphasise that Alexander's health may have been in general decline after years of heavy drinking and his suffering severe wounds (including one in India that nearly claimed his life). Furthermore, the anguish that Alexander felt after Hephaestion's death may have contributed to his declining health. The most probable cause of Alexanders death is however, the result of overdosing on medicine made from Hellebore, deadly in large doses. The very few things we do know about his death, can today be explained only with accidental hellebore-poisoning.
Alexander's body was placed in a gold anthropoid sarcophagus, which was in turn placed in a second gold casket. According to Aelian, a seer called Aristander foretold that the land where Alexander was laid to rest "would be happy and unvanquishable forever". Perhaps more likely, the successors may have seen possession of the body as a symbol of legitimacy (it was a royal prerogative to bury the previous king). At any rate, Ptolemy stole the funeral cortege, and took it to Memphis. His successor, Ptolemy II Philadelphus, transferred the sarcophagus to Alexandria, where it remained until at least Late Antiquity. Ptolemy IX Lathyros, one of the last successors of Ptolemy I, replaced Alexander's sarcophagus with a glass one so he could melt the original down for issues of his coinage. Pompey, Julius Caesar and Augustus all visited the tomb in Alexandria, the latter allegedly accidentally knocking the nose off the body. Caligula was said to have taken Alexander's breastplate from the tomb for his own use. In c. AD 200, Emperor Septimius Severus closed Alexander's tomb to the public. His son and successor, Caracalla, was a great admirer of Alexander, and visited the tomb in his own reign. After this, details on the fate of the tomb are sketchy. The so-called "Alexander Sarcophagus", discovered near Sidon and now in the Istanbul Archaeology Museum, is so named not because it was thought to have contained Alexander's remains, but because its bas-reliefs depict Alexander and his companions hunting and in battle with the Persians. It was originally thought to have been the sarcophagus of Abdalonymus (died 311 BC), the king of Sidon appointed by Alexander immediately following the battle of Issus in 331. However, more recently, it has been suggested that it may date from earlier than Abdalonymus' death.
Alexander had no obvious or legitimate heir, his son Alexander IV by Roxane being born after Alexander's death. This left the huge question as to who would rule the newly-conquered, and barely-pacified Empire. According to Diodorus, Alexander's companions asked him when he was on his deathbed to whom he bequeathed his kingdom; his laconic reply was "tôi kratistôi"—"to the strongest". Given that Arrian and Plutarch have Alexander speechless by this point, it is possible that this is an apocryphal story. Diodorus, Curtius and Justin also have the more plausible story of Alexander passing his signet ring to Perdiccas, one of his bodyguard and leader of the companion cavalry, in front of witnesses, thereby possibly nominating Perdiccas as his successor. In any event, Perdiccas initially avoided explicitly claiming power, instead suggesting that Roxane's baby would be king, if male; with himself, Craterus, Leonnatus and Antipater as guardians. However, the infantry, under the command of Meleager, rejected this arrangement since they had been excluded from the discussion. Instead, they supported Alexander's half-brother Philip Arrhidaeus. Eventually, the two sides reconciled, and after the birth of Alexander IV, he and Philip III were appointed joint kings of the Empire—albeit in name only. It was not long, however, before dissension and rivalry began to afflict the Macedonians. The satrapies handed out by Perdiccas at the Partition of Babylon became power bases each general could use to launch his own bid for power. After the assassination of Perdiccas in 321 BC, all semblance of Macedonian unity collapsed, and 40 years of war between "The Successors" ("Diadochi") ensued before the Hellenistic world settled into four stable power blocks: the Ptolemaic kingdom of Egypt, the Seleucid Empire in the east, the Kingdom of Pergamon in Asia Minor, and Macedon. In the process, both Alexander IV and Philip III were murdered.
Green provides a description of Alexander's appearance, based on ancient sources:Physically, Alexander was not prepossessing. Even by Macedonian standards he was very short, though stocky and tough. His beard was scanty, and he stood out against his hirsute Macedonian barons by going clean-shaven. His neck was in some way twisted, so that he appeared to be gazing upward at an angle. His eyes (one blue, one brown) revealed a dewy, feminine quality. He had a high complexion and a harsh voice. Many descriptions and statues portray Alexander with the aforementioned gaze looking upward and outward. Both his father Philip II and his brother Philip Arrhidaeus also suffered from physical deformities, which had led to the suggestion that Alexander suffered from a congenital scoliotic disorder (familial neck and spinal deformity). Furthermore, it has been suggested that this may have contributed to his death. During his last years, sculptor Lysippus sculpted an image of Alexander. Lysippus had captured in the stone Alexander's appearance characteristics; slightly left-turned neck and peculiar gaze. Lysippus' sculpture, which is opposite to his often vigorous portrayal, especially in coinage of the time, is thought to be the most faithful depiction of Alexander.
Alexander's personality is well described by the ancient sources. Some of his strongest personality traits formed in response to his parents. His mother had huge ambitions for Alexander, and encouraged him to believe it was his destiny to conquer the Persian Empire. Indeed, Olympias may have gone to the extent of poisoning Philip Arrhidaeus so as to disable him, and prevent him being a rival for Alexander. Olympias's influence instilled huge ambition and a sense of destiny in Alexander, and Plutarch tells us that his ambition "kept his spirit serious and lofty in advance of his years". Alexander's relationship with his father generated the competitive side of his personality; he had a need to out-do his father, as his reckless nature in battle suggests. While Alexander worried that his father would leave him "no great or brilliant achievement to be displayed to the world", he still attempted to downplay his father's achievements to his companions. Alexander's most evident personality traits were his violent temper and rash, impulsive nature, which undoubtedly contributed to some of his decisions during his life. Plutarch thought that this part of his personality was the cause of his weakness for alcohol. Although Alexander was stubborn and did not respond well to orders from his father, he was easier to persuade by reasoned debate. Indeed, set beside his fiery temperament, there was a calmer side to Alexander; perceptive, logical, and calculating. He had a great desire for knowledge, a love for philosophy, and was an avid reader. This was no doubt in part due to his tutelage by Aristotle; Alexander was intelligent and quick to learn. The tale of his "solving" the Gordian knot neatly demonstrates this. He had great self-restraint in "pleasures of the body", contrasting with his lack of self control with alcohol. The intelligent and rational side to Alexander is also amply demonstrated by his ability and success as a general. Alexander was undoubtedly erudite, and was a patron to both the arts and sciences. However, he had little interest in sports, or the Olympic games (unlike his father), seeking only the Homeric ideals of glory and fame. He had great charisma and force of personality, characteristics, which made him a great leader. This is further emphasised by the inability of any of his generals to unite the Macedonians and retain the Empire after his death only Alexander had the personality to do so.
During his final years, and especially after the death of Hephaestion, Alexander began to exhibit signs of megalomania and paranoia. His extraordinary achievements, coupled with his own ineffable sense of destiny and the flattery of his companions, may have combined to produce this effect. His delusions of grandeur are readily visible in the testament that he ordered Craterus to fulfil, and in his desire to conquer all non-Greek peoples. He seems to have come to believe himself a deity, or at least sought to deify himself. Olympias always insisted to him that he was the son of Zeus, a theory apparently confirmed to him by the oracle of Amun at Siwa. He began to identify himself as the son of Zeus-Ammon. Alexander adopted some elements of Persian dress and customs at his court, notably the custom of "proskynesis", a practice of which the Macedonians disapproved, and were loathe to perform. Such behaviour cost him much in the sympathies of many of his countrymen.
The greatest emotional relationship of Alexander's life was with his friend, general, and bodyguard Hephaestion, the son of a Macedonian noble. Hephaestion's death devastated Alexander, sending him into a period of grieving. This event may have contributed to Alexander's failing health, and detached mental state during his final months. Alexander married twice: Roxana, daughter of a Bactrian nobleman, Oxyartes, out of love; and Stateira, a Persian princess and daughter of Darius III of Persia out of political interest. He apparently had two sons, Alexander IV of Macedon of Roxana and, possibly, Heracles of Macedon from his mistress Barsine; and lost another child when Roxana miscarried at Babylon. Alexander's sexuality has been the subject of speculation and controversy. Nowhere in the ancient sources is it stated that Alexander had homosexual relationships, or that Alexander's relationship with Hephaestion was sexual. Aelian, however, writes of Alexander's visit to Troy where "Alexander garlanded the tomb of Achilles and Hephaestion that of Patroclus, the latter riddling that he was a beloved of Alexander, in just the same way as Patroclus was of Achilles". Noting that the word "eromenos" (ancient Greek for beloved) does not necessarily bear sexual meaning, Alexander may indeed have been bisexual, which in his time was not ethically controversial. Green argues that there is little evidence in the ancient sources Alexander had much interest in women, particularly since he did not produce an heir until the very end of his life. However, he was relatively young when he died, and Ogden suggests that Alexander's matrimonial record is more impressive than his father's at the same age. Apart from wives, Alexander had many more female companions. Alexander had accumulated a harem in the style of Persian kings but he used it rather sparingly; showing great self-control in "pleasures of the body". It is possible that Alexander was simply not a highly-sexed person. Nevertheless, Plutarch describes how Alexander was infatuated by Roxanne while complimenting him on not forcing himself on her. Green suggests that, in the context of the period, Alexander formed quite strong friendships with women, including Ada of Caria, who adopted Alexander, and even Darius's mother Sisygambis, who supposedly died from grief when Alexander died.
Alexander's most obvious legacy was the introduction of Macedonian rule to huge new swathes of Asia. Many of these areas would remain in Macedonian hands, or under Greek influence for the next 200–300 years. The successor states that emerged were, at least initially, dominant forces during this epoch, and these 300 years are often referred to as the Hellenistic Period. The eastern borders of Alexander's empire began to collapse even during his lifetime. However, the power vacuum he left in the northwest of the Indian subcontinent directly gave rise to one of the most powerful Indian dynasties in history. Taking advantage of the neglect shown to this region by the successors, Chandragupta Maurya (referred to in European sources as Sandrokotto), of relatively humble origin, took control of the Punjab, and then with that power base proceeded to conquer the Nanda Empire of northern India. In 305 BC, Seleucus, one of the successors, marched to India to reclaim the territory; instead, he ceded the area to Chandragupta in return for 500 war elephants. These in turn played a pivotal role in the Battle of Ipsus, the result of which did much to settle the division of the Empire.
Hellenization is a term coined by the German historian Johann Gustav Droysen to denote the spread of Greek language, culture, and population into the former Persian empire after Alexander's conquest. That this export took place is undoubted, and can be seen in the great Hellenistic cities of, for instance, Alexandria (one of around twenty towns founded by Alexander), Antioch and Seleucia (south of modern Baghdad). However, exactly how widespread and deeply permeating this was, and to what extent it was a deliberate policy, is debatable. Alexander certainly made deliberate efforts to insert Greek elements into Persian culture and in some instances he attempted to hybridize Greek and Persian culture, culminating in his aspiration to homogenise the populations of Asia and Europe. However, the successors explicitly rejected such policies after his death. Nevertheless, Hellenization occurred throughout the region, and moreover, was accompanied by a distinct and opposite 'Orientalization' of the Successor states. The core of Hellenistic culture was essentially Athenian by origin. The Athenian koine dialect had been adopted long before Philip II for official use and was thus spread throughout the Hellenistic world, becoming the lingua franca through Alexander's conquests. Furthermore, town planning, education, local government, and art current in the Hellenistic period were all based on Classical Greek ideals, evolving though into distinct new forms commonly grouped as Hellenistic. Aspects of the Hellenistic culture were still evident in the traditions of the Byzantine Empire up until the mid-15th century. Some of the most unusual effects of Hellenization can be seen in India, in the region of the relatively late-arising Indo-Greek kingdoms. There, isolated from Europe, Greek culture apparently hybridised with Indian, and especially Buddhist, influences. The first realistic portrayals of the Buddha appeared at this time; they are modelled on Greek statues of Apollo. Several Buddhist traditions may have been influenced by the ancient Greek religion: the concept of Boddhisatvas is reminiscent of Greek divine heroes, and some Mahayana ceremonial practices (burning incense, gifts of flowers, and food placed on altars) are similar to those practiced by the ancient Greeks. Zen Buddhism draws in part on the ideas of Greek stoics, such as Zeno. One Greek king, Menander I, probably became Buddhist, and is immortalized in Buddhist literature as 'Milinda'.
Alexander and his exploits were admired by many Romans who wanted to associate themselves with his achievements. Polybius started his "Histories" by reminding Romans of his role, and thereafter subsequent Roman leaders saw him as his inspirational role model. Julius Caesar reportedly wept in Spain at the sight of Alexander's statue, because he thought he had achieved so little by the same age that Alexander had conquered the world. Pompey the Great searched the conquered lands of the east for Alexander's 260-year-old cloak, which he then wore as a sign of greatness. In his zeal to honor Alexander, Augustus accidentally broke the nose off the Macedonian's mummified corpse while laying a wreath at the Alexander's tomb Alexandria. The Macriani, a Roman family that in the person of Macrinus briefly ascended to the imperial throne, kept images of Alexander on their persons, either on jewelry, or embroidered into their clothes. In the summer of 1995, a statue of Alexander was recovered in an excavation of a Roman house in Alexandria, which was richly decorated with mosaic and marble pavements and probably was constructed in the 1st century AD and occupied until the 3rd century.
There are many legendary accounts surrounding the life of Alexander the Great, with a relatively large number deriving from his own lifetime, probably encouraged by Alexander himself. His court historian Callisthenes portrayed the sea in Cilicia as drawing back from him in proskynesis. Writing shortly after Alexander's death, another participant, Onesicritus, went so far as to invent a tryst between Alexander and Thalestris, queen of the mythical Amazons. When Onesicritus read this passage to his patron, Alexander's general and later King Lysimachus reportedly quipped, "I wonder where I was at the time." In the first centuries after Alexander's death, probably in Alexandria, a quantity of the more legendary material coalesced into a text known as the "Alexander Romance", later falsely ascribed to the historian Callisthenes and therefore known as "Pseudo-Callisthenes". This text underwent numerous expansions and revisions throughout Antiquity and the Middle Ages. The Alexander legend is also believed to extend to Alexander the Great in the Qur'an, where he appears as a man called Dhul-Qarnayn. In ancient and modern culture. Alexander the Great's accomplishments and legacy have been preserved and depicted in many ways. Alexander has figured in works of both high and popular culture from his own era to the modern day. In Punjab, the land of his final conquest, the name "Secunder" is commonly given to children even today. This is both due to respect and admiration for Alexander and also as a momento to the fact that fighting the people of Punjab fatigued his army to the point that they revolted against him. A common proverb in the Punjab, reads "jit jit key jung, secunder jay haar", translation, "alexander wins so many battles that he loses the war" used to address anyone who is good at winning but never taking advantage of those wins.
Texts written by people who actually knew Alexander or who gathered information from men who served with Alexander are all lost apart from a few inscriptions and fragments. Contemporaries who wrote accounts of his life include Alexander's campaign historian Callisthenes; Alexander's generals Ptolemy and Nearchus; Aristobulus, a junior officer on the campaigns; and Onesicritus, Alexander's chief helmsman. These works have been lost, but later works based on these original sources survive. The five main surviving accounts are by Arrian, Curtius, Plutarch, Diodorus, and Justin.
He was born in Warsaw, Russian Empire. He came from an aristocratic polish family whose members had worked as mathematicians, scientists, and engineers for generations. He learned Polish at home and Russian in the schools; and having a French governess and a German governess, he became fluent in these four languages as a child. Korzybski was educated at the Warsaw University of Technology in engineering. During the First World War Korzybski served as an intelligence officer in the Russian Army. After being wounded in his leg and suffering other injuries, he came to North America in 1916 (first to Canada, then the United States) to coordinate the shipment of artillery to the war front. He also lectured to Polish-American audiences about the conflict, promoting the sale of war bonds. Following the War, he decided to remain in the United States, becoming a naturalized citizen in 1940. His first book, "Manhood of Humanity", was published in 1921. In the book, he proposed and explained in detail a new theory of humankind: mankind as a time-binding class of life.
Korzybski's work culminated in the founding of a discipline that he called general semantics (GS). As Korzybski explicitly said, GS should not be confused with semantics, a different subject. The basic principles of general semantics, which include time-binding, are outlined in "Science and Sanity", published in 1933. In 1938 Korzybski founded the Institute of General Semantics and directed it until his death in Lakeville, Connecticut, USA. Korzybski's work held a view that human beings are limited in what they know by (1) the structure of their nervous systems, and (2) the structure of their languages. Human beings cannot experience the world directly, but only through their "abstractions" (nonverbal impressions or "gleanings" derived from the nervous system, and verbal indicators expressed and derived from language). Sometimes our perceptions and our languages actually mislead us as to the "facts" with which we must deal. Our understanding of what is going on sometimes lacks "similarity of structure" with what is actually going on. He stressed training in awareness of abstracting, using techniques that he had derived from his study of mathematics and science. He called this awareness, this goal of his system, "consciousness of abstracting". His system included modifying the way we approach the world, e.g., with an attitude of "I don't know; let's see," to better discover or reflect its realities as shown by modern science. One of these techniques involved becoming inwardly and outwardly quiet, an experience that he called, "silence on the objective levels".
Many supporters and critics of Korzybski reduced his rather complex system to a simple matter of what he said about the verb 'to be.' His system, however, is based primarily on such terminology as the different 'orders of abstraction,' and formulations such as 'consciousness of abstracting.' It is often said that Korzybski "opposed" the use of the verb "to be," an unfortunate exaggeration (see 'Criticisms' below). He thought that "certain uses" of the verb "to be", called the "is of identity" and the "is of predication", were faulty in structure, e.g., a statement such as, "Joe is a fool" (said of a person named 'Joe' who has done something that we regard as foolish). In Korzybski's system, one's assessment of Joe belongs to a higher order of abstraction than Joe himself. Korzybski's remedy was to "deny" identity; in this example, to be continually aware that 'Joe' is "not" what we "call" him. We find Joe not in the verbal domain, the world of words, but the nonverbal domain (the two, he said, amount to different orders of abstraction). This was expressed in Korzybski's most famous premise, "the map is not the territory". Note that this premise uses the phrase "is not", a form of "to be"; this and many other examples show that he did not intend to abandon "to be" as such. In fact, he expressly said that there were no structural problems with the verb "to be" when used as an auxiliary verb or when used to state existence or location. It was even 'OK' sometimes to use the faulty forms of the verb 'to be,' as long as one was aware of their structural limitations. This was developed into E-prime by one of his students 15 years after his death.
One day, Korzybski was giving a lecture to a group of students, and he suddenly interrupted the lesson in order to retrieve a packet of biscuits, wrapped in white paper, from his briefcase. He muttered that he just had to eat something, and he asked the students on the seats in the front row, if they would also like a biscuit. A few students took a biscuit. "Nice biscuit, don't you think," said Korzybski, while he took a second one. The students were chewing vigorously. Then he tore the white paper from the biscuits, in order to reveal the original packaging. On it was a big picture of a dog's head and the words "Dog Cookies." The students looked at the package, and were shocked. Two of them wanted to throw up, put their hands in front of their mouths, and ran out of the lecture hall to the toilet. "You see, ladies and gentlemen," Korzybski remarked, "I have just demonstrated that people don't just eat food, but also words, and that the taste of the former is often outdone by the taste of the latter." Apparently his prank aimed to illustrate how some human suffering originates from the confusion or conflation of linguistic representations of reality and reality itself.
Korzybski's work influenced Gestalt Therapy, Rational Emotive Behavior Therapy, and Neuro-linguistic programming (especially the Meta model, Korzybski's critique of cause-effect thinking, and ideas behind human modeling for performance). As reported in the Third Edition of "Science and Sanity", The U.S. Army in World War II used his system to treat battle fatigue in Europe under the supervision of Dr. Douglas M. Kelley, who also became the psychiatrist in charge of the Nazi prisoners at Nuremberg. Other individuals influenced by Korzybski include Kenneth Burke, William S. Burroughs, Frank Herbert, Albert Ellis, Gregory Bateson, John Grinder, Buckminster Fuller, Douglas Engelbart, Stuart Chase, Alvin Toffler, Robert A. Heinlein (Korzybski is mentioned in the 1940 short story "Blowups Happen" and the 1949 novella "Gulf"), L.Ron Hubbard, A. E. van Vogt, Robert Anton Wilson, Alan Watts, entertainer Steve Allen, and Tommy Hall (lyricist for the 13th Floor Elevators); and scientists such as William Alanson White (psychiatry), physicist P. W. Bridgman, and researcher W. Horsley Gantt (a former student and colleague of Pavlov). He also influenced the Belgian surrealist writer of comics Jan Bucquoy in the seventh part of the comics series "Jaunes": "Labyrinthe", with explicit reference in the plot to Korzybski's "the map is not the territory." In part the General Semantics tradition was upheld by Samuel I. Hayakawa, who did have a falling out with Korzybski. When asked over what, Hayakawa is said to have replied: "Words."
"Asteroids" is a video arcade game released in 1979 by Atari Inc. It was one of the most popular and influential games of the Golden Age of Arcade Games. "Asteroids" uses vector graphics and a two-dimensional view that wraps around in both screen axes (a toroidal topology). The player controls a spaceship in an asteroid field which is periodically traversed by flying saucers. The object of the game is to shoot and destroy asteroids and saucers while not colliding with either, or being hit by the saucers' counter-fire.
The game was conceived by Lyle Rains and programmed and designed by Ed Logg. "Asteroids" was a hit in the United States and became Atari's best selling game of all time. Atari had been in the process of releasing another vector game, "Lunar Lander", but demand for "Asteroids" was so high they stopped further production of "Lunar Lander" so they could begin building "Asteroids". The first 200 "Asteroids" machines were sent out in "Lunar Lander" cabinets. "Asteroids" was so popular that video arcade owners sometimes had to install larger boxes to hold the amount of coins that were spent by players. "Asteroids" is also the first game to use Atari's "QuadraScan" vector-refresh system. A full-color version known as "Color-QuadraScan" was later developed for games such as "Space Duel" and "Tempest".
The objective of "Asteroids" is to score as many points as possible by destroying asteroids and flying saucers. The player controls a triangular-shaped ship that can rotate left and right, fire shots straight forward, and thrust forward. As the ship moves, momentum is not conserved — the ship eventually comes to a stop again when not thrusting. The player can also send their ship into hyperspace, causing it to disappear and reappear in a random location on the screen (with the risk of self-destructing or appearing on top of an asteroid). Each stage starts with a few asteroids drifting in random directions on the screen. Objects wrap around screen edges — for instance, an asteroid that drifts off the top edge of the screen reappears at the bottom and continues moving in the same direction. As the player shoots asteroids, they break into smaller asteroids that frequently move faster and are more difficult to hit. Smaller asteroids also score higher points. Periodically, a flying saucer appears on one side of the screen and moves across to the other before disappearing again. The saucers are of two kinds: Large saucers fire in random directions, while small saucers aim at the player's ship. The minimalist soundtrack features a memorable deep-toned electronic "heartbeat", which quickens as the asteroid density is reduced by the player's fire. Once the screen has been cleared of all asteroids and flying saucers, a new set of large asteroids appears. The number of asteroids increases each round up to a maximum of twelve. The game is over when the player has lost all of his/her lives. Like many games of its time, "Asteroids" contains several bugs that were mostly the result of the original programmers underestimating the game's popularity or the skill of its players. The maximum possible score in this game is 99,990 points, after which it "rolls over" back to zero. Also, an oversight in the small saucer's programming gave rise to a popular strategy known as "lurking" — because the saucer could only shoot directly at the player's position on the screen, the player could "hide" at the opposite end of the screen and shoot across the screen boundary, while remaining relatively safe. This led to experienced players being able to play indefinitely on a single credit. This oversight was addressed in the game's sequel, "Asteroids Deluxe", and led to significant changes in the way game developers designed and tested their games in the future. On some early versions of the game, it was also possible to hide the ship in the score area indefinitely without being hit by asteroids.
The "Asteroids" arcade machine is a vector game. This means that the game graphics are composed entirely of lines which are drawn on a vector monitor. The hardware consists primarily of a standard MOS 6502 CPU, which executes the game program, and the Digital Vector Generator (DVG), vector processing circuitry developed by Atari themselves. As the 6502 by itself was too slow to control both the game play and the vector hardware at the same time, the latter task was delegated to the DVG. The original design concepts of the DVG came out of Atari's off-campus research lab in Grass Valley, CA, in 1978. The prototype was given to engineer Howard Delman, who refined it, produced it, and then added additional features for Atari's first vector game, "Lunar Lander". When it was decided that "Asteroids" would be a vector game as well, Delman modified a "Lunar Lander" circuit board for Ed Logg. More memory was added, as was the circuitry for the many sounds in the game. That original "Asteroids" prototype board still exists, and is currently in Delman's personal collection. For each picture frame, the 6502 writes graphics commands for the DVG into a defined area of RAM (the vector RAM), and then asks the DVG to draw the corresponding vector image on the screen. The DVG reads the commands and generates appropriate signals for the vector monitor. There are DVG commands for positioning the cathode ray, for drawing a line to a specified destination, calling a subroutine with further commands, and so on. "Asteroids" also features various sound effects, each of which is implemented by its own circuitry. There are seven distinct audio circuits, designed by Howard Delman. The CPU activates these audio circuits (and other hardware components) by writing to special memory addresses (memory mapped ports). The inputs from the player's controls (buttons) are also mapped into the CPU address space The main "Asteroids" game program uses only 6 KB of ROM code. Another 2 KB of vector ROM contains the descriptions of the main graphical elements (rocks, saucer, player's ship, explosion pictures, letters, and digits) in the form of DVG commands.
Due to the game's success, a sequel followed in 1980 dubbed "Asteroids Deluxe". Though similar to the original game, several changes and additions occurred, with the onscreen objects now tinted blue and a shield that depleted with use replacing the hyperspace feature. In addition a new enemy dubbed a "killer satellite" was added to the game, and would break apart into two smaller ships that homed on the player's position if shot. Another two sequels followed this, "Space Duel" in 1982 and "Blasteroids" in 1987. The Killer List of Videogames (KLOV) credits this game as one of the "Top 100 Videogames." Readers of the KLOV credit it as the seventh most popular game. The gameplay in "Asteroids" was imitated by many games that followed. For example, one of the objects of "Sinistar" is to shoot asteroids to get them to release resources which the player needs to collect.
"Asteroids" has been ported to multiple systems, including many of Atari's systems (Atari 2600, 7800, Atari Lynx) and many others. The 2600 port was the first game to utilize a bank-switched cartridge, doubling available ROM space. A port was in development for the 5200 and advertised as a launch title but never officially released, although an unofficial release was produced by AtariAge. 1993 saw a release for PCs with Windows 3.1 as part of the original "Microsoft Arcade" package. Also, a new version of "Asteroids" was developed for PlayStation, Nintendo 64, Windows, and the Game Boy Color in the late 1990s. A port was also included on Atari's Cosmos system, but the system never saw release. Many of the recent TV Games series of old Atari games have included either the 2600 or arcade versions of "Asteroids". Atari has also used the game for its other late '90s and 2000's anthology series. "Asteroids Hyper 64" is an update to the 1979 arcade shooter "Asteroids" released for the Nintendo 64 on December 14, 1999. It includes fully 3D environments, new weapons, over 50 levels, and a 2 player split-screen mode; including a Versus mode, a Co-op mode, and a Team mode. In 2001, Infogrames released "Atari Anniversary Edition" for the Sega Dreamcast and PC compatibles which included emulated versions of Asteroids and other classics. In 2004, "Asteroids" (Including both the Atari 2600 port and the arcade original, along with "Asteroids Deluxe") were included as part of "Atari Anthology" for both Xbox and PlayStation 2, using Digital Eclipse's emulation technology. (This package was released for the PC a year earlier under the title "Atari: 80 Classic Games in One".) "Asteroids" was released via Xbox Live Arcade for the Xbox 360 on November 28, 2007, with an option for special revamped HD graphics and a high-speed "throttle monkey" mode. Glu Mobile released a licensed cellular phone version of "Asteroids" that includes the original game as well as updated gameplay, skins, and modes. Also, a port for Rockbox was released, named "Spacerocks".
On November 13, 1982, 15 year old Scott Safran of Cherry Hill, NJ, set a world record of 41,336,440 points on the arcade game "Asteroids". He beat the 40,101,910 point score set by Leo Daniels of Carolina Beach on February 6, 1982. To congratulate Safran on his accomplishment, the Twin Galaxies Intergalactic Scoreboard searched for him for four years until 2002, when it was discovered that he had died in an accident in 1989. In a ceremony in Philadelphia on April 27, 2002, Walter Day of Twin Galaxies presented an award to the surviving members of Safran's family, commemorating the Asteroid Champion's achievement.
Note: "+..." = optional segregate family, that may be split off from the preceding family. APG II has consolidated some of the families in the earlier APG system, while recognizing an alternative, that allows smaller families to be segregated and still follow the 'APG system'. Under the new classification system a taxonomist could, for example, correctly choose to include the daylilies ("Hemerocallis") in family Hemerocallidaceae, or in family Xanthorrhoeaceae.
Thus circumscribed, the order contains about 165 genera in 14 families, with a cosmopolitan distribution. Most of the families are composed of herbaceous plants, commonly found in aquatic environments. The flowers are usually arranged in inflorescences, and the mature seeds lack endosperm. The biggest departure from earlier systems (see below) is the inclusion of family Araceae. By its inclusion the order has grown enormously in number of species. The family Araceae alone accounts for about a hundred genera, totalling over two thousand species. The rest of families together contain just about five hundred species. The Cronquist subclass Alismatidae conformed fairly closely to the order Alismatales as circumscribed by APG, minus the family Araceae. The Dahlgren superorder Alismatanae conformed fairly closely to the order Alismatales as circumscribed by APG, minus the family Araceae. The Wettstein system, last version in 1935, and the Engler system, update in 1964, used the name Helobiae for the order.
The Apiales are an order of flowering plants. The families given at right are typical of newer classifications, though there is some slight variation, and in particular the Torriceliaceae may be divided. These families are placed within the asterid group of eudicots as circumscribed by the APG II system. Within the asterids, Apiales belongs to an unranked group called the campanulids. Under this definition well-known members include carrots, celery, parsley, and ivy. Under the Cronquist system, only the Apiaceae and Araliaceae were included here, and the restricted order was placed among the rosids rather than the asterids. The Pittosporaceae were placed within the Rosales, and many of the other forms within the family Cornaceae. Pennantia was in the family Icacinaceae.
Asterales is an order of dicotyledonous flowering plants that includes the composite family (Asteraceae) and its related families. The order is a cosmopolite, and includes mostly herbaceous species, although a small number of trees (such as some members of the genus "Lobelia") and shrubs are also present. The Asterales can be characterized on the morphological and molecular level. Synapomorphies include the oligosaccharide inulin, a nutrient storage molecule, and unique stamen morphology. The stamens are usually found around the style, either aggregated densely or fused into a tube, probably an adaptation in association with the plunger (or secondary) pollination that is common among the families of the order.
The order Asterales includes about eleven families, the largest of which are the Asteraceae, with about 25,000 species, and the Campanulaceae, with about 2,000 species. The remaining families count together for less than 500 species. The two large families are cosmopolitan, with many of their species found in the northern hemisphere, and the smaller families are usually confined to Australia and the adjacent areas, or sometimes South America. Under the Cronquist system, Asteraceae was the only family in the group, but newer systems (e. g. APG II) have expanded it.
The Asterales order probably originated in the Cretaceous on the supercontinent Gondwana, in the area that is now Australia and Asia. Although most extant species are herbaceous, the examination of the basal families in the order suggests that the common ancestor of the order was an arborescent plant. Fossil evidence of the Asterales is rare and belongs to rather recent epochs, so the precise estimation of the order's age is quite difficult. An Oligocene pollen is known for Asteraceae and Goodeniaceae, and seeds from Oligocene and Miocene are known for Menyanthaceae and Campanulaceae respectively.
Asteroids, sometimes called minor planets or planetoids, are small Solar System bodies in orbit around the Sun, especially in the inner Solar System; they are smaller than planets but larger than meteoroids. The term "asteroid" has historically been applied primarily to minor planets of the inner Solar System, as the outer Solar System was poorly known when it came into common usage. The distinction between asteroids and comets is made on visual appearance: Comets show a perceptible coma while asteroids do not.
Traditionally, small bodies orbiting the Sun were classified as asteroids, comets or meteoroids, with anything smaller than ten metres across being called a meteoroid. The term "asteroid" is somewhat ill-defined. It never had a formal definition, with the broader term minor planet being preferred by the International Astronomical Union until 2006, when the term "small Solar System body" (SSSB) was introduced to cover both minor planets and comets. The 2006 definition of SSSB says that they "include most of the Solar System asteroids, most Trans-Neptunian Objects (TNOs), comets, and other small bodies". Other languages prefer "planetoid" (Greek for "planet-like"), and this term is occasionally used in English for the larger asteroids. The word "planetesimal" has a similar meaning, but refers specifically to the small building blocks of the planets that existed at the time the Solar System was forming. The term "planetule" was coined by the geologist William Daniel Conybeare to describe minor planets, but is not in common use. When found, asteroids were seen as a class of objects distinct from comets, and there was no unified term for the two until "small Solar System body" was coined in 2006. The main difference between an asteroid and a comet is that a comet shows a coma due to sublimation of near surface ices by solar radiation. A few objects have ended up being dual-listed because they were first classified as minor planets but later showed evidence of cometary activity. Conversely, some (perhaps all) comets are eventually depleted of their surface volatile ices and become asteroids. A further distinction is that comets typically have more eccentric orbits than most asteroids; most "asteroids" with notably eccentric orbits are probably dormant or extinct comets. For almost two centuries, from the discovery of the first asteroid, Ceres, in 1801 until the discovery of the first centaur, 2060 Chiron, in 1977, all known asteroids spent most of their time at or within the orbit of Jupiter, though a few such as 944 Hidalgo ventured far beyond Jupiter for part of their orbit. When astronomers started finding additional small bodies that permanently resided further out than Jupiter, now called centaurs, they numbered them among the traditional asteroids, though there was debate over whether they should be classified as asteroids or as a new type of object. Then, when the first trans-Neptunian object, 1992 QB1, was discovered in 1992, and especially when large numbers of similar objects started turning up, new terms were invented to sidestep the issue: Kuiper Belt object (KBO), trans-Neptunian object (TNO), scattered-disc object (SDO), and so on. These inhabit the cold outer reaches of the Solar System where ices remain solid and comet-like bodies are not expected to exhibit much cometary activity; if centaurs or TNOs were to venture close to the Sun, their volatile ices would sublimate, and traditional approaches would classify them as comets rather than asteroids. The innermost of these are the Kuiper Belt Objects (KBOs), called "objects" partly to avoid the need to classify them as asteroids or comets. KBOs are believed to be predominantly comet-like in composition, though some may be more akin to asteroids. Furthermore, most do not have the highly eccentric orbits associated with comets, and the ones so far discovered are very much larger than traditional comet nuclei. (The much more distant Oort cloud is hypothesized to be the main reservoir of dormant comets.) Other recent observations, such as the analysis of the cometary dust collected by the Stardust probe, are increasingly blurring the distinction between comets and asteroids, suggesting "a continuum between asteroids and comets" rather than a sharp dividing line. The minor planets beyond Jupiter's orbit are rarely directly referred to as "asteroids", but all are commonly lumped together under the term "asteroid" in popular presentations. For instance, a joint NASA-JPL public-outreach website states, It is, however, becoming increasingly common for the term "asteroid" to be restricted to minor planets of the inner Solar System, and therefore this article will restrict itself for the most part to the classical asteroids: objects of the main asteroid belt, Jupiter trojans, and near-Earth objects. When the IAU introduced the class small solar system bodies in 2006 to include most objects previously classified as minor planets and comets, they created the class of dwarf planets for the largest minor planets—those which have sufficient mass to have become ellipsoidal under their own gravity. According to the IAU, "the term 'minor planet' may still be used, but generally the term 'small solar system body' will be preferred." Currently only the largest object in the asteroid belt, Ceres, at about across, has been placed in the dwarf planet category, although there are several large asteroids (Vesta, Pallas, and Hygiea) that may be classified as dwarf planets when their shapes are better known.
It is believed that planetesimals in the main asteroid belt evolved much like the rest of the Solar Nebula until Jupiter neared its current mass, at which point excitation from orbital resonances with Jupiter ejected over 99% of planetesimals in the belt. Both simulations and a discontinuity in spin rate and spectral properties suggest that asteroids larger than approximately in diameter accreted during that early era, whereas smaller bodies are fragments from collisions between asteroids during or after the Jovian disruption. At least two asteroids, Ceres and Vesta, grew large enough to melt and differentiate, with heavy metallic elements sinking to the core, leaving rocky minerals in the crust. In the Nice model, a large number of Kuiper Belt objects are captured in the outer Main Belt, at distances greater than 2.6 AU. Most were subsequently ejected by Jupiter, but those that remained may be the D-type asteroids, and possibly include Ceres.
Objects in the main asteroid belt vary greatly in size, from a diameter of 950 kilometres for the dwarf planet Ceres and over 500 kilometres for the asteroids 2 Pallas and 4 Vesta down to rocks just tens of metres across. The three largest are very much like miniature planets: they are roughly spherical, have at least partially differentiated interiors, and indeed are thought to be surviving protoplanets. The vast majority, however, are much smaller and are irregularly shaped; they are thought to be either surviving planetesimals or fragments of larger bodies. The physical composition of asteroids is varied and in most cases poorly understood. Ceres appears to be composed of a rocky core covered by an icy mantle, whereas Vesta is thought to have a nickel-iron core, olivine mantle, and basaltic crust. 10 Hygiea, on the other hand, which appears to have a uniformly primitive composition of carbonaceous chondrite, is thought to be the largest undifferentiated asteroid. Many, perhaps most, of the smaller asteroids are piles of rubble held together loosely by gravity. Some have moons or are co-orbiting binary asteroids. The rubble piles, moons, binaries, and scattered asteroid families are believed to be the results of collisions which disrupted a parent asteroid. Asteroids contain traces of amino-acids and other organic compounds, and some speculate that asteroid impacts may have seeded the early Earth with the chemicals necessary to initiate life, or may have even brought life itself to Earth. (See also panspermia.) Only one asteroid, 4 Vesta, which has a particularly reflective surface, is normally visible to the naked eye, and this only in very dark skies when it is favorably positioned. Very rarely, small asteroids passing close to Earth may be naked-eye visible for a short period of time. The orbits of asteroids are often influenced by the gravity of other bodies in the solar system or the Yarkovsky effect. Distribution within the Solar System. The vast majority of known asteroids orbit within the main asteroid belt between the orbits of Mars and Jupiter, generally in relatively low-eccentricity (i.e., not very elongated) orbits. This belt is currently estimated to contain between 1.1 and 1.9 million asteroids larger than in diameter, and millions of smaller ones. It is thought that these asteroids are remnants of the protoplanetary disk, and in this region the accretion of planetesimals into planets during the formative period of the solar system was prevented by large gravitational perturbations by Jupiter. Although fewer Trojan asteroids sharing Jupiter's orbit are currently known, it is thought that there are as many as there are asteroids in the main belt. The dwarf planet Ceres is the largest object in the asteroid belt, with a diameter of over. The next largest are the asteroids 2 Pallas and 4 Vesta, both with diameters of over. Normally Vesta is the only main belt asteroid that can, on occasion, become visible to the naked eye. However, on some very rare occasions, a near-Earth asteroid may briefly become visible without technical aid; see 99942 Apophis. The mass of all the objects of the Main asteroid belt, lying between the orbits of Mars and Jupiter, is estimated to be about 3.0-3.6 kg, or about 4 percent of the mass of the Moon. Of this, Ceres comprises 0.95 kg, some 32 percent of the total. Adding in the next three most massive objects, Vesta (9%), Pallas (7%), and Hygiea (3%), brings this figure up to 51%; while the three after that, 511 Davida (1.2%), 704 Interamnia (1.0%), and 52 Europa (0.9%), only add another 3% to the total mass. The number of asteroids then increases rapidly as their individual masses decrease. Various classes of asteroid have been discovered outside the main asteroid belt. Near-Earth asteroids have orbits in the vicinity of Earth's orbit. Trojan asteroids are gravitationally locked into synchronisation with Jupiter, either leading or trailing the planet in its orbit. A couple trojans have been found orbiting with Mars. A group of asteroids called Vulcanoids are hypothesised by some to lie very close to the Sun, within the orbit of Mercury, but none has so far been found.
Many asteroids have been placed in groups and families based on their orbital characteristics. Apart from the broadest divisions, it is customary to name a group of asteroids after the first member of that group to be discovered. Groups are relatively loose dynamical associations, whereas families are much tighter and result from the catastrophic break-up of a large parent asteroid sometime in the past. Families have only been recognized within the main asteroid belt. They were first recognised by Kiyotsugu Hirayama in 1918 and are often called Hirayama families in his honor. About 30% to 35% of the bodies in the main belt belong to dynamical families each thought to have a common origin in a past collision between asteroids. A family has also been associated with the plutoid dwarf planet.
Some asteroids have unusual horseshoe orbits that are co-orbital with the Earth or some other planet. Examples are 3753 Cruithne and. The first instance of this type of orbital arrangement was discovered between Saturn's moons Epimetheus and Janus. Sometimes these horseshoe objects temporarily become quasi-satellites for a few decades or a few hundred years, before returning to their prior status. Both Earth and Venus are known to have quasi-satellites. Such objects, if associated with Earth or Venus or even hypothetically Mercury, are a special class of Aten asteroids. However, such objects could be associated with outer planets as well.
In 1975, an asteroid taxonomic system based on colour, albedo, and spectral shape was developed by Clark R. Chapman, David Morrison, and Ben Zellner. These properties are thought to correspond to the composition of the asteroid's surface material. The original classification system had three categories: C-types for dark carbonaceous objects (75% of known asteroids), S-types for stony (silicaceous) objects (17% of known asteroids) and U for those that did not fit into either C or S. This classification has since been expanded to include a number of other asteroid types. The number of types continues to grow as more asteroids are studied. The two most widely used taxonomies currently used are the Tholen classification and SMASS classification. The former was proposed in 1984 by David J. Tholen, and was based on data collected from an eight-color asteroid survey performed in the 1980s. This resulted in 14 asteroid categories. In 2002, the Small Main-Belt Asteroid Spectroscopic Survey resulted in a modified version of the Tholen taxonomy with 24 different types. Both systems have three broad categories of C, S, and X asteroids, where X consists of mostly metallic asteroids, such as the M-type. There are also a number of smaller classes. Note that the proportion of known asteroids falling into the various spectral types does not necessarily reflect the proportion of all asteroids that are of that type; some types are easier to detect than others, biasing the totals.
Originally, spectral designations were based on inferences of an asteroid's composition. However, the correspondence between spectral class and composition is not always very good, and there are a variety of classifications in use. This has led to significant confusion. While asteroids of different spectral classifications are likely to be composed of different materials, there are no assurances that asteroids within the same taxonomic class are composed of similar materials. At present, the spectral classification based on several coarse resolution spectroscopic surveys in the 1990s is still the standard. Scientists have been unable to agree on a better taxonomic system, largely due to the difficulty of obtaining detailed measurements consistently for a large sample of asteroids (e.g. finer resolution spectra, or non-spectral data such as densities would be very useful).
The first named minor planet, Ceres, was discovered in 1801 by Giuseppe Piazzi, and was originally considered a new planet. This was followed by the discovery of other similar bodies, which with the equipment of the time appeared to be points of light, like stars, showing little or no planetary disc (though readily distinguishable from stars due to their apparent motions). This prompted the astronomer Sir William Herschel to propose the term "asteroid", from Greek "αστεροειδής", "asteroeidēs" = star-like, star-shaped, from ancient Greek "Aστήρ", "astēr" = star. In the early second half of the nineteenth century, the terms "asteroid" and "planet" (not always qualified as "minor") were still used interchangeably; for example, the, page 316, reads "Professor J. Watson has been awarded by the Paris Academy of Sciences, the astronomical prize, Lalande foundation, for the discovery of 8 new asteroids in one year. The planet Lydia (No. 110), discovered by M. Borelly at the Marseilles Observatory [...] M. Borelly had previously discovered 2 planets bearing the numbers 91 and 99 in the system of asteroids revolving between Mars and Jupiter" (emphasis added).
Asteroid discovery methods have dramatically improved over the past two centuries. In the last years of the 18th century, Baron Franz Xaver von Zach organized a group of 24 astronomers to search the sky for the missing planet predicted at about 2.8 AU from the Sun by the Titius-Bode law, partly as a consequence of the discovery, by Sir William Herschel in 1781, of the planet Uranus at the distance predicted by the law. This task required that hand-drawn sky charts be prepared for all stars in the zodiacal band down to an agreed-upon limit of faintness. On subsequent nights, the sky would be charted again and any moving object would, hopefully, be spotted. The expected motion of the missing planet was about 30 seconds of arc per hour, readily discernible by observers. The first asteroid, 1 Ceres, was not discovered by a member of the group, but rather by accident in 1801 by Giuseppe Piazzi, director of the observatory of Palermo in Sicily. He discovered a new star-like object in Taurus and followed the displacement of this object during several nights. His colleague, Carl Friedrich Gauss, used these observations to determine the exact distance from this unknown object to the Earth. Gauss' calculations placed the object between the planets Mars and Jupiter. Piazzi named it after Ceres, the Roman goddess of agriculture. Three other asteroids (2 Pallas, 3 Juno, and 4 Vesta) were discovered over the next few years, with Vesta found in 1807. After eight more years of fruitless searches, most astronomers assumed that there were no more and abandoned any further searches. However, Karl Ludwig Hencke persisted, and began searching for more asteroids in 1830. Fifteen years later, he found 5 Astraea, the first new asteroid in 38 years. He also found 6 Hebe less than two years later. After this, other astronomers joined in the search and at least one new asteroid was discovered every year after that (except the wartime year 1945). Notable asteroid hunters of this early era were J. R. Hind, Annibale de Gasparis, Robert Luther, H. M. S. Goldschmidt, Jean Chacornac, James Ferguson, Norman Robert Pogson, E. W. Tempel, J. C. Watson, C. H. F. Peters, A. Borrelly, J. Palisa, the Henry brothers and Auguste Charlois. In 1891, however, Max Wolf pioneered the use of astrophotography to detect asteroids, which appeared as short streaks on long-exposure photographic plates. This dramatically increased the rate of detection compared with previous visual methods: Wolf alone discovered 248 asteroids, beginning with 323 Brucia, whereas only slightly more than 300 had been discovered up to that point. Still, a century later, only a few thousand asteroids were identified, numbered and named. It was known that there were many more, but most astronomers did not bother with them, calling them "vermin of the skies". Manual methods of the 1900s and modern reporting. Until 1998, asteroids were discovered by a four-step process. First, a region of the sky was photographed by a wide-field telescope, or Astrograph. Pairs of photographs were taken, typically one hour apart. Multiple pairs could be taken over a series of days. Second, the two films of the same region were viewed under a stereoscope. Any body in orbit around the Sun would move slightly between the pair of films. Under the stereoscope, the image of the body would appear to float slightly above the background of stars. Third, once a moving body was identified, its location would be measured precisely using a digitizing microscope. The location would be measured relative to known star locations. These first three steps do not constitute asteroid discovery: the observer has only found an apparition, which gets a provisional designation, made up of the year of discovery, a letter representing the half-month of discovery, and finally a letter and a number indicating the discovery's sequential number (example:). The final step of discovery is to send the locations and time of observations to the Minor Planet Center, where computer programs determine whether an apparition ties together previous apparitions into a single orbit. If so, the object receives a catalogue number and the observer of the first apparition with a calculated orbit is declared the discoverer, and granted the honor of naming the object subject to the approval of the International Astronomical Union.
There is increasing interest in identifying asteroids whose orbits cross Earth's, and that could, given enough time, collide with Earth (see Earth-crosser asteroids). The three most important groups of near-Earth asteroids are the Apollos, Amors, and Atens. Various asteroid deflection strategies have been proposed, as early as the 1960s. The near-Earth asteroid 433 Eros had been discovered as long ago as 1898, and the 1930s brought a flurry of similar objects. In order of discovery, these were: 1221 Amor, 1862 Apollo, 2101 Adonis, and finally 69230 Hermes, which approached within 0.005 AU of the Earth in 1937. Astronomers began to realize the possibilities of Earth impact. Two events in later decades increased the level of alarm: the increasing acceptance of Walter Alvarez' hypothesis that an impact event resulted in the Cretaceous-Tertiary extinction, and the 1994 observation of Comet Shoemaker-Levy 9 crashing into Jupiter. The U.S. military also declassified the information that its military satellites, built to detect nuclear explosions, had detected hundreds of upper-atmosphere impacts by objects ranging from one to 10 metres across. The LINEAR system alone has discovered 97,470 asteroids, as of September 18, 2008. Between all of the automated systems, 4711 near-Earth asteroids have been discovered including over 600 more than in diameter. The rate of discovery peaked in 2000, when 38,679 minor planets were numbered, and has been going down steadily since then (719 minor planets were numbered in 2007).
A newly discovered asteroid is given a provisional designation (such as) consisting of the year of discovery and an alphanumeric code indicating the half-month of discovery and the sequence within that half-month. Once an asteroid's orbit has been confirmed, it is given a number, and later may also be given a name (e.g. 433 Eros). The formal naming convention uses parentheses around the number (e.g. (433) Eros), but dropping the parentheses is quite common. Informally, it is common to drop the number altogether, or to drop it after the first mention when a name is repeated in running text.
The first few asteroids discovered were assigned symbols like the ones traditionally used to designate Earth, the Moon, the Sun and planets. The symbols quickly became ungainly, hard to draw and recognize. By the end of 1851 there were 15 known asteroids, each (except one) with its own symbol(s). Johann Franz Encke made a major change in the Berliner Astronomisches Jahrbuch (BAJ, Berlin Astronomical Yearbook) for 1854. He introduced encircled numbers instead of symbols, although his numbering began with Astraea, the first four asteroids continuing to be denoted by their traditional symbols. This symbolic innovation was adopted very quickly by the astronomical community. The following year (1855), Astraea's number was bumped up to 5, but Ceres through Vesta would be listed by their numbers only in the 1867 edition. A few more asteroids (28 Bellona, 35 Leukothea, and 37 Fides) would be given symbols as well as using the numbering scheme. The circle would become a pair of parentheses, and the parentheses sometimes omitted altogether over the next few decades.
Until the age of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes and their shapes and terrain remained a mystery. The best modern ground-based telescopes, as well as the Earth-orbiting Hubble Space Telescope, can resolve a small amount of detail on the surfaces of the very largest asteroids, but even these mostly remain little more than fuzzy blobs. Limited information about the shapes and compositions of asteroids can be inferred from their light curves (their variation in brightness as they rotate) and their spectral properties, and asteroid sizes can be estimated by timing the lengths of star occulations (when an asteroid passes directly in front of a star). Radar imaging can yield good information about asteroid shapes and orbital and rotational parameters, especially for near-Earth asteroids. The first close-up photographs of asteroid-like objects were taken in 1971 when the Mariner 9 probe imaged Phobos and Deimos, the two small moons of Mars, which are probably captured asteroids. These images revealed the irregular, potato-like shapes of most asteroids, as did subsequent images from the Voyager probes of the small moons of the gas giants. The first true asteroid to be photographed in close-up was 951 Gaspra in 1991, followed in 1993 by 243 Ida and its moon Dactyl, all of which were imaged by the Galileo probe en route to Jupiter. The first dedicated asteroid probe was NEAR Shoemaker, which photographed 253 Mathilde in 1997, before entering into orbit around 433 Eros, finally landing on its surface in 2001. Other asteroids briefly visited by spacecraft en route to other destinations include 9969 Braille (by Deep Space 1 in 1999), and 5535 Annefrank (by Stardust in 2002). In September 2005, the Japanese Hayabusa probe started studying 25143 Itokawa in detail and may return samples of its surface to earth. The Hayabusa mission has been plagued with difficulties, including the failure of two of its three control wheels, rendering it difficult to maintain its orientation to the sun to collect solar energy. Following that, the next asteroid encounters will involve the European Rosetta probe (launched in 2004), which flew by 2867 Šteins in 2008 and will buzz 21 Lutetia in 2010. In September 2007, NASA launched the Dawn Mission, which will orbit the dwarf planet Ceres and the asteroid 4 Vesta in 2011-2015, with its mission possibly then extended to 2 Pallas. It has been suggested that asteroids might be used in the future as a source of materials which may be rare or exhausted on earth (asteroid mining), or materials for constructing space habitats (see Colonization of the asteroids). Materials that are heavy and expensive to launch from earth may someday be mined from asteroids and used for space manufacturing and construction.
Generally, to allocute in law means "to speak out formally." In the field of apologetics, allocution is generally done in defense of a belief. In politics, one may allocute before a legislative body in an effort to influence their position on an issue. In law, it is generally meant to state specifically and in detail what one did and for what reason, often in relation to commission of a crime. In most United States jurisdictions a defendant is allowed the opportunity to allocute—that is, explain himself—before sentence is passed. Some jurisdictions hold this as an absolute right, and in its absence, a sentence may potentially be overturned, with the result that a new sentencing hearing must be held. Allocution is sometimes required of a defendant who pleads guilty to a crime in a plea bargain in exchange for a reduced sentence. In this instance, allocution can serve to provide closure for victims or their families. In principle, it removes any doubt as to the exact nature of the defendant's guilt in the matter. There have however, been many cases in which the defendant allocuted to a crime that he or she did not commit, often because this was a requirement to receiving a lesser sentence. The term "allocution" is generally only in use in jurisdictions in the United States, though there are vaguely similar processes in other common law countries. For example in Australia the term "allocutus" will be used. It will be used by the Clerk of Arraigns or another formal associate of the Court. It will generally be phrased as "Prisoner at the Bar, you have been found Guilty by a jury of your peers of the offense of XYZ. Do you have anything to say as to why the sentence of this Court should not now be passed upon you?". The defense counsel will then make a "plea in mitigation" (also called "submissions on penalty") wherein he or she will attempt to mitigate the relative seriousness of the offense and heavily refer to and rely upon the defendant's previous good character and good works (if any). In Australia, the right to make a plea in mitigation is absolute. If a judge or magistrate were to refuse to hear such a plea, or obviously fail to properly consider it, then the sentence would, without doubt, be overturned on appeal. In many other jurisdictions it is for the defense lawyer to mitigate on his client's behalf, and the defendant himself will rarely have the opportunity to speak.
Allocution refers to the one way dissemination of information through a media channel. It assumes that one party has an unlimited amount of information (usually through some kind of expertise) and can act as the ‘information services provider’ (pg 268) while the other party acts as the ‘information services consumer’ (Bordewijk and Kaam, 1986:268) The term allocution differs from distribution as distribution implies that the original party loses some kind of control over the information. One party can tell many others a piece of information without losing it themselves, the original information store never becomes empty. (Bordewijk and Kaam, 1986:268) The original party holds all control over the information. They decide when, how and how much information to give to the information services consumer. The consumer has no control over it in this model. Examples of this type of communication include radio and traditional television programs such as the news. Bordewijk, Jan L. and van Kaam, Ben (2002) [1986] “Towards a New Classification of Tele-Information Services,” in Denis McQuail (ed.) McQuail’s Reader in Mass Communication Theory, Sage, London, pp. 113–24
According to the Catholic Encyclopedia, an Allocution is a solemn form of address or speech from the throne employed by the Pope on certain occasions. It is delivered only in a secret consistory at which the cardinals alone are present. The term allocutio was used by the ancient Romans for the speech made by a commander to his troops, either before a battle or during it, to animate and encourage them. The term when adopted into ecclesiastical usage retained much of its original significance. An allocution of the Pope often takes the place of a manifesto when a struggle between the Holy See and the secular powers has reached an acute stage.
An affidavit is a formal sworn statement of fact, signed by the author, who is called the affiant or deponent, and witnessed as to the authenticity of the affiant's signature by a taker of oaths, such as a notary public or commissioner of oaths. The name is Medieval Latin for "he has declared upon oath". An affidavit is a type of verified statement or showing, or in other words, it contains a verification, meaning it is under oath or penalty of perjury, and this serves as evidence to its veracity and is required for court proceedings. If an affidavit is notarized or authenticated, it will also include a caption with a venue and title in reference to judicial proceedings. In some cases, an introductory clause, called a preamble, is added attesting that the affiant personally appeared before the authenticating authority.
In American jurisprudence, under the rules for hearsay, admission of an unsupported affidavit as evidence is unusual (especially if the affiant is not available for cross-examination) with regard to material facts which may be dispositive of the matter at bar. Affidavits from persons who are dead or otherwise incapacitated, or who cannot be located or made to appear may be accepted by the court, but usually only in the presence of corroborating evidence. An affidavit which reflected a better grasp of the facts close in time to the actual events may be used to refresh a witness' recollection. Materials used to refresh recollection are admissible as evidence. If the affiant is a party in the case, the affiant's opponent may be successful in having the affidavit admitted as evidence, as statements by a party-opponent are not considered hearsay. Some types of motions will not be accepted by the court unless accompanied by an independent sworn statement or other evidence, in support of the need for the motion. In such a case, a court will accept an affidavit from the filing attorney in support of the motion, as certain assumptions are made, to wit: The affidavit in place of sworn testimony promotes judicial economy. The lawyer is an officer of the court and knows that a false swearing by him, if found out, could be grounds for severe penalty up to and including disbarment. The lawyer if called upon would be able to present independent and more detailed evidence to prove the facts set forth in his affidavit. The acceptance of an affidavit by one society does not confirm its acceptance as a legal document in other jurisdictions. Equally, the acceptance that a lawyer is an officer of the court (for swearing the affidavit) is not a given. This matter is addressed by the use of the Apostille, a means of certifying the legalization of a document for international use under the terms of the 1961 Hague Convention Abolishing the Requirement of Legalization for Foreign Public Documents. Documents which have been notarized by a notary public, and certain other documents, and then certified with a conformant apostille are accepted for legal use in all the nations that have signed the Hague Convention. Thus most Affidavits now require to be Apostilled if used for cross border issues.
Affidavits are made in a similar way as to England and Wales, although "make oath" is sometimes omitted. A declaration may be substituted for an affidavit in most cases for those opposed to swearing oaths. The person making the affidavit is known as the deponent but does not sign the affidavit. The affidavit concludes in the standard format "sworn (declared) before me, [name of commissioner for oaths/solicitor], a commissioner for oaths (solicitor), on the [date] at [location] in the county/city of [county/city], and I know the deponent (declarant)", and it is signed and stamped by the commissioner for oaths.
In the ancient Babylonian calendar given in the stone tablets known as the "MUL.APIN", the constellation now known as Aries was the final station along the ecliptic. It was known as, "The Agrarian Worker". It is unclear how the "Agrarian Worker" became the "Ram" of Greek tradition, but John H. Rogers has suggested that it may have been via association with the legendary figure of Dumuzi the Shepherd. Aries only rose to its prominent position in the Neo-Babylonian revision of the Babylonian zodiac as the first point of Aries came to represent vernal equinox, replacing the Pleiades which had had this function during the Bronze Age. In Greek mythology, the constellation of Aries represents the golden ram that rescued Phrixos, taking him to the land of Colchis. Phrixos sacrificed the ram to the gods and hung its skin in a temple, where it was known as the Golden Fleece.
Aquarius is a constellation of the zodiac, situated between Capricornus and Pisces. Its name is Latin for "water-bearer" or "cup-bearer", and its symbol is (), a representation of water. Aquarius is one of the oldest of the recognized constellations along the zodiac (the sun's apparent path). It is found in a region often called the Sea due to its profusion of constellations with watery associations such as Cetus the whale, Pisces the fish and Eridanus the river.
Image:Aquariusurania.jpg|thumb|left|300px|Aquarius pours water from a jar into the mouth of the southern fish, as depicted in In illustrations, the brightest stars of Aquarius are represented as the figure of a man, while the fainter naked eye stars are represented as a vessel from which is pouring a stream of water. The water flows southwards into the mouth of the southern fish, Piscis Austrinus.
Aquarius is identified as "The Great One" in the Babylonian star catalogues and represents the god Ea himself. It contained the winter solstice in the Early Bronze Age. In Old Babylonian astronomy, Ea was the ruler of the southernmost quarter of the Sun's path, the "Way of Ea", corresponding to the period of 45 days on either side of winter solstice. In the Greek tradition, the constellation became represented as simply a single vase from which a stream poured down to Piscis Austrinus. The name in the Hindu zodiac is likewise "kumbha" "water-pitcher", showing that the zodiac reached India via Greek intermediaries. Aquarius is sometimes identified with Ganymede, a beautiful youth in Greek mythology with whom Zeus fell in love and, in the disguise of an eagle (represented by the constellation Aquila) carried off to Olympus to be cup-bearer to the gods. The constellation of Crater is sometimes identified as his cup. Aquarius has also been identified as the pourer of the waters that flooded the Earth in the ancient Greek version of the Great Flood myth. As such, the constellation Eridanus the river is sometimes identified as a river being poured by Aquarius. Aquarius may also, together with the constellation Pegasus, be part of the origin of the myth of the Mares of Diomedes, which forms one of The Twelve Labours of Heracles. Its association with pouring out rivers, and the nearby constellation of Capricornus, may be the source of the myth of the Augean stable, which forms another of the labours.
Anime in English usually refers to a style of animation originating in Japan, heavily influenced by the manga (Japanese comics) style and typically featuring characters with large eyes, big hair and elongated limbs, exaggerated facial expressions, brush-stroked outlines, limited motion and other distinctive features. The term may also be used for other animation connected to Japan or to anime proper, irrespective of style. The word comes from Japanese アニメ "anime", meaning "animation" in general, and is typically pronounced or in English. While the earliest known Japanese animation dates from 1917, and many original Japanese cartoons were produced in the ensuing decades, the characteristic anime style developed in the 1960s - notably with the work of Osamu Tezuka - and became known outside Japan in the 1980s. Anime, like manga, has a large audience in Japan and high recognition throughout the world. Distributors can release anime via television broadcasts, directly to video, or theatrically, as well as online. Both hand-drawn and computer-animated anime exist. It is used in television series, films, video, video games, commercials, and internet-based releases, and represents most, if not all, genres of fiction. Anime gained early popularity in East and Southeast Asia and has garnered more-recent popularity in the Western World.
Anime began at the start of the 20th century, when Japanese filmmakers experimented with the animation techniques also pioneered in France, Germany, the United States, and Russia. The oldest known anime in existence first screened in 1917 – a two-minute clip of a samurai trying to test a new sword on his target, only to suffer defeat. Early pioneers included Shimokawa Oten, Jun'ichi Kouchi, and Seitarō Kitayama. By the 1930s animation became an alternative format of storytelling to the live-action industry in Japan. But it suffered competition from foreign producers and many animators, such as Noburō Ōfuji and Yasuji Murata still worked in cheaper cutout not cel animation, although with masterful results. Other creators, such as Kenzō Masaoka and Mitsuyo Seo, nonetheless made great strides in animation technique, especially with increasing help from a government using animation in education and propaganda. The first talkie anime was "Chikara to Onna no Yo no Naka", produced by Masaoka in 1933. The first feature length animated film was "Momotaro's Divine Sea Warriors" directed by Seo in 1945 with sponsorship by the Imperial Japanese Navy. The success of The Walt Disney Company's 1937 feature film "Snow White and the Seven Dwarfs" influenced Japanese animators. In the 1960s, manga artist and animator Osamu Tezuka adapted and simplified many Disney animation-techniques to reduce costs and to limit the number of frames in productions. He intended this as a temporary measure to allow him to produce material on a tight schedule with inexperienced animation-staff. The 1970s saw a surge of growth in the popularity of manga – many of them later animated. The work of Osamu Tezuka drew particular attention: he has been called a "legend" and the "god of manga". His work – and that of other pioneers in the field – inspired characteristics and genres that remain fundamental elements of anime today. The giant robot genre (known as "Mecha" outside Japan), for instance, took shape under Tezuka, developed into the Super Robot genre under Go Nagai and others, and was revolutionized at the end of the decade by Yoshiyuki Tomino who developed the Real Robot genre. Robot anime like the "Gundam" and "The Super Dimension Fortress Macross" series became instant classics in the 1980s, and the robot genre of anime is still one of the most common in Japan and worldwide today. In the 1980s, anime became more accepted in the mainstream in Japan (although less than manga), and experienced a boom in production. Following a few successful adaptations of anime in overseas markets in the 1980s, anime gained increased acceptance in those markets in the 1990s and even more at the turn of the 21st century.
Japanese write the English term "animation" in "katakana" as アニメーション ("animēshon", pronounced), and it is widely assumed that the term アニメ ("anime", pronounced in Japanese) emerged in the 1970s as an abbreviation. Others claim that the word derives from the French phrase "dessin animé". Japanese-speakers use both the original and abbreviated forms interchangeably, but the shorter form occurs more commonly. The pronunciation of "anime" in Japanese, differs significantly from the Standard English, which has different vowels and stress. (In Japanese each mora carries equal stress.) As with a few other Japanese words such as "saké", "Pokémon", and "Kobo Abé," English-language texts sometimes spell "anime" as "animé" (as in French), with an acute accent over the final "e", to cue the reader to pronounce the letter, not to leave it silent as English orthography might suggest.
In Japan, the term "anime" does not specify an animation's nation of origin or style; instead, it serves as a blanket term to refer to all forms of animation from around the world. English-language dictionaries define "anime" as "a Japanese style of motion-picture animation" or as "a style of animation developed in Japan". Non-Japanese works that borrow stylization from anime are commonly referred to as "anime-influenced animation" but it is not unusual for a viewer who does not know the country of origin of such material to refer to it as simply "anime". Some works result from co-productions with non-Japanese companies, such as most of the traditionally animated Bass works, the Cartoon Network and Production I.G series "IGPX" or "Ōban Star-Racers"; different viewers may or may not consider these anime. In English, "anime", when used as a common noun, normally functions as a mass noun (for example: "Do you watch anime?", "How much anime have you collected?"). However, in casual usage the word also appears as a count noun. "Anime" can also be used as a suppletive adjective or classifier noun ("The anime "Guyver" is different from the movie "Guyver").
English-speakers occasionally refer to anime as "Japanimation", but this term has fallen into disuse. "Japanimation" saw the most usage during the 1970s and 1980s, but the term "anime" supplanted it in the mid-1990s as the material became more widely known in English-speaking countries. In general, the term now only appears in nostalgic contexts. Although the term was coined outside Japan to refer to animation imported from Japan, it is now used primarily "in" Japan, to refer to domestic animation; since "anime" does not identify the country of origin in Japanese usage, "Japanimation" is used to distinguish Japanese work from that of the rest of the world. In Japan, "manga" can additionally refer to both animation and comics (although the use of "manga" to refer to animation mostly occurs only among non-fans). Among English speakers, "manga" usually has the stricter meaning of "Japanese comics", in parallel to the usage of "anime" in and outside of Japan. An alternate explanation is that it is due to the prominence of Manga Entertainment, a distributor of anime to the US and UK markets. Because Manga Entertainment originated in the UK, the term occurs commonly outside Japan. The term "ani-manga" has been used to collectively refer to anime and manga, though it is also a term used to describe comics produced from animation cels.
Many commentators refer to anime as an art form. As a visual medium, it can emphasize visual styles. The styles can vary from artist to artist or from studio to studio. Some titles make extensive use of common stylization: "FLCL", for example, has a reputation for wild, exaggerated stylization. Other titles use different methods: "Only Yesterday" or "Jin-Roh" take much more realistic approaches, featuring few stylistic exaggerations; "Pokémon" uses drawings which specifically do not distinguish the nationality of characters. While different titles and different artists have their own artistic styles, many stylistic elements have become so common that people describe them as definitive of anime in general. However, this does not mean that all modern anime share one strict, common art-style. Many anime have a very different art style from what would commonly be called "anime style", yet fans still use the word "anime" to refer to these titles. Generally, the most common form of anime drawings include "exaggerated physical features such as large eyes, big hair and elongated limbs... and dramatically shaped speech bubbles, speed lines and onomatopoeic, exclamatory typography." The influences of Japanese calligraphy and Japanese painting also characterize linear qualities of the anime style. The round ink brush traditionally used for writing kanji and for painting, produces a stroke of widely varying thickness. Anime also tends to borrow many elements from manga, including text in the background and panel layouts. For example, an opening may employ manga panels to tell the story, or to dramatize a point for humorous effect. See for example the anime "Kare Kano".
Body proportions emulated in anime come from proportions of the human body. The height of the head is considered as the base unit of proportion. Head heights can vary as long as the remainder of the body remains proportional. Most anime characters are about seven to eight heads tall, and extreme heights are set around nine heads tall. Variations to proportion can be modded. Super-deformed characters feature a non-proportionally small body compared to the head. Sometimes specific body parts, like legs, are shortened or elongated for added emphasis. Most super deformed characters are two to four heads tall. Some anime works like "Crayon Shin-chan" completely disregard these proportions, such that they resemble Western cartoons. For exaggeration, certain body features are increased in proportion.
Many anime and manga characters feature large eyes. Osamu Tezuka, who is believed to have been the first to use this technique, was inspired by the exaggerated features of American cartoon characters such as Betty Boop, Mickey Mouse, and Disney's "Bambi". Tezuka found that large eyes style allowed his characters to show emotions distinctly. When Tezuka began drawing "Ribbon no Kishi", the first manga specifically targeted at young girls, Tezuka further exaggerated the size of the characters' eyes. Indeed, through "Ribbon no Kishi", Tezuka set a stylistic template that later "shōjo" artists tended to follow. Coloring is added to give eyes, particularly to the cornea, some depth. The depth is accomplished by applying variable color shading. Generally, a mixture of a light shade, the tone color, and a dark shade is used. Cultural anthropologist Matt Thorn argues that Japanese animators and audiences do not perceive such stylized eyes as inherently more or less foreign. However, not all anime have large eyes. For example, some of the work of Hayao Miyazaki and Toshiro Kawamoto are known for having realistically proportioned eyes, as well as realistic hair colors on their characters. In addition many other productions also have been known to use smaller eyes. This design tends to have more resemblance to traditional Japanese art. Some characters have even smaller eyes, where simple black dots are used. However, many western audiences associate anime with large detailed eyes.
Anime characters may employ wide variety of facial expressions to denote moods and thoughts. These techniques are often different in form than their counterparts in western animation. There are a number of other stylistic elements that are common to conventional anime as well but more often used in comedies. Characters that are shocked or surprised will perform a "face fault", in which they display an extremely exaggerated expression. Angry characters may exhibit a "vein" or "stress mark" effect, where lines representing bulging veins will appear on their forehead. Angry women will sometimes summon a mallet from nowhere and strike someone with it, leading to the concept of Hammerspace and cartoon physics. Male characters will develop a bloody nose around their female love interests (typically to indicate arousal, based on an old wives' tale). Embarrassed characters either produce a massive sweat-drop (which has become one of the most widely recognized motifs of conventional anime) or produce a visibly red blush or set of parallel (sometimes squiggly) lines beneath the eyes, especially as a manifestation of repressed romantic feelings. Some anime, usually with political plots and other more serious subject matters, have abandoned the use of these techniques.
Like all animation, the production processes of storyboarding, voice acting, character design, cel production and so on still apply. With improvements in computer technology, computer animation increased the efficiency of the whole production process. Anime is often considered a form of limited animation. That means that stylistically, even in bigger productions the conventions of limited animation are used to fool the eye into thinking there is more movement than there is. Many of the techniques used are comprised with cost-cutting measures while working under a set budget. Anime scenes place emphasis on achieving three-dimensional views. Backgrounds depict the scenes' atmosphere. For example, anime often puts emphasis on changing seasons, as can be seen in numerous anime, such as "Tenchi Muyo!". Sometimes actual settings have been duplicated into an anime. The backgrounds for the "Melancholy of Haruhi Suzumiya" are based on various locations within the suburb of Nishinomiya, Hyogo, Japan. Camera angles, camera movement, and lighting play an important role in scenes. Directors often have the discretion of determining viewing angles for scenes, particularly regarding backgrounds. In addition, camera angles show perspective. Directors can also choose camera effects within cinematography, such as panning, zooming, facial closeup, and panoramic. The large majority of anime uses traditional animation, which better allows for division of labor, pose to pose approach and checking of drawings before they are shot – practices favoured by the anime industry. Other mediums are mostly limited to independently-made short films, examples of which are the silhouette and other cutout animation of Noburō Ōfuji, the stop motion puppet animation of Tadahito Mochinaga, Kihachirō Kawamoto and Tomoyasu Murata and the computer animation of Satoshi Tomioka (most famously "Usavich").
While anime had entered markets beyond Japan in the 1960s, it grew as a major cultural export during its market expansion during the 1980s and 1990s. The anime market for the United States alone is "worth approximately $4.35 billion, according to the Japan External Trade Organization". Anime has also been a commercial success in Asia, Europe and Latin America, where anime has become even more mainstream than in the United States. For example, the "Saint Seiya" video game was released in Europe due to the popularity of the show even years after the series has been off-air. Anime distribution companies handled the licensing and distribution of anime outside Japan. Licensed anime is modified by distributors through dubbing into the language of the country and adding language subtitles to the Japanese language track. Using a similar global distribution pattern as Hollywood, the world is divided into five regions. Some editing of cultural references may occur to better follow the references of the non-Japanese culture. Certain companies may remove any objectionable content, complying with domestic law. This editing process was far more prevalent in the past (e.g. "Voltron"), but its use has declined because of the demand for anime in its original form. This "light touch" approach to localization has favored viewers formerly unfamiliar with anime. The use of such methods is evident by the success of "Naruto" and Cartoon Network's Adult Swim programming block, both of which employ minor edits. "Robotech" and "Star Blazers" were the earliest attempts to present anime (albeit still modified) to North American television audiences without harsh censoring for violence and mature themes. With the advent of DVD, it became possible to include multiple language tracks into a simple product. This was not the case with VHS cassette, in which separate VHS media were used and with each VHS cassette priced the same as a single DVD. The "light touch" approach also applies to DVD releases as they often include both the dubbed audio and the original Japanese audio with subtitles, typically unedited. Anime edited for television is usually released on DVD "uncut", with all scenes intact. TV networks regularly broadcast anime programming. In Japan, major national TV networks, such as TV Tokyo broadcast anime regularly. Smaller regional stations broadcast anime under the UHF. In the United States, cable TV channels such as Cartoon Network, Disney, Syfy, and others dedicate some of their timeslots to anime. Some, such as the Anime Network and the FUNimation Channel, specifically show anime. Sony-based Animax and Disney's Jetix channel broadcast anime within many countries in the world. AnimeCentral solely broadcasts anime in the UK. Although it violates copyright laws in many countries, some fans add subtitles to anime on their own. These are distributed as fansubs. The ethical implications of producing, distributing, or watching fansubs are topics of much controversy even when fansub groups do not profit from their activities. Once the series has been licensed outside of Japan, fansub groups often cease distribution of their work. In one case, Media Factory Incorporated requested that no fansubs of their material be made, which was respected by the fansub community. In another instance, Bandai specifically thanked fansubbers for their role in helping to make "The Melancholy of Haruhi Suzumiya" popular in the English speaking world. The Internet has played a significant role in the exposure of anime beyond Japan. Prior to the 1990s, anime had limited exposure beyond Japan's borders. Coincidentally, as the popularity of the Internet grew, so did interest in anime. Much of the fandom of anime grew through the Internet. The combination of internet communities and increasing amounts of anime material, from video to images, helped spur the growth of fandom. As the Internet gained more widespread use, Internet advertising revenues grew from 1.6 billion yen to over 180 billion yen between 1995 and 2005.
Anime has become commercially profitable in western countries, as early commercially successful western adaptations of anime, such as "Astro Boy", have revealed. The phenomenal success of Nintendo's multi-billion dollar "Pokémon" franchise was helped greatly by the spin-off anime series that, first broadcast in the late 1990s, is still running worldwide to this day. In doing so, anime has made significant impacts upon Western culture. Since the 19th century, many Westerners have expressed a particular interest towards Japan. Anime dramatically exposed more Westerners to the culture of Japan. Aside from anime, other facets of Japanese culture increased in popularity. Worldwide, the number of people studying Japanese increased. In 1984, the Japanese Language Proficiency Test was devised to meet increasing demand. Anime-influenced animation refers to non-Japanese works of animation that emulate the visual style of anime. Most of these works are created by studios in the United States, Europe, and non-Japanese Asia; and they generally incorporate stylizations, methods, and gags described in anime physics, as in the case of '. Often, production crews either are fans of anime or are required to view anime. Some creators cite anime as a source of inspiration with their own series. Furthermore, a French production team for "Ōban Star-Racers" moved to Tokyo to collaborate with a Japanese production team from Hal Film Maker. Critics and the general anime fanbase do not consider them as anime. Some American animated television-series have singled out anime styling with satirical intent, for example "South Park" (with "Chinpokomon" and with "Good Times with Weapons"). "South Park" has a notable drawing style, itself parodied in "Brittle Bullet", the fifth episode of the anime "FLCL", released several months after "Chinpokomon" aired. This intent on satirizing anime is the springboard for the basic premise of "Kappa Mikey", a Nicktoons Network original cartoon. Even clichés normally found in anime are parodied in some series, such as "Perfect Hair Forever". Anime conventions began to appear in the early 1990s, during the Anime boom, starting with Anime Expo, Animethon, Otakon, and JACON. Currently anime conventions are held annually in various cities across the Americas, Asia, and Europe. Many attendees participate in cosplay, where they dress up as anime characters. Also, guests from Japan ranging from artists, directors, and music groups are invited. In addition to anime conventions, anime clubs have become prevalent in colleges, high schools, and community centers as a way to publicly exhibit anime as well as broadening Japanese cultural understanding.
The Japanese term "otaku" is used in America as a term for anime fans, more particularly the obsessive ones. The negative connotations associated with the word in Japan have disappeared in its American context, where it instead connotes the pride of the fans. Only in the recent decade or so has there been a more casual viewership outside the devoted "otaku" fan base, which can be attributed highly to technological advances. Also, shows like "Pokémon" and "Dragon Ball Z" provided a pivotal introduction of anime's conventions, animation methods, and Shinto influences to many American children. Ancient Japanese myths – often deriving from the animistic nature worship of Shinto – have influenced anime greatly, but most American audiences not accustomed to anime know very little of these foreign texts and customs. For example, an average American viewing the live-action TV show "Hercules" will be no stranger to the Greek myths and legends it is based on, while the same person watching the show "Tenchi Muyo!" might not understand that the pleated ropes wrapped around the "space trees" are influenced by the ancient legend of "Amaterasu and Susano".
Ankara is the capital of Turkey and the country's second largest city after Istanbul. The city has a mean elevation of, and as of 2007 the city had a population of 4,751,360, which includes eight districts under the city's administration. Ankara also serves as the capital of Ankara Province. As with many ancient cities, Ankara has gone by several names over the ages: The Hittites gave it the name "Ankuwash" before 1200 BC. The Galatians and Romans called it "Ancyra". In the classical, Hellenistic, and Byzantine periods it was known as ("Ánkyra", meaning "Anchor") in Greek. The city was also known in the European languages as "Angora" after its conquest by the Seljuk Turks in 1073, and continued to be internationally called with this name until it was officially renamed "Ankara" with the Turkish Postal Service Law of 1930. Centrally located in Anatolia, Ankara is an important commercial and industrial city. It is the center of the Turkish Government, and houses all foreign embassies. It is an important crossroads of trade, strategically located at the centre of Turkey's highway and railway networks, and serves as the marketing centre for the surrounding agricultural area. The city was famous for its long-haired Angora goat and its prized wool (mohair), a unique breed of cat (Angora cat), white rabbits and their prized wool (Angora wool), pears, honey, and the region's muscat grapes. The historical center of Ankara is situated upon a steep and rocky hill, which rises above the plain on the left bank of the "Ankara Çayı", a tributary of the Sakarya (Sangarius) river. The city is located at 39°52'30" North, 32°52' East (), about to the southeast of Istanbul, the country's largest city. Although situated in one of the driest places of Turkey and surrounded mostly by steppe vegetation except for the forested areas on the southern periphery, Ankara can be considered a green city in terms of green areas per inhabitant, which is 72 m2 per head. Ankara is a very old city with various Hittite, Phrygian, Hellenistic, Roman, Byzantine, and Ottoman archaeological sites. The hill which overlooks the city is crowned by the ruins of the old castle, which adds to the picturesqueness of the view, but only a few historic structures surrounding the old citadel have survived to our date. There are, however, many finely preserved remains of Hellenistic, Roman and Byzantine architecture, the most remarkable being the Temple of Augustus and Rome (20 BC) which is also known as the "Monumentum Ancyranum".
The oldest settlements in and around the city centre of Ankara belong to the Hatti civilization which existed during the Bronze Age. The city grew significantly in size and importance under the Phrygians starting around 1000 BC, and experienced a large expansion following the mass migration from Gordion, (the capital of Phrygia), after an earthquake which severely damaged that city around that time. In Phrygian tradition, King Midas was venerated as the founder of Ancyra, but Pausanias mentions that the city was actually far older, which accords with present archaeological knowledge. Phrygian rule was succeeded first by Lydian and later by Persian rule, though the strongly Phrygian character of the peasantry remained, as evidenced by the gravestones of the much later Roman period. Persian sovereignty lasted until the Persians' defeat at the hands of Alexander the Great who conquered the city in 333 BC. Alexander came from Gordion to Ankara and stayed in the city for a short period. After his death at Babylon in 323 BC and the subsequent division of his empire amongst his generals, Ankara and its environs fell into the share of Antigonus. Another important expansion took place under the Greeks of Pontos who came there around 300 BC and developed the city as a trading centre for the commerce of goods between the Black Sea ports and Crimea to the north; Assyria, Cyprus, and Lebanon to the south; and Georgia, Armenia and Persia to the east. By that time the city also took its name "Áγκυρα" ("Ànkyra", meaning "Anchor" in Greek) which in slightly modified form provides the modern name of "Ankara".
In 278 BC, the city, along with the rest of central Anatolia, was occupied by the Celtic race of Galatians, who were the first to make Ankara one of their main tribal centres, the headquarters of the Tectosages tribe. Other centres were Pessinos, today's "Balhisar", for the Trocmi tribe, and Tavium, to the east of Ankara, for the "Tolstibogii" tribe. The city was then known as "Ancyra". The Celtic element was probably relatively small in numbers; a warrior aristocracy which ruled over Phrygian-speaking peasants. However, the Celtic language continued to be spoken in Galatia for many centuries. At the end of the 4th century AD, St. Jerome, a native of Galatia, observed that the language spoken around Ankara was very similar to that being spoken in the northwest of the Roman world near Trier.
The city was subsequently conquered by Augustus in 25 BC and passed under the control of the Roman Empire. Now the capital city of the Roman province of Galatia, Ancyra continued to be a center of great commercial importance. Ankara is also famous for the "Monumentum Ancyranum" ("Temple of Augustus and Rome") which contains the official record of the "Acts of Augustus", known as the "Res Gestae Divi Augusti", an inscription cut in marble on the walls of this temple. The ruins of Ancyra still furnish today valuable bas-reliefs, inscriptions and other architectural fragments. Augustus decided to make Ancyra one of three main administrative centres in central Anatolia. The town was then populated by Phrygians and Celts—the "Galatians" who spoke a language closely related to Welsh and Gaelic. Ancyra was the center of a tribe known as the "Tectosages", and Augustus upgraded it into a major provincial capital for his empire. Two other Galatian tribal centres, Tavium near Yozgat, and Pessinus (Balhisar) to the west, near Sivrihisar, continued to be reasonably important settlements in the Roman period, but it was Ancyra that grew into a grand metropolis. An estimated 200,000 people lived in Ancyra in good times during the Roman Empire, a far greater number than was to be the case from after the fall of the Roman Empire until the early twentieth century. A small river, the Ankara Çayı, ran through the centre of the Roman town. It has now been covered over and diverted, but it formed the northern boundary of the old town during the Roman, Byzantine and Ottoman periods. Çankaya, the rim of the majestic hill to the south of the present city center, stood well outside the Roman city, but may have been a summer resort. In the 19th century, the remains of at least one Roman villa or large house were still standing not far from where the Çankaya Presidential Residence stands today. To the west, the Roman city extended until the area of the Gençlik Park and Railway Station, while on the southern side of the hill, it may have extended downwards as far as the site presently occupied by Hacettepe University. It was thus a sizeable city by any standards and much larger than the Roman towns of Gaul or Britannia. Ancyra's importance rested on the fact was that it was the junction point where the roads in northern Anatolia running north-south and east-west intersected. The great imperial road running east passed through Ankara and a succession of emperors and their armies came this way. They were not the only ones to use the Roman highway network, which was equally convenient for invaders. In the second half of the 3rd century, Ancyra was invaded in rapid succession by the Goths coming from the west (who rode far into the heart of Cappadocia, taking slaves and pillaging) and later by the Arabs. For about a decade, the town was one of the western outposts of one of the most brilliant queens of the ancient world, the Arab empress Zenobia from Palmyra in the Syrian desert, who took advantage of a period of weakness and disorder in the Roman Empire to set up a short-lived state of her own. The town was reincorporated into the Roman Empire under the Emperor Aurelian in 272. The tetrarchy, a system of multiple (up to four) emperors introduced by Diocletian (284-305), seems to have engaged in a substantial programme of rebuilding and of road construction from Ankara westwards to Germe and Dorylaeum (now Eskişehir). In its heyday, Roman Ankara was a large market and trading center but it also functioned as a major administrative capital, where a high official ruled from the city's Praetorium, a large administrative palace or office. During the 3rd century, life in Ancyra, as in other Anatolian towns, seems to have become somewhat militarised in response to the invasions and instability of the town. In this period, like other cities of central Anatolia, Ankara was also undergoing Christianisation. Early martyrs, about whom little is known, included Proklos and Hilarios who were natives of the otherwise unknown village of Kallippi, near Ancyra, and suffered repression under the emperor Trajan (98-117). In the 280s AD we hear of Philumenos, a Christian corn merchant from southern Anatolia, being captured and martyred in Ankara, and Eustathius. As in other Roman towns, the reign of Diocletian marked the culmination of the persecution of the Christians. In 303, Ancyra was one of the towns where the co-Emperors Diocletian and his deputy Galerius launched their anti-Christian persecution. In Ancyra, their first target was the 38-year-old Bishop of the town, whose name was Clement. Clement's life describes how he was taken to Rome, then sent back, and forced to undergo many interrogations and hardship before he, and his brother, and various companions were put to death. The remains of the church of St. Clement can be found today in a building just off Işıklar Caddesi in the Ulus district. Quite possibly this marks the site where Clement was originally buried. Four years later, a doctor of the town named Plato and his brother Antiochus also became celebrated martyrs under Galerius. Theodotus of Ancyra is also venerated as a saint. However, the persecution proved unsuccessful and in 314 Ancyra was the center of an important council of the early church; which considered ecclesiastical policy for the reconstruction of the Christian church after the persecutions, and in particular the treatment of 'lapsi'—Christians who had given in and conformed to paganism during these persecutions. Three councils were held in the former capital of Galatia in Asia Minor, during the 4th century. The first, an orthodox plenary synod, was held in 314, and its 25 disciplinary canons constitute one of the most important documents in the early history of the administration of the Sacrament of Penance. Nine of them deal with conditions for the reconciliation of the lapsi; the others, with marriage, alienations of church property, etc. Though paganism was probably tottering in Ancyra in Clement's day, it may still have been the majority religion. Twenty years later, Christianity and monotheism had taken its place. Ancyra quickly turned into a Christian city, with a life dominated by monks and priests and theological disputes. The town council or senate gave way to the bishop as the main local figurehead. During the middle of the 4th century, Ancyra was involved in the complex theological disputes over the nature of Christ, and a form of Arianism seems to have originated there. The synod of 358 was a Semi-Arian conciliabulum, presided over by Basil of Ancyra. It condemned the grosser Arian blasphemies, but set forth an equally heretical doctrine in the proposition that the Son was in all things similar to the Father, but not identical in substance. In 362-363, the Emperor Julian the Apostate passed through Ancyra on his way to an ill-fated campaign against the Persians, and according to Christian sources, engaged in a persecution of various holy men. The stone base for a statue, with an inscription describing Julian as "Lord of the whole world from the British Ocean to the barbarian nations", can still be seen, built into the eastern side of the inner circuit of the walls of Ankara Castle. The Column of Julian which was erected in honor of the emperor's visit to the city in 362 still stands today. In 375, Arian bishops met at Ancyra and deposed several bishops, among them St. Gregory of Nyssa. The modern Ankara, also known in some Western texts as "Angora", remains a Roman Catholic titular see in the former Roman province of Galatia in Asia Minor, suffragan of Laodicea. Its episcopal list is given in Gams, "Series episc. Eccl. cath."; also that of another Ancyra in Phrygia Pacatiana. In the later 4th century Ancyra became something of an imperial holiday resort. After Constantinople became the East Roman capital, emperors in the 4th and 5th centuries would retire from the humid summer weather on the Bosporus to the drier mountain atmosphere of Ancyra. Theodosius II (408-450) kept his court in Ancyra in the summers. Laws issued in Ancyra testify to the time they spent there. The city's military as well as logistical significance lasted well into the long Byzantine rule. Although Ancyra temporarily fell into the hands of several Arab Muslim armies numerous times after the seventh century, it remained an important crossroads polis within the Byzantine Empire until the late 11th century. It was also the capital of the powerful Opsician Theme, and after ca. 750 of the Bucellarian Theme.
In 1071, the Turkish Seljuk Sultan Alparslan conquered much of eastern and central Anatolia after his victory at the Battle of Manzikert (Malazgirt). He then annexed Ankara, an important location for military transportation and natural resources, to his territory in 1073. After Battle of Kösedağ in 1243 which Mongols defeated Seljuks, most of Anatolia became dominion of Mongols. Taking advantage of Seljuk decline, a semi religious cast of craftsmen and trade people named "Ahiler" chose Ankara as their independent city state in 1290. Orhan I, the second Bey of the Ottoman Empire, captured the city in 1356. Timur defeated the Ottomans at the Battle of Ankara in 1402 and took the city, but in 1403 Ankara was again under Ottoman control. Following the Ottoman defeat at World War I, the Ottoman capital Istanbul and much of Anatolia were occupied by the Allies, who planned to share these lands between Armenia, France, Greece, Italy and the United Kingdom, leaving for the Turks the core piece of land in central Anatolia. In response, the leader of the Turkish nationalist movement, Mustafa Kemal Atatürk, established the headquarters of his resistance movement in Ankara in 1920 (see the Treaty of Sèvres and the Turkish War of Independence.) After the War of Independence was won and the Treaty of Sèvres was superseded by the Treaty of Lausanne, the Turkish nationalists replaced the Ottoman Empire with the Republic of Turkey on 29 October 1923. A few days earlier, Ankara had officially replaced Istanbul (formerly Constantinople) as the new Turkish capital city, on 13 October 1923. After Ankara became the capital of the newly founded Republic of Turkey, new development divided the city into an old section, called "Ulus", and a new section, called "Yenişehir". Ancient buildings reflecting Roman, Byzantine, and Ottoman history and narrow winding streets mark the old section. The new section, now centered around "Kızılay", has the trappings of a more modern city: wide streets, hotels, theaters, shopping malls, and high-rises. Government offices and foreign embassies are also located in the new section. Ankara has experienced a phenomenal growth since it was made Turkey's capital. It was "a small town of no importance" when it was made the capital of Turkey. In 1924, the year after the government had moved there, Ankara had about 35,000 residents. By 1927 there were 44,553 residents and by 1950 the population had grown to 286,781.
Ankara has a continental climate, with cold, snowy winters and hot, dry summers. Rainfall occurs mostly during the spring and autumn. Under Köppen's climate classification, Ankara features the rare Continental Mediterranean climate (Köppen Csb) due to its elevation, the forementioned cold, snowy winters and hot dry summers and peaks of precipitation during the spring and autumn. It borders on a cold semi-arid climate (Köppen BSk) due to the low average annual precipitation. Because of Ankara's high altitude and its dry summers, nightly temperatures in the summer months are cool. Precipitation levels are low, but precipitation can be observed throughout the year.
The foundations of the citadel or castle were laid by the Galatians on a prominent lava outcrop, and the rest was completed by the Romans. The Byzantines and Seljuks further made restorations and additions. The area around and inside the citadel, being the oldest part of Ankara, contains many fine examples of traditional architecture. There are also recreational areas to relax. Many restored traditional Turkish houses inside the citadel area have found new life as restaurants, serving local cuisine. The citadel was depicted in various Turkish banknotes during 1927-1952 and 1983-1989.
The remains, the stage, and the backstage can be seen outside the castle. Roman statues that were found here are exhibited in the Museum of Anatolian Civilizations (see above). The seating area is still under excavation. Temple of Augustus and Rome. The temple, also known as the Monumentum Ancyranum, was built between 25 BC - 20 BC following the conquest of Central Anatolia by the Roman Empire and the formation of the Roman province of Galatia, with Ancyra (modern Ankara) as its administrative capital. After the death of Augustus in 14 AD, a copy of the text of Res Gestae Divi Augusti was inscribed on the interior of the "pronaos" in Latin, whereas a Greek translation is also present on an exterior wall of the "cella". The temple, on the ancient Acropolis of Ancyra, was enlarged by the Romans in the 2nd century. In the 5th century it was converted into a church by the Byzantines. It is located in the Ulus quarter of the city.
Erected in 1927 on Zafer Square in the Sıhhiye quarter, it depicts Atatürk in uniform. Monument to a Secure, Confident Future. This monument, located in Güven Park near Kızılay Square, was erected in 1935 and bears Atatürk's advice to his people: "Turk! Be proud, work hard, and believe in yourself." The monument was depicted on the reverse of the Turkish 5 lira banknote of 1937-1952 and of the 1000 lira banknotes of 1939-1946.
Ankara has many parks and open spaces mainly established in the early years of the Republic and well maintained and expanded thereafter. The most important of these parks are: Gençlik Park (houses an amusement park with a large pond for rowing), the Botanical Garden, Seğmenler Park, Anayasa Park, Kuğulu Park (famous for the swans received as a gift from the Chinese government), Abdi İpekçi Park, Güven Park (see above for the monument), Kurtuluş Park (has an ice-skating rink), Altınpark (also a prominent exposition/fair area), Harikalar Diyarı (claimed to be Biggest Park of Europe inside city borders) and Göksu Park. Gençlik Park was depicted on the reverse of the Turkish 100 lira banknotes of 1952-1976. Atatürk Forest Farm and Zoo ("Atatürk Orman Çiftliği") is an expansive recreational farming area which houses a zoo, several small agricultural farms, greenhouses, restaurants, a dairy farm and a brewery. It is a pleasant place to spend a day with family, be it for having picnics, hiking, biking or simply enjoying good food and nature. There is also an exact replica of the house where Atatürk was born in 1881, in Thessaloniki, Greece. Visitors to the "Çiftlik" (farm) as it is affectionately called by Ankarans, can sample such famous products of the farm such as old-fashioned beer and ice cream, fresh dairy products and meat rolls/kebaps made on charcoal, at a traditional restaurant ("Merkez Lokantası", Central Restaurant), cafés and other establishments scattered around the farm.
Foreign visitors to Ankara usually like to visit the old shops in "Çıkrıkçılar Yokuşu" (Weavers' Road) near Ulus, where myriad things ranging from traditional fabrics, hand-woven carpets and leather products can be found at bargain prices. "Bakırcılar Çarşısı" (Bazaar of Coppersmiths) is particularly popular, and many interesting items, not just of copper, can be found here like jewelry, carpets, costumes, antiques and embroidery. Up the hill to the castle gate, there are many shops selling a huge and fresh collection of spices, dried fruits, nuts, and other produce. Modern shopping areas are mostly found in Kızılay, or on Tunalı Hilmi Avenue, including the modern mall of Karum (named after the ancient Assyrian merchant colonies (Karum) that were established in central Anatolia at the beginning of the 2nd millennium BC) which is located towards the end of the Avenue; and in the Atakule Tower at Çankaya, the quarter with the highest elevation in the city, which commands a magnificent view over the whole city and also has a revolving restaurant at the top where the complete panorama can be enjoyed in a more leisurely fashion. The symbol of the Armada Shopping Mall is an anchor, and there's a large anchor monument at its entrance, as a reference to the ancient Greek name of the city, Ἄγκυρα (Ánkyra), which means anchor. Likewise, the anchor is also related with the Spanish name of the mall, Armada, which means naval fleet. As Ankara started expanding westward in the 1970s, several modern, suburbia-style developments and mini-cities began to rise along the western highway, also known as the Eskişehir Road. The "Armada" and "CEPA" malls on the highway, the "Galleria" in Ümitköy, and a huge mall, "Real" in Bilkent Center, offer North American and European style shopping opportunities (these places can be reached through the Eskişehir Highway.) There is also the newly expanded "Ankamall" at the outskirts, on the Istanbul Highway, which houses most of the well-known international brands. This mall is the largest throughout the Ankara region.
Esenboğa International Airport, located in the north-east of the city, is the main airport of Ankara. () is an important part of the bus network which covers every neighbourhood in the city. The central train station, "Ankara Garı" of the Turkish State Railways (), is an important hub connecting the western and eastern parts of the country. High-speed rail services are to be operated between Ankara and Istanbul, beginning in 2009. The "Electricity, Gas, Bus General Directorate" (EGO) operates the Ankara Metro and other forms of public transportation. Ankara is currently served by suburban rail and two subway lines with about 300,000 total daily commuters, and three additional subway lines are under construction.
As with all other cities of Turkey, football is the most popular sport in Ankara. The city has four football clubs currently competing in the Turkcell Super League: Ankaragücü founded in 1910 is the oldest club in Ankara and associated with Ankara's military arsenal manufacturing company MKE. They were the Turkish Cup winners in 1972 and 1981. Their rival is Gençlerbirliği founded in 1923 known as Ankara Wind or the Poppies because of their colours: red and black. They were the Turkish Cup winners in 1987 and 2001. Gençler's B team, Hacettepe SK (formerly known as Gençlerbirliği OFTAŞ) has been allowed to ascend to the Super League along with its A team as long as they have 2 different chairmen. All these three teams have their home at the Ankara 19 Mayıs Stadium in Ulus, which has a capacity of 21,250 (all-seater). The fourth team is owned by the Municipality, Büyükşehir Belediye Ankaraspor who are nicknamed the Leopards. Their home is the Yenikent Asaş Stadium in the Sincan district of Yenikent, outside the city center. Ankara has a large number of minor teams, playing at regional levels: Bugsaşspor in Sincan; Etimesgut Şekerspor in Etimesgut; Türk Telekom owned by the phone company in Yenimahalle; Demirspor in Çankaya; Keçiörengücü, Keçiörenspor, Pursaklarspor, Bağlumspor in Keçiören; and Petrol Ofisi Spor. In the Turkish Basketball League, Ankara is represented by Türk Telekom, whose home is the ASKI Sport Hall, and CASA TED Kolejliler, whose home is the TOBB Sports Hall. Ankara Buz Pateni Sarayı is where the ice skating and ice hockey competitions take place in the city. There are many popular spots for skateboarding which is active in the city since the 1980s. Skaters in Ankara usually meet in the park near the Grand National Assembly of Turkey.
Ankara is home to a world famous cat breed — the Turkish Angora, called "Ankara kedisi" (Ankara cat) in Turkish. It is a breed of domestic cat. Turkish Angoras are one of the ancient, naturally-occurring cat breeds, having originated in Ankara and its surrounding region in central Anatolia. They mostly have a white, silky, medium to long length coat, no undercoat and a fine bone structure. There seems to be a connection between the Angora Cats and Persians, and the Turkish Angora is also a distant cousin of the Turkish Van. Although they are known for their shimmery white coat, currently there are more than twenty varieties including black, blue and reddish fur. They come in tabby and tabby-white, along with smoke varieties, and are in every color other than pointed, lavender, and cinnamon (all of which would indicate breeding to an outcross.) Eyes may be blue, green, or amber, or even one blue and one amber or green. The W gene which is responsible for the white coat and blue eye is closely related to the hearing ability, and the presence of a blue eye can indicate that the cat is deaf to the side the blue eye is located. However, a great many blue and odd-eyed white cats have normal hearing, and even deaf cats lead a very normal life if kept indoors. Ears are pointed and large, eyes are almond shaped and the head is massive with a two plane profile. Another characteristic is the tail, which is often kept parallel to the back.
The Angora rabbit () is a variety of domestic rabbit bred for its long, soft hair. The Angora is one of the oldest types of domestic rabbit, originating in Ankara and its surrounding region in central Anatolia, along with the Angora cat and Angora goat. The rabbits were popular pets with French royalty in the mid 1700s, and spread to other parts of Europe by the end of the century. They first appeared in the United States in the early 1900s. They are bred largely for their long Angora wool, which may be removed by shearing, combing, or plucking (gently pulling loose wool.) Angoras are bred mainly for their wool because it is silky and soft. They have a humorous appearance, as they oddly resemble a fur ball. Most are calm and docile but should be handled carefully. Grooming is necessary to prevent the fiber from matting and felting on the rabbit. A condition called "wool block" is common in Angora rabbits and should be treated quickly. Sometimes they are shorn in the summer as the long fur can cause the rabbits to overheat.
The Angora goat () is a breed of domestic goat that originated in Ankara and its surrounding region in central Anatolia. This breed was first mentioned in the time of Moses, roughly in 1500 BC. The first Angora goats were brought to Europe by Charles V, Holy Roman Emperor, about 1554, but, like later imports, were not very successful. Angora goats were first introduced in the United States in 1849 by Dr. James P. Davis. Seven adult goats were a gift from Sultan Abdülmecid I in appreciation for his services and advice on the raising of cotton. The fleece taken from an Angora goat is called mohair. A single goat produces between five and eight kilograms of hair per year. Angoras are shorn twice a year, unlike sheep, which are shorn only once. Angoras have high nutritional requirements due to their rapid hair growth. A poor quality diet will curtail mohair development. The United States, Turkey, and South Africa are the top producers of mohair. For a long period of time, Angora goats were bred for their white coat. In 1998, the Colored Angora Goat Breeders Association was set up to promote breeding of colored Angoras. Now Angora goats produce white, black (deep black to greys and silver), red (the color fades significantly as the goat gets older), and brownish fiber. Angora goats were depicted on the reverse of the Turkish 50 lira banknotes of 1938-1952.
Arabic (', () or ') is a Central Semitic language, thus related to and classified alongside other Semitic languages such as Hebrew and the Neo-Aramaic languages. In terms of speakers, Arabic is the largest member of the Semitic language family. It is spoken by more than 280 million people as a first language, most of whom live in the Middle East and North Africa, and by 250 million more as a second language. Arabic has many different, geographically-distributed spoken varieties, some of which are mutually unintelligible. Modern Standard Arabic is widely taught in schools, universities, and used in workplaces, government and the media. Modern Standard Arabic derives from Classical Arabic, the only surviving member of the Old North Arabian dialect group, attested in Pre-Islamic Arabic inscriptions dating back to the 4th century. Classical Arabic has also been a literary language and the liturgical language of Islam since its inception in the 7th century. Arabic has lent many words to other languages of the Islamic world. During the Middle Ages, Arabic was a major vehicle of culture in Europe, especially in science, mathematics and philosophy. As a result, many European languages have also borrowed many words from it. Arabic influence is seen in Mediterranean languages, particularly Spanish, Portuguese, and Sicilian, owing to both the proximity of European and Arab civilizations and 700 years of Islamic rule in the Iberian peninsula (see Al-Andalus). Arabic has also borrowed words from many languages, including Hebrew, Persian and Syriac in early centuries, Turkish in medieval times and contemporary European languages in modern times. Classical, Modern Standard, and colloquial Arabic. "Arabic" usually designates one of three main variants: Classical Arabic; Modern Standard Arabic; "colloquial" or "dialectal" Arabic. Classical Arabic (فصحى "fuṣḥā") is the language found in the Qur'an and used from the period of Pre-Islamic Arabia to that of the Abbasid Caliphate. Classical Arabic is considered normative; modern authors attempt to follow the syntactic and grammatical norms laid down by classical grammarians (such as Sibawayh), and use the vocabulary defined in classical dictionaries (such as the Lisān al-Arab). Based on Classical Arabic, Modern Standard Arabic (فصحى "fuṣḥā") is the literary language used in most current, printed Arabic publications, spoken by the Arabic media across North Africa and the Middle East, and understood by most educated Arabic speakers. "Literary Arabic" and "Standard Arabic" are less strictly defined terms that may refer to Modern Standard Arabic and/or Classical Arabic. "Colloquial" or "dialectal" Arabic refers to the many national or regional varieties which constitute the everyday spoken language. Colloquial Arabic has many different regional variants; these sometimes differ enough to be mutually unintelligible and some linguists consider them distinct languages. The varieties are typically unwritten. They are often used in informal spoken media, such as soap operas and talk shows, as well as occasionally in certain forms of written media, such as poetry and printed advertising. The only variety of modern Arabic to have acquired official language status is Maltese, spoken in (predominately Roman Catholic) Malta and written with the Latin alphabet. It is descended from Classical Arabic through Siculo-Arabic and is not mutually intelligible with other varieties of Arabic. Most linguists list it as a separate language rather than as a dialect of Arabic. The sociolinguistic situation of Arabic in modern times provides a prime example of the linguistic phenomenon of diglossia, which is the normal use of two separate varieties of the same language, usually in different social situations. In the case of Arabic, educated Arabs of any nationality can be assumed to speak both their local dialect and their school-taught Standard Arabic. When educated Arabs of different dialects engage in conversation (for example, a Moroccan speaking with a Lebanese), many speakers code-switch back and forth between the dialectal and standard varieties of the language, sometimes even within the same sentence. Arabic speakers often improve their familiarity with other dialects via music or film. Like other languages, Modern Standard Arabic continues to evolve. Many modern terms have entered into common usage, in some cases taken from other languages (for example, فيلم "film") or coined from existing lexical resources (for example, هاتف "hātif" "telephone" < "caller"). Structural influence from foreign languages or from the colloquial varieties has also affected Modern Standard Arabic. For example, texts in Modern Standard Arabic sometimes use the format "A, B, C, and D" when listing things, whereas Classical Arabic prefers "A and B and C and D", and subject-initial sentences may be more common in Modern Standard Arabic than in Classical Arabic. For these reasons, Modern Standard Arabic is generally treated separately in non-Arab sources. Influence of Arabic on other languages. The influence of Arabic has been most important in Islamic countries. Arabic is a major source of vocabulary for languages such as Amharic, Baluchi, Bengali, Berber, Catalan, Cypriot Greek, Gujarati, Hindustani, Indonesian, Kurdish, Malay, Marathi, Pashto, Persian, Portuguese, Punjabi, Rohingya, Sindhi, Spanish, Swahili, Tagalog, Turkish and Urdu as well as other languages in countries where these languages are spoken. For example, the Arabic word for "book" (/kitāb/) has been borrowed in all the languages listed, with the exception of Spanish, Catalan and Portuguese which use the Latin-derived words "libro","llibre" and "livro", respectively, and Tagalog which uses "aklat". In addition, English has quite a few Arabic loan words, some directly but most through the medium of other Mediterranean languages. Other languages such as Maltese and Kinubi derive from Arabic, rather than merely borrowing vocabulary or grammar rules. The terms borrowed range from religious terminology (like Berber "prayer" < salat), academic terms (like Uyghur "mentiq" "logic"), economic items (like English "sugar") to placeholders (like Spanish "fulano" "so-and-so") and everyday conjunctions (like Hindustani "lekin" "but", or Spanish "hasta" "until"). Most Berber varieties (such as Kabyle), along with Swahili, borrow some numbers from Arabic. Most Islamic religious terms are direct borrowings from Arabic, such as "salat" 'prayer' and "imam" 'prayer leader.' In languages not directly in contact with the Arab world, Arabic loanwords are often transferred indirectly via other languages rather than being transferred directly from Arabic. For example, most Arabic loanwords in Hindustani entered through Persian, and many older Arabic loanwords in Hausa were borrowed from Kanuri. Some words in English and other European languages are derived from Arabic, often through other European languages, especially Spanish and Italian. Among them are commonly-used words like "sugar" ("sukkar"), "cotton" (') and "magazine" ("makhzen"). English words more recognizably of Arabic origin include "algebra", "alcohol", "alchemy", "alkali", "zenith" and "nadir". Some words in common use, such as "intention" and "information", were originally calques of Arabic philosophical terms. Arabic words also made their way into several West African languages as Islam spread across the Sahara. Variants of Arabic words such as "kitaab" (book) have spread to the languages of African groups who had no direct contact with Arab traders. Arabic was influenced by other languages as well. The most important sources of borrowings into (pre-Islamic) Arabic are Aramaic, which used to be the principal, international language of communication throughout the ancient Near and Middle East, Ethiopic, and to a lesser degree Hebrew (mainly religious concepts). As Arabic occupied a position similar to Latin (in Europe) throughout the Islamic world many of the Arabic concepts in the field of science, philosophy, commerce etc., were often coined by non-native Arabic speakers, notably by Aramaic and Persian translators. This process of using Arabic roots in notably Turkish and Persian, to translate foreign concepts continued right until the 18th and 19th century, when large swaths of Arab-inhabited lands were under Ottoman rule.
Arabic is the language of the Qur'an. Arabic is often associated with Islam, but it is also spoken by Arab Christians, Mizrahi Jews and Iraqi Mandaeans. Most of the world's Muslims do not speak Arabic as their native language but many can read the script and recite the words of religious texts. Some Muslims consider the Arabic language to be "the language chosen by God to speak to mankind" and the original revealed language spoken by man from which all other languages were derived having been corrupted. It is most notably understood by Muslims as being the lingua franca of the afterlife.
The earliest surviving texts in Proto-Arabic, or Ancient North Arabian, are the Hasaean inscriptions of eastern Saudi Arabia, from the 8th century BC, written not in the modern Arabic alphabet, nor in its Nabataean ancestor, but in variants of the epigraphic South Arabian "musnad". These are followed by 6th-century BC Lihyanite texts from southeastern Saudi Arabia and the Thamudic texts found throughout Arabia and the Sinai, and not actually connected with Thamud. Later come the Safaitic inscriptions beginning in the 1st century BC, and the many Arabic personal names attested in Nabataean inscriptions (which are, however, written in Aramaic). From about the 2nd century BC, a few inscriptions from Qaryat al-Faw (near Sulayyil) reveal a dialect which is no longer considered "Proto-Arabic", but Pre-Classical Arabic. By the fourth century AD, the Arab kingdoms of the Lakhmids in southern Iraq, the Ghassanids in southern Syria the Kindite Kingdom emerged in Central Arabia. Their courts were responsible for some notable examples of pre-Islamic Arabic poetry, and for some of the few surviving pre-Islamic Arabic inscriptions in the Arabic alphabet.
"Colloquial Arabic" is a collective term for the spoken varieties of Arabic used throughout the Arab world, which differ radically from the literary language. The main dialectal division is between the North African dialects and those of the Middle East, followed by that between sedentary dialects and the much more conservative Bedouin dialects. Speakers of some of these dialects are unable to converse with speakers of another dialect of Arabic. In particular, while Middle Easterners can generally understand one another, they often have trouble understanding North Africans (although the converse is not true, in part due to the popularity of Middle Eastern—especially Egyptian—films and other media). One factor in the differentiation of the dialects is influence from the languages previously spoken in the areas, which have typically provided a significant number of new words, and have sometimes also influenced pronunciation or word order; however, a much more significant factor for most dialects is, as among Romance languages, retention (or change of meaning) of different classical forms. Thus Iraqi "aku", Levantine "fīh", and North African "kayən" all mean "there is", and all come from Classical Arabic forms ("yakūn", "fīhi", "kā'in" respectively), but now sound very different.
See Arabic alphabet for explanations on the IPA phonetic symbols found in this chart. Arabic has consonants traditionally termed "emphatic", which exhibit simultaneous pharyngealization as well as varying degrees of velarization. This simultaneous articulation is described as "Retracted Tongue Root" by phonologists. In some transcription systems, emphasis is shown by capitalizing the letter, for example, is written ‹D›; in others the letter is underlined or has a dot below it, for example. Vowels and consonants can be phonologically short or long. Long (geminate) consonants are normally written doubled in Latin transcription (i.e. bb, dd, etc.), reflecting the presence of the Arabic diacritic mark shaddah, which indicates doubled consonants. In actual pronunciation, doubled consonants are held twice as long as short consonants. This consonant lengthening is phonemically contrastive: "qabala" "he accepted" vs. "qabbala" "he kissed."
Arabic has two kinds of syllables: open syllables (CV) and (CVV)—and closed syllables (CVC), (CVVC), and (CVCC), the latter two, which are (CVVC) and (CVCC) occurring only at the end of the sentence. Every syllable begins with a consonant. Syllables cannot begin with a vowel. Arabic phonology recognizes the glottal stop as an independent consonant, so in cases where a word begins with a vowel sound, as the definite article "al", for example, the word is recognized in Arabic as beginning with the consonant (glottal stop). When a word ends in a vowel and the following word begins with a glottal stop, then the glottal stop and the initial vowel of the word are in some cases elided, and the following consonant closes the final syllable of the preceding word, for example, "baytu al-mudi:r" "house (of) the director," which becomes.
For example: "ki-TAA-bun" "book", "KAA-ti-bun" "writer", "MAK-ta-bun" "desk", "ma-KAA-ti-bu" "desks", "mak-TA-ba-tun" "library", "KA-ta-buu" (Modern Standard Arabic) "they wrote" = "KA-ta-bu" (dialect), "ka-ta-BUU-hu" (Modern Standard Arabic) "they wrote it" = "ka-ta-BUU" (dialect), "ka-TA-ba-taa" (Modern Standard Arabic) "they (dual, fem) wrote", "ka-TAB-tu" (Modern Standard Arabic) "I wrote" = "ka-TABT" (dialect). Doubled consonants count as two consonants: "ma-JAL-la" "magazine", "ma-HALL" "place". Some dialects have different stress rules. In the Cairo (Egyptian Arabic) dialect, for example, a heavy syllable may not carry stress more than two syllables from the end of a word, hence "mad-RA-sa" "school", "qaa-HI-ra" "Cairo". In the Arabic of Sana, stress is often retracted: "BAY-tayn" "two houses", "MAA-sat-hum" "their table", "ma-KAA-tiib" "desks", "ZAA-rat-hiin" "sometimes", "mad-RA-sat-hum" "their school". (In this dialect, only syllables with long vowels or diphthongs are considered heavy; in a two-syllable word, the final syllable can be stressed only if the preceding syllable is light; and in longer words, the final syllable cannot be stressed.)
In some dialects, there may be more or fewer phonemes than those listed in the chart above. For example, non-Arabic is used in the Maghrebi dialects as well in the written language mostly for foreign names. Semitic became extremely early on in Arabic before it was written down; a few modern Arabic dialects, such as Iraqi (influenced by Persian and Turkish) distinguish between and. Interdental fricatives (and) are rendered as stops and in some dialects (such as Egyptian, Levantine, and much of the Maghreb); some of these dialects render them as and in "learned" words from the Standard language. Early in the expansion of Arabic, the separate emphatic phonemes and coallesced into a single phoneme, becoming one or the other. "Predictably, dialects without interdental fricatives use exclusively, while dialects with such fricatives use." Again, in "learned" words from the Standard language, is rendered as (in Egypt & the Levant) or (in North Africa) in dialects without interdental fricatives.
Compared with other Semitic language systems, Classical Arabic is distinguished by, "its almost (too perfect) algebraic-looking grammar, i.e. root pattern and morphology." Nouns in Literary Arabic have three grammatical cases (nominative, accusative, and genitive [also used when the noun is governed by a preposition]); three numbers (singular, dual and plural); two genders (masculine and feminine); and three "states" (indefinite, definite, and construct). The cases of singular nouns (other than those that end in long ā) are indicated by suffixed short vowels (/-u/ for nominative, /-a/ for accusative, /-i/ for genitive). The feminine singular is often marked by /-at/, which is reduced to /-ah/ or /-a/ before a pause. Plural is indicated either through endings (the sound plural) or internal modification (the broken plural). Definite nouns include all proper nouns, all nouns in "construct state" and all nouns which are prefixed by the definite article /al-/. Indefinite singular nouns (other than those that end in long ā) add a final /-n/ to the case-marking vowels, giving /-un/, /-an/ or /-in/ (which is also referred to as nunation or tanwīn). Verbs in Literary Arabic are marked for person (first, second, or third), gender, and number. They are conjugated in two major paradigms (termed perfective and imperfective, or past and non-past); two voices (active and passive); and five moods in the imperfective (indicative, imperative, subjunctive, jussive and energetic). There are also two participles (active and passive) and a verbal noun, but no infinitive. As indicated by the differing terms for the two tense systems, there is some disagreement over whether the distinction between the two systems should be most accurately characterized as tense, aspect or a combination of the two. The perfective aspect is constructed using fused suffixes that combine person, number and gender in a single morpheme, while the imperfective aspect is constructed using a combination of prefixes (primarily encoding person) and suffixes (primarily encoding gender and number). The moods other than imperative are primarily marked by suffixes (/u/ for indicative, /a/ for subjunctive, no ending for jussive, /an/ for energetic). The imperative has the endings of the jussive but lacks any prefixes. The passive is marked through internal vowel changes. Plural forms for the verb are only used when the subject is not mentioned, or precedes it, and the feminine singular is used for all non-human plurals. Adjectives in Literary Arabic are marked for case, number, gender and state, as for nouns. However, the plural of all non-human nouns is always combined with a singular feminine adjective, which takes the /-ah/ or /-at/ suffix. Pronouns in Literary Arabic are marked for person, number and gender. There are two varieties, independent pronouns and enclitics. Enclitic pronouns are attached to the end of a verb, noun or preposition and indicate verbal and prepositional objects or possession of nouns. The first-person singular pronoun has a different enclitic form used for verbs (/-ni/) and for nouns or prepositions (/-ī/ after consonants, /-ya/ after vowels). Nouns, verbs, pronouns and adjectives agree with each other in all respects. However, non-human plural nouns are grammatically considered to be feminine singular. Furthermore, a verb in a verb-initial sentence is marked as singular regardless of its semantic number when the subject of the verb is explicitly mentioned as a noun. Numerals between three and ten show "chiasmic" agreement, in that grammatically masculine numerals have feminine marking and vice versa. The spoken dialects have lost the case distinctions and make only limited use of the dual (it occurs only on nouns and its use is no longer required in all circumstances). They have lost the mood distinctions other than imperative, but many have since gained new moods through the use of prefixes (most often /bi-/ for indicative vs. unmarked subjunctive). They have also mostly lost the indefinite "nunation" and the internal passive. Modern Standard Arabic maintains the grammatical distinctions of Literary Arabic except that the energetic mood is almost never used; in addition, Modern Standard Arabic sometimes drop the final short vowels that indicate case and mood. As in many other Semitic languages, Arabic verb formation is based on a (usually) triconsonantal root, which is not a word in itself but contains the semantic core. The consonants, for example, indicate "write", indicate "read", indicate "eat", etc. Words are formed by supplying the root with a vowel structure and with affixes. (Traditionally, Arabic grammarians have used the root, "do", as a template to discuss word formation.) From any particular root, up to fifteen different verbs can be formed, each with its own template; these are referred to by Western scholars as "form I", "form II", and so on through "form XV". These forms, and their associated participles and verbal nouns, are the primary means of forming vocabulary in Arabic. Forms XI to XV are incidental.
The Arabic alphabet derives from the Aramaic script through Nabatean, to which it bears a loose resemblance like that of Coptic or Cyrillic script to Greek script. Traditionally, there were several differences between the Western (North African) and Middle Eastern version of the alphabet—in particular, the "fa" and "qaf" had a dot underneath and a single dot above respectively in the Maghreb, and the order of the letters was slightly different (at least when they were used as numerals). However, the old Maghrebi variant has been abandoned except for calligraphic purposes in the Maghreb itself, and remains in use mainly in the Quranic schools (zaouias) of West Africa. Arabic, like all other Semitic languages (except for the Latin-written Maltese, and the languages with the Ge'ez script), is written from right to left. There are several styles of script, notably Naskh which is used in print and by computers, and Ruq'ah which is commonly used in handwriting.
After Khalil ibn Ahmad al Farahidi finally fixed the Arabic script around 786, many styles were developed, both for the writing down of the Qur'an and other books, and for inscriptions on monuments as decoration. Arabic calligraphy has not fallen out of use as calligraphy has in the Western world, and is still considered by Arabs as a major art form; calligraphers are held in great esteem. Being cursive by nature, unlike the Latin alphabet, Arabic script is used to write down a verse of the Qur'an, a Hadith, or simply a proverb, in a spectacular composition. The composition is often abstract, but sometimes the writing is shaped into an actual form such as that of an animal. One of the current masters of the genre is Hassan Massoudy
There are a number of different standards of Arabic transliteration: methods of accurately and efficiently representing Arabic with the Latin alphabet. There are multiple conflicting motivations for transliteration. Scholarly systems are intended to accurately and unambiguously represent the phonemes of Arabic, generally making the phonetics more explicit than the original word in the Arabic alphabet. These systems are heavily reliant on diacritical marks such as "š" for the sound equivalently written "sh" in English. In some cases, the "sh" or "kh" sounds can be represented by italicizing or underlining them that way, they can be distinguished from separate "s" and "h" sounds or "k" and "h" sounds, respectively. (Compare "gashouse" to "gash".) At first sight, this may be difficult to recognize. Less scientific systems often use digraphs (like "sh" and "kh"), which are usually more simple to read, but sacrifice the definiteness of the scientific systems. Such systems may be intended to help readers who are neither Arabic speakers nor linguists to intuitively pronounce Arabic names and phrases. An example of such a system is the Bahá'í orthography. A third type of transliteration seeks to represent an equivalent of the Arabic spelling with Latin letters, for use by Arabic speakers when Arabic writing is not available (for example, when using an ASCII communication device). An example is the system used by the US military, Standard Arabic Technical Transliteration System or SATTS, which represents each Arabic letter with a unique symbol in the ASCII range to provide a one-to-one mapping from Arabic to ASCII and back. This system, while facilitating typing on English keyboards, presents its own ambiguities and disadvantages. During the last few decades and especially since the 1990s, Western-invented text communication technologies have become prevalent in the Arab world, such as personal computers, the World Wide Web, email, Bulletin board systems, IRC, instant messaging and mobile phone text messaging. Most of these technologies originally had the ability to communicate using the Latin alphabet only, and some of them still do not have the Arabic alphabet as an optional feature. As a result, Arabic speaking users communicated in these technologies by transliterating the Arabic text using the Latin script, sometimes known as IM Arabic. To handle those Arabic letters that cannot be accurately represented using the Latin script, numerals and other characters were appropriated. For example, the numeral "3" may be used to represent the Arabic letter "ع", "ayn". There is no universal name for this type of transliteration, but some have named it Arabic Chat Alphabet. Other systems of transliteration exist, such as using dots or capitalization to represent the "emphatic" counterparts of certain consonants. For instance, using capitalization, the letter "د", or "daal", may be represented by d. Its emphatic counterpart, "ض", may be written as D.
In most of present-day North Africa, the Western Arabic numerals (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) are used. However in Egypt and Arabic-speaking countries to the east of it, the Eastern Arabic numerals () are in use. When representing a number in Arabic, the lowest-valued position is placed on the right, so the order of positions is the same as in left-to-right scripts. Sequences of digits such as telephone numbers are read from left to right, but numbers are spoken in the traditional Arabic fashion, with units and tens reversed from the modern English usage. For example, 24 is said "four and twenty", and 1975 is said "one thousand and nine hundred and five and seventy."
Because the Quran is written in Arabic and all Islamic terms are in Arabic, millions of Muslims (both Arab and non-Arab) study the language. Arabic has been taught worldwide in many elementary and secondary schools, especially Muslim schools. Universities around the world have classes that teach Arabic as part of their foreign languages, Middle Eastern studies, and religious studies courses. Arabic language schools exist to assist students in learning Arabic outside of the academic world. Many Arabic language schools are located in the Arab world and other Muslim countries. Software and books with tapes are also important part of Arabic learning, as many of Arabic learners may live in places where there are no academic or Arabic language school classes available. Radio series of Arabic language classes are also provided from some radio stations. A number of websites on the Internet provide online classes for all levels as a means of distance education.
Sir Alfred Joseph Hitchcock, KBE (13 August 1899 – 29 April 1980) was an English filmmaker and producer who pioneered many techniques in the suspense and psychological thriller genres. After a successful career in his native United Kingdom in both silent films and early talkies, Hitchcock moved to Hollywood. In 1956 he became an American citizen while retaining his British citizenship. Hitchcock directed more than fifty feature films in a career spanning six decades. Often regarded as the greatest British filmmaker, he came first in a 2007 poll of film critics in Britain's "Daily Telegraph" newspaper, which said: "Unquestionably the greatest filmmaker to emerge from these islands, Hitchcock did more than any director to shape modern cinema, which would be utterly different without him. His flair was for narrative, cruelly withholding crucial information (from his characters and from us) and engaging the emotions of the audience like no one else."
Hitchcock was born on 13 August 1899 in Leytonstone, London, the second son and youngest of three children of William Hitchcock (1862–1914), a greengrocer and poulterer, and Emma Jane Hitchcock ("née" Whelan; 1863–1942). He was named after his father's brother, Alfred. His family was mostly Roman Catholic, with his mother and paternal grandmother being of Irish extraction. Hitchcock was sent to the Jesuit Classic school St Ignatius' College in Stamford Hill, London. He often described his childhood as being very lonely and sheltered, a situation compounded by his obesity. Hitchcock said he was sent by his father on numerous occasions to the local police station with a note asking the officer to lock him away for ten minutes as punishment for behaving badly. This idea of being harshly treated or wrongfully accused is frequently reflected in Hitchcock's films. Hitchcock's mother would often make him address her while standing at the foot of her bed, especially if he behaved badly, forcing him to stand there for hours. These experiences would later be used for the portrayal of the character of Norman Bates in his movie "Psycho". Hitchcock's father died when he was 14. In the same year, Hitchcock left St. Ignatius to study at the London County Council School of Engineering and Navigation in Poplar, London. After graduating, he became a draftsman and advertising designer with a cable company. During this period, Hitchcock became intrigued by photography and started working in film production in London, working as a title-card designer for the London branch of what would become Paramount Pictures. In 1920, he received a full-time position at Islington Studios with its American owner, Famous Players-Lasky and their British successor, Gainsborough Pictures, designing the titles for silent movies. His rise from title designer to film director took five years.
Hitchcock's last collaboration with Graham Cutts led him to Germany in 1924. The film "Die Prinzessin und der Geiger" (UK title "The Blackguard", 1925), directed by Cutts and co-written by Hitchcock, was produced in the Babelsberg Studios in Potsdam near Berlin. Hitchcock also worked as an art-director on the set of F. W. Murnau's film "Der letzte Mann" (1924). He was very impressed with Murnau's work and later used many techniques for the set design in his own productions. In his book-length interview with François Truffaut, "Hitchcock/Truffaut" (Simon and Schuster, 1967), Hitchcock also said he was influenced by Fritz Lang's film "Destiny" (1921). Hitchcock's first few films faced a string of bad luck. His first directing project came in 1922 with the aptly-titled "Number 13". However, the production was canceled due to financial problems and the few scenes that were finished at that point were apparently lost. In 1925, Michael Balcon of Gainsborough Pictures gave Hitchcock another opportunity for a directing credit with "The Pleasure Garden" made at UFA Studios in Germany. Unfortunately, The film was a commercial flop. Next, Hitchcock directed a drama called "The Mountain Eagle" (released under the title "Fear o' God" in the United States). This film was also eventually lost. In 1926, Hitchcock's luck changed with his first thriller, '. The film, released in January 1927, was a major commercial and critical success in the United Kingdom. As with many of his earlier works, this film was influenced by Expressionist techniques Hitchcock had witnessed first-hand in Germany. Some commentators regard this piece as the first truly "Hitchcockian" film, incorporating such themes as the "wrong man". Following the success of "The Lodger", Hitchcock hired a publicist to help enhance his growing reputation. On 2 December 1926, Hitchcock married his assistant director, Alma Reville at the Brompton Oratory. Their only child, a daughter Patricia, was born on 7 July 1928. Alma was to become Hitchcock's closest collaborator. She wrote some of his screenplays and (though often uncredited) worked with him on every one of his films. In 1929, Hitchcock began work on his tenth film "Blackmail". While the film was still in production, the studio, British International Pictures (BIP), decided to make it one of the UK's first sound pictures. With the climax of the film taking place on the dome of the British Museum, "Blackmail" began the Hitchcock tradition of using famous landmarks as a backdrop for suspense sequences. In the PBS series "The Men Who Made The Movies", Hitchcock had explained how he used early sound recording as a special element of the film, emphasizing the word "knife" in a conversation with the woman suspected of murder. During this period, Hitchcock directed segments for a BIP musical film revue "Elstree Calling" (1930) and directed a short film featuring two "Film Weekly" scholarship winners, "An Elastic Affair" (1930). Another BIP musical revue, "Harmony Heaven" (1929), reportedly had minor input from Hitchcock, but his name does not appear in the credits. In 1933, Hitchcock was once again working for Michael Balcon at Gaumont-British Picture Corporation. His first film for the company, "The Man Who Knew Too Much" (1934), was a success and his second, "The 39 Steps" (1935), is often considered one of the best films from his early period. This film was also one of the first to introduce the concept of the "MacGuffin", a plot device around which a whole story seems to revolve, but ultimately has nothing to do with the true meaning or ending of the story. In "The 39 Steps", the Macguffin is a stolen set of design plans. (Hitchcock told French director François Truffaut: "There are two men sitting in a train going to Scotland and one man says to the other, 'Excuse me, sir, but what is that strange parcel you have on the luggage rack above you?', 'Oh', says the other, 'that's a Macguffin.', 'Well', says the first man, 'what's a Macguffin?', The other answers, 'It's an apparatus for trapping lions in the Scottish Highlands.', 'But', says the first man, 'there are no lions in the Scottish Highlands.', 'Well', says the other, 'then that's no Macguffin.'") Hitchcock's next major success was in 1938 with his film "The Lady Vanishes", a clever and fast-paced film about the search for a kindly old Englishwoman Miss Froi (Dame May Whitty), who disappears while on board a train in the fictional country of Vandrika (a thinly-veiled version of Nazi Germany). By 1938, Hitchcock had become known for his observation, "Actors are cattle". He once said that he first made this remark as early as the late 1920s, in connection to stage actors who were snobbish about motion pictures. However, Michael Redgrave said that Hitchcock had made the statement during the filming of "The Lady Vanishes". The phrase would haunt Hitchcock for years to come and would result in an incident during the filming of his 1941 production of "Mr. & Mrs. Smith" where Carole Lombard brought some heifers onto the set — with name tags of Lombard, Robert Montgomery, and Gene Raymond, the stars of the film — to surprise the director. Hitchcock said he was misquoted: "I said 'Actors should be "treated" like cattle'." At the end of the 1930s, David O. Selznick signed Hitchcock to a seven-year contract beginning in March 1939, when the Hitchcocks moved to the United States.
The suspense and the gallows humor that had become Hitchcock's trademark in film continued to appear in his productions. The working arrangements with Selznick were less than optimal. Selznick suffered from perennial money problems, and Hitchcock was often displeased with Selznick's creative control over his films. In a later interview, Hitchcock summarised the working relationship thus:\ Selznick loaned Hitchcock to the larger studios more often than producing Hitchcock's films himself. In addition, Selznick, as well as fellow independent producer Samuel Goldwyn, made only a few films each year, so Selznick did not always have projects for Hitchcock to direct. Goldwyn had also negotiated with Hitchcock on a possible contract, only to be outbid by Selznick. Hitchcock was quickly impressed with the superior resources of the American studios compared to the financial restrictions he had frequently encountered in England. Hitchcock's fondness for his homeland resulted in numerous American films set in, or filmed in, the United Kingdom, including his penultimate film, "Frenzy". With the prestigious Selznick picture "Rebecca" in 1940, Hitchcock made his first American movie, set in England and based on a novel by English author Daphne du Maurier. The film starred Laurence Olivier and Joan Fontaine. This Gothic melodrama explores the fears of a naive young bride who enters a great English country home and must adapt to the extreme formality and coldness she finds there. The film won the Academy Award for Best Picture of 1940. The statuette was given to Selznick, as the film's producer. The film did not win the Best Director award for Hitchcock. There were additional problems between Selznick and Hitchcock. Selznick was known to impose very restrictive rules upon Hitchcock who was forced to shoot the film as Selznick wanted. At the same time, Selznick complained about Hitchcock's "goddamn jigsaw cutting", which meant that the producer did not have nearly the leeway to create his own film as he liked, but had to follow Hitchcock's vision of the finished product. The film was the fourth longest of Hitchcock's films, at 130 minutes, exceeded only by "The Paradine Case" (132 minutes), "North by Northwest" (136 minutes), and "Topaz" (142 minutes). Hitchcock's second American film, the European-set thriller "Foreign Correspondent" (1940), based on Vincent Sheean's "Personal History" and produced by Walter Wanger, was nominated for Best Picture that year. The movie was filmed in the first year of World War II and was apparently inspired by the rapidly-changing events in Europe, as fictionally covered by an American newspaper reporter portrayed by Joel McCrea. The film mixed actual footage of European scenes and scenes filmed on a Hollywood back lot. In compliance with Hollywood's Production Code censorship, the film avoided direct references to Germany and Germans.
Hitchcock's films during the 1940s were diverse, ranging from the romantic comedy "Mr. & Mrs. Smith" (1941) to the courtroom drama "The Paradine Case" (1947), to the dark and disturbing film noir "Shadow of a Doubt" (1943). In September 1940, the Hitchcocks purchased the Cornwall Ranch, located near Scotts Valley in the Santa Cruz Mountains in northern California. The Ranch became the primary residence of the Hitchcocks for the rest of their lives, although they kept their Bel Air home. "Suspicion" (1941) marked Hitchcock's first film as a producer as well as director. Hitchcock used the north coast of Santa Cruz, California for the English coastline sequence. This film was to be actor Cary Grant's first time working with Hitchcock, and it was one of the few times that Grant would be cast in a sinister role. Joan Fontaine won Best Actress Oscar and the New York Film Critics Circle Award for her "outstanding performance in "Suspicion". "Grant plays an irresponsible husband whose actions raise suspicion and anxiety by his wife (Fontaine)". In what critics regard as a classic scene, Hitchcock uses a light bulb to illuminate what might be a fatal glass of milk that Grant is bringing to his wife. In the book upon which the movie is based ("Before the Fact" by Francis Iles), the Grant character is a killer, but Hitchcock and the studio felt Grant's image would be tarnished by that ending. Though a homicide would have suited him better, as he stated to François Truffaut, Hitchcock settled for an ambiguous finale. "Saboteur" (1942) was the first of two films that Hitchcock made for Universal, a studio where he would continue his career during his later years. Hitchcock was forced to use Universal contract players Robert Cummings and Priscilla Lane, both known for their work in comedies and light dramas. Breaking with Hollywood conventions of the time, Hitchcock did extensive location filming, especially in New York City, and depicted a confrontation between a suspected saboteur (Cummings) and a real saboteur (Norman Lloyd) atop the Statue of Liberty. "Shadow of a Doubt" (1943), Hitchcock's personal favourite of all his films and the second of the early Universal films, was about young Charlotte "Charlie" Newton (Teresa Wright), who suspects her beloved uncle Charlie Oakley (Joseph Cotten) of being a serial murderer. Critics have said that in its use of overlapping characters, dialogue, and closeups it has provided a generation of film theorists with psychoanalytic potential, including Jacques Lacan and Slavoj Žižek. Hitchcock again filmed extensively on location, this time in the Northern California city of Santa Rosa, California, during the summer of 1942. The director showcased his own personal fascination with crime and criminals when he had two of his characters discuss various ways of killing people, to the obvious annoyance of Charlotte. Working at 20th Century Fox, Hitchcock adapted a script of John Steinbeck's that chronicled the experiences of the survivors of a German U-boat attack in the film "Lifeboat" (1944). The action sequences were shot on the small boat. The locale also posed problems for Hitchcock's traditional cameo appearance. That was solved by having Hitchcock's image appear in a newspaper that William Bendix is reading in the boat, showing the director in a before-and-after advertisement for "Reduco-Obesity Slayer". While at Fox, Hitchcock seriously considered directing the film version of A.J. Cronin's novel about a Catholic priest in China, "The Keys of the Kingdom", but the plans for this fell through. John M. Stahl ended up directing the 1944 film, which was produced by Joseph L. Mankiewicz and starred Gregory Peck, among other luminaries. Returning to England for an extended visit in late 1943 and early 1944, Hitchcock made two short films for the Ministry of Information, "Bon Voyage" and "Aventure Malgache". These - made for the Free French - were the only films Hitchcock made in the French language, and "feature typical Hitchcockian touches". In the 1990s, the two films were shown by Turner Classic Movies and released on home video. In 1945, Hitchcock served as "treatment advisor" (in effect, a film editor) for a Holocaust documentary produced by the British Army. The film, which recorded the liberation of Nazi Concentration Camps, remained unreleased until 1985, when it was completed by PBS Frontline and distributed under the title "Memory of the Camps". Hitchcock worked for Selznick again when he directed "Spellbound", which explored the then-fashionable subject of psychoanalysis and featured a dream sequence designed by Salvador Dalí. Gregory Peck is amnesiac Dr. Anthony Edwardes under the treatment of analyst Dr. Peterson (Ingrid Bergman), who falls in love with him while trying to unlock his repressed past. The dream sequence as it actually appears in the film is considerably shorter than was originally envisioned, which was to be several minutes long, because it proved to be too disturbing for the audience. Some of the original musical score by Miklós Rózsa (which makes use of the theremin) was later adapted by the composer into a concert piano concerto. "Notorious" (1946) followed "Spellbound". According to Hitchcock, in his book-length interview with François Truffaut, Selznick sold the director, the two stars (Grant and Bergman) and the screenplay (by Ben Hecht) to RKO Radio Pictures as a "package" for $500,000 due to cost overruns on Selznick's "Duel in the Sun" (1946). "Notorious" starred Hitchcock regulars Ingrid Bergman and Cary Grant, and features a plot about Nazis, uranium, and South America. It was a huge box office success and has remained one of Hitchcock's most acclaimed films. His use of uranium as a plot device led to Hitchcock's being briefly under FBI surveillance. McGilligan writes that Hitchcock consulted Dr. Robert Millikan of Caltech about the development of an atomic bomb. Selznick complained that the notion was "science fiction" only to be confronted by the news stories of the detonation of two atomic bombs on Hiroshima and Nagasaki in Japan in August 1945. After completing his final film for Selznick, "The Paradine Case" (a courtroom drama that critics found lost momentum because it apparently ran too long and exhausted its resource of ideas), Hitchcock filmed his first color film, "Rope", which appeared in 1948. Here Hitchcock experimented with marshalling suspense in a confined environment, as he had done earlier with "Lifeboat" (1943). He also experimented with exceptionally long takes — up to ten minutes long. Featuring James Stewart in the leading role, "Rope" was the first of four films Stewart would make for Hitchcock. It was based on the Leopold and Loeb case of the 1920s. Somehow Hitchcock's cameraman managed to move the bulky, heavy Technicolor camera quickly around the set as it followed the continuous action of the long takes. "Under Capricorn" (1949), set in nineteenth-century Australia, also used the short-lived technique of long takes, but to a more limited extent. He again used Technicolor in this production, then returned to black and white films for several years. For "Rope" and "Under Capricorn". Hitchcock formed a production company with Sidney Bernstein, called Transatlantic Pictures, which became inactive after these two unsuccessful pictures. Hitchcock continued to produce his own films for the rest of his life.
In 1950, Hitchcock filmed "Stage Fright" on location in the UK. For the first time, Hitchcock matched one of Warner Brothers' biggest stars, Jane Wyman, with the sultry German actress Marlene Dietrich. Hitchcock used a number of prominent British actors, including Michael Wilding, Richard Todd, and Alastair Sim. This was Hitchcock's first production for Warner Brothers, which had distributed "Rope" and "Under Capricorn", because Transatlantic Pictures was experiencing financial difficulties. With the film "Strangers on a Train" (1951), based on the novel by Patricia Highsmith, Hitchcock combined many elements from his preceding films. Hitchcock approached Dashiell Hammett to write the dialogue but Raymond Chandler took over, then left over disagreements with the director. Two men casually meet and speculate on removing people who are causing them difficulty. One of the men takes this banter entirely seriously. With Farley Granger reprising some elements of his role from "Rope", "Strangers" continued the director's interest in the narrative possibilities of blackmail and murder". Robert Walker, previously known for "boy-next-door" roles, plays the villain. MCA head Lew Wasserman, whose client list included James Stewart, Janet Leigh and other actors who would appear in Hitchcock's films, had a significant impact in packaging and marketing Hitchcock's films beginning in the 1950s. Three very popular films starring Grace Kelly followed. "Dial M for Murder" (1954) was adapted from the popular stage play by Frederick Knott. Ray Milland plays the "suave and scheming" villain, an ex-tennis pro, who tries to murder his innocent wife Grace Kelly for her money. When the murder goes awry and the assassin is killed by her in self-defense, he manipulates the evidence to pin the murder of the assassin on his wife. Her lover Mark Halliday (Robert Cummings) and police inspector Hubbard (John Williams) work urgently to save her from execution. Hitchcock experimented with 3D cinematography, although the film was not released in this format at first. However, it was shown in 3D in the early 1980s. The film marked a return to Technicolor productions for Hitchcock. Hitchcock moved to Paramount Pictures and filmed "Rear Window" (1954), starring James Stewart and Kelly again, as well as Thelma Ritter and Raymond Burr. Here, the wheelchair-using Stewart, a photographer based on Robert Capa, seems obsessed with observing his neighbours across the courtyard, and becomes convinced one of them (Raymond Burr) has murdered his wife. Stewart tries to sway both his glamorous model-girlfriend (Kelly) and his policeman buddy (Wendell Corey) to his theory, and eventually succeeds.. Like "Lifeboat" and "Rope", the movie was photographed almost entirely within the confines of a small space: Stewart's tiny studio apartment overlooking the massive courtyard set. Hitchcock uses closeups of Stewart's face to show his character's reactions to all he sees, "from the comic voyeurism directed at his neighbors to his helpless terror watching Kelly and Burr in the villain's apartment". The third Kelly film "To Catch a Thief" (1955), set in the French Riviera, stars Kelly with Cary Grant again and John Williams. Grant plays retired thief John Robie who becomes the prime suspect for a spate of robberies in the Riviera. An American heiress played by Kelly surmises his true identity, attempts to seduce him with her own jewels, and even offers to assist him in his alleged life of crime. "Despite the obvious age disparity between Grant and Kelly and a lightweight plot, the witty script (loaded with double-entendres) and the good-natured acting proved a commercial success." It was Hitchcock's last film with Kelly because she married Prince Rainier of Monaco in 1956 and the residents of her new homeland refused to allow her to make any more films. The successful remake of Hitchcock's own 1934 film, "The Man Who Knew Too Much", in 1956 followed, this time starring Stewart and Doris Day, who sang the theme song, "Whatever Will Be, Will Be (Que Sera, Sera)" (which won the Oscar for "Best Music", and became a big hit for Day). Stewart and Day, distraught over the kidnapping of their son, struggle with both their emotions and their urgent quest to find their child and stop an assassination, until the song helps re-unite the family. "The Wrong Man" (1957), Hitchcock's final film for Warner Brothers, was a low-key black and white production based on a real-life case of mistaken identity reported in Life Magazine in 1953. This was the only film of Hitchcock's to star Henry Fonda. Fonda plays a Stork Club musician mistaken for a liquor store thief who is arrested and tried for robbery while his wife (newcomer Vera Miles) emotionally collapses under the strain. Hitchcock told Truffaut that his lifelong fear of the police attracted him to the subject and was embedded in many scenes. "Vertigo" (1958) again starred Stewart, this time with Kim Novak and Barbara Bel Geddes. Stewart plays "Scottie", a former police investigator suffering from acrophobia, who develops an obsession with a woman he is shadowing (Kim Novak). Scottie's obsession leads to tragedy, and this time Hitchcock does not opt for a happy ending. Though the film is widely considered a classic today, "Vertigo" met with negative reviews and poor box office receipts upon its release, and marked the last collaboration between Stewart and Hitchcock. The film is now placed highly in the "Sight & Sound" decade polls. It was premiered in the San Sebastián International Film Festival, where Hitchcock won a Silver Seashell. Late 1950s, 1960s and 1970s. By this time, Hitchcock had filmed in many areas of the United States. He followed "Vertigo" with three more successful films. All are also recognized as among his very best films: "North by Northwest" (1959), "Psycho" (1960) and "The Birds" (1963). In "North by Northwest", Cary Grant is Roger Thornhill, a Madison Avenue ad executive who is mistaken for a government agent. He is hotly pursued by enemy agents across America who try to kill him, one of whom is foreign agent Eve Kendall (Eva Marie Saint), who is really an American agent. She seduces Thornhill, sets him up, but then falls in love with him and aids his escape. "Psycho" is considered by some to be Hitchcock's most famous film. Produced on a highly constrained budget of $800,000, it was shot in black-and-white on a spare set. The unprecedented violence of the shower scene, the early demise of the heroine, the innocent lives extinguished by a disturbed murderer were all hallmarks of Hitchcock, copied in many subsequent horror films. After completing "Psycho", Hitchcock moved to Universal, where he made the remainder of his films. "The Birds", inspired by a Daphne Du Maurier short story and by an actual news story about a mysterious infestation of birds in California, was Hitchcock's 49th film. He signed up Tippi Hedren as his latest blonde heroine opposite Rod Taylor. The scenes of the birds attacking included hundreds of shots mixing actual and animated sequences. The cause of the birds' attack is left unanswered, "perhaps highlighting the mystery of forces unknown". The latter two films were particularly notable for their unconventional soundtracks, both orchestrated by Bernard Herrmann: the screeching strings played in the murder scene in "Psycho" exceeded the limits of the time, and "The Birds" dispensed completely with conventional instruments, instead using an electronically-produced soundtrack and an unaccompanied song by school children (just prior to the infamous attack at the historic Bodega Bay School). Also notable was that Santa Cruz was mentioned again as the place where the bird-phenomenon was said to have first occurred. These films are considered his last great films, after which it is said his career started to lose pace (although some critics such as Robin Wood and Donald Spoto contend that "Marnie", from 1964, is first-class Hitchcock, and some have argued that "Frenzy" is unfairly overlooked). Failing health took its toll on Hitchcock, reducing his output during the last two decades of his career. Hitchcock filmed two spy thrillers. The first, "Torn Curtain" (1966), with Paul Newman and Julie Andrews, was a Cold War thriller. "Torn Curtain" displays the bitter end of the twelve-year collaboration between Hitchcock and composer Bernard Herrmann. Herrmann was fired when Hitchcock was unsatisfied with his score, so John Addison was hired in Herrmann's place. In 1969, "Topaz", another Cold War-themed film (based on a Leon Uris novel), was released. Both received mixed reviews from critics. In 1972, Hitchcock returned to London to film "Frenzy", his last major triumph. After two only moderately successful espionage films, the plot marks a return to the murder thriller genre that he made so many films out of earlier in his career. The basic story recycles his early film "The Lodger". Richard Blaney (Jon Finch), volatile barkeeper with a history of explosive anger, becomes the likely perpetrator of the "Necktie Murders", which are actually committed by his friend Bob Rusk (Barry Foster), a fruit seller. This time Hitchcock makes the victim and villain twins, rather than opposites, as in "Strangers on a Train". Only one of them, however, has crossed the line to murder. For the first time, Hitchcock allowed nudity and profane language, which had before been taboo, in one of his films. He also shows rare sympathy for the Chief Inspector and his comic domestic life. Biographers have noted that Hitchcock had always pushed the limits of film censorship, often managing to fool Joseph Breen, the longtime head of Hollywood's Production Code. Many times Hitchcock slipped in subtle hints of improprieties forbidden by censorship until the mid-1960s. Yet Patrick McGilligan wrote that Breen and others often realized that Hitchcock was inserting such things and were actually amused as well as alarmed by Hitchcock's "inescapable inferences". Beginning with "Torn Curtain", Hitchcock was finally able to blatantly include plot elements previously forbidden in American films and this continued for the remainder of his film career. "Family Plot" (1976) was Hitchcock's last film. It related the escapades of "Madam" Blanche Tyler played by Barbara Harris, a fraudulent spiritualist, and her taxi driver lover Bruce Dern making a living from her phony powers. William Devane, Karen Black and Cathleen Nesbitt co-starred. It was the only Hitchcock film scored by John Williams. Last film work and death. Near the end of his life, Hitchcock had worked on the script for a projected spy thriller, "The Short Night", collaborating with screenwriters James Costigan and Ernest Lehman. Despite some preliminary work, the story was never filmed. This was due, primarily, to Hitchcock's own failing health and his concerns over the health of his wife, Alma, who had suffered a stroke. The script was eventually published posthumously, in a book on Hitchcock's last years. Hitchcock died from kidney failure in his Bel Air, Los Angeles, California home at the age of 80. His wife Alma Reville, and their daughter, Patricia Hitchcock O'Connell, both survived him. His funeral service was held at Good Shepherd Catholic Church in Beverly Hills. Hitchcock's body was cremated and his ashes were scattered over the Pacific. Themes, plot devices and motifs. Hitchcock returned several times to cinematic devices such as suspense, the audience as voyeur, and his well-known "McGuffin", an apparently minor detail serving as a pivot upon which the narrative turns.
Hitchcock seemed to delight in the technical challenges of film making. In the film "Lifeboat", Hitchcock stages the entire action of the movie in a small boat, yet manages to keep the cinematography from monotonous repetition (his trademark cameo appearance was a dilemma, given the limitations of the setting; so Hitchcock appears in a fictitious magazine for a weight loss product). Similarly, the entire action in "Rear Window" either takes place in or is seen from a single apartment. In "Spellbound", two unprecedented point-of-view shots were achieved by constructing a large wooden hand (which would appear to belong to the character whose point of view the camera took) and out-sized props for it to hold: a bucket-sized glass of milk and a large wooden gun. For added novelty and impact, the climactic gunshot was hand-colored red on some copies of the black-and-white print of the film. "Rope" (1948) was another technical challenge: a film that appears to have been shot entirely in a single take. The film was actually shot in 10 takes ranging from four and a half to 10 minutes each; a 10 minute length of film being the maximum a camera's film magazine could hold. Some transitions between reels were hidden by having a dark object fill the entire screen for a moment. Hitchcock used those points to hide the cut, and began the next take with the camera in the same place. Hitchcock's 1958 film "Vertigo" contains a camera technique developed by Irmin Roberts that has been imitated and re-used many times by filmmakers, wherein the image appears to "stretch". This is achieved by moving the camera in the opposite direction of the camera's zoom. It has become known as the Dolly zoom or "Vertigo Effect." Signature appearances in his films. Hitchcock appeared briefly in many of his own films, usually playing upon his portly figure in an incongruous manner, for example, seen struggling to get a double bass onto a train.
Hitchcock's films sometimes feature characters struggling in their relationships with their mothers. In "North by Northwest" (1959), Roger Thornhill (Cary Grant's character) is an innocent man ridiculed by his mother for insisting that shadowy, murderous men are after him. In "The Birds" (1963), the Rod Taylor character, an innocent man, finds his world under attack by vicious birds, and struggles to free himself of a clinging mother (Jessica Tandy). The killer in "Frenzy" (1972) has a loathing of women but idolizes his mother. The villain Bruno in "Strangers on a Train" hates his father, but has an incredibly close relationship with his mother (played by Marion Lorne). Sebastian (Claude Rains) in "Notorious" has a clearly conflictual relationship with his mother, who is (correctly) suspicious of his new bride Alicia Huberman (Ingrid Bergman). And, of course, Norman Bates' troubles with his mother in "Psycho" are well known. Hitchcock heroines tend to be lovely, cool blondes who seem proper at first but, when aroused by passion or danger, respond in a more sensual, animal, or even criminal way. As noted, the famous victims in "The Lodger" are all blondes. In "The 39 Steps", Hitchcock's glamorous blonde star, Madeleine Carroll, is put in handcuffs. In "Marnie" (1964), the title character (played by Tippi Hedren) is a kleptomaniac. In "To Catch a Thief" (1955), Francie (Grace Kelly) offers to help a man she believes is a burglar. In "Rear Window", Lisa (Grace Kelly again) risks her life by breaking into Lars Thorwald's apartment. The best known example is in "Psycho" where Janet Leigh's unfortunate character steals $40,000 and is murdered by a reclusive psychopath. Hitchcock's last blonde heroine was - years after Dany Robin and her "daughter" Claude Jade in "Topaz" - Barbara Harris as a phony psychic turned amateur sleuth in his final film, 1976's "Family Plot". In the same film, the diamond smuggler played by Karen Black could also fit that role, as she wears a long blonde wig in various scenes and becomes increasingly uncomfortable about her line of work. Some critics and Hitchcock scholars, including Donald Spoto and Roger Ebert, agree that "Vertigo" represents the director's most personal and revealing film, dealing with the obsessions of a man who crafts a woman into the woman he desires. "Vertigo" explores more frankly and at greater length his interest in the relation between sex and death than any other film in his filmography. Hitchcock often said that his favorite film (of his own work) was "Shadow of a Doubt".
Hitchcock's films were strongly believed to have been extensively storyboarded to the finest detail by the majority of commentators over the years. He was reported to have never even bothered looking through the viewfinder, since he didn't need to do so, though in publicity photos he was shown doing so. He also used this as an excuse to never have to change his films from his initial vision. If a studio asked him to change a film, he would claim that it was already shot in a single way, and that there were no alternate takes to consider. However, this view of Hitchcock as a director who relied more on pre-production than on the actual production itself, has been challenged by the book, "Hitchcock At Work", written by Bill Krohn, the American correspondent of "Cahiers du cinéma". Krohn after investigating several script revisions, notes to other production personnel written by or to Hitchcock alongside inspection of storyboards and other production material has observed that Hitchcock's work often deviated from how the screenplay was written or how the film was originally envisioned. He noted that the myth of storyboards in relation to Hitchcock, often regurgitated by generations of commentators on his movies was to a great degree perpetuated by Hitchcock himself or the publicity arm of the studios. A great example would be the celebrated crop spraying sequence of "North by Northwest" which was not storyboarded at all. After the scene was filmed, the publicity department asked Hitchcock to make storyboards to promote the film and Hitchcock in turn hired an artist to match the scenes in detail. Even on the occasions when storyboards were made, the scene which was shot did differ from it significantly. Krohn's extensive analysis of the production of Hitchcock classics like "Notorious" reveals that Hitchcock was flexible enough to change a film's conception during its production. Another example Krohn notes is the American remake of "The Man Who Knew Too Much" whose shooting schedule commenceed without a finished script and moreover went over schedule, something which as Krohn notes was not an uncommon occurrence on many of Hitchcock's films including "Strangers on a Train" and "Topaz". While Hitchcock did do a great deal of preparation for all his movies, he was fully cognizant that the actual film-making process often deviated from the best laid plans and was flexible to adapt to the changes and needs of production as his films were not free from the normal hassles faced and common routines utilised during many other film productions. Krohn's work also sheds light on Hitchcock's practice of generally shooting in chronological order. A practice which he notes often sent many of his films over budget and over schedule and more importantly differed from the standard operating procedure of Hollywood in the Studio System Era. Equally important is Hitchcock's tendency of shooting alternate takes of scenes. This differed from coverage in that the films weren't necessarily shot from varying angles so as to give the editor options to shape the film how he/she chooses (often under the producer's aegis). Rather they represented Hitchcock's tendency of giving himself options in the editing room where he would provide advice to his editors after viewing a rough cut of the work so as to give him space for other possibilities in the editing room. According to Krohn, this and numerous other information revealed through his research of Hitchcock's personal papers, script revisions and the like refute the notion of Hitchcock as a director who was always in control of his films, whose vision of his films did not change during production, which Krohn notes has remained the central long-standing myth of Alfred Hitchcock. His fastidiousness and attention to detail also found its way to each film poster for his films. Hitchcock preferred to work with the best talent of his day—film poster designers such as Bill Gold and Saul Bass -- and kept them busy with countless rounds of revision until he felt that the single image of the poster accurately represented his entire film.
Similarly, much of Hitchcock's supposed dislike of actors has been exaggerated. Hitchcock simply did not tolerate the method approach, as he believed that actors should only concentrate on their performances and leave work on script and character to the directors and screenwriters. In a "Sight and Sound" interview, he stated that, 'the method actor is OK in the theatre because he has a free space to move about. But when it comes to cutting the face and what he sees and so forth, there must be some discipline'. During the making of "Lifeboat", Walter Slezak, who played the German character, stated that Hitchcock knew the mechanics of acting better than anyone he knew. Several critics have observed that despite his reputation as a man who disliked actors, several actors who worked with him gave fine, often brilliant performances and these performances contribute to the film's success. Regarding Hitchcock's sometimes less than pleasant relationship with actors, there was a persistent rumor that he had said that actors were cattle. Hitchcock later denied this, typically tongue-in-cheek, clarifying that he had only said that actors should be treated like cattle. Carole Lombard, tweaking Hitchcock and drumming up a little publicity, brought some cows along with her when she reported to the set of "Mr. and Mrs. Smith". For Hitchcock, the actors, like the props, were part of the film's setting. In the late 1950s, French New Wave critics, especially Éric Rohmer, Claude Chabrol and François Truffaut, were among the first to see and promote Hitchcock's films as artistic works. Hitchcock was one of the first directors to whom they applied their auteur theory, which stresses the artistic authority of the director in the film-making process. Hitchcock's innovations and vision have influenced a great number of filmmakers, producers, and actors. His influence helped start a trend for film directors to control artistic aspects of their movies without answering to the movie's producer.
"Rebecca", which Hitchcock directed, won the 1940 Best Picture Oscar for its producer David O. Selznick. In addition to "Rebecca" and "Suspicion", two other films Hitchcock directed, "Foreign Correspondent" and "Spellbound", were nominated for Best Picture. Hitchcock is considered the Best Film Director of all time by The Screen Directory website. Sixteen films directed by Hitchcock earned Oscar nominations, though only six of those films earned Hitchcock himself a nomination. The total number of Oscar nominations (including winners) earned by films he directed is fifty. Four of those films earned Best Picture nominations. "Spellbound" won the Academy Award for Best Original Music Score. Actor Joan Fontaine won the Academy Award for Best Actress for her performance in "Suspicion", the only Academy Award–winning performance under Hitchcock's direction. Six of Hitchcock's films are in the National Film Registry: "Vertigo", "Rear Window", "North by Northwest", "Shadow of a Doubt", "Notorious", and "Psycho"; all but "Shadow of a Doubt" and "Notorious" were also in 1998's AFI's 100 best American films and the AFI's 2007 update. In 2008, four of Hitchcock's films were named among the ten best mystery films of all time in the AFI's 10 Top 10. Those films are "Vertigo" (at No. 1); "Rear Window" (No. 3); "North by Northwest" (No. 7); and "Dial M for Murder" (No. 9). Alfred Hitchcock received the AFI Life Achievement Award in 1979. Hitchcock was made a Knight Commander of the Order of the British Empire by Queen Elizabeth II in the 1980 New Year's Honours. Although he had adopted American citizenship in 1956, he was entitled to use the title "Sir" because he had remained a British subject. Hitchcock died just four months later, on 29 April, before he could be formally invested.
Hitchcock became famous for his expert and largely unrivaled control of pace and suspense, and his films draw heavily on both fear and fantasy. The films are known for their droll humour and witticisms, and these cinematic works often portray innocent people caught up in circumstances beyond their control or understanding. Hitchcock began his directing career in the United Kingdom in 1922. From 1939 onward, he worked primarily in the United States. In September, 1940, Hitchcock had purchased a mountaintop estate for the sum of $40,000. Known as the 1870 Cornwall Ranch or 'Heart o' the Mountain', the property was perched high above Scotts Valley, California, at the end of Canham Road. The Hitchcocks resided there from 1940 to 1972. The Hitchcocks became close friends with the parents of Joan Fontaine, after she starred in his film, "Rebecca". Years later, after a break-in at his estate, Hitchcock replaced all of the accumulated paintings with studio-made copies. The family sold the estate in 1974, six years before Hitchcock's death. Hitchcock and family also purchased a second home in late 1942 at 10957 Bellagio Road in Los Angeles, just across from the Bel Air Country Club. "Rebecca" was the only Hitchcock film to win the Academy Award for Best Picture (though the award did not go to Hitchcock); four other films were nominated. In 1967 he was awarded the Irving G. Thalberg Memorial Award for lifetime achievement. He never won an Academy Award for direction of a film.
Along with Walt Disney, Hitchcock was among the first prominent motion picture producers to fully envisage just how popular the medium of television would become. From 1955 to 1965, Hitchcock was the host and producer of a television series entitled "Alfred Hitchcock Presents". While his films had made Hitchcock's name strongly associated with suspense, the TV series made Hitchcock a celebrity himself. His irony-tinged voice and signature droll delivery, gallows humor, iconic image and mannerisms became instantly recognizable and were often the subject of parody. The title-theme of the show pictured a minimalist caricature of Hitchcock's profile (he drew it himself; it is composed of only nine strokes) which his real silhouette then filled. His introductions before the stories in his program always included some sort of wry humor, such as the description of a recent multi-person execution hampered by having only one electric chair, while two are now shown with a sign "Two chairs--no waiting!" He directed a few episodes of the TV series himself, and he upset a number of movie production companies when he insisted on using his TV production crew to produce his motion picture "Psycho". In the late 1980s, a new version of "Alfred Hitchcock Presents" was produced for television, making use of Hitchcock's original introductions in a colorised form. "Alfred Hitchcock Presents" was parodied by Friz Freleng's 1961 cartoon "The Last Hungry Cat", which contains a plot similar to "Blackmail". "Hitch" used a curious little tune by the French composer Charles Gounod (1818–1893), the composer of the 1859 opera "Faust", as the theme "song" for his television programs, after it was suggested to him by composer Bernard Herrmann. Arthur Fiedler and the Boston Pops Orchestra included the piece, "Funeral March of a Marionette", in one of their extended play 45 rpm discs for RCA Victor during the 1950s. Hitchcock appears as a character in the popular juvenile detective book series, "Alfred Hitchcock and the Three Investigators". The long-running detective series was created by Robert Arthur, who wrote the first several books, although other authors took over after he left the series. The Three Investigators—Jupiter Jones, Bob Andrews and Peter Crenshaw—were amateur detectives, slightly younger than the Hardy Boys. In the introduction to each book, "Alfred Hitchcock" introduces the mystery, and he sometimes refers a case to the boys to solve. At the end of each book, the boys report to Hitchcock, and sometimes give him a memento of their case. When the real Hitchcock died, the fictional Hitchcock in the Three Investigators books was replaced by a retired detective named Hector Sebastian. At this time, the series title was changed from "Alfred Hitchcock and the Three Investigators" to "The Three Investigators". At the height of Hitchcock's success, he was also asked to introduce a set of books with his name attached. The series was a collection of short stories by popular short-story writers, primarily focused on suspense and thrillers. These titles included "Alfred Hitchcock's Anthology", "Alfred Hitchcock Presents: Stories to be Read with the Door Locked", "Alfred Hitchcock's Monster Museum", "Alfred Hitchcock's Supernatural Tales of Terror and Suspense", "Alfred Hitchcock's Spellbinders in Suspense", "Alfred Hitchcock's Witch's Brew", "Alfred Hitchcock's Ghostly Gallery", "Alfred Hitchcock's A Hangman's Dozen" and "Alfred Hitchcock's Haunted Houseful." Hitchcock himself was not actually involved in the reading, reviewing, editing or selection of the short stories; in fact, even his introductions were ghost-written. The entire extent of his involvement with the project was to lend his name and collect a check. Some notable writers whose works were used in the collection, include Shirley Jackson ("Strangers in Town", "The Lottery"), T.H. White ("The Once and Future King"), Robert Bloch, H. G. Wells ("The War of the Worlds"), Robert Louis Stevenson, Sir Arthur Conan Doyle, Mark Twain and the creator of "The Three Investigators", Robert Arthur. Hitchcock also wrote a mystery story for "Look" magazine in 1943, "The Murder of Monty Woolley". This was a sequence of captioned photographs inviting the reader to inspect the pictures for clues to the murderer's identity; Hitchcock cast the performers as themselves; such as Woolley, Doris Merrick and make up man Guy Pearce, whom Hitchcock identified, in the last photo, as the murderer. The article was reprinted in "Games" Magazine in November/December 1980.
Anacondas are large, nonvenomous boas found in tropical South America. Although the name actually applies to a group of snakes, it is often used to refer only to one species in particular, the green anaconda, "Eunectes murinus", one of the largest snakes in the world, and (together with the "reticulated python" of southeast Asia) possibly the longest. They live mostly in water, such as the Amazon River. While this snake poses a danger to humans, and there are several ascertained cases of people being killed by it, it does not regularly hunt humans. Its standard prey includes fish, river fowl, and occasionally domesticated goats or ponies that venture near or into the water. Threat from dangerous anacondas is a familiar plot in comics, movies and adventure stories set in the Amazon jungle.
The Oxford English Dictionary gives a first source as John Ray's "List of Indian Serpents" from the Leyden Museum, as "anacandaia of the Ceylonese, i.e. he that crushes the limbs of buffaloes and yoke beasts," but that "anacandaia" is "not now a native name in Sri Lanka, and not satisfactorily explained either in Cingalese [Sinhalese] or Tamil"—though Henry Yule lists "āṇaik'k'onḍa" to means "having killed an elephant" in Tamil.
Altaic is a disputed language family that is generally held by its proponents to include the Turkic, Mongolic, Tungusic, and Japonic language families and the Korean language isolate (Georg et al. 1999:73–74). These languages are spoken in a wide arc stretching from northeast Asia through Central Asia to Anatolia and eastern Europe (Turks, Kalmyks). The group is named after the Altai Mountains, a mountain range in Central Asia. These language families share numerous characteristics. The debate is over the origin of their similarities. One camp, often called the "Altaicists", views these similarities as arising from common descent from a Proto-Altaic language spoken several thousand years ago. The other camp, often called the "anti-Altaicists", views these similarities as arising from areal interaction between the language groups concerned. Some linguists believe the case for either interpretation is about equally strong; they have been called the "skeptics" (Georg et al. 1999:81). Another view accepts Altaic as a valid family but includes in it only Turkic, Mongolic, and Tungusic. This view was widespread prior to the 1960s, but has almost no supporters among specialists today (Georg et al. 1999:73–74). The expanded grouping, including Korean and Japanese, came to be known as "Macro-Altaic", leading to the designation by back-formation of the smaller grouping as "Micro-Altaic". Most proponents of Altaic continue to support the inclusion of Korean and Japanese. Micro-Altaic would include about 66 living languages, to which Macro-Altaic would add Korean, Japanese, and the Ryukyuan languages for a total of about 74. (These are estimates, depending on what is considered a language and what is considered a dialect. They do not include earlier states of language, such as Old Japanese.) Micro-Altaic would have a total of about 348 million speakers today, Macro-Altaic about 558 million. History of the Altaic idea. The idea that the Turkic, Mongolic, and Tungusic languages are each others' closest relatives was allegedly first published in 1730 by Philip Johan von Strahlenberg, a Swedish officer who traveled in the eastern Russian Empire while a prisoner of war after the Great Northern War. However, as has been pointed out by Alexis Manaster Ramer and Paul Sidwell (1997), Strahlenberg actually opposed the idea of a closer relationship between the languages which later became known as "Altaic". The term "Altaic", as the name for a language family, was introduced in 1844 by Matthias Castrén, a pioneering Finnish philologist who made major contributions to the study of the Uralic languages. As originally formulated by Castrén, Altaic included not only Turkic, Mongolian, and Manchu-Tungus (=Tungusic) but also Finno-Ugric and Samoyed (Poppe 1965:126). Finno-Ugric and Samoyed are not included in later formulations of Altaic. They came to be grouped in a separate family, known as Uralic (though doubts long persisted about its validity). Castrén's Altaic is thus equivalent to what later came to be known as Ural-Altaic (ib. 127). More precisely, Ural-Altaic came to subgroup Finno-Ugric and Samoyedic as "Uralic" and Turkic, Mongolic, and Tungusic as "Altaic", with Korean sometimes added to Altaic, and less often Japanese. For much of the 19th and early 20th centuries, many linguists who studied Turkic, Mongolic, and Tungusic regarded them as members of a common Ural-Altaic family, together with Finno-Ugric and Samoyedic, based on such shared features as vowel harmony and agglutination. While the Ural-Altaic hypothesis can still be found in encyclopedias, atlases, and similar general reference works, it has not had any adherents in the linguistics community for decades. It has been characterized by Sergei Starostin as "an idea now completely discarded" (Starostin et al. 2003:8). In 1857, the Austrian scholar Anton Boller suggested adding Japanese to Altaic or more precisely to Ural-Altaic (Miller 1986:34). For Korean, G.J. Ramstedt and E.D. Polivanov put forward additional etymologies in favor of its inclusion in the 1920s. The culmination of decades of research and publication on the part of the author, Ramstedt's two-volume work "Einführung in die altaische Sprachwissenschaft" ('Introduction to Altaic Linguistics') was published in 1952–1957. It rejected grouping the Uralic languages in a common family with the Altaic ones and included Korean in Altaic, an inclusion followed by most leading Altaicists to date. Ramstedt's first volume, "Lautlehre" ('Phonology'), contained the first comprehensive attempt to identify regular correspondences between the sound systems of the Altaic language families. The second volume was "Formenlehre" ('Morphology'). (The second volume was actually published first, in 1952, with the first volume following in 1957.) Ramstedt did not live to see the publication of his great work. He died in 1950, and the work was edited and seen through the press by Pentti Aalto, a student of his. In 1960, Nicholas Poppe presented what was in effect a heavily revised version of Ramstedt’s volume on phonology (Miller 1991:298) that has since set the standard in Altaic studies. Further contributions to Altaic linguistics in the 1960s were made by scholars such as Karl H. Menges and, on particular points, by Vladislav Illich-Svitych and others. In the meantime, knowledge of the branches of Altaic and the individual languages of which they are composed made great strides, thanks in large part to the efforts of Vera Cincius (also spelled Tsintsius) on Tungusic (Poppe 1965:97–98) and of Poppe himself on Mongolic, with contributions by many other scholars. Ramstedt and Cincius each had several students who carried on and extended their work (Poppe 1965:136, 98), as did Poppe. Poppe (1965:148) considered the issue of the relationship of Korean to Turkic-Mongolic-Tungusic was not settled. In his view, there were three real possibilities: (1) Korean did not belong with the other three genealogically, but had been influenced by an Altaic substratum; (2) Korean was related to the other three at the same level they were related to each other; (3) Korean had split off from the other three before they underwent a series of characteristic changes. Poppe leaned toward the third possibility (ib.), but did not commit himself to it in this work. Roy Andrew Miller's 1971 book "Japanese and the Other Altaic Languages" convinced most Altaicists that Japanese also belonged to Altaic (Poppe 1976:470). Since then, the standard set of languages included in Altaic has comprised Turkic, Mongolic, Tungusic, Korean, and Japanese. An alternative classification, though one with much less currency among Altaicists, was proposed by John C. Street (1962), according to which Turkic-Mongolic-Tungusic forms one grouping and Korean-Japanese-Ainu another, the two being linked in a common family that Street designated as "North Asiatic". The same schema was adopted by James Patrie (1982) in the context of an attempt to classify the Ainu language. The Turkic-Mongolic-Tungusic and Korean-Japanese-Ainu groupings were also posited by Joseph Greenberg (2000–2002) who, however, treated them as independent members of a larger family, which he termed Eurasiatic. <span id="Controversy" /> <span id="The controversy over Altaic" /> A language family or a Sprachbund? Even as Ramstedt's "Einführung" was making converts and generating the modern school of Altaic studies, a newly invigorated attack on the validity of the Altaic language family was taking shape. Gerard Clauson (1956), Gerhard Doerfer (1963), and Alexander Shcherbak argued that the words and features shared by Turkic, Mongolic, and Tungusic were for the most part borrowings and that the rest could be attributed to chance resemblances. They argued that while there were words shared by Turkic and Mongolic, by Mongolic and Tungusic, and by all three, there were none (Doerfer: few) shared by Turkic and Tungusic but not Mongolic. If all three families had a common ancestor, we should expect losses to happen at random, not only at the geographical margins of the family; on the other hand, we should expect exactly the observed pattern if borrowing is responsible. Furthermore, they argued that many of the typological features of the supposed Altaic languages, such as agglutinative morphology and SOV word order, usually occur together in languages. In sum, the idea was that Turkic, Mongolic, and Tungusic form a Sprachbund – the result of convergence through intensive borrowing and long contact among speakers of languages that are not necessarily closely related. The proponents of this hypothesis are sometimes called "the Anti-Altaicists". Doubt was also raised about the affinities of Korean and Japanese; in particular, some authors tried to connect Japanese to the Austronesian languages (Starostin et al. 2003:8–9). Since then, the debate has raged back and forth, with defenses of Altaic in the wide sense (e.g. Sergei Starostin 1991), advocacy of a family consisting of Tungusic, Korean, and Japonic but not Turkic or Mongolic ("Macro-Tungusic", J. Marshall Unger 1990), and wholesale rejections (e.g. Doerfer 1988) being published. Starostin's (1991) lexicostatistical research showed that the proposed Altaic groups shared about 15–20% of potential cognates within a 110-word Swadesh-Yakhontov list (e.g. Turkic–Mongolic 20%, Turkic–Tungusic 18%, Turkic–Korean 17%, Mongolic–Tungusic 22%, Mongolic–Korean 16%, Tungusic–Korean 21%). Altogether, Starostin concluded that the Altaic grouping was substantiated, though "older than most other language families in Eurasia, such as Indo-European or Finno-Ugric, and this is the reason why the modern Altaic languages preserve few common elements". A further step in the debate was the publication of "An Etymological Dictionary of the Altaic Languages" by Sergei Starostin, Anna V. Dybo, and Oleg A. Mudrak in 2003. The research for the dictionary included contributions by several young Altaic scholars, among them Ilya Gruntov and Martine Robbeets. The result of some twenty years of work, it contains 2800 proposed cognate sets, a complete set of regular sound correspondences based on those proposed sets, and a number of grammatical correspondences, as well as a few important changes to the reconstruction of Proto-Altaic. For example, while most of today's Altaic languages have vowel harmony, Proto-Altaic as reconstructed by Starostin et al. lacked it – instead various vowel assimilations between the first and second syllables of words occurred in Turkic, Mongolic, Tungusic, Korean, and Japonic. It tries hard to distinguish loans between Turkic and Mongolic and between Mongolic and Tungusic from cognates, and it suggests words that occur in Turkic and Tungusic but not Mongolic (Starostin et al. 2003:20); all other combinations between the five branches also occur in the book. It lists 144 items of shared basic vocabulary (2003:230–234) (mostly already present in Starostin 1991 (2003:234)), including words for such items as 'eye', 'ear', 'neck', 'bone', 'blood', 'water', 'stone', 'sun', and 'two'. This work has not changed the mind of any of the principal authors in the field, however. The debate continues unabated—e.g. S. Georg 2004, A. Vovin 2005, S. Georg 2005 (anti-Altaic); S. Starostin 2005, V. Blažek 2006, M. Robbeets 2007, A. Dybo and G. Starostin 2008 (pro-Altaic).
The earliest known texts in a language attributed to Altaic by its proponents are the Orkhon inscriptions, dating from the 8th century AD. They are written in a Turkic language. They were deciphered in 1893 by the Danish linguist Vilhelm Thomsen in a scholarly race with his rival, the Germano-Russian linguist Wilhelm Radloff. However, Radloff was the first to publish the inscriptions. All of these methods remain to be applied to the languages attributed to Altaic with the same degree of focus and intensity they have been applied to the Indo-European family (e.g. Mallory 1989, Anthony 2007). In the absence of more extensive studies in this area, most claims about the prehistory of the Altaic-speaking peoples must be viewed as extremely preliminary. This includes the following remarks. According to one line of reasoning, if the languages grouped as Altaic are genetically related, their great differences from each other would point to a very ancient date for their proto-language, in the Mesolithic or even the Upper Paleolithic period. (Miller 1991 however emphasizes the commonalities of these languages in all major areas: phonology, vocabulary, inflections, and syntax). Speakers of an Altaic protolanguage might have entered Central Asia following the disappearance of the West Siberian Glacial Lake, which almost completely covered the flatlands of western Siberia up to the foothills of the Kuznetsk Alatau and Altai mountain ranges. With the Late Glacial warming, up to the Atlantic Phase of the Post-Glacial Optimum, Mesolithic groups moved north into this area from the Hissar (6000–4000 BCE) and Keltiminar (5500–3500 BCE) cultures. These groups brought with them the bow and arrow and the dog, elements of what Kent Flannery has called the "broad-spectrum revolution". The Keltiminar culture occupied the semi-desert and desert areas of the Karakum and Kyzyl Kum deserts and the deltas of the Amu Darya and Zeravshan rivers (Whitney Coolidge 2005). The Keltiminar people practised a mobile hunting, gathering, and fishing subsistence system. Over time, they adopted stockbreeding. Some seek the origin of the proposed Micro-Altaic group in the spread of the Karasuk culture and the appearance of northern Mongol Dinlin elements. The Karasuk culture is the result of a migration of the eastern part of the Dinlins. Its influence extended as far as the Ordos region of China and across into Manchuria and northern Korea. The Karasuk people lived in permanent settlements in frame-type houses. The economy was complex. They bred large-horned livestock, horses, and sheep. They developed a high level of bronze metallurgy. Characteristic of the Karasuk culture are extensive cemeteries. Tombs are fenced with stone slabs laid on crest. Others equate the Karasuk culture with the origin of the Karasuk languages, a recently proposed language family that includes the Yeniseian languages and Burushaski but none of the suggested members of Altaic. Associating languages with archeological discoveries in the absence of written evidence is always a delicate matter. This hypothesis was dealt a major blow when the Yeniseian languages were firmly linked to the Na-Dené languages of North America in a family now called Dené-Yeniseian ("Bulletin of the Society for the Study of the Indigenous Languages of the Americas" 264, 31 March 2008). According to one view, Turkic and Mongolic are more closely related to each other than either is to Tungusic. If so, the split between Turkic and Mongolic would have been the last division within the Altaic group. It has been suggested that this occurred just prior to the Xiongnu period of Central Asian history. This would imply a considerably more shallow time depth for Proto-Altaic, or at least Proto-Micro-Altaic, than the late Stone Age. Such conflicts remain to be resolved. List of Altaicists and critics of Altaic. "Note: This list is limited to linguists who have worked specifically on the Altaic problem since the publication of the first volume of Ramstedt's "Einführung" in 1952. The dates given are those of works concerning Altaic. For Altaicists, the version of Altaic they favor is given at the end of the entry."
It is not clear whether /æ/, /ø/, /y/ were monophthongs as shown here (presumably) or diphthongs (); the evidence is equivocal. In any case, however, they only occurred in the first (and sometimes only) syllable of any word. Every vowel occurred in long and short versions which were different phonemes in the first syllable. Starostin et al. (2003) treat length together with pitch as a prosodic feature.
If a Proto(-Macro)-Altaic language really existed, it should be possible to reconstruct regular sound correspondences between that protolanguage and its descendants; such correspondences would make it possible to distinguish cognates from loanwords (in many cases). Such attempts have repeatedly been made. The latest version is reproduced here, taken from Blažek's (2006) summary of the newest Altaic etymological dictionary (Starostin et al. 2003) and transcribed into the IPA. When a Proto-Altaic phoneme developed differently depending on its position in a word (beginning, interior, or end), the special case (or all cases) is marked with a hyphen; for example, Proto-Altaic disappears (marked "0") or becomes /j/ at the beginning of a Turkic word and becomes /p/ elsewhere in a Turkic word.
Only single consonants are considered here. In the middle of words, clusters of two consonants were allowed in Proto-Altaic as reconstructed by Starostin et al. (2003); the correspondence table of these clusters spans almost 7 pages in their book (83–89), and most clusters are only found in one or a few of the reconstructed roots. ¹ The Khalaj language has /h/ instead. (It also retains a number of other archaisms.) However, it has also added /h/ in front of words for which no initial consonant (except in some cases /ŋ/, as expected) can be reconstructed for Proto-Altaic; therefore, and because it would make them dependent on whether Khalaj happens to have preserved any given root, Starostin et al. (2003:26–28) have not used Khalaj to decide whether to reconstruct an initial in any given word and have not reconstructed a /h/ for Proto-Turkic even though it was probably there. ² The Monguor language has /f/ here instead (Kaiser & Shevoroshkin 1988); it is therefore possible that Proto-Mongolian also had /f/ which then became /h/ (and then usually disappeared) in all descendants except Monguor. Tabgač and Kitan, two extinct Mongolic languages not considered by Starostin et al. (2003), even preserve /p/ in these places (Blažek 2006). ³ This happened when the next consonant in the word was, or. 5 When the next consonant in the word was /h/. 6 This happened "in syllables with original high pitch" (Starostin et al. 2003:135). 7 When followed by /æ/, /ø/, /y/. 8 When the next consonant in the word was /r/. 9 When the preceding consonant was,, or, or when the next consonant was /g/. 10 When the following vowel was /a/, /ə/, or followed by /j/. 11 When followed by /i/ and then another vowel, or by /j/. 12 When preceded by a vowel preceded by /i/. 14 Starostin et al. (2003) follow a minority opinion (Vovin 1993) in interpreting the sound of the Middle Korean letter as or rather than [z]. (Dybo & Starostin 2008:footnote 50) 16 When followed by /a/, /o/, or /e/. 17 When followed by /i/ or /u/.
The following table is a brief selection of further proposed cognates in basic vocabulary across the Altaic family (from Starostin et al. [2003]). 1 Contains the Proto-Altaic dual suffix: "both breasts" – "chest" – "heart". 2 Contains the Proto-Altaic singulative suffix -/nV/: "one breast". 3 Compare Baekje */turak/ "stone" (Blažek 2006). 4 This is in the Jurchen language. In modern Manchu it is "usiha". 5 This is disputed by Georg (2004), who states: "The traditional Tungusological reconstruction "*yāsa" [=] cannot be replaced by the nasal-initial one espoused here, needed for the comparison." However, Starostin (2005) mentions evidence from several Tungusic languages cited by Starostin et al. (2003). Georg (2005) does not accept this, referring to Georg (1999/2000) and an upcoming paper. By that time, Starostin was already dead (Starostin 2005 was published posthumously).
3 Kitan has "2" (Blažek 2006). 4 is probably a contraction of -/ubu/-. 5 The /y/- of "3" "may also reflect the same root, although the suffixation is not clear." (Starostin et al. 2003:223) 6 Compare Silla /mir/ "3" (Blažek 2006). 7 Compare Goguryeo /mir/ "3" (Blažek 2006). 8 "third (or next after three = fourth)", "consisting of three objects" 9 "song with three out of four verses rhyming (first, second and fourth)" 10 Kitan has /dur/ "4" (Blažek 2006). 11 Kitan has /tau/ "5" (Blažek 2006). 12 "(the prefixed i- is somewhat unclear: it is also used as a separate word meaning ‘fifty’, but the historical root here is no doubt "*tu-")" (Starostin et al. 2003:223). – Blažek (2006) also considers Goguryeo * "5" (from */uti/) to be related. 13 Kitan has /nir/ "6" (Blažek 2006). 14 Middle Korean has "6", which may fit here, but the required loss of initial "is not quite regular" (Starostin et al. 2003:224). 15 The Mongolian forms "may suggest an original proto-form" or /ladi/ "with dissimilation or metathesis in" Proto-Mongolic (Starostin et al. 2003:224). – Kitan has /dol/ "7". 16 in Early Middle Korean(タリクニ/チリクヒ in 二中歴). 17 "Problematic" (Starostin et al. 2003:224). 18 Compare Goguryeo /tok/ "10" (Blažek 2006). 19 Manchu "a very big number". 20 Orok "a bundle of 10 squirrels", Nanai "collection, gathering". 21 "Hundred" in names of hundreds. 22 Starostin et al. (2003) suspect this to be a reduplication: * "20 + 20". 23 /kata-ti/ would be expected; Starostin et al. (2003) think that this irregular change from /k/ to /p/ is due to influence from "2" /puta-tu/.
Austrian German ("German: Österreichisches Deutsch"), or "Austrian Standard German", is the national standard variety of the German language spoken in Austria and in South Tyrol (Italy). The standardized form of Austrian German for official texts and schools is defined by the Austrian dictionary ("Österreichisches Wörterbuch"), published under the authority of the ministry of education, art and culture.
As German is a pluricentric language, Austrian German is another standard variety in addition to the German spoken in Germany. Much like the relationship between American and British English, Austrian German is simply another standard form of the German language. The "Österreichisches Wörterbuch" states specific grammar rules and is a dictionary using Austrian spelling. In addition to this standard variety, in everyday life most Austrians speak one of a number of High German dialects.
With German being a pluricentric language, Austrian dialects should not be confused with the variety of Standard German spoken by most Austrians, which is distinct from that of Germany or Switzerland. Distinctions in vocabulary persist, for example, in culinary terms, where communication with Germans is frequently difficult, and administrative and legal language, which is due to Austria's exclusion from the development of a German nation-state in the late 19th century and its manifold particular traditions. A comprehensive collection of Austrian-German legal, administrative and economic terms is offered in: "Markhardt, Heidemarie: Wörterbuch der österreichischen Rechts-, Wirtschafts- und Verwaltungsterminologie" (Peter Lang, 2006).
When Austria became a member of the European Union, the Austrian variety of the German language (limited to 23 agricultural terms) was “protected” in the so-called Protocol no. 10 () regarding the use of specific Austrian terms of the German language in the framework of the European Union, which forms part of the Austrian EU accession treaty. Austrian German is the only variety of a pluricentric language recognised under international law / EU primary law. All facts concerning “Protocol no. 10” are documented in Markhardt, Heidemarie: "Das österreichische Deutsch im Rahmen der EU", Peter Lang, 2005.
In Austria, as in the German-speaking parts of Switzerland and in southern Germany, verbs that express a state tend to use "sein" as the auxiliary verb in the perfect tense, as well as verbs of movement. Verbs which fall into this category include "sitzen" (to sit), "liegen" (to lie) and, in parts of Carinthia, "schlafen" (to sleep). Therefore the perfect tense of these verbs would be "ich bin gesessen", "ich bin gelegen" and "ich bin geschlafen" respectively (note: "ich bin geschlafen" is a very rare form, usually you will hear "ich habe geschlafen"; but "ich bin eingeschlafen" (I fell asleep) is quite normal). (In the variant of German that is spoken in Germany, the words "stehen" (to stand) and "gestehen" (to confess) are identical in the present perfect tense: "habe gestanden". The Austrian variant avoids this potential ambiguity ("bin gestanden" from "stehen", "habe gestanden" from "gestehen"). Also, the preterite (simple past) is very rarely used in Austria, especially in the spoken language, except for some modal verbs ("ich war", "ich wollte").
There are many official terms that differ in Austrian German from their usage in most parts of Germany. These include Jänner (January) rather than "Januar", Feber (February) rather than Februar, heuer (this year) rather than "dieses Jahr", Kasten (wardrobe) instead of "Schrank", Kiste (crate) instead of "Schachtel", Sessel (chair) instead of "Stuhl", Stiege (stairs) instead of "Treppe", Rauchfang (chimney) instead of "Schornstein", Vorzimmer (floor) instead of "Diele", many administrative, legal and political terms - and a whole series of foods and vegetables such as: Erdäpfel (potatoes) German "Kartoffeln" (but Dutch "Aardappel"), Schlagobers (whipped cream) German "Schlagsahne", Faschiertes (ground beef) German "Hackfleisch", Fisolen (green beans) German "Gartenbohne", Karfiol (cauliflower) German "Blumenkohl", Karotte (carrot) German "Möhre", Kohlsprossen (Brussels sprouts) German "Rosenkohl", Marillen (apricots) German "Aprikosen", Paradeiser (tomatoes) German "Tomaten", Palatschinken (pancakes) German "Pfannkuchen", Topfen (a semi-sweet cottage cheese) German "Quark" and Kren (horseradish) German "Meerrettich". Austrians, in particular (and especially in the countryside and in conservative settings), will say "Grüß Gott!" (literally "greet God!", meaning "May God bless you") when greeting someone, rather than the "Guten Tag!" ("[Have a] good day!") used by many Germans. Beside the official Austrian German, occasionally also Austrian dialects from various regions are seen in written form, containing a large number of contractions and abbreviations compared to standard German, which can be hard to understand for non-native speakers (although the same applies to German dialects in Germany and Switzerland). Dialects: Intercomprehensibility and regional accents. While strong forms of the various dialects are not normally fully comprehensible to Northern Germans, there is virtually no communication barrier to speakers from Bavaria. The Central Austro-Bavarian dialects are more intelligible to speakers of Standard German than the Southern Austro-Bavarian dialects of Tyrol. Viennese, the Austro-Bavarian dialect of Vienna, is most frequently used in Germany for impersonations of the typical inhabitant of Austria. The people of Graz, the capital of Styria, speak yet another dialect which is not very Styrian and more easily understood by people from other parts of Austria than other Styrian dialects, for example from western Styria. Simple words in the various dialects are very similar, but pronunciation is distinct for each and, after listening to a few spoken words it may be possible for an Austrian to realise which dialect is being spoken. However, in regard to the dialects of the deeper valleys of the Tirol, other Tyroleans are often unable to understand them. Speakers from the different states of Austria can easily be distinguished from each other by their particular accents (probably more so than Bavarians), those of Carinthia, Styria, Vienna, Upper Austria, and the Tirol being very characteristic. Speakers from those regions, even those speaking Standard German, can usually be easily identified by their accent, even by an untrained listener. Several of the dialects have been influenced by contact with non-Germanic linguistic groups, such as the dialect of Carinthia, where in the past many speakers were bilingual with Slovene, and the dialect of Vienna, which has been influenced by immigration during the Austro-Hungarian period, particularly from what is today the Czech Republic. The German dialects of Bolzano-Bozen (Alto Adige/South Tyrol) have been influenced by local Romance languages, in particular with many loan words from Italian, and Ladin. Interestingly, the geographic borderlines between the different accents (isoglosses) coincide strongly with the borders of the states and also with the border with Bavaria, with Bavarians having a markedly different rhythm of speech in spite of the similarities in the language.
In mathematics, the axiom of choice, or AC, is an axiom of set theory. Informally put, the axiom of choice says that given any collection of bins, each containing at least one object, it is possible to make a selection of exactly one object from each bin, even if there are infinitely many bins and there is no "rule" for which object to pick from each. The axiom of choice is not required if the number of bins is finite or if such a selection "rule" is available. The axiom of choice was formulated in 1904 by Ernst Zermelo. Although originally controversial, it is now used without reservation by most mathematicians. One motivation for this use is that a number of important mathematical results, such as Tychonoff's theorem, require the axiom of choice for their proofs. Contemporary set theorists also study axioms that are not compatible with the axiom of choice, such as the axiom of determinacy. Unlike the axiom of choice, these alternatives are not ordinarily proposed as axioms for mathematics, but only as principles in set theory with interesting consequences.
There are many other equivalent statements of the axiom of choice. These are equivalent in the sense that, in the presence of other basic axioms of set theory, they imply the axiom of choice and are implied by it. One variation avoids the use of choice functions by, in effect, replacing each choice function with its range. Authors who use this formulation often speak of the "choice function on A", but be advised that this is a slightly different notion of choice function. Its domain is the powerset of "A" (with the empty set removed), and so makes sense for any set "A", whereas with the definition used elsewhere in this article, the domain of a choice function on a "collection of sets" is that collection, and so only makes sense for sets of sets. With this alternate notion of choice function, the axiom of choice can be compactly stated as
Until the late 19th century, the axiom of choice was often used implicitly, although it had not yet been formally stated. For example, after having established that the set "X" contains only non-empty sets, a mathematician might have said "let "F(s)" be one of the members of "s" for all "s" in "X"." In general, it is impossible to prove that "F" exists without the axiom of choice, but this seems to have gone unnoticed until Zermelo. Not every situation requires the axiom of choice. For finite sets "X", the axiom of choice follows from the other axioms of set theory. In that case it is equivalent to saying that if we have several (a finite number of) boxes, each containing at least one item, then we can choose exactly one item from each box. Clearly we can do this: We start at the first box, choose an item; go to the second box, choose an item; and so on. The number of boxes is finite, so eventually our choice procedure comes to an end. The result is an explicit choice function: a function that takes the first box to the first element we chose, the second box to the second element we chose, and so on. (A formal proof for all finite sets would use the principle of mathematical induction.) For certain infinite sets "X", it is also possible to avoid the axiom of choice. For example, suppose that the elements of "X" are sets of natural numbers. Every nonempty set of natural numbers has a smallest element, so to specify our choice function we can simply say that it maps each set to the least element of that set. This gives us a definite choice of an element from each set and we can write down an explicit expression that tells us what value our choice function takes. Any time it is possible to specify such an explicit choice, the axiom of choice is unnecessary. The difficulty appears when there is no natural choice of elements from each set. If we cannot make explicit choices, how do we know that our set exists? For example, suppose that "X" is the set of all non-empty subsets of the real numbers. First we might try to proceed as if "X" were finite. If we try to choose an element from each set, then, because "X" is infinite, our choice procedure will never come to an end, and consequently, we will never be able to produce a choice function for all of "X". So that won't work. Next we might try the trick of specifying the least element from each set. But some subsets of the real numbers don't have least elements. For example, the open interval (0,1) does not have a least element: If "x" is in (0,1), then so is "x"/2, and "x"/2 is always strictly smaller than "x". So taking least elements doesn't work, either. The reason that we are able to choose least elements from subsets of the natural numbers is the fact that the natural numbers come pre-equipped with a well-ordering: Every nonempty subset of the natural numbers has a unique least element under the natural ordering. Perhaps if we were clever we might say, "Even though the usual ordering of the real numbers does not work, it may be possible to find a different ordering of the real numbers which is a well-ordering. Then our choice function can choose the least element of every set under our unusual ordering." The problem then becomes that of constructing a well-ordering, which turns out to require the axiom of choice for its existence; every set can be well-ordered if and only if the axiom of choice is true.
A proof requiring the axiom of choice is, in one meaning of the word, nonconstructive: even though the proof establishes the existence of an object, it may be impossible to define the object in the language of set theory. For example, while the axiom of choice implies that there is a well-ordering of the real numbers, there are models of set theory with the axiom of choice in which no well-ordering of the reals is definable. As another example, a subset of the real numbers that is not Lebesgue measurable can be proven to exist using the axiom of choice, but it is consistent that no such set is definable. The axiom of choice produces these intangibles (objects that are proven to exist by a nonconstructive proof, but cannot be explicitly constructed), which may conflict with some philosophical principles. Because there is no canonical well-ordering of all sets, a construction that relies on a well-ordering may not produce a canonical result, even if a canonical result is desired (as is often the case in category theory). In constructivism, all existence proofs are required to be totally explicit. That is, one must be able to construct, in an explicit and canonical manner, anything that is proven to exist. This foundation rejects the full axiom of choice because it asserts the existence of an object without uniquely determining its structure. In fact the Diaconescu–Goodman–Myhill theorem shows how to derive the constructively unacceptable law of the excluded middle, or a restricted form of it, in constructive set theory from the assumption of the axiom of choice. Another argument against the axiom of choice is that it implies the existence of counterintuitive objects. One example of this is the Banach–Tarski paradox which says that it is possible to decompose ("carve up") the 3-dimensional solid unit ball into finitely many pieces and, using only rotations and translations, reassemble the pieces into two solid balls each with the same volume as the original. The pieces in this decomposition, constructed using the axiom of choice, are extremely complicated. The majority of mathematicians accept the axiom of choice as a valid principle for proving new results in mathematics. The debate is interesting enough, however, that it is considered of note when a theorem in ZFC is logically equivalent (with just the ZF axioms) to the axiom of choice, and mathematicians look for results that require the axiom of choice to be false, though this type of deduction is less common than the type which requires the axiom of choice to be true. It is possible to prove many theorems using neither the axiom of choice nor its negation; this is common in constructive mathematics. Such statements will be true in any model of Zermelo–Fraenkel set theory (ZF), regardless of the truth or falsity of the axiom of choice in that particular model. The restriction to ZF renders any claim that relies on either the axiom of choice or its negation unprovable. For example, the Banach–Tarski paradox is neither provable nor disprovable from ZF alone: it is impossible to construct the required decomposition of the unit ball in ZF, but also impossible to prove there is no such decomposition. Similarly, all the statements listed below which require choice or some weaker version thereof for their proof are unprovable in ZF, but since each is provable in ZF plus the axiom of choice, there are models of ZF in which each statement is true. Statements such as the Banach–Tarski paradox can be rephrased as conditional statements, for example, "If AC holds, the decomposition in the Banach–Tarski paradox exists." Such conditional statements are provable in ZF when the original statements are provable from ZF and the axiom of choice.
By work of Kurt Gödel and Paul Cohen, the axiom of choice is logically independent of the other axioms of Zermelo–Fraenkel set theory (ZF). This means that neither it nor its negation can be proven to be true in ZF, if ZF is consistent. Consequently, if ZF is consistent, then ZFC is consistent and ZF¬C is also consistent. So the decision whether or not it is appropriate to make use of the axiom of choice in a proof cannot be made by appeal to other axioms of set theory. The decision must be made on other grounds. One argument given in favor of using the axiom of choice is that it is convenient to use it: using it cannot hurt (cannot result in contradiction) and makes it possible to prove some propositions that otherwise could not be proved. Many theorems which are provable using choice are of an elegant general character: every ideal in a ring is contained in a maximal ideal, every vector space has a basis, and every product of compact spaces is compact. Without the axiom of choice, these theorems may not hold for mathematical objects of large cardinality. The proof of the independence result also shows that a wide class of mathematical statements, including all statements that can be phrased in the language of Peano arithmetic, are provable in ZF if and only if they are provable in ZFC. Statements in this class include the statement that P = NP, the Riemann hypothesis, and many other unsolved mathematical problems. When one attempts to solve problems in this class, it makes no difference whether ZF or ZFC is employed if the only question is the existence of a proof. It is possible, however, that there is a shorter proof of a theorem from ZFC than from ZF. The axiom of choice is not the only significant statement which is independent of ZF. For example, the generalized continuum hypothesis (GCH) is not only independent of ZF, but also independent of ZF plus the axiom of choice (ZFC). However, ZF plus GCH implies AC, making GCH a strictly stronger claim than AC, even though they are both independent of ZF.
The axiom of constructibility and the generalized continuum hypothesis both imply the axiom of choice, but are strictly stronger than it. In class theories such as Von Neumann–Bernays–Gödel set theory and Morse–Kelley set theory, there is a possible axiom called the axiom of global choice which is stronger than the axiom of choice for sets because it also applies to proper classes. And the axiom of global choice follows from the axiom of limitation of size.
There are several results in category theory which invoke the axiom of choice for their proof. These results might be weaker than, equivalent to, or stronger than the axiom of choice, depending on the strength of the technical foundations. For example, if one defines categories in terms of sets, that is, as sets of objects and morphisms (usually called a small category), or even locally small categories, whose hom-objects are sets, then there is no category of all sets, and so it is difficult for a category-theoretic formulation to apply to all sets. On the other hand, other foundational descriptions of category theory are considerably stronger, and an identical category-theoretic statement of choice may be stronger than the standard formulation, à la class theory, mentioned above.
There are several weaker statements that are not equivalent to the axiom of choice, but are closely related. One example is the axiom of dependent choice (DC). A still weaker example is the axiom of countable choice (ACω or CC), which states that a choice function exists for any countable set of nonempty sets. These axioms are sufficient for many proofs in elementary mathematical analysis, and are consistent with some principles, such as the Lebesgue measurability of all sets of reals, that are disprovable from the axiom of choice. Other choice axioms weaker than axiom of choice include the Boolean prime ideal theorem and the axiom of uniformization. Results requiring AC (or weaker forms) but weaker than it===. One of the most interesting aspects of the axiom of choice is the large number of places in mathematics that it shows up. Here are some statements that require the axiom of choice in the sense that they are not provable from ZF but are provable from ZFC (ZF plus AC). Equivalently, these statements are true in all models of ZFC but false in some models of ZF. Stronger forms of the negation of AC. Now, consider stronger forms of the negation of AC. For example, if we abbreviate by BP the claim that every set of real numbers has the property of Baire, then BP is stronger than ¬AC, which asserts the nonexistence of any choice function on perhaps only a single set of nonempty sets. Note that strengthened negations may be compatible with weakened forms of AC. For example, ZF + DC + BP is consistent, if ZF is. It is also consistent with ZF + DC that every set of reals is Lebesgue measurable; however, this consistency result, due to Robert M. Solovay, cannot be proved in ZFC itself, but requires a mild large cardinal assumption (the existence of an inaccessible cardinal). The much stronger axiom of determinacy, or AD, implies that every set of reals is Lebesgue measurable, has the property of Baire, and has the perfect set property (all three of these results are refuted by AC itself). ZF + DC + AD is consistent provided that a sufficiently strong large cardinal axiom is consistent (the existence of infinitely many Woodin cardinals). Statements consistent with the negation of AC. There are models of Zermelo-Fraenkel set theory in which the axiom of choice is false. We will abbreviate "Zermelo-Fraenkel set theory plus the negation of the axiom of choice" by ZF¬C. For certain models of ZF¬C, it is possible to prove the negation of some standard facts. Note that any model of ZF¬C is also a model of ZF, so for each of the following statements, there exists a model of ZF in which that statement is true. For proofs, see Thomas Jech, "The Axiom of Choice", American Elsevier Pub. Co., New York, 1973.
"The Axiom of Choice is obviously true, the well-ordering principle obviously false, and who can tell about Zorn's lemma?" — Jerry Bona "The Axiom of Choice is necessary to select a set from an infinite number of socks, but not an infinite number of shoes." — Bertrand Russell "Tarski tried to publish his theorem [the equivalence between AC and 'every infinite set "A" has the same cardinality as "A"x"A, see above] in Comptes Rendus, but Fréchet and Lebesgue refused to present it. Fréchet wrote that an implication between two well known [true] propositions is not a new result, and Lebesgue wrote that an implication between two false propositions is of no interest". "The axiom gets its name not because mathematicians prefer it to other axioms." — A. K. Dewdney
Attila (or; 406–453), widely known as Attila the Hun, was the Emperor of the Huns from 434 until his death in 453. He was leader of the Hunnic Empire which stretched from Germany to the Ural River and from the River Danube to the Baltic Sea (see map below). During his rule, he was one of the most fearsome of the Western and Eastern Roman Empires' enemies: he invaded the Balkans twice and marched through Gaul (modern France) as far as Orléans before being defeated at the Battle of Châlons. He refrained from attacking either Constantinople or Rome. His story, that the Sword of Attila had come to his hand by miraculous means, was reported by the Roman Priscus. In much of Western Europe, he is remembered as the epitome of cruelty and rapacity. However, in Hungary, Turkey, and other Turkic-speaking countries in Central Asia, he is regarded as a hero and his name is revered. Some histories and chronicles describe him as a great and noble king, and he plays major roles in three Old Norse works: "Atlakviða", "Völsungasaga", and "Atlamál".
The Huns were a group of Eurasian nomads who, appearing from beyond the Volga, migrated into Europe c. 370 and built up an enormous empire in Europe. Their main military technique was mounted archery. They were possibly the descendants of the Xiongnu who had been northern neighbours of China three hundred years before and may be the first expansion of Turkic people across Eurasia. The origin and language of the Huns has been the subject of debate for centuries. One scholar suggests a relationship to Yeniseian. The leading current theory is that their leaders at least may have spoken a Turkic language, perhaps closest to the modern Chuvash language.
The death of Rugila (also known as Rua or Ruga) in 434 left his nephews Attila and Bleda (also known as Buda), the sons of his brother Mundzuk (,), in control over all the united Hun tribes. At the time of their accession, the Huns were bargaining with Byzantine emperor Theodosius II's envoys over the return of several renegades (possibly Hunnic nobles not in agreement with the brothers' leadership) who had taken refuge within the Byzantine Empire. The following year Attila and Bleda met with the imperial legation at Margus (present-day Požarevac) and, all seated on horseback in the Hunnic manner, negotiated a successful treaty: the Romans agreed not only to return the fugitives, but also to double their previous tribute of 350 Roman pounds (ca. 115 kg) of gold, open their markets to Hunnish traders, and pay a ransom of eight "solidi" for each Roman taken prisoner by the Huns. The Huns, satisfied with the treaty, decamped from the empire and returned to their home in the Hungarian Great Plain, perhaps to consolidate and strengthen their empire. Theodosius used this opportunity to strengthen the walls of Constantinople, building the city's first sea wall, and to build up his border defenses along the Danube. The Huns remained out of Roman sight for the next few years as a Hunnic force invaded the Sassanid Empire. A defeat in Armenia by the Sassanids caused them to abandon this attempt and return their attentions to Europe. In 440 they reappeared in force on the borders of the Roman Empire, attacking the merchants at the market on the north bank of the Danube that had been established by the treaty. Crossing the Danube, they laid waste to the cities of Illyricum and forts on the river, among them, according to Priscus, Viminacium, which was a city of Moesia. Their advance began at Margus, whose bishop they had demanded for retaining property which Attila regarded as his; when the Romans discussed handing over the offending bishop, he slipped away secretly to the Huns and betrayed the city to them. As the Huns conquered the Danube defences, the Vandals, under the leadership of Geiseric, captured the Western Roman province of Africa with its capital of Carthage in 440 and the Sassanid Shah Yazdegerd II invaded Armenia in 441. Stripping the Balkan defenses of forces requested by the West Romans, in order to launch an attack on the Vandals in Africa (which was the richest province of the Western empire and a main source of the food supply of Rome) left Attila and Bleda a clear path through Illyricum into the Balkans, which they invaded in 441. The Hunnish army, having sacked Margus and Viminacium, took Singidunum (modern Belgrade) and Sirmium before halting. A lull followed in 442 and during this time Theodosius recalled his troops from Sicily and ordered a large new issue of coins to finance operations against the Huns. Having made these preparations, he thought it safe to refuse the Hunnish kings' demands. Attila responded with a campaign in 443. Striking along the Danube, the Huns overran the military centres of Ratiara and successfully besieged Naissus (modern Niš) with battering rams and rolling siege towers—military sophistication that was new to the Hun repertoire. Then, pushing along the Nisava River, they took Serdica (Sofia), Philippopolis (Plovdiv), and Arcadiopolis. They encountered and destroyed a Roman army outside Constantinople and were stopped by the double walls of the Eastern capital. A second army was defeated near Callipolis (modern Gallipoli) and Theodosius, now without any armed forces to respond, admitting defeat, sent the "Magister militum per Orientem" Anatolius to negotiate peace terms. These were harsher than the previous treaty: the Emperor agreed to hand over 6,000 Roman pounds (ca. 2000 kg) of gold as punishment for having disobeyed the terms of the treaty during the invasion; the yearly tribute was tripled, rising to 2,100 Roman pounds (ca. 700 kg) in gold; and the ransom for each Roman prisoner rose to 12 "solidi". Their demands met for a time, the Hun kings withdrew into the interior of their empire. According to Jordanes (following Priscus), during the peace following the Huns' withdrawal from Byzantium (probably around 445), Bleda died (killed in a hunting accident arranged by his brother, according to the classical sources), Attila took the throne for himself, and became the sole ruler of the Huns.
The barbarian nation of the Huns, which was in Thrace, became so great that more than a hundred cities were captured and Constantinople almost came into danger and most men fled from it. … And there were so many murders and blood-lettings that the dead could not be numbered. Ay, for they took captive the churches and monasteries and slew the monks and maidens in great numbers. (Callinicus, in his "Life of Saint Hypatius")
In 450, Attila proclaimed his intent to attack the powerful Visigoth kingdom of Toulouse, making an alliance with Emperor Valentinian III in order to do so. He had previously been on good terms with the Western Roman Empire and its "de facto" ruler Flavius Aëtius. Aëtius had spent a brief exile among the Huns in 433, and the troops Attila provided against the Goths and Bagaudae had helped earn him the largely honorary title of "magister militum" in the west. The gifts and diplomatic efforts of Geiseric, who opposed and feared the Visigoths, may also have influenced Attila's plans. However, Valentinian's sister was Honoria, who, in order to escape her forced betrothal to a Roman senator, had sent the Hunnish king a plea for help – and her engagement ring – in the spring of 450. Though Honoria may not have intended a proposal of marriage, Attila chose to interpret her message as such. He accepted, asking for half of the western Empire as dowry. When Valentinian discovered the plan, only the influence of his mother Galla Placidia convinced him to exile, rather than kill, Honoria. He also wrote to Attila strenuously denying the legitimacy of the supposed marriage proposal. Attila sent an emissary to Ravenna to proclaim that Honoria was innocent, that the proposal had been legitimate, and that he would come to claim what was rightfully his. Attila interfered in a succession struggle after the death of a Frankish ruler. Attila supported the elder son, while Aëtius supported the younger. Attila gathered his vassals—Gepids, Ostrogoths, Rugians, Scirians, Heruls, Thuringians, Alans, Burgundians, among others and began his march west. In 451 he arrived in Belgica with an army exaggerated by Jordanes to half a million strong. J.B. Bury believes that Attila's intent, by the time he marched west, was to extend his kingdom – already the strongest on the continent – across Gaul to the Atlantic Ocean. On April 7, he captured Metz. Other cities attacked can be determined by the hagiographic "vitae" written to commemorate their bishops: Nicasius was slaughtered before the altar of his church in Rheims; Servatus is alleged to have saved Tongeren with his prayers, as Saint Genevieve is to have saved Paris. Lupus, bishop of Troyes, is also credited with saving his city by meeting Attila in person. Aëtius moved to oppose Attila, gathering troops from among the Franks, the Burgundians, and the Celts. A mission by Avitus, and Attila's continued westward advance, convinced the Visigoth king Theodoric I (Theodorid) to ally with the Romans. The combined armies reached Orléans ahead of Attila, thus checking and turning back the Hunnish advance. Aëtius gave chase and caught the Huns at a place usually assumed to be near Catalaunum (modern Châlons-en-Champagne). The two armies clashed in the Battle of Châlons, whose outcome is commonly considered to be a strategic victory for the Visigothic-Roman alliance. Theodoric was killed in the fighting and Aëtius failed to press his advantage, according to Edward Gibbon and Edward Creasy, because he feared the consequences of an overwhelming Visigothic triumph as much as he did a defeat. From Aëtius' point of view, the best outcome was what occurred: Theodoric died, Attila was in retreat and disarray, and the Romans had the benefit of appearing victorious. Invasion of Italy and death. Attila returned in 452 to claim his marriage to Honoria anew, invading and ravaging Italy along the way. The city of Venice was founded as a result of these attacks when the residents fled to small islands in the Venetian Lagoon. His army sacked numerous cities and razed Aquileia completely, leaving no trace of it behind. Legend has it he built a castle on top of a hill north of Aquileia to watch the city burn, thus founding the town of Udine, where the castle can still be found. Aëtius, who lacked the strength to offer battle, managed to harass and slow Attila's advance with only a shadow force. Attila finally halted at the River Po. By this point disease and starvation may have broken out in Attila's camp, thus helping to stop his invasion. Emperor Valentinian III sent three envoys, the high civilian officers Gennadius Avienus and Trigetius, as well as the Bishop of Rome Leo I, who met Attila at Mincio in the vicinity of Mantua, and obtained from him the promise that he would withdraw from Italy and negotiate peace with the emperor. Prosper of Aquitaine gives a short, reliable description of the historic meeting, but gives all the credit of the successful negotiation to Leo. The later anonymous account, a pious "fable which has been represented by the pencil of Raphael and the chisel of Algardi" (as Gibbon called it) says that the Pope, aided by Saint Peter and Saint Paul, convinced him to turn away from the city. According to a later mediaeval Hungarian chronicle, the Pope promised Attila that if he left Rome in peace, one of his successors would receive a holy crown. Priscus reports that superstitious fear of the fate of Alaric—who died shortly after sacking Rome in 410—gave him pause. After Attila left Italy and returned to his palace across the Danube, he planned to strike at Constantinople again and reclaim the tribute which Marcian had stopped. (Marcian was the successor of Theodosius and had ceased paying tribute in late 450 while Attila was occupied in the west; multiple invasions by the Huns and others had left the Balkans with little to plunder.) However Attila died in the early months of 453. The conventional account, from Priscus, says that at a feast celebrating his latest marriage to the beautiful and young Ildico (if uncorrupted, the name suggests a Gothic origin) he suffered a severe nosebleed and choked to death in a stupor. An alternative theory is that he succumbed to internal bleeding after heavy drinking or a condition called esophageal varices, where dilated veins in the lower part of the esophagus rupture leading to death by haemorrhage. Another account of his death, first recorded 80 years after the events by the Roman chronicler Count Marcellinus, reports that "Attila, King of the Huns and ravager of the provinces of Europe, was pierced by the hand and blade of his wife." The "Volsunga saga" and the "Poetic Edda" also claim that King Atli (Attila) died at the hands of his wife, Gudrun. Most scholars reject these accounts as no more than hearsay, preferring instead the account given by Attila's contemporary Priscus. Priscus' version, however, has recently come under renewed scrutiny by Michael A. Babcock. Based on detailed philological analysis, Babcock concludes that the account of natural death, given by Priscus, was an ecclesiastical "cover story" and that Emperor Marcian (who ruled the Eastern Roman Empire from 450-457) was the political force behind Attila's death. Jordanes says: "The greatest of all warriors should be mourned with no feminine lamentations and with no tears, but with the blood of men." His horsemen galloped in circles around the silken tent where Attila lay in state, singing in his dirge, according to Cassiodorus and Jordanes: "Who can rate this as death, when none believes it calls for vengeance?" Then they celebrated a "strava" (lamentation) over his burial place with great feasting. Legend says that he was laid to rest in a triple coffin made of gold, silver, and iron, along with some of the spoils of his conquests. His men diverted a section of the river, buried the coffin under the riverbed, and then were killed to keep the exact location a secret. His sons Ellac (his appointed successor), Dengizich, and Ernakh fought over the division of his legacy, specifically which vassal kings would belong to which brother. As a consequence they were divided, defeated and scattered the following year in the Battle of Nedao by the Ostrogoths and the Gepids under Ardaric who had been Attila's most prized chieftain. Attila's many children and relatives are known by name and some even by deeds, but soon valid genealogical sources all but dry up and there seems to be no verifiable way to trace Attila's descendants. This has not stopped many genealogists from attempting to reconstruct a valid line of descent for various medieval rulers. One of the most credible claims has been that of the khans of Bulgaria (see Nominalia of the Bulgarian khans). A popular, but ultimately unconfirmed, attempt tries to relate Attila to Charlemagne.
Attila has been portrayed in various ways, sometimes as a noble ruler, sometimes as a cruel barbarian. Attila is known in Western history and tradition as the grim "flagellum dei" (Latin: "Scourge of God"), and his name has become a byword for cruelty and barbarism. Some of this may have arisen from confusion between him and later steppe warlords such as Genghis Khan (Timuchin) and Timur (Tamerlane). All have been regarded as cruel, clever, and blood-thirsty lovers of battle and pillage; all have been recorded mainly by their enemies. The reality of his character is probably more complex. Priscus also recounts his meeting with an eastern Roman captive who admired Hunnic governance over Roman, so that he had no desire to return to his former country, and the Byzantine historian's description of Attila's humility and simplicity is unambiguous in its admiration. The origin of Attila's name is not known with confidence. Most suggestions assume Turkish roots. The etymology "oceanic (universal) [ruler]" has been proposed, supposing that the Hunnic language was Danube-Bulgarian. Alternatively the word might originate from Turkic "Atyl/Atal/Atil/Itil" meaning water, river (also, ancient name of Volga river), with adjective suffix -ly. (Compare also Turkic medieval notable title "atalyk" – "senior as father"). Old-Turkic might have used the word "atta" ("father") (as in Atatürk) then added the diminutive suffix "-ila", which means ("little father") from "Attaila" 'Attila' has many variants: Atli and Atle in Norse, Ætla, Attle and Atlee in English, Attila/Atilla/Etele in Hungarian (all the three name variants are used in Hungary; Attila is the most popular variant), Etzel in the German Nibelungenlied, or Attila, Atila or Atilla in modern Turkish. In Hungary and in Turkey "Attila" is commonly used as a male first name. In Turkey sometimes the name is spelled with double ll rather than double tt (Atilla). The Polish Chronicle represents Attila's name as "Aquila" derived from the Latin "aqua". Others believe that the name may have a connection to Hungarian "ítélet" meaning "judgement".
In Hungary, several public places are named after Attila; for instance, in Budapest there are 10 Attila Streets and an Attila Lane, one of which is an important street behind the Buda Castle. See: Public place names of Budapest In Turkey many military operations were named after Attila. When the Turkish army intervened Cyprus in 1974 the operation was nicknamed Attila as well. Turks also named hundreds of streets and regions after his name in different cities and towns across Turkey.
The Aegean Sea (, "Egeo Pelagos";) is an elongated embayment of the Mediterranean Sea located between the southern Balkan and Anatolian peninsulas, i.e., between the mainlands of Greece and Turkey respectively. In the north, it is connected to the Marmara Sea and Black Sea by the Dardanelles and Bosporus. The Aegean Islands are within the sea and some bound it on its southern periphery, including Crete and Rhodes. The Aegean Region consists of nine provinces in southwestern Turkey, in part bordering on the Aegean sea. The sea was traditionally known as "Archipelago" (in Greek, "Αρχιπέλαγος"), the general sense of which has since changed to refer to the Aegean Islands and, generally, to any island group because the Aegean Sea is remarkable for its large number of islands.
The Aegean Sea covers about in area, and measures about longitudinally and latitudinally. The sea's maximum depth is, east of Crete. The Aegean Islands are found within its waters, with the following islands delimiting the sea on the south (generally from west to east): Kythera, Antikythera, Crete, Kasos, Karpathos and Rhodes. The word "archipelago" was originally applied specifically to the Aegean Sea and its islands. Many of the Aegean Islands, or chains of islands, are actually extensions of the mountains on the mainland. One chain extends across the sea to Chios, another extends across Euboea to Samos, and a third extends across the Peloponnese and Crete to Rhodes, dividing the Aegean from the Mediterranean. The bays and gulfs of the Aegean beginning and the South and moving clockwise include on Crete, the Mirabelli, Almyros, Souda and Chania bays or gulfs, on the mainland the Myrtoan Sea to the west, the Saronic Gulf northwestward, the Petalies Gulf which connects with the South Euboic Sea, the Pagasetic Gulf which connects with the North Euboic Sea, the Thermian Gulf northwestward, the Chalkidiki Peninsula including the Cassandra and the Singitic Gulfs, northward the Strymonian Gulf and the Gulf of Kavala and the rest are in Turkey; Saros Gulf, Edremit Gulf, Dikili Gulf, Çandarlı Gulf, İzmir Gulf, Kuşadası Gulf, Gulf of Gökova, Güllük Gulf.
"On the South." A line running from Cape Aspro (28°16'E) in Asia Minor, to Cum Burnù (Capo della Sabbia) the Northeast extreme of the Island of Rhodes, through the island to Cape Prasonisi, the Southwest point thereof, on to Vrontos Point (35°33'N) in Skarpanto [Karpathos, through this island to Castello Point, the South extreme thereof, across to Cape Plaka (East extremity of Crete), through Crete to Agria Grabusa, the Northwest extreme thereof, thence to Cape Apolitares in Antikithera Island, through the island to Psira Rock (off the Northwest point) and across to Cape Trakhili in Kithera Island, through Kithera to the Northwest point (Cape Karavugia) and thence to Cape Santa Maria () in the Morea. "In the Dardanelles". A line joining Kum Kale (26°11'E) and Cape Helles.
Aegean surface water circulates in a counter-clockwise gyre, with hypersaline Mediterranean water moving northward along the west coast of Turkey, before being displaced by less dense Black Sea outflow. The dense Mediterranean water sinks below the Black Sea inflow to a depth of, then flows through the Dardanelles Strait and into the Marmara at velocities of 5–15 cm/s. The Black Sea outflow moves westward along the northern Aegean Sea, then flows southwards along the east coast of Greece. The physical oceanography of the Aegean Sea is controlled mainly by the regional climate, the fresh water discharge from major rivers draining southeastern Europe, and the seasonal variations in the Black Sea surface water outflow through the Dardanelles Strait.
In ancient times there were various explanations for the name "Aegean". It was said to have been named after the Greek town of Aegae, or after Aegea, a queen of the Amazons who died in the sea, or Aigaion, the "sea goat", another name of Briareus, one of the archaic Hecatonchires, or, especially among the Athenians, Aegeus, the father of Theseus, who drowned himself in the sea when he thought his son had died. A possible etymology is a derivation from the Greek word ' – "aiges" = "waves" (Hesychius of Alexandria; metaphorical use of ("aix") "goat"), hence "wavy sea", cf. also (aigialos) "coast".
The current coastline dates back to about 4000 BC. Before that time, at the peak of the last ice age (c. 16,000 BC) sea levels everywhere were 130 metres lower, and there were large well-watered coastal plains instead of much of the northern Aegean. When they were first occupied, the present-day islands including Milos with its important obsidian production were probably still connected to the mainland. The present coastal arrangement appeared c. 7000 BC, with post-ice age sea levels continuing to rise for another 3000 years after that. The subsequent Bronze Age civilizations of Greece and the Aegean Sea have given rise to the general term "Aegean civilization". In ancient times the sea was the birthplace of two ancient civilizations the Minoans of Crete and the Mycenean Civilization of the Peloponnese. Later arose the city-states of Athens and Sparta among many others that constituted the Athenian Empire and Hellenic Civilization. Plato described the Greeks living round the Aegean "like frogs around a pond". The Aegean Sea was later invaded by the Persians and the Romans, and inhabited by the Byzantine Empire, the Venetians, the Seljuk Turks, and the Ottoman Empire. The Aegean was the site of the original democracies, and its seaways were the means of contact among several diverse civilizations of the Eastern Mediterranean.
Many of the islands in the Aegean have safe harbours and bays. In ancient times, navigation through the sea was easier than traveling across the rough terrain of the Greek mainland (and to some extent the coastal areas of Anatolia). Many of the islands are volcanic, and marble and iron are mined on other islands. The larger islands have some fertile valleys and plains. Of the main islands in the Aegean Sea, two belong to Turkey – Bozcaada (Tenedos) and Gökçeada (Imbros); the rest belong to Greece. Between the two countries, there are political disputes over several aspects of political control over the Aegean space, including the size of territorial waters, air control and the delimitation of economic rights to the continental shelf.
"A Clockwork Orange" (1962) is a dystopian novel by Anthony Burgess. The title is taken from an old Cockney expression, "as queer as a clockwork orange"¹, and alludes to the prevention of the main character's exercise of his free will through the use of a classical conditioning technique. With this technique, the subject’s emotional responses to violence are systematically paired with a negative stimulation in the form of nausea caused by an emetic medicine administered just before the presentation of films depicting "ultra-violent" situations. Written from the perspective of a seemingly biased and unapologetic protagonist, the novel also contains an experiment in language: Burgess creates a new speech that is the teenage slang of the not-too-distant future. The novel has been adapted for cinema in a controversial movie by Stanley Kubrick, and also by Andy Warhol; adaptations have also been made for television, radio, and the stage. As well as inspiring a concept album, the novel and films are referred to in, and have inspired, a number of songs and bands.
Alex, living in near-future England, leads his gang on nightly orgies of opportunistic, random violence. Alex's friends ("droogs" in the novel's Anglo-Russified slang) are Dim, a slow-witted bruiser who is the gang's muscle, Georgie, and Pete. Alex, who is quick-witted and possessing an often disconcerting sense of humour, is clearly the smartest of the group and seemingly very cultured. The novel begins with the droogs sitting in their favourite milkbar, drinking drugged milk to hype themselves for the night's mayhem. They assault a scholar walking home from the library, stamp on a panhandling derelict, scuffle with a rival gang led by Billyboy, rob a newsagent and leave its owners unconscious, then steal a car. Joyriding in the countryside, they break into an isolated cottage and maul the young couple living there, beating the husband and raping his wife. The droogs ditch the car, and Dim and Georgie make clear their dissatisfaction with Alex's domination of the gang. At home in his dreary flat, Alex plays classical music thunderously while bringing himself to climax with fantasies of even more orgiastic violence. Alex skips school the following morning and is visited by P. R. Deltoid, a "post-corrective advisor" assigned to remediate his juvenile delinquency. Visiting his favourite music shop, Alex meets a pair of underage girls and takes them back to his parents' flat, where he feeds them alcohol and sexually assaults them while they are intoxicated. Alex later chats with his parents, who are sceptical of his claims about having a night job, yet too intimidated to press the issue. Arriving late to meet the droogs, who have already pumped themselves up with "the old knifey moloko" (i.e., drugged milk), Alex is at a disadvantage. Georgie challenges Alex for leadership of the gang, demanding that they pull a "man-sized" job by robbing a wealthy old woman who lives alone with her cats. Alex quells the rebellion by slashing Dim's hand and fighting with Georgie, then in a show of generosity takes them to a bar for some fortifying drinks. Georgie and Dim are ready to call it a night, but Alex bullies them into proceeding with the burglary. At the woman's house, she's reluctant to open the door and calls the police. His droogs lift Alex through a second-floor window and, after a farcical struggle, he knocks the old woman unconscious. With sirens in the distance Alex flees. His droogs await him at the front door, and Dim hits Alex with a chain across the eyes, causing him extreme pain and temporary blindness (the chain was replaced with a full milk bottle in the film version). They leave Alex to fend for himself, and the police find and arrest him. At the police station they ask him questions about the invasion. P.R. Deltoid shows up and renounces Alex, spitting in his face and telling him that he can't intercede on his behalf any longer. Alex is later summoned from his jail cell and learns that his victim has died and he is now guilty of murder. Part 2: The Ludovico Technique. After enduring prison life for two years, Alex gets a job as an assistant to the prison chaplain. He feigns an interest in religion and amuses himself by reading the Bible for its lurid descriptions of "the old yahoodies (Jews) tolchocking (beating) each other" and imagining himself taking part in "the nailing-in" (the Crucifixion of Jesus). Alex learns of his ex-droog Georgie's death by an intended victim during a botched robbery. He also hears about an experimental rehabilitation programme called "the Ludovico technique", which promises that the prisoner will be released upon completion of the two-week treatment and, as a result, will not commit any crimes afterwards. The prison chaplain warns against it, arguing that moral choice is necessary to humanity — a theme introduced earlier during the home invasion scene, when Alex reads a passage from the victimised husband's work in progress. After speaking out in line when being reviewed, Alex is selected to become the subject in the first full-scale trial of the Ludovico Technique. The technique itself is a form of aversion therapy, in which Alex is given a drug that induces extreme nausea while being forced to watch graphically violent films for two weeks. Strapped into a seat before a large screen, Alex is forced to watch an unrelenting series of violent acts. During the sessions, Alex begins to realise that not only the violent acts but the music on the soundtrack is triggering his nausea attacks (Kubrick's film version narrows this down so that only Beethoven's Ninth Symphony has this effect.) Alex pleads with the supervising doctors to remove the music, crying that it is a sin to take away his love of music and adding that "Ludwig Van" did nothing wrong and "only made music", claiming that it was wrong to use the composer in that way, but they refuse, saying that it is for his own good and that the music may be the "punishment element". By the end of the treatment, Alex is unable to listen to his favourite classical pieces without experiencing nausea and distress. A few weeks later, Alex is presented to an audience of prison and government officials as a successfully rehabilitated inmate and potential member of society. Alex's conditioning makes him unable to defend himself against a pummelling bully and cripples him with nausea when the sight of a scantily clad woman arouses his predatory sexual impulses. The prison chaplain rises to denounce the treatment and accuses the state of stripping Alex of the ability to choose good over evil. "Padre, these are subtleties", a government official replies. "The point is that it works". And so Alex is released into society.
The Ludovico treatment leaves him ill when he attempts violence, so he is powerless. Alex returns home joyful at the thought of starting afresh, but finds that his parents have rented out his room to a lodger named Joe, essentially "replacing" their son. Alex runs into old victims, and is powerless when they seek their revenge. Despondently wandering, Alex stops at the Korova Milk Bar and drinks synthemesc-laced milk, as opposed to his usual drencrom-laced milk. He visits the music store, but the technique has rendered him incapable of listening to his beloved classical music. Alex decides to commit suicide, yet is unable to because the technique prevents him from committing any act of violence, including against himself. In the public library, Alex is quickly recognised by the elderly librarian whom he had beaten up with his droogs in chapter one. With his friends, the librarian attacks and beats Alex. The police (summoned by the librarian) turn out to be Dim and Billyboy. Taking advantage of their positions, they take Alex to the town's edge, beat him, and leave him for dead. Alex wanders in a daze through the countryside until he collapses at the door of an isolated cottage. Too late he realises this is the home he and his droogs invaded at the start of the book. He is taken in by F. Alexander, the husband of the woman the droogs gang-raped; Mr. Alexander doesn't recognise Alex because the droogs were wearing masks during the assault. We learn that Mrs. Alexander died of the injuries inflicted during the rape, and her husband has decided to continue "where her fragrant memory persists" despite the horrid memories. Alex has been careless with words during his time in Mr. Alexander's care, and the writer begins to suspect they have met before. Mr. Alexander recognises Alex from newspaper publicity about the behaviour modification treatment, and sees an opportunity to use him as a political weapon by turning him into a poster child for the victims of fascism. One of Mr. Alexander's political activist friends takes Alex aside and puts the question to him bluntly: Alex, cornered, makes a non-denial denial by saying "Lord knows I've suffered". "We'll speak no more of it", the friend assures him, but later on Alex is taken to another house, locked into a room, and tormented with classical music, triggering the maddening effect of the Ludovico treatment. Driven to insanity by the music, Alex jumps from his bedroom window in an attempt to end his life. Alex wakes up in a hospital, where he learns that the government, trying to reverse the bad publicity it incurred in the wake of Alex's suicide attempt, has reversed the effects of the Ludovico treatment. Mr. Alexander has been incarcerated in a mental institution, "for his own protection and for yours," Alex is told. In return for agreeing to cooperate with the powers that be, Alex is promised a cushy job at high salary. His parents offer to take him back in, and Alex happily ponders returning to his life of ultra-violence. In the final chapter, Alex finds himself back at the milkbar. He is half-heartedly preparing for yet another night of crime with a new trio of droogs, who are bemused at the discovery of a photograph of a baby in Alex's pocket. After watching them beat an innocent stranger walking home with a newspaper, Alex begins to feel bored with his life of violence. He abandons the gang, then has a chance encounter with his old droog Pete, who has reformed and married. Alex acknowledges to the reader that the reason he was carrying a photograph of a baby is because he would like a son of his own. He begins contemplating giving up crime to become a productive member of society and start a family, while reflecting on the notion that his own children could be just as destructive as he was himself. Omission of the final chapter. The book has three parts of seven chapters each. Burgess has stated that the total of 21 chapters was an intentional nod to the age of 21 being recognised as a milestone in human maturation. The 21st chapter was omitted from the editions published in the United States prior to 1986. In the introduction to the updated American text (these newer editions include the missing 21st chapter), Burgess explains that when he first brought the book to an American publisher, he was told that U.S. audiences would never go for the final chapter, in which Alex sees the error of his ways, decides he has lost all energy for and thrill from violence and resolves to turn his life around (a slow-ripening but classic moment of metanoia—the moment at which one's protagonist realises that everything he thought he knew was wrong). At the American publisher's insistence, Burgess allowed their editors to cut the redeeming final chapter from the U.S. version, so that the tale would end on a note of bleak despair, with young Alex succumbing to his darker nature—an ending which the publisher insisted would be 'more realistic' and appealing to a U.S. audience. The film adaptation, directed by Stanley Kubrick, is based on this "badly flawed" (Burgess' words, ibid.) American edition of the book. Kubrick called Chapter 21 "an extra chapter" and claimed that he had not read the original version until he had virtually finished the screenplay, and that he had never given serious consideration to using it.
Burgess wrote that the title was a reference to an old Cockney expression, "as queer as a clockwork orange".¹ Due to his time serving in the British Colonial Office in Malaysia, Burgess thought that the phrase could be used punningly to refer to a mechanically responsive (clockwork) human ("orang", Malay for "man"). Burgess wrote an introduction to the 1986 edition, titled "A Clockwork Orange Resucked", that a creature who can only perform good or evil is "a clockwork orange—meaning that he has the appearance of an organism lovely with colour and juice, but is in fact only a clockwork toy to be wound up by God or the Devil; or the "almighty state". In his essay "Clockwork Oranges" ², Burgess asserts that "this title would be appropriate for a story about the application of Pavlovian or mechanical laws to an organism which, like a fruit, was capable of colour and sweetness". This title alludes to the protagonist's positively conditioned responses to feelings of evil which prevent the exercise of his free will.
"A Clockwork Orange" is written using a narrative first-person singular perspective of a seemingly biased and unreliable narrator. The protagonist, Alex, never justifies his actions in the narration, giving a sense that he is somewhat sincere; a narrator who, as unlikeable as he may attempt to seem, evokes pity from the reader by telling of his unending suffering, and later through his realization that the cycle will never end. Alex's perspective is effective in that the way that he describes events is easy to relate to, even if the situations themselves are not.
The book, narrated by Alex, contains many words in a slang argot which Burgess invented for the book, called Nadsat. It is a mix of modified Slavic words, rhyming slang, derived Russian (like "baboochka"), and words invented by Burgess himself. For instance, these terms have the following meanings in Russian - 'droog' means 'friend'; 'korova' means 'cow'; 'golova' (gulliver) means 'head'; 'malchick' or 'malchickiwick' means 'boy'; 'soomka' means 'sack' or 'bag'; 'Bog' means 'God'; 'khorosho' (horrorshow) means 'good', 'prestoopnick' means 'criminal'; 'rooka' is 'hand', 'cal' is 'crap', 'veck' ('chelloveck') is 'man' or 'guy'; 'litso' is 'face'; 'malenky' is 'little'; and so on. One of Alex's doctors explains the language to a colleague as "Odd bits of old rhyming slang; a bit of gypsy talk, too. But most of the roots are Slav propaganda. Subliminal penetration." Some words are not derived from anything, but merely easy to guess, e.g. 'in-out, in-out' or 'the old in-out' means sexual intercourse. 'Cutter', however, means money, because 'cutter' rhymes with 'bread-and-butter'; this is rhyming slang, which is intended to be impenetrable to outsiders (especially eavesdropping policemen). In the first edition of the book, no key was provided, and the reader was left to interpret the meaning from the context. In his appendix to the restored edition, Burgess explained that the slang would keep the book from seeming dated, and served to muffle "the raw response of pornography" from the acts of violence. Furthermore, in a novel where a form of brainwashing plays a role, the narrative itself brainwashes the reader into understanding Nadsat. The term "ultraviolence", referring to excessive and/or unjustified violence, was coined by Burgess in the book, which includes the phrase "do the ultra-violent". The term's association with aesthetic violence has led to its use in the media. In fact, the second-highest difficulty level in the first-person shooter "Doom" is named "Ultra Violence."
In 1985, Burgess published the book "Flame into Being: The Life and Work of D. H. Lawrence" (Heinemann, London), and while discussing "Lady Chatterley's Lover" in the concluding chapter, he compared that novel's notoriety with "A Clockwork Orange": "We all suffer from the popular desire to make the known notorious. The book I am best known for, or only known for, is a novel I am prepared to repudiate: written a quarter of a century ago, a "jeu d'esprit" knocked off for money in three weeks, it became known as the raw material for a film which seemed to glorify sex and violence. The film made it easy for readers of the book to misunderstand what it was about, and the misunderstanding will pursue me till I die. I should not have written the book because of this danger of misinterpretation, and the same may be said of Lawrence and "Lady Chatterley's lover"." Awards and nominations and rankings. The novel was chosen by "Time" Magazine as one of the 100 best English-language novels from 1923 to 2005.
After Kubrick's film was released, Burgess wrote a "Clockwork Orange" stage play. In it, Dr. Branom defects from the psychiatric clinic when he grasps that the aversion treatment has destroyed Alex's ability to enjoy music. The play restores the novel's original ending. In 1988, a German adaptation of "Clockwork Orange" at the intimate theatre of Bad Godesberg featured a musical score by the German punk rock band Die Toten Hosen which, combined with orchestral clips of Beethoven's Ninth Symphony and "other dirty melodies" (so stated by the subtitle), was released on the album Ein kleines bisschen Horrorschau. The track "Hier kommt Alex" became one of the band's signature songs. In February 1990, another musical version was produced at the Barbican Theatre in London by the Royal Shakespeare Company. Titled 'A Clockwork Orange:2004', it received mostly negative reviews, with John Peter of The Sunday Times of London calling it "only an intellectual 'Rocky Horror Show' " and John Gross of The Sunday Telegraph calling it "a clockwork lemon." Even Burgess himself, who wrote the script based on his novel, was disappointed. According to The Evening Standard, he called the score, written by Bono and the Edge of the rock group U2, "neo-wallpaper." Burgess had originally worked alongside the director of the production, Ron Daniels, and envisioned a musical score that was entirely classical. Unhappy with the decision to abandon that score, he heavily criticised the band's experimental mix of Hip Hop, liturgical and gothic music. Lise Hand of The Irish Independent reported U2's The Edge as saying that Burgess's original conception was "a score written by a novelist rather than a songwriter". Calling it "meaningless glitz", Jane Edwardes of 20/20 Magazine said that watching this production was "like being invited to an expensive French Restaurant - and being served with a Big Mac". In 2001, UNI Theatre (Mississauga, Ontario) presented the Canadian premiere of the play under the direction of. In 2002, Godlight Theatre Company presented the New York Premiere adaptation of Anthony Burgess's 'A Clockwork Orange' at Manhattan Theatre Source. The production went on to play at the SoHo Playhouse (2002), Ensemble Studio Theatre (2004), 59E59 Theaters (2005) and the Edinburgh Festival Fringe (2005). While at Edinburgh, the production received rave reviews from the press while playing to sold-out audiences. The production was directed by Godlight's Artistic Director, Joe Tantalo. In 2003, Los Angeles director Brad Mays and the Ark Theatre Company staged a controversial multi-media adaptation of "A Clockwork Orange", which was named "Pick Of The Week" by the LA Weekly and nominated for three of the 2004 LA Weekly Theater Awards: Direction, Revival Production (of a 20th-century work), and Leading Female Performance. This inventive production utilised three separate video streams outputted to seven onstage video monitors - six 19 inch and one 40 inch. In order to preserve the first-person narrative of the book, a pre-recorded video stream of Alex, "your humble narrator", was projected onto the 40 inch monitor, thereby freeing the onstage character during passages which would have been awkward or impossible to sustain in the breaking of the fourth wall. According to the LA Weekly, "Mays' visceral, fast-paced multimedia show brings into stark relief the Freudian struggle between the primal self and the civilised self for domination over the human spirit. The director deftly conveys the horror of violence by subjecting the audience to an onslaught of images of war, torture and hardcore porn projected on seven TV screens."
Amsterdam (; Dutch) is the capital and largest city of the Netherlands, located in the province of North Holland in the west of the country. The city, which had a population (including suburbs) of 1.36 million on 1 January 2008, comprises the northern part of the Randstad, the sixth-largest metropolitan area in Europe, with a population of around 6.7 million. Its name is derived from "Amstellerdam", indicative of the city's origin: a dam in the river Amstel. Settled as a small fishing village in the late 12th century, Amsterdam became one of the most important ports in the world during the Dutch Golden Age, a result of its innovative developments in trade. During that time, the city was the leading centre for finance and diamonds. In the 19th and 20th centuries, the city expanded, and many new neighbourhoods and suburbs were formed. The city is the financial and cultural capital of the Netherlands. Many large Dutch institutions have their headquarters there, and 7 of the world's top 500 companies, including Philips and ING, are based in the city. The Amsterdam Stock Exchange, the oldest stock exchange in the world is located in the city centre. Amsterdam's main attractions, including its historic canals, the Rijksmuseum, the Van Gogh Museum, Hermitage Amsterdam, Anne Frank House, its red-light district, and its many cannabis coffee shops draw more than 3.66 million international visitors annually.
The earliest recorded use of the name "Amsterdam" is from a certificate dated 27 October 1275, when the inhabitants, who had built a bridge with a dam across the Amstel, were exempted from paying a bridge toll by Count Floris V. The certificate describes the inhabitants as "homines manentes apud Amestelledamme" (people living near "Amestelledamme"). By 1327, the name had developed into "Aemsterdam". Amsterdam's founding is relatively recent compared with much older Dutch cities such as Nijmegen, Rotterdam, and Utrecht. In October 2008, historical geographer Chris de Bont suggested that the land around Amsterdam was being reclaimed as early as the late 10th century. This does not necessarily mean that there was already a settlement then since reclamation of land may not have been for farming—it may have been for peat, used as fuel. Amsterdam was granted city rights in either 1300 or 1306. From the 14th century on, Amsterdam flourished, largely because of trade with the Hanseatic League. In 1345, an alleged Eucharistic miracle in the Kalverstraat rendered the city an important place of pilgrimage until the adoption of the Protestant faith. The "Stille Omgang"—a silent procession in civil attire—is today a remnant of the rich pilgrimage history. In the 16th century, the Dutch rebelled against Philip II of Spain and his successors. The main reasons for the uprising were the imposition of new taxes, the tenth penny, and the religious persecution of Protestants by the Spanish Inquisition. The revolt escalated into the Eighty Years' War, which ultimately led to Dutch independence. Strongly pushed by Dutch Revolt leader William the Silent, the Dutch Republic became known for its relative religious tolerance. Jews from the Iberian Peninsula, Huguenots from France, prosperous merchants and printers from Flanders, and economic and religious refugees from the Spanish-controlled parts of the Low Countries found safety in Amsterdam. The influx of Flemish printers and the city's intellectual tolerance made Amsterdam a centre for the European free press. The 17th century is considered Amsterdam's "Golden Age", during which it became the wealthiest city in the world. Ships sailed from Amsterdam to the Baltic Sea, North America, and Africa, as well as present-day Indonesia, India, Sri Lanka, and Brazil, forming the basis of a worldwide trading network. Amsterdam's merchants had the largest share in both the Dutch East India Company and the Dutch West India Company. These companies acquired overseas possessions that later became Dutch colonies. Amsterdam was Europe's most important point for the shipment of goods and was the leading Financial Centre of the world. In 1602, the Amsterdam office of the Dutch East India Company became the world's first stock exchange by trading in its own shares. Amsterdam lost over 10% of its population to plague in 1623–1625, and again in 1635–1636, 1655, and 1664. Nevertheless, the population of Amsterdam rose in the 17th century (largely through immigration) from 50,000 to 200,000. Amsterdam's prosperity declined during the 18th and early 19th centuries. The wars of the Dutch Republic with England and France took their toll on Amsterdam. During the Napoleonic Wars, Amsterdam's significance reached its lowest point, with Holland being absorbed into the French Empire. However, the later establishment of the United Kingdom of the Netherlands in 1815 marked a turning point. The end of the 19th century is sometimes called Amsterdam's second Golden Age. New museums, a train station, and the Concertgebouw were built; in this same time, the Industrial Revolution reached the city. The Amsterdam-Rhine Canal was dug to give Amsterdam a direct connection to the Rhine, and the North Sea Canal was dug to give the port a shorter connection to the North Sea. Both projects dramatically improved commerce with the rest of Europe and the world. In 1906, Joseph Conrad gave a brief description of Amsterdam as seen from the seaside, in "The Mirror of the Sea". Shortly before the First World War, the city began expanding, and new suburbs were built. Even though the Netherlands remained neutral in this war, Amsterdam suffered a food shortage, and heating fuel became scarce. The shortages sparked riots in which several people were killed. These riots are known as the "Aardappeloproer" (Potato rebellion). People started looting stores and warehouses in order to get supplies, mainly food. Germany invaded the Netherlands on 10 May 1940 and took control of the country. Some Amsterdam citizens sheltered Jews, thereby exposing themselves and their families to the high risk of being imprisoned or sent to concentration camps. More than 100,000 Dutch Jews were deported to Nazi concentration camps. Perhaps the most-famous deportee was the young Jewish girl Anne Frank, who died in the Bergen-Belsen concentration camp. At the end of the Second World War, communication with the rest of the country broke down, and food and fuel became scarce. Many citizens traveled to the countryside to forage. Dogs, cats, raw sugar beets, and Tulip bulbs—cooked to a pulp—were consumed to stay alive. Most of the trees in Amsterdam were cut down for fuel, and all the wood was taken from the apartments of deported Jews. Many new suburbs, such as Osdorp, Slotervaart, "Slotermeer", and "Geuzenveld", were built in the years after the Second World War. These suburbs contained many public parks and wide, open spaces, and the new buildings provided improved housing conditions with larger and brighter rooms, gardens, and balconies. Because of the war and other incidents of the 20th century, almost the entire city centre had fallen into disrepair. As society was changing, politicians and other influential figures made plans to redesign large parts of it. There was an increasing demand for office buildings and new roads as the automobile became available to most common people. A metro started operating in 1977 between the new suburb of Bijlmer and the centre of Amsterdam. Further plans were to build a new highway above the metro to connect the Central Station and city centre with other parts of the city. The incorporated large-scale demolitions began in Amsterdam's formerly Jewish neighbourhood. Smaller streets, such as the "Jodenbreestraat", were widened and saw almost all of their houses demolished. During the destruction's peak, the "Nieuwmarktrellen" (Nieuwmarkt riots) broke out, where people expressed their fury about the demolition caused by the restructuring of the city. As a result, the demolition was stopped, and the highway was never built, with only the metro being finished. Only a few streets remained widened. The new city hall was built on the almost completely demolished "Waterlooplein". Meanwhile, large private organisations, such as "Stadsherstel Amsterdam", were founded with the aim of restoring the entire city centre. Although the success of this struggle is visible today, efforts for further restoration are still ongoing. The entire city centre has reattained its former splendor and, as a whole, is now a protected area. Many of its buildings have become monuments, and plans exist to make the "Grachtengordel" (Herengracht, Keizersgracht, and Prinsengracht) a UNESCO World Heritage Site.
Amsterdam is part of the province of North-Holland and is located in the west of the Netherlands next to the provinces of Utrecht and Flevoland. The river Amstel terminates in the city centre and connects to a large number of canals that eventually terminate in the IJ. Amsterdam is situated 2 metres above sea level. The surrounding land is flat as it is formed of large polders. To the southwest of the city lies a man-made forest called "het Amsterdamse Bos". Amsterdam is connected to the North Sea through the long North Sea Canal. Amsterdam is intensely urbanized, as is the Amsterdam metropolitan area surrounding the city. Comprising 219.4 square kilometres of land, the city proper has 4,457 inhabitants per km2 and 2,275 houses per km2. Parks and nature reserves make up 12% of Amsterdam's land area.
Amsterdam has a cool oceanic climate (Köppen climate classification "Cfb"), strongly influenced by its proximity to the North Sea to the west, with prevailing north-western winds and gales. Winter temperatures are mild, seldom below. Amsterdam, as well as most of North-Holland province, lies in USDA Hardiness zone 9, the northernmost such occurrence in continental Europe. Frosts mainly occur during spells of easterly or northeasterly winds from the inner European continent, from Scandinavia, Russia, or even Siberia. Even then, because Amsterdam is surrounded on three sides by large bodies of water, as well as enjoying a significant heat island effect, nights rarely fall below, while it could easily be in Hilversum, 25 kilometres southeast. Summers are moderately warm but rarely hot. The average daily high in August is, and or higher is only measured on average on 3 days, placing Amsterdam in AHS Heat zone 2. Days with measurable precipitation are common, on average 175 days per year. Nevertheless, Amsterdam's average annual precipitation is less than 760 mm. Most of this precipitation is in the form of protracted drizzle or light rain, making cloudy and damp days common during the cooler months of October through March. Only the occasional European windstorm brings significant rain in a short period of time, requiring it to be pumped out to higher ground or to the seas around the city.
Amsterdam fans out south from the Amsterdam Centraal railway station. The Damrak is the main street and leads into the street Rokin. The oldest area of the town is known as "de Wallen" (the quays). It lies to the east of Damrak and contains the city's famous red light district. To the south of de Wallen is the old Jewish quarter of Waterlooplein. The 17th century girdle of concentric canals, known as the "Grachtengordel", embraces the heart of the city where homes have interesting gables. Beyond the Grachtengordel are the former working class areas of Jordaan and de Pijp. The Museumplein with the city's major museums, the Vondelpark, a 19th century park named after the Dutch writer Joost van den Vondel, and the Plantage neighbourhood, with the zoo, are also located outside the Grachtengordel. Several parts of the city and the surrounding urban area are polders. This can be recognized by the suffix "-meer" which means "lake", as in Aalsmeer, Bijlmermeer, Haarlemmermeer, and Watergraafsmeer.
The Amsterdam canal system is the result of conscious city planning. In the early 17th century, when immigration was at a peak, a comprehensive plan was developed that was based on four concentric half-circles of canals with their ends emerging at the IJ bay. Known as the "Grachtengordel", three of the canals were mostly for residential development: the "Herengracht" (Gentlemen's or more accurately Patricians' Canal), "Keizersgracht" (Emperor's Canal), and "Prinsengracht" (Prince's Canal). The fourth and outermost canal, the "Singelgracht" (not to be confused with the older "Singel"), served the purposes of defense and water management. The defenses took the form of a moat and earthen dikes, with gates at transit points, but otherwise no masonry superstructures. The original plans have been lost, so historians, such as Ed Taverne, need to speculate on the original intentions: it is thought that the considerations of the layout were purely practical and defensive rather than ornamental. Construction started in 1613 and proceeded from west to east, across the breadth of the layout, like a gigantic windshield wiper as the historian Geert Mak calls it — and "not" from the centre outwards, as a popular myth has it. The canal construction in the southern sector was completed by 1656. Subsequently, the construction of residential buildings proceeded slowly. The eastern part of the concentric canal plan, covering the area between the Amstel river and the IJ bay, has never been implemented. In the following centuries, the land was used for parks, senior citizens' homes, theaters, other public facilities, and waterways without much planning. Over the years, several canals have been filled in, becoming streets or squares, such as the Nieuwezijds Voorburgwal and the Spui.
After the development of Amsterdam's canals in the 17th century, the city did not grow beyond its borders for two centuries. During the 19th century, Samuel Sarphati devised a plan based on the grandeur of Paris and London at that time. The plan envisaged the construction of new houses, public buildings and streets just outside the "grachtengordel". The main aim of the plan, however, was to improve public health. Although the plan did not expand the city, it did produce some of the largest public buildings to date, like the "Paleis voor Volksvlijt". Following Sarphati, "Van Niftrik" and "Kalff" designed an entire ring of 19th century neighbourhoods surrounding the city’s centre. Most of these neighbourhoods became home to the working class. In response to overcrowding, two plans were designed at the beginning of the 20th century which were very different from anything Amsterdam had ever seen before: "Plan Zuid", designed by the architect Berlage, and "West". These plans involved the development of new neighbourhoods consisting of "housing blocks" for all social classes. After the Second World War, large new neighbourhoods were built in the western, southeastern, and northern parts of the city. These new neighbourhoods were built to relieve the city's shortage of living space and give people affordable houses with modern conveniences. The neighbourhoods consisted mainly of large housing blocks situated among green spaces, connected to wide roads, making the neighbourhoods easily accessible by motor car. The western suburbs which were built in that period are collectively called the "Westelijke Tuinsteden". The area to the southeast of the city built during the same period is known as the "Bijlmer".
Amsterdam has a rich architectural history. The oldest building in Amsterdam is the Oude Kerk (Old Church), at the heart of the Wallen, consecrated in 1306. The oldest wooden building is "het Houten Huys" at the Begijnhof. It was constructed around 1425 and is one of only two existing wooden buildings. It is also one of the few examples of Gothic architecture in Amsterdam. In the 16th century, wooden buildings were razed and replaced with brick ones. During this period, many buildings were constructed in the architectural style of the Renaissance. Buildings of this period are very recognizable, since they have a façade which ends at the top in the shape of a stairway. This is, however, the common Dutch Renaissance style. Amsterdam quickly developed its own Renaissance architecture. These buildings were built according to the principles of the architect Hendrick de Keyser. One of the most striking buildings designed by Hendrick de Keyer is the Westerkerk. In the 17th century baroque architecture became very popular, as it was elsewhere in Europe. This roughly coincided with Amsterdam’s Golden Age. The leading architects of this style in Amsterdam were Jacob van Campen, Philip Vingboons and Daniel Stalpaert. Philip Vingboons designed splendid merchants' houses throughout the city. A famous building in baroque style in Amsterdam is the Royal Palace on Dam Square. Throughout the 18th century, Amsterdam was heavily influenced by French culture. This is reflected in the architecture of that period. Around 1815, architects broke with the baroque style and started building in different neo-styles. Most Gothic style buildings date from that era and are therefore said to be built in a neo-gothic style. At the end of the 19th century, the Jugendstil or Art Nouveau style became popular and many new buildings were constructed in this architectural style. Since Amsterdam expanded rapidly during this period, new buildings adjacent to the city centre were also built in this style. The houses in the vicinity of the Museum Square in Amsterdam Oud-Zuid are an example of Jugendstil. The last style that was popular in Amsterdam before the modern era was Art Deco. Amsterdam had its own version of the style, which was called the Amsterdamse School. Whole districts were built this style, such as the "Rivierenbuurt". A notable feature of the façades of buildings designed in Amsterdamse School is that they are highly decorated and ornate, with oddly shaped windows and doors. The old city centre is the focal point of all the architectural styles before the end of the 19th century. Jugendstil and Art Deco are mostly found outside the city’s centre in the neighbourhoods built in the early 20th century, although there are also some striking examples of these styles in the city centre. Most historic buildings in the city centre and nearby are houses, such as the famous merchants' houses lining the canals.
The administration of the municipality of Amsterdam is divided into 15 boroughs or "stadsdelen"; the central one, Centrum, being circled by Westerpark, Bos en Lommer, De Baarsjes, Oud-West, Oud-Zuid, Watergraafsmeer, Zeeburg and Amsterdam-Noord, with the six outer boroughs creating a further encirclement. On 1 May 2010, the number of boroughs will be reduced to 8 (Centrum, Noord, Oost, Zuid, West, Nieuw-West, Zuidoost and Westpoort).
"Amsterdam" is usually understood to refer to the municipality of Amsterdam. Colloquially, some areas within the municipality, such as the village of Durgerdam, may not be considered part of Amsterdam. Statistics Netherlands uses three other definitions of Amsterdam: metropolitan agglomeration Amsterdam ("Grootstedelijke Agglomeratie Amsterdam", not to be confused with "Grootstedelijk Gebied Amsterdam", a synonym of "Groot Amsterdam"), Greater Amsterdam ("Groot Amsterdam", a COROP region) and the urban region Amsterdam ("Stadsgewest Amsterdam"). These definitions are not synonymous with the terms urban area and metropolitan area, which are commonly used in English speaking countries for the purpose of defining large conurbations. The Amsterdam Department for Research and Statistics uses a fourth conurbation, namely the City region Amsterdam. This region is similar to Greater Amsterdam but includes the municipalities Zaanstad and Wormerland. It excludes Graft-De Rijp. The smallest of these areas is the municipality, with a population of 742,981 in 2006. The metropolitan agglomeration had a population of 1,021,870 in 2006. It includes the municipalities of Zaanstad, Wormerland, Oostzaan, Diemen and Amstelveen only, as well as the municipality of Amsterdam. Greater Amsterdam includes 15 municipalities, and had a population of 1,211,503 in 2006. Though much larger in area, the population of this area is only slightly larger, because the definition excludes the relatively populous municipality of Zaanstad. The largest area by population, the Amsterdam Metropolitan Area (Dutch: Metropoolregio Amsterdam), has a population of 2,22 million. It includes for instance Zaanstad, Wormerveer, Muiden, Abcoude, Haarlem, Almere and Lelystad but excludes Graft De Rijp. Amsterdam is also part of the conglomerate metropolitan area Randstad, with a total population of 6,659,300 inhabitants.
As with all Dutch municipalities, Amsterdam is governed by a mayor, aldermen, and the municipal council. However, unlike most other Dutch municipalities, Amsterdam is subdivided into fifteen "stadsdelen" (boroughs), a system that was implemented in the 1980s to improve local governance. The stadsdelen are responsible for many activities that had previously been run by the central city. Fourteen of these have their own council, chosen by a popular election. The fifteenth, Westpoort, covers the harbour of Amsterdam, has very few residents, and is governed by the central municipal council. Local decisions are made at borough level, and only affairs pertaining to the whole city, such as major infrastructure projects, are handled by the central city council. The borough system is currently being revised, and the number of boroughs will likely be reduced to seven in the following years.
The present version of the Dutch constitution mentions "Amsterdam" and "capital" only in one place, chapter 2, article 32: The king's confirmation by oath and his coronation take place in "the capital Amsterdam" ("de hoofdstad Amsterdam"). spoke of "the city of Amsterdam" ("de stad Amsterdam"), without mention of capital. In any case, the seat of the government, parliament and supreme court of the Netherlands is (and always has been, with the exception of a brief period between 1808 and 1810) located at The Hague. Foreign embassies are also in The Hague. The capital of North Holland is Haarlem.
The coat of arms of Amsterdam is composed of several historical elements. First and centre are three St Andrew's crosses, aligned in a vertical band on the city's shield (although Amsterdam's patron saint was Saint Nicholas). These St Andrew's crosses can also be found on the cityshields of neighbours Amstelveen and Ouder-Amstel. This part of the coat of arms is the basis of the flag of Amsterdam, flown by the city government, but also as civil ensign for ships registered in Amsterdam. Second is the Imperial Crown of Austria. In 1489, out of gratitude for services and loans, Maximilian I awarded Amsterdam the right to adorn its coat of arms with the king's crown. Then, in 1508, this was replaced with Maximilian's imperial crown when he was crowned Holy Roman Emperor. In the early years of the 17th century, Maximilian's crown in Amsterdam's coat of arms was again replaced, this time with the crown of Emperor Rudolph II, a crown that became the Imperial Crown of Austria. The lions date from the late 16th century, when city and province became part of the Republic of the Seven United Netherlands. Last came the city's official motto: "Heldhaftig, Vastberaden, Barmhartig" ("Valiant, Determined, Compassionate"), bestowed on the city in 1947 by Queen Wilhelmina, in recognition of the city's bravery during the Second World War.
Amsterdam is the financial and business capital of the Netherlands. Amsterdam is currently one of the best European cities in which to locate an International Business. It is ranked fifth in this category and is only surpassed by London, Paris, Frankfurt and Barcelona. Many large Dutch corporations and banks have their headquarters in Amsterdam, including ABN AMRO, Akzo Nobel, Heineken International, ING Group, Ahold, TomTom, "Delta Lloyd Group" and Philips. KPMG International's global headquarters is located in nearby Amstelveen. Though many small offices are still located on the old canals, companies are increasingly relocating outside the city centre. The Zuidas (English: South Axis) has become the new financial and legal hub. The five largest law firms of the Netherlands, a number of Dutch subsidiaries of large consulting firms like Boston Consulting Group and Accenture, and the World Trade Center Amsterdam are also located in Zuidas. There are three other smaller financial districts in Amsterdam. The first is the area surrounding Amsterdam Sloterdijk railway station, where several newspapers like De Telegraaf have their offices. Also, the municipal public transport company ("Gemeentelijk Vervoersbedrijf") and the Dutch tax offices ("Belastingdienst") are located there. The second Financial District is the area surrounding Amsterdam Arena. The third is the area surrounding Amsterdam Amstel railway station. The tallest building in Amsterdam, the Rembrandt Tower, is situated there, as is the headquarters of Philips. The Amsterdam Stock Exchange (AEX), nowadays part of Euronext, is the world's oldest stock exchange and is one of Europe's largest bourses. It is situated near Dam Square in the city's centre.
Amsterdam is one of the most popular tourist destinations in Europe, receiving more than 3.66 million international visitors annually. The number of visitors has been growing steadily over the past decade. This can be attributed to an increasing number of European visitors. Two thirds of these hotels are located in the city's centre. Hotels with 4 or 5 stars contribute 42% of the total beds available and 41% of the overnight stays in Amsterdam. The room occupation rate was 78% in 2006, up from 70% in 2005. The majority of tourists (74%) originate from Europe. The largest group of non-European visitors come from the United States, accounting for 14% of the total. Certain years have a theme in Amsterdam to attract extra tourists. For example, the year 2006 was designated "Rembrandt 400", to celebrate the 400th birthday of Rembrandt van Rijn. Some hotels offer special arrangements or activities during these years. The average number of guests per year staying at the four campsites around the city range from 12,000 to 65,000.
De Wallen, also known as Walletjes or Rosse Buurt, is a designated area for legalized prostitution and is Amsterdam's largest and most well known red-light district. This neighborhood has become a famous tourist attraction. It consists of a network of roads and alleys containing several hundred small, one-room apartments rented by female sex workers who offer their services from behind a window or glass door, typically illuminated with red lights. The area also has a number of sex shops, sex theatres, peep shows, a sex museum, a cannabis museum, and a number of coffee shops offering various cannabis products.
Shops in Amsterdam range from large department stores such as De Bijenkorf founded in 1870 and Maison de Bonneterie a Parisian style store founded in 1889, to small specialty shops. Amsterdam's high-end shops are found in the streets "Pieter Cornelisz Hooftstraat" and "Cornelis Schuytstraat", which are located in the vicinity of the Vondelpark. One of Amsterdam's busiest high streets is the narrow, medieval "Kalverstraat" in the heart of the city. Another shopping area is the "Negen Straatjes": nine narrow streets within the "Grachtengordel", the concentric canal system of Amsterdam. The Negen Straatjes differ from other shopping districts with the presence of a large diversity of privately owned shops. The city also features a large number of open-air markets such as the Albert Cuypmarkt, "Westermarkt", "Ten Katemarkt", and "Dappermarkt".
Fashion brands like G-star, Gsus, BlueBlood, Iris van Herpen, 10 feet and Warmenhoven & Venderbos, and fashion designers like Mart Visser, Viktor & Rolf, Sheila de Vries, Marlies Dekkers and Frans Molenaar are based in Amsterdam. Modelling agencies Elite Models, Touche models and Tony Jones have opened branches in Amsterdam. Supermodels Yfke Sturm, Doutzen Kroes and Kim Noorda started their careers in Amsterdam. Amsterdam has its garment centre in the World Fashion Center. Buildings which formerly housed brothels in the red light district have been converted to ateliers for young fashion designers.
In the 16th and 17th century non-Dutch immigrants to Amsterdam were mostly Huguenots, Flemings, Sephardi Jews and Westphalians. Huguenots came after the Edict of Fontainebleau in 1685, while the Flemish Protestants came during the Eighty Years' War. The Westphalians came to Amsterdam mostly for economic reasons – their influx continued through the 18th and 19th centuries. Before the Second World War, 10% of the city population was Jewish. The first mass immigration in the 20th century were by people from Indonesia, who came to Amsterdam after the independence of the Dutch East Indies in the 1940s and 1950s. In the 1960s guest workers from Turkey, Morocco, Italy and Spain emigrated to Amsterdam. After the independence of Suriname in 1975, a large wave of Surinamese settled in Amsterdam, mostly in the Bijlmer area. Other immigrants, including asylum seekers and illegal immigrants, came from Europe, America, Asia, and Africa. In the seventies and eighties, many 'old' Amsterdammers moved to 'new' cities like Almere and Purmerend, prompted by the third planological bill of the Dutch government. This bill promoted suburbanization and arranged for new developments in so called "groeikernen", literally "cores of growth". Young professionals and artists moved into neighbourhoods de Pijp and the Jordaan abandoned by these Amsterdammers. The non-Western immigrants settled mostly in the social housing projects in Amsterdam-West and the Bijlmer. Today, people of non-Western origin make up approximately one-third of the population of Amsterdam, and more than 50% of children. The largest religious group are Christians, who are divided between Roman Catholics and Protestants. The next largest religion is Islam, most of whose followers are Sunni. In 1578 the previously Roman Catholic city of Amsterdam joined the revolt against Spanish rule, late in comparison to other major northern Dutch cities. In line with Protestant procedure of that time, all churches were converted to Protestant worship. Calvinism became the dominant religion, and although Catholicism was not forbidden and priests allowed to serve, the Catholic hierarchy was prohibited. This led to the establishment of "schuilkerken", covert churches, behind seemingly ordinary canal side house fronts. One example is the current debate centre de Rode Hoed. A large influx of foreigners of many religions came to 17th-century Amsterdam, in particular Sefardic Jews from Spain and Portugal, Huguenots from France, and Protestants from the Southern Netherlands. This led to the establishment of many non-Dutch-speaking religious churches. In 1603, the first notification was made of Jewish religious service. In 1639, the first Jewish synagogue was consecrated. As they became established in the city, other Christian denominations used converted Catholic chapels to conduct their own services. The oldest English-language church congregation in the world outside the United Kingdom is found at the Begijnhof. Regular services there are still offered in English under the auspices of the Church of Scotland. The Huguenots accounted for nearly 20% of Amsterdam's inhabitants in 1700. Being Calvinists, they soon integrated into the Dutch Reformed Church, though often retaining their own congregations. Some, commonly referred by the moniker 'Walloon', are recognizable today as they offer occasional services in French. In the second half of the 17th century, Amsterdam experienced an influx of Ashkenazim, Jews from Central and Eastern Europe, which continued into the 19th century. Jews often fled the pogroms in those areas. The first Ashkenazi who arrived in Amsterdam were refugees from the Chmielnicki Uprising in Poland and the Thirty Years War. They not only founded their own synagogues, but had a strong influence on the 'Amsterdam dialect' adding a large Yiddish local vocabulary. Despite an absence of an official Jewish ghetto, most Jews preferred to live in the eastern part of the old medieval heart of the city. The main street of this Jewish neighborhood was the "Jodenbreestraat". The neighborhood comprised the "Waterlooplein" and the Nieuwmarkt. Buildings in this neighborhood fell into disrepair after the Second World War, and a large section of the neighbourhood was demolished during the construction of the subway. This led to riots, and as a result a small part of the old neighbourhood was saved. Catholic Churches in Amsterdam have been constructed since the restoration of the bishopric hierarchy in 1853. One of the principal architects behind the city's Catholic churches, Cuypers, was also responsible for the Amsterdam Central Station and the Rijksmuseum, which led to a refusal of Protestant King William III to open 'that monastery'. In 1924, the Roman Catholic Church of the Netherlands hosted the International Eucharistic Congress in Amsterdam, and numerous Catholic prelates visited the city, where festivities were held in churches and stadiums. Catholic processions on the public streets, however, were still forbidden under law at the time. Only in the twentieth century was Amsterdam's relation to Catholicism normalized, but despite its far larger population size, the Catholic clergy chose to place its bishopric seat of the city in the nearby provincial town of Haarlem. The most recent religious changes in Amsterdam have been influenced by large-scale immigration from former colonies. Immigrants from Suriname have introduced Evangelical Protestantism and Lutheranism, from the Hernhutter variety; Hinduism has been introduced mainly from Suriname; and several distinct branches of Islam have been brought from various parts of the world. Islam is now the largest non-Christian religion in Amsterdam. The large community of Ghanaian and Nigerian immigrants have established African churches, often in parking garages in the Bijlmer area, where many have settled. In addition, a broad array of other religious movements have established congregations, including Buddhism, Confucianism and Hinduism. Although the saying "Leven en laten leven" or "Live and let live" summarises the Dutch and especially the Amsterdam open and tolerant society, the increased influx of many races, religions, and cultures after the Second World War, has on a number of occasions strained social relations. With 176 different nationalities, Amsterdam is home to one of the widest varieties of nationalities of any city in the world.
Amsterdam is one of the most bicycle-friendly large cities in the world and is a centre of bicycle culture with good facilities for cyclists such as bike paths and bike racks. In 2006, there were about 465,000 bicycles in Amsterdam. Theft is widespread - in 2005, about 54,000 bicycles were stolen in Amsterdam. Bicycles are used by all socio-economic groups because of their convenience, Amsterdam's small size, the large number of bike paths, the flat terrain, and the arguable inconvenience of driving an automobile. A wide variety of bicycles are used, such as road bicycles, mountain bikes, racing bikes and even recumbent bikes, but the vast majority of bicycles are second-hand, older-model, heavy bikes with one gear and back-pedal coaster brakes. Bicycle traffic, and traffic in general, is relatively safe - in 2007, Amsterdam had a total of 18 traffic deaths, compared with 26 people murdered. In the city centre, driving a car is discouraged. Parking fees are expensive, and many streets are closed to cars or are one-way. The local government sponsors carsharing and carpooling initiatives such as "Autodelen" and "Meerijden.nu". Public transport in Amsterdam mainly consists of bus and tram lines, operated by Gemeentelijk Vervoerbedrijf, Connexxion and Arriva. Currently, there are 16 different tramlines. There are currently four metro lines, with a fifth line, the South line, under construction. Three free ferries carry pedestrians and cyclists across the IJ to Amsterdam-Noord, and two fare-charging ferries run east and west along the harbour. There are also water taxis, a water bus, a boat sharing operation, electric rental boats (Boaty) and canal cruises, that transport people along Amsterdam's waterways. The A10 ringroad surrounding the city connects Amsterdam with the Dutch national network of freeways. Interchanges on the A10 allow cars to enter the city by transferring to one of the eighteen "city roads", numbered S101 through to S118. These city roads are regional roads without grade separation, and sometimes without a central reservation. Most are accessible by cyclists. The S100 "Centrumring" is a smaller ringroad circumnavigating the city's centre. Amsterdam was intended in 1932 to be the hub, a kind of Kilometre Zero, of the highway system of the Netherlands, with freeways numbered one through eight planned to originate from the city. The outbreak of the Second World War and shifting priorities led to the current situation, where only roads A1, A2, and A4 originate from Amsterdam according to the original plan. The A3 road to Rotterdam was cancelled in 1970 in order to conserve the Groene Hart. Road A8, leading north to Zaandam and the A10 Ringroad were opened between 1968 and 1974. Besides the A1, A2, A4 and A8, several freeways, such as the A7 and A6, carry traffic mainly bound for Amsterdam. Amsterdam is served by ten stations of the Nederlandse Spoorwegen (Dutch Railways). Five are intercity stops: Sloterdijk, Zuid, Amstel, Bijlmer ArenA and Amsterdam Centraal. The stations for local services are: Lelylaan, RAI, Holendrecht, Muiderpoort and Science Park. Amsterdam Centraal is also an international train station. From the station there are regular services to destinations such as Austria, Belarus, Belgium, the Czech Republic, Denmark, France, Germany, Hungary, Poland, Russia and Switzerland. Among these trains are international trains of the Nederlandse Spoorwegen and the Thalys(Amsterdam-Brussels-Paris-Cologne), CityNightLine, and InterCityExpress. Eurolines has coaches from Amsterdam to destinations all over Europe. Amsterdam Airport Schiphol is less than 20 minutes by train from Amsterdam Central Station. It is the biggest airport in the Netherlands, the fifth largest in Europe, and the twelfth largest in the world in terms of passengers. It handles about 46 million passengers per year and is the home base of three airlines, KLM, transavia.com and Martinair. Schiphol was, in 2006, the third busiest airport in the world measured by international passengers.
Amsterdam has two universities: the University of Amsterdam (Universiteit van Amsterdam), and the VU University Amsterdam (Vrije Universiteit or "VU" - often referred to, in English, as "The Free"). Other institutions for higher education include an art school – Gerrit Rietveld Academie, the Hogeschool van Amsterdam, and the Amsterdamse Hogeschool voor de Kunsten. Amsterdam's International Institute of Social History is one of the world's largest documentary and research institutions concerning social history, and especially the history of the labour movement. Amsterdam's Hortus Botanicus, founded in the early 17th century, is one of the oldest botanical gardens in the world, with many old and rare specimens, among them the coffee plant that served as the parent for the entire coffee culture in Central and South America. Some of Amsterdam's primary schools base their teachings on particular pedagogic theories like the various Montessori schools. The biggest Montessori High School in Amsterdam is the Montessori Lyceum Amsterdam. Many schools, however, are based on religion. This used to be primarily Roman Catholicism and various Protestant denominations, but with the influx of Muslim immigrants there has been a rise in the number of Islamic schools. Jewish schools can be found in the southern suburbs of Amsterdam. Amsterdam is noted for having three independent grammar schools (Dutch: gymnasia), the Vossius Gymnasium, Barlaeus Gymnasium, and St. Ignatius Gymnasium, where a classical curriculum including Latin and classical Greek is taught. Though believed until recently by many to be an anachronistic and elitist concept that would soon die out, the gymnasia have recently experienced a revival, leading to the formation of a fourth and fifth grammar school in which the three aforementioned schools participate. Most secondary schools in Amsterdam offer a variety of different levels of education in the same school.
The housing market is heavily distorted by regulation. In Amsterdam, 55% of existing housing and 80% of new housing is owned by Housing Associations, which are Government sponsored entities and as such offer social housing, e.g. housing where rents are not set by the market, but directly or indirectly by the State, typically with subsidy of some kind. There is no right-to-buy with these properties, which leads to a remarkably large proportion of Amsterdam's housing market being rental. At the same time, the national Government is acting to increase the percentage of owner-occupied dwellings, by removing income tax from the part of the mortgage payment which is interest. This figure has risen by 125% over the last 15 years, along with a concomitant increase in personal debt, as house buyers seek to obtain the largest possible mortgage to maximize income tax relief gains. Due to the large proportion of the housing market being socialised, supply is unresponsive to demand, since prices largely are not set by the market (and so building is unprofitable) and also due to restrictive development laws. The uncontrolled segment of the housing market experiences shortage of supply, because so many properties belong to Housing Associations; as such, along with a large body of regulation which very strongly favours the tenant rather than the landlord (making letting risky) uncontrolled rent prices are very high and the number of properties available is low. A second consequence of the large number of subsidized rent Housing Association properties is the division of the renting population into 'haves' and 'have nots'; those able to gain entry into the subsidized rent market are favoured with unusually low rents. Those unable to do so are penalized with unusually high rents. Finally, Housing Associations have proved inefficient at developing new property, unable to meet even modest annual targets for new development, further increasing rental pressure. Squat properties are common throughout Amsterdam, due to property law strongly favouring tenants. A number of these squats have become well known, such as OT301, Vrankrijk (closed down by city government), and the Binnenpret, and several are now businesses, such as health clubs and licensed restaurants.
During the later part of the 16th century Amsterdam's Rederijkerskamer (Chamber of Rhetoric) organized contests between different Chambers in the reading of poetry and drama. In 1638, Amsterdam opened its first theatre. Ballet performances were given in this theatre as early as 1642. In the 18th century, French theatre became popular. While Amsterdam was under the influence of German music in the 19th century there were few national opera productions; the Hollandse Opera of Amsterdam was built in 1888 for the specific purpose of promoting Dutch opera. In the 19th century, popular culture was centred around the Nes area in Amsterdam (mainly vaudeville and music-hall). The metronome, one of the most important advances in European classical music, was invented here in 1812 by Dietrich Nikolaus Winkel. At the end of this century, the Rijksmuseum and were built. In 1888, the Concertgebouworkest was established. With the 20th century came cinema, radio and television. Though most studios are located in Hilversum and Aalsmeer, Amsterdam's influence on programming is very strong. Many people who work in the television industry live in Amsterdam. Also, the headquarters of SBS 6 is located in Amsterdam.
The most important museums of Amsterdam are located on "het Museumplein" (Museum Square), located at the southern side of the Rijksmuseum. It was created in the last quarter of the 19th century on the grounds of the former World Exposition. The northern part of the square is bordered by the very large Rijksmuseum. In front of the Rijksmuseum on the square itself is a long, rectangular, pond. This is transformed into an ice rink in winter. The western part of the square is bordered by the Van Gogh Museum, Stedelijk Museum, House of Bols Cocktail & Genever Experience and Coster Diamonds. The southern border of the Museum Square is the Van Baerlestraat, which is a major thoroughfare in this part of Amsterdam. The Concertgebouw is situated across this street from the square. To the east of the square are situated a number of large houses, one of which contains the American consulate. A parking garage can be found underneath the square, as well as a supermarket. "Het Museumplein" is covered almost entirely with a lawn, except for the northern part of the square which is covered with gravel. The current appearance of the square was realized in 1999, when the square was remodeled. The square itself is the most prominent site in Amsterdam for festivals and outdoor concerts, especially in the summer. Plans were made in 2008 to remodel the square again, because many inhabitants of Amsterdam are not happy with its current appearance. The Rijksmuseum possesses the largest and most important collection of classical Dutch art. It opened in 1885. Its collection consists of nearly one million objects. The artist most associated with Amsterdam is Rembrandt, whose work, and the work of his pupils, is displayed in the Rijksmuseum. Rembrandt's masterpiece the Nightwatch is one of top pieces of art of the museum. It also houses paintings from artists like Van der Helst, Vermeer, Frans Hals, Ferdinand Bol, Albert Cuijp, Van Ruysdael and Paulus Potter. Aside from paintings, the collection consists of a large variety of decorative art. This ranges from Delftware to giant dollhouses from the 17th century. The architect of the gothic revival building was P.J.H. Cuypers. At present, the museum is being expanded, renovated, and a new main entrance for the museum created. Only one wing of the Rijksmuseum is currently open to the public, with a selection of master pieces on display. The full museum will re-open in 2012 or 2013. Van Gogh lived in Amsterdam for a short while, so there is a dedicated to his early work. The museum is housed in one of the few modern buildings in this area of Amsterdam. The building was designed by Gerrit Rietveld. This building is where the permanent collection is displayed. A new building was added to the museum in 1999. This building, known as the performance wing, was designed by Japanese architect Kisho Kurokawa. Its purpose is to house temporary exhibitions of the museum. Some of Van Gogh's most famous paintings, like the Aardappeleters ("The Potato Eaters") and "Zonnenbloemen", are present in the collection. The Van Gogh museum is the most visited museum in Amsterdam. Next to the Van Gogh museum stands the Stedelijk Museum. This is Amsterdam's largest museum concerning modern art. The museum opened its doors at around the same time the Museum Square was created. The permanent collection consists of works of art from artists like Piet Mondriaan, Karel Appel, and Kazimir Malevich. This museum is also currently being renovated and expanded. The main entrance will be relocated from the Paulus Potterstraat to the Museum Square itself. It will be open again to public in 2009. Amsterdam contains many other museums throughout the city. They range from small museums such as the "Verzetsmuseum", the Anne Frank House, and the Rembrandthuis, to the very large, like the Tropenmuseum, Amsterdams Historisch Museum, and Joods Historisch Museum.
The Heineken Music Hall is a concert hall located near the Amsterdam ArenA. Its main purpose is to serve as a podium for pop concerts for big audiences. Many famous international artists have performed there. Two other notable venues, Paradiso and the Melkweg are located near the Leidseplein. Both focus on broad programming, ranging from indie rock to hip hop, R&B, and other popular genres. Other more subculturally focused music venues are OCCII, OT301, De Nieuwe Anita, Winston Kingdom. Jazz has a strong following in Amsterdam, with the Bimhuis being the premier venue.
Amsterdam has a world-class symphony orchestra, the Royal Concertgebouw Orchestra. Their home is the Concertgebouw, which is across the Van Baerlestraat from the Museum Square. It is considered by critics to be a concert hall with some of the best acoustics in the world. The building contains three halls, Grote Zaal, Kleine Zaal, and Spiegelzaal. Eight hundred concerts per year are performed there for approximately 850,000 patrons. The opera house of Amsterdam is situated adjacent to the city hall. Therefore, the two buildings combined are often called the Stopera. This word is derived from the Dutch words "stadhuis" (city hall) and opera. This huge modern complex, opened in 1986, lies in the former Jewish neighbourhood at "Waterlooplein" next to the river Amstel. The "Stopera" is the homebase of De Nederlandse Opera, Het Nationale Ballet and the "Holland Symfonia". Muziekgebouw aan 't IJ is a concert hall, which is situated in the IJ near the central station. Its concerts perform mostly modern classical music. Located adjacent to it, is the "Bimhuis", a concert hall for improvised and Jazz music.
The main theatre building of Amsterdam is the Stadsschouwburg Amsterdam at the Leidseplein. It is the home base of the "Toneelgroep Amsterdam". The current building dates from 1894. Most plays are performed in the Grote Zaal (Great Hall). The normal programm of events encompasses all sorts of theatrical forms. The Stadsschouwburg is currently being renovated and expanded. The third theater space, to be operated jointly with next door Melkweg, will open in late 2009 or early 2010. Other theatres are: Royal Theatre Carré, Bellevue theatres, the Stopera and "de kleine comedie".
The Netherlands has a tradition of cabaret or kleinkunst, which combines music, storytelling, commentary, theatre and comedy. Cabaret dates back to the 1930s and artists like Wim Kan, Wim Sonneveld and Toon Hermans were pioneers of this form of art in the Netherlands. In Amsterdam is the Kleinkunstacademie (English: Cabaret Academy). Contemporary popular artists are Youp van 't Hek, Freek de Jonge, Herman Finkers, Hans Teeuwen, Theo Maassen, Javier Guzman, Herman van Veen, Najib Amhali, Raoul Heertje, Jörgen Raymann, De Vliegende Panters and Comedytrain. The English spoken comedy scene was established with the founding of Boom Chicago in 1993. They have an own theatre at Leidse Plein.
Amsterdam is famous for its vibrant and diverse nightlife. The two main nightlife areas are the Leidseplein and the Rembrandtplein. Amsterdam has many cafes. They range from large and modern to small and cozy. The typical "Bruine Kroeg" (brown cafe) breathe a more old fashioned atmosphere with dimmed lights, candles, and somewhat older clientele. Most cafes have terraces in summertime. A common sight on the Leidseplein during summer is a square full of terraces packed with people drinking beer or wine. Many restaurants can be found in Amsterdam as well. Since Amsterdam is a multicultural city, a lot of different ethnic restaurants can be found. Restaurants range from being rather luxurious and expensive to being ordinary and affordable. Amsterdam also possesses many discothèques. Most of these 'clubs' are situated near the Leidseplein and Rembrandtplein. The Paradiso, Melkweg and Sugar Factory are cultural centres, which turn into discothèques on some nights. Examples of discothèques near the Rembrandtplein are the Escape and Club Home. Also noteworthy are Panama, Hotel Arena (East), The Sand and The Powerzone. The Reguliersdwarsstraat is the main street for the GLBT community and nightlife. Hollywood films are primarily featured at cinemas owned by Pathe. Tuschinski is a heritage art deco building with a beautiful lobby and six screens. Theater One is an architectural treasure with comfortable seats, two balconies and recently restored ceilings. The Pathe cinema is modern and is located at De Munt. Pathe Arena is located a short metro ride from the centre and is Amsterdam's most technically advanced and modern cinema. Pathe City is scheduled to reopen in October 2009. Art films can be found at Tuschinski, and the independent The Movies, Cinecenter, Kriterion, Ketelhuis, Uitkijk, and the Filmmuseum.
In 2008, there were 140 festivals and events in Amsterdam. Famous festivals and events in Amsterdam include: Koninginnedag (Queen's Day); the Holland Festival for the performing arts; the yearly Prinsengrachtconcert (classical concerto on the Prinsen canal) in August; the 'Stille Omgang' (a silent Roman Catholic evening procession held every March); Amsterdam Gay Pride; The Cannabis Cup; and the Uitmarkt. On Koninginnedag—held each year on April 30—hundreds of thousands of people travel to Amsterdam to celebrate with the city's residents. The entire city becomes overcrowded with people buying products from the "freemarket," or visiting one of the many music concerts. The yearly Holland Festival attracts international artists and visitors from all over Europe. Amsterdam Gay Pride is a yearly local LGBT parade of boats in Amsterdam's canals, held on the first Saturday in August. The Gay Pride event is a frequent source of both criticism and praise. The annual Uitmarkt is a three-day cultural event at the start of the cultural season in late August. It offers previews of many different artists, such as musicians and poets, who perform on podia.
Amsterdam is home of the "Eredivisie" football club Ajax Amsterdam. The stadium Amsterdam ArenA is the home of Ajax. It is located in the south-east of the city next to the new Amsterdam Bijlmer ArenA railway station. Before it moved to its current location in 1996, Ajax played their regular matches in De Meer Stadion. In 1928, Amsterdam hosted the 1928 Summer Olympics. The Olympic Stadium built for the occasion has been completely restored and is now used for cultural and sporting events, such as the Amsterdam Marathon. The ice hockey team Amstel Tijgers play in the Jaap Eden ice rink. The team competes in the Dutch ice hockey premier league. Speed skating championships have been held on the lane of this ice rink. Amsterdam holds two American Football franchises, the Amsterdam Crusaders, playing at Amsterdam Sloten, and the Amsterdam Panthers. The Amsterdam Pirates baseball team competes in the Dutch Major League. There are three field hockey teams, Amsterdam, Pinoké and Hurley, who play their matches around the Wagener Stadium in the nearby city of Amstelveen. The basketball team MyGuide Amsterdam competes in the Dutch premier division and play their games in the Sporthallen Zuid, near the Olympic Stadium. Since 1999 the city of Amsterdam honours the best sportsmen and women at the Amsterdam Sports Awards. Boxer Raymond Joval and field hockey midfielder Carole Thate were the first to receive the awards in 1999.
Audi AG (Xetra:) is a German manufacturer of automobiles marketed under the Audi brand. The company is headquartered in Ingolstadt, Germany and has been a wholly-owned (99.55%) subsidiary of the Volkswagen Group (Volkswagen AG) since 1964. Volkswagen Group relaunched the Audi brand with the 1965 introduction of the Audi 60 range. Shortly thereafter the name was acquired as part of Volkswagen's purchase of the Auto Union assets from former owner, Daimler-Benz. The company name is based on the surname of the founder August Horch, the name itself an English cognate with the English word "hark", meaning "listen" — which when translated into Latin, becomes "Audi". Birth of the company and its name. The company traces its origins back to 1909 and August Horch. The first Audi automobile, the Audi Type A 10 / 22 hp Sport-Phaeton, was produced in 1910 in Zwickau. In 1909, Horch was forced out of the company he had founded. He then started a new company in Zwickau and continued using the Horch brand. His former partners sued him for trademark infringement and the German Supreme Court (Reichsgericht in Berlin) finally determined that the Horch brand belonged to his former company. August Horch was barred from using his own family name in his new car business, so he called a meeting with his best business friends, Paul and Franz Fikentscher from Zwickau. At the apartment of Franz Fikentscher they discussed how to come up with a new name for his company. During this meeting Franz's son was quietly studying Latin in a corner of the room. Several times he looked like he was on the verge of saying something but would just swallow his words and continue working, until he finally blurted out, "Father "audiatur et altera pars"... wouldn't it be a good idea to call it "audi" instead of "horch"?" "Horch!" in German means "Hark!" or "hear", which is "Audi" in Latin (compare audible). The idea was enthusiastically accepted by everyone attending the meeting. Audi started with a 2,612 cc (2.6 litre) four cylinder model followed by a 3564 cc (3.6 L) model, as well as 4680 cc (4.7 L) and 5720 cc (5.7L) models. These cars were successful even in sporting events. The first six cylinder model, 4655 cc (4.7 L) appeared in 1924. August Horch left the Audi company in 1920 for a high position at the ministry of transport, but he was still involved with Audi as a member of the board of trustees. In September 1921, Audi became the first German car manufacturer to present a production car, the Audi Type K, with left-handed drive. Left-hand drive spread and established dominance during the 1920s because it provided a better view of oncoming traffic, making overtaking skidoos safer.
In August 1928 Jørgen Rasmussen, the owner of DKW, acquired the majority of shares in Audiwerke AG. In the same year, Rasmussen bought the remains of the US automobile manufacturer Rickenbacker, including the manufacturing equipment for eight cylinder engines. These engines were used in "Audi Zwickau" and "Audi Dresden" models that were launched in 1929. At the same time, six cylinder and four cylinder (licensed from Peugeot) models were manufactured. Audi cars of that era were luxurious cars equipped with special bodywork. In 1932, Audi merged with Horch, DKW and Wanderer, to form Auto Union. It was during this period that the company offered the Audi Front which was the first European car to combine a six cylinder engine with front-wheel drive, using a unit shared with Wanderer but turned through 180 degrees so that the drive shaft faced the front. Before World War II, Auto Union used the four interlinked rings that make up the Audi badge today, representing these four brands. This badge was used, however, only on Auto Union racing cars in that period while the member companies used their own names and emblems. The technological development became more and more concentrated and some Audi models were propelled by Horch or Wanderer built engines. Reflecting the economic pressures of the time, Auto Union concentrated increasingly on smaller cars through the 1930s, so that by 1938 the company's DKW brand accounted for 17.9% of the German car market while Audi held only 0.1%.
Like most German manufacturing, at the onset of World War II the Auto Union plants were retooled for military production, and immediately were subjected heavy bombing for the rest of the war, leaving them all severely damaged. Over run by the Russian Army, in 1945 on the orders of the Soviet Union military administration, they were dismantled as part of war reparations. Following this, the company’s entire assets were expropriated without compensation. On 17 August 1948 Auto Union AG of Chemnitz was deleted from the commercial register. These actions had the effect of liquidating Germany's Auto Union AG. The remains of the Audi plant of Zwickau became the VEB (for "People Owned Enterprise") Automobilwerk Zwickau, AWZ for short (which translates into English as Automobile factory Zwickau). The former Audi factory in Zwickau, restarted assembly of the pre-war-models in 1949. These DKW models were renamed to IFA F8 and IFA F9 and were similar to the West German versions. West and East German models were equipped with the traditional and renowned DKW two-stroke engines.
A new West German head quartered Auto Union was launched in Ingolstadt, Bavaria with loans from the Bavarian state government and Marshall Plan aid. The reformed company was launched 3 September 1949 and continued DKW's tradition of producing front-wheel drive vehicles with two-stroke engines. This included production of a small but sturdy 125 cc motorcycle and a DKW delivery van, the DKW F 89 L. In 1958 Daimler-Benz took an 87% holding in the Auto Union company, and this was increased to a 100% holding in 1959. However, small two-stroke cars were not the focus of the company's interests, and while the early 1960s saw major investment in new Mercedes models, the Auto Union business at this time did not benefit from the economic boom of the time to the same extent as competitor manufacturers such as Volkswagen and Opel. It appears that the decision to dispose of the Auto Union business was based on its lack of profitability. Ironically, by the time they sold the business it also included a near production-ready thoroughly modern four stroke engine, which would enable the Auto Union business, under a new owner and with the benefit of a rediscovered name, Audi, to become one of Germany's most successful auto-makers during the second half of the 1960s. In 1964 Volkswagen Group acquired a 50% holding in the business, which included the new factory in Ingolstadt and the trademark rights of the Auto Union. 18 months later Volkswagen bought complete control of Ingolstadt, and by 1966 were using the spare capacity of the Ingolstadt plant to assemble an additional 60,000 Volkswagen Beetles per year. Two-stroke engines became less popular during the 1960s as customers were more attracted to the smoother four-stroke engines. In September 1965, the DKW F102 got a four-stroke engine implanted and some front and rear styling changes. Volkswagen dumped the DKW brand because of its associations with two-stroke technology, and having classified the model internally as the F103, sold it simply as the "Audi." Later developments of the model were named for their horsepower ratings and sold as the Audi 60, 75, 80, and Super 90, selling until 1972. In 1969, Auto Union merged with NSU, based in Neckarsulm, near Stuttgart. In the 1950s, NSU had been the world's largest manufacturer of motorcycles, but had moved on to produce small cars like the NSU Prinz, the TT and TTS versions of which are still popular as vintage race cars. NSU then focused on new rotary engines based on the ideas of Felix Wankel. In 1967, the new NSU Ro 80 was a space-age car, well ahead of its time in technical details such as aerodynamics, light weight, and safety but teething problems with the rotary engines put an end to the independence of NSU. Today the Neckarsulm plant is used to produce the larger Audi models A6 and A8. The Neckarsulm factory is also home of the quattro GmbH, this subsidiary is responsible for development and production of the Audi high performance cars: the R8 and the "RS" model range. The mid-sized car that NSU had been working on, the K70, was intended to slot between the rear-engined Prinz models and the futuristic NSU Ro 80. However, Volkswagen took the K70 for its own range, spelling the end of NSU as a separate brand.
The new merged company was known as Audi NSU Auto Union AG, and saw the emergence of Audi as a separate brand for the first time since the pre-war era. Volkswagen introduced the Audi brand to the United States for the 1970 model year. The first new car of this regime was the Audi 100 of 1968. This was soon joined by the Audi 80/Fox (which formed the basis for the 1973 Volkswagen Passat) in 1972 and the Audi 50 (later rebadged as the Volkswagen Polo) in 1974. The Audi 50 was a seminal design in many ways, because it was the first incarnation of the Polo concept, one that led to a hugely successful world car. The Audi image at this time was a conservative one, and so, a proposal from chassis engineer Jörg Bensinger was accepted to develop the four-wheel drive technology in Volkswagen's Iltis military vehicle for an Audi performance car and rally racing car. The performance car, introduced in 1980, was named the "Audi Quattro," a turbocharged coupé which was also the first German large-scale production vehicle to feature permanent all-wheel drive through a centre differential. Commonly referred to as the "Ur-Quattro" (the "Ur-" prefix is a German augmentative used, in this case, to mean "original" and is also applied to the first generation of Audi's S4 and S6 sport sedans, as in "UrS4" and "UrS6"), few of these vehicles were produced (all hand-built by a single team), but the model was a great success in rallying. Prominent wins proved the viability of all-wheel drive racecars, and the Audi name became associated with advances in automotive technology. In 1985, with the Auto Union and NSU brands effectively dead, the company's official name was now shortened to simply Audi AG. In 1986, as the Passat-based Audi 80 was beginning to develop a kind of "grandfather's car" image, the "type 89" was introduced. This completely new development sold extremely well. However, its modern and dynamic exterior belied the low performance of its base engine, and its base package was quite spartan (even the passenger-side mirror was an option.) In 1987, Audi put forward a new and very elegant Audi 90, which had a much superior set of standard features. In the early 1990s, sales began to slump for the Audi 80 series, and some basic construction problems started to surface. In the early part of the 21st century, Audi set forth on a German racetrack to claim and maintain several World Records, such as Top Speed Endurance. This effort was in-line with the company's heritage from the 1930s racing era Silver Arrows.
Audi's U.S. sales fell after a series of recalls from 1982-1987 of Audi 5000 models associated with reported incidents of sudden unintended acceleration linked to six deaths and 700 accidents. At the time, NHTSA was investigating 50 car models from 20 manufacturers for sudden surges of power. A "60 Minutes" report aired 23 November 1986, featuring interviews with six people who had sued Audi after reporting unintended acceleration, showing an Audi 5000 ostensibly suffering a problem when the brake pedal was pushed. Subsequent investigation revealed that "60 Minutes" had engineered the failure — fitting a canister of compressed air on the passenger-side floor, linked via a hose to a hole drilled into the transmission. Audi contended, prior to findings by outside investigators, that the problems were caused by driver error, specifically pedal misapplication. Subsequently, the National Highway Traffic Safety Administration (NHTSA) concluded that the majority of unintended acceleration cases, including all the ones that prompted the "60 Minutes" report, were caused by driver error such as confusion of pedals. CBS did not acknowledge the test results of involved government agencies, but did acknowledge the similar results of another study. With the series of recall campaigns, Audi made several modifications; the first adjusted the distance between the brake and accelerator pedal on automatic-transmission models. Later repairs, of 250,000 cars dating back to 1978, added a device requiring the driver to press the brake pedal before shifting out of park. A legacy of the Audi 5000 and other reported cased of sudden unintended acceleration are intricate gear stick patterns and brake interlock mechanisms to prevent inadvertent shifting into forward or reverse. Audi’s U.S. sales, which had reached 74,061 in 1985, dropped to 12,283 in 1991 and remained level for three years. — with resale values falling dramatically. Audi subsequently offered increased warranty protection and renamed the affected models — with the "5000" becoming the "100" and "200" in 1989 — and only reached the same sales levels again by model year 2000. A 2010 "BusinessWeek" article — outlining possible parallels between Audi's experience and 2009–2010 Toyota vehicle recalls — noted a class-action lawsuit filed in 1987 by about 7,500 Audi Audi 5000-model owners remains unsettled and is currently being contested in county court in Chicago after appeals at the Illinois state and U.S. federal levels.
The largest shareholder of Audi AG is Volkswagen AG, which holds over 99 percent of the share capital. Volkswagen AG includes the consolidated accounts of Audi AG in its own consolidated financial statements. In recent years, the possibility of Audi being spun off or otherwise divested by Volkswagen has been mooted Audi's sales grew strongly in the 2000s, with deliveries to customers increasing from 653,000 in 2000 to 1,003,000 in 2008. The largest sales increases came from Eastern Europe (+19.3%), Africa (+17.2%) and the Middle East (+58.5%).. China in particular has become a key market, representing 108,000 out of 705,000 cars delivered in the first three quarters of 2009.
Audi produces 100% galvanised cars to prevent corrosion, and was the first mass-market vehicle to do so, following introduction of the process by Porsche, c.1975. Along with other precautionary measures, the full-body zinc coating has proved to be very effective in preventing rust. The body's resulting durability even surpassed Audi's own expectations, causing the manufacturer to extend its original 10-year warranty against corrosion perforation to currently 12 years (except for aluminium bodies which don't rust). An all-aluminium car was brought forward by Audi, and in 1994 the Audi A8 was launched, which introduced aluminium space frame technology (called "Audi Space Frame"). Audi introduced a new series of vehicles in the mid-nineties and continues to pursue leading-edge technology and high performance. Prior to that effort, Audi used examples of the Type 44 chassis fabricated out of aluminium as test-beds for the technique.
In all its post Volkswagen-era models, Audi has firmly refused to adopt the traditional rear-wheel drive layout favoured by its two arch rivals Mercedes-Benz and BMW, favouring either front-wheel drive or all wheel drive. To achieve this, Audi has usually engineered its cars with a longitudinally front mounted engine, in an "overhung" position, over the front wheels in front of the axle line. While this allows for the easy adoption of all wheel drive, it goes against the ideal 50:50 weight distribution (as do all front wheel drive cars). Audi has recently applied the "quattro" badge to models such as the A3 and TT which do not actually use the Torsen-based system as in prior years, with a mechanical centre differential, but with the Swedish Haldex Traction electro-mechanical clutch AWD system.
In the 1980s, Audi, along with Volvo, was the champion of the inline 5 cylinder, 2.2 L engine as a longer lasting alternative to more traditional 6 cylinder engines. This engine was used not only in production cars but also in their race cars. The 2.1 L inline 5 cylinder engine was used as a base for the rally cars in the 1980s, providing well over 400 horsepower (298 kW) after modification. Before 1990, there were engines produced with a displacement between 2.0 L and 2.3 L. This range of engine capacity was a good combination of good fuel economy (which was on the mind of every motorist in the 1980s) and, of course, a good amount of power.
Through the early 1990s, Audi began to move more towards the position of being a real competitor in its target market against global luxury leaders Mercedes-Benz and BMW. This began with the release of the Audi V8 in 1990. It was essentially a new engine fitted to the Audi 100/200, but with noticeable bodywork differences. Most obvious was the new grille that was now incorporated in the bonnet. By 1991, Audi had the 4 cylinder Audi 80, the 5 cylinder Audi 90 and Audi 100, the turbocharged Audi 200 and the Audi V8. There was also a coupe version of the 80/90 with both 4 and 5 cylinder engines. Although the five cylinder engine was a successful and very robust powerplant, it was still a little too different for the target market. With the introduction of an all-new Audi 100 in 1992, Audi introduced a 2.8L V6 engine. This engine was also fitted to a face-lifted Audi 80 (all 80 and 90 models were now badged 80 except for the USA), giving this model a choice of 4, 5 and 6 cylinder engines, in saloon/sedan, coupé and Cabriolet body styles. The 5 cylinder was soon dropped as a major engine choice; however, a turbocharged 230 hp (169 kW) version remained. The engine, initially fitted to the 200 quattro 20V of 1991, was a derivative of the engine fitted to the Sport Quattro. It was fitted to the Audi Coupé, and named the S2 and also to the Audi 100 body, and named the S4. These two models were the beginning of the mass-produced S series of performance cars.
The Audi A2 was a futuristic super mini born from the Al2 concept. It featured many features that gave Audi the cutting edge technology that had lacked for years, like the aluminium space frame which was a first in production car design. In the A2 Audi further expanded their TDI technology through the use of frugal three cylinder engines. The A2 was extremely aerodynamic and was designed around a wind tunnel. The Audi A2 was criticised for its high price and was never really a sales success but it planted Audi as a cutting edge manufacturer. The model, a Mercedes-Benz A-Class competitor, sold relatively well in Europe. However, the A2 was discontinued in 2005 and Audi decided not to develop an immediate replacement.
The next major model change was in 1995 when the Audi A4 replaced the Audi 80. The new nomenclature scheme was applied to the Audi 100 to become the Audi A6 (with a minor facelift). This also meant the S4 became the S6 and a new S4 was introduced in the A4 body. The S2 was discontinued. The Audi Cabriolet continued on (based on the Audi 80 platform) until 1999, gaining the engine upgrades along the way. A new A3 hatchback model (sharing the Volkswagen Golf Mk4's platform) was introduced to the range in 1996, and the radical Audi TT coupé and roadster were debuted in 1998 based on the same underpinnings. The engines available throughout the range were now a 1.4L, 1.6L and 1.8L 4 cylinder, 1.8L 4-cylinder turbo, 2.6L and 2.8L V6, 2.2L turbo-charged 5 cylinder and the 4.2L V8 engine. The V6s were replaced by new 2.4L and 2.8L 30V V6s in 1998, with marked improvement in power, torque and smoothness. Further engines were added along the way, including a 3.7L V8 and 6.0L W12 engine for the A8.
At the turn of the century, Volkswagen introduced the Direct-Shift Gearbox (DSG), a type of dual clutch transmission. It is an automated semi-automatic transmission, drivable like a conventional automatic transmission. Based on the gearbox found in the Group B S1, the system includes dual electrohydraulically controlled clutches instead of a torque converter. This is implemented in some VW Golfs, Audi A3 and TT models where DSG is called S-tronic.
Audi has recently started offering a computerised control system for its cars, called Multi Media Interface (MMI). This came amid criticism of BMW's iDrive control. It is essentially a rotating control knob and 'segment' buttons - designed to control all in-car entertainment devices (radio, CD changer, iPod, TV tuner), satellite navigation, heating and ventilation, and other car controls with a screen. MMI was widely reported to be a considerable improvement on BMW's iDrive, although BMW has since made their iDrive more user-friendly. MMI has been generally well-received, as it requires less menu-surfing with its segment buttons around a central knob, along with 'main function' direct access buttons - with shortcuts to the radio or phone functions. The screen, either colour or monochrome, is mounted on the upright dashboard, and on the A4 (new), A5, A6, A8, and Q7, the controls are mounted horizontally. An "MMI-like" system is also available on the A3, TT, A4 (B7), and R8 models - when equipped with the Audi Navigation System Plus (RNS-E) satellite navigation system.
In 1980, Audi released the Quattro, a four-wheel drive (4WD) turbocharged car that went on to win rallies and races worldwide. It is considered one of the most significant rally cars of all time, because it was one of the first to take advantage of the then-recently changed rules which allowed the use of four-wheel drive in competition racing. Many critics doubted the viability of four-wheel drive racers, thinking them to be too heavy and complex, yet the Quattro was to become a successful car. Leading its first rally it went off the road, however the rally world had been served notice 4WD was the future. The Quattro went on to achieve much success in the World Rally Championship. It won the 1983 (Hannu Mikkola) and the 1984 (Stig Blomqvist) drivers' titles, and brought Audi the manufacturers' title in 1982 and 1984. In 1984, Audi launched the short-wheelbase Sport Quattro which dominated rally races in Monte Carlo and Sweden, with Audi taking all podium places, but succumbed to problems further into WRC contention. In 1985, after another season mired in mediocre finishes, Walter Röhrl finished the season in his Sport Quattro S1, and helped place Audi second in the manufacturers' points. Audi also received rally honours in the Hong Kong to Beijing rally in that same year. Michèle Mouton, the only female driver to win a round of the World Rally Championship and a driver for Audi, took the Sport Quattro S1, now simply called the "S1", and raced in the Pikes Peak International Hill Climb. The climb race pits a driver and car to drive up a 4,302 metre high mountain in Colorado and in 1985, Michèle Mouton set a new record of 11:25.39, and being the first woman to set a Pikes Peak record. In 1986, Audi formally left international rally racing following an accident in Portugal involving driver Joaquim Santos in his Ford RS200. Santos swerved to avoid hitting spectators in the road, and left the track into the crowd of spectators on the side, killing three and injuring 30. Bobby Unser used an Audi in that same year to claim a new record for the Pikes Peak Hill Climb at 11:09.22. In 1987, Walter Röhrl claimed the title for Audi setting a new Pikes Peak International Hill Climb record of 10:47.85 in his Audi S1, which he had retired from the WRC two years earlier. The Audi S1 employed Audi's time-tested inline five cylinder turbocharged engine, with the final version generating. The engine was mated to a six-speed gearbox and ran on Audi's famous four-wheel drive system. All of Audi's top drivers drove this car; Hannu Mikkola, Stig Blomqvist, Walter Röhrl and Michèle Mouton. This Audi S1 started the range of Audi 'S' cars, which now represents an increased level of sports-performance equipment within the mainstream Audi model range.
In 1990, having completed their objective to market cars in North America, Audi returned to Europe, turning first to the Deutsche Tourenwagen Meisterschaft (DTM) series with the Audi V8, and then in 1993, being unwilling to build cars for the new formula, they turned their attention to the fast growing Super Touring series, which are a series of national championships. Audi first entered in the French Supertourisme and Italian Superturismo. In the following year, Audi would switch to the German Super Tourenwagen Cup (known as STW), and then to British Touring Car Championship (BTCC) the year after that. The Fédération Internationale de l'Automobile (FIA), having difficulty regulating the quattro four wheel drive system, and the impact it had on the competitors, would eventually ban all four wheel drive cars from competing in 1998, but by then, Audi switched all their works efforts to sports car racing. By 2000, Audi would still compete in the US with their RS4 for the SCCA Speed World GT Challenge, through dealer/team Champion Racing competing against Corvettes, Vipers, and smaller BMWs (where it is one of the few series to permit 4WD cars). In 2003, Champion Racing entered an RS6. Once again, the quattro four wheel drive was superior, and Champion Audi won the championship. They returned in 2004 to defend their title, but a newcomer, Cadillac with the new Omega Chassis CTS-V, gave them a run for their money. After four victories in a row, the Audis were sanctioned with several negative changes that deeply affected the car's performance. Namely, added ballast weights, and Champion Audi deciding to go with different tyres, and reducing the boost pressure of the turbocharger. In 2004, after years of competing with the TT-R in the revitalised DTM series, with privateer team Abt Racing/Christian Abt taking the 2002 title with Laurent Aïello, Audi returned as a full factory effort to touring car racing by entering two factory supported Joest Racing A4 DTM cars.
Starting in 1999, Audi built the Audi R8R (open-cockpit 'roadster' prototype) and the Audi R8C (closed-cockpit 'coupé' GT-prototype) to compete in sports car racing, including the Le Mans Prototype LMP900 class at the 24 Hours of Le Mans. For the 2000 season, Audi focussed mainly on the new Audi R8, due to favourable rules for open-cockpit prototypes. The factory-supported Joest Racing team won at Le Mans three times in a row with the Audi R8 (2000 — 2002), as well as winning every race in the American Le Mans Series in its first year. Audi also sold the car to customer teams such as Champion Racing. In 2003, two Bentley Speed 8s, with engines designed by Audi, and driven by Joest drivers "loaned" to the fellow Volkswagen Group company, competed in the GTP class, and finished the race in the top two positions, while the Champion Racing R8 finished third overall, and first in the LMP900 class. Audi returned to the winner's podium at the 2004 race, with the top three finishers all driving R8s: Audi Sport Japan Team Goh finished first, Audi Sport UK Veloqx second, and Champion Racing third. At the 2005 24 Hours of Le Mans, Champion Racing entered two R8s, along with an R8 from the Audi PlayStation Team Oreca. The R8s (which were built to old LMP900 regulations) received a narrower air inlet restrictor, reducing power, and an additional of weight compared to the newer LMP1 chassis. On average, the R8s were about 2–3 seconds off pace compared to the Pescarolo-Judd. But with a team of excellent drivers and experience, both Champion R8s were able to take first and third, while the ORECA team took fourth. The Champion team was also the first American team to win Le Mans since the Gulf Ford GT's in 1967. This also ends the long era of the R8; however, its replacement for 2006, called the Audi R10 TDI, was unveiled on 13 December 2005. The R10 TDI employs many new features, its most obvious was the twin-turbocharged direct injection diesel engine. Its first race was the 2006 12 Hours of Sebring as a race-test for the 2006 24 Hours of Le Mans, which it later went on to win. Audi has been on the forefront of sports car racing, claiming a historic win in the first ever diesel sports car at 12 Hours of Sebring. As well as winning the 24 Hours of Le Mans in 2006 making history, the R10 TDI has also shown its capabilities by beating the Peugeot 908 HDi FAP in 2007, and beating Peugeot again in 2008.
The Audi emblem is four overlapping rings that represent the four marques of Auto Union. The Audi emblem symbolises the amalgamation of Audi with DKW, Horch and Wanderer: the first ring represents Audi, the second represents DKW, third is Horch, and the fourth and last ring Wanderer. Its similarity to the Olympic rings caused the International Olympic Committee to sue Audi in Rochester, MN small claims Court in 1995. As part of Audi's centennial celebration in 2009, the company updated the logo, changing the font to left-aligned Audi Type, and altering the shading for the overlapping rings.
Audi's corporate tagline is "Vorsprung durch Technik", meaning "Progress through Technology". The German-language tagline is used in many European countries, including the United Kingdom, and in other markets, such as Latin America, Oceania and parts of Asia including Japan. A few years ago, the North American tagline was "Innovation through technology", but in Canada the German tagline "Vorsprung durch Technik" was used in advertising. More recently, however, Audi has updated the tagline to "Truth in Engineering" in the U.S.
Audi is a strong partner of different kinds of sports. In soccer, long partnerships exist between Audi and various clubs like the FC Bayern Munich, Real Madrid CF, FC Barcelona, AC Milan and Ajax Amsterdam. Audi also sponsors winter sports: The Audi FIS Alpine Ski World Cup is named after the company. Additionally, Audi supports the German Ski Association (DSV) as well as the alpine skiing national teams of Switserland, Sweden, Finland, France, Liechtenstein, Italy, Austria and the US. For almost two decades Audi fosters golf sport: for example with the Audi quattro Cup and the HypoVereinsbank Ladies German Open presented by Audi. In sailing, Audi is engaged in the Medcup regatta and supports the team Luna Rossa during the Louis Vuitton Pacific Series and also is the primary sponsor of the Melges 20 sailboat. Further, Audi sponsors the regional teams ERC Ingolstadt (hockey) and FC Ingolstadt 04 (soccer). In 2009, the year of Audis 100th anniversary, the company organises the Audi Cup for the first time. In a two-day-tournament, the teams of FC Bayern Munich, AC Milan, Manchester United F.C. and CA Boca Juniors will compete against each other.
In 2001, Audi promoted the new multitronic continuously variable transmission with television commercials throughout Europe, featuring an impersonator of musician and actor Elvis Presley. A prototypical dashboard figure - later named "Wackel-Elvis" ("Wobble Elvis" or "Wobbly Elvis") - appeared in the commercials to demonstrate the smooth ride in an Audi equipped with the multitronic transmission. The dashboard figure was originally intended for use in the commercials only, but after they aired the demand for Wackel-Elvis fans grew among fans and the figure was mass produced in China and marketed by Audi in their factory outlet store.
In PlayStation Home, the PlayStation 3's online community-based service, Audi has supported Home by releasing a dedicated Home space in the European version of Home. Audi is the first carmaker to develop a space for Home. On 17 December 2009, Audi released the Audi Space as two spaces; the Audi Home Terminal and the Audi Vertical Run.. The Audi Home Terminal features an Audi TV channel delivering video content, an Internet Browser feature, and a view of a city. The Audi Vertical Run is where users can access the mini-game Vertical Run, a futuristic mini-game featuring Audi's e-tron concept. Players collect energy and race for the highest possible speeds and the fastest players earn a place in the Audi apartments located in a large tower in the centre of the Audi Space. In both the Home Terminal and Vertical Run spaces, there are teleports where users can teleport back and forth between the two spaces. Audi has stated that additional content will be added in 2010. "Most young people gain their first driving experience from video games," explains Kai Mensing, who is responsible for video games and virtual worlds in Online Marketing at Audi. "With the Audi Space, we can bring this target group into contact with our brand in a highly emotion-packed and interactive environment, and demonstrate our 'Vorsprung durch Technik' with the virtual e-tron race." Kai Mensing's most recent statement on the Audi Space was that "As a brand, Audi sees great value and potential in the experiences and level of interaction the world of gaming affords. PlayStation Home gives Audi the perfect environment to connect with gamers in an unexpected way and we made sure to push the platform to its limits with the new Audi Space."
As part of Audi's attempt to promote its Diesel technology in 2009, the company began Audi Mileage Marathon. The driving tour featured a fleet of 23 Audi TDI vehicles from 4 models (Audi Q7 3.0 TDI, Audi Q5 3.0 TDI, Audi A4 3.0 TDI, Audi A3 Sportback 2.0 TDI with S tronic transmission) travelling across the American continent from New York to Los Angeles, passing major cities like Chicago, Dallas and Las Vegas during the 13 daily stages, as well as natural wonders including the Rocky Mountains, Death Valley and the Grand Canyon.
An aircraft is a vehicle which is able to fly by being supported by the air, or in general, the atmosphere of a planet. An aircraft counters the force of gravity by using either static lift (as with balloons, blimps and dirigibles) or by using the dynamic lift of an airfoil (as with vehicles that plane the air with wings in a straight manner, such as airplanes and gliders, or vehicles that generate lift with wings in a rotary manner, such as helicopters or gyrocopters). Although rockets and missiles also travel through the atmosphere, most are not considered aircraft because they use rocket thrust instead of aerodynamics as the primary means of lift (A cruise missile may be considered to be an aircraft because it relies on a lifting wing). The human activity which surrounds aircraft is called "aviation". Manned aircraft are flown by an onboard pilot. Unmanned aerial vehicles may be remotely controlled or self-controlled by onboard computers. Target drones are an example of UAVs. Lighter than air – aerostats. Aerostats use buoyancy to float in the air in much the same way that ships float on the water. They are characterized by one or more large gasbags or canopies, filled with a relatively low density gas such as helium, hydrogen or hot air, which is less dense than the surrounding air. When the weight of this is added to the weight of the aircraft structure, it adds up to the same weight as the air that the craft displaces. Small hot air balloons called sky lanterns date back to the 3rd century BC, and were only the second type of aircraft to fly, the first being kites. Originally, a balloon was any aerostat, while the term airship was used for large, powered aircraft designs – usually fixed-wing – though none had yet been built. The advent of powered balloons, called dirigible balloons, and later of rigid hulls allowing a great increase in size, began to change the way these words were used. Huge powered aerostats, characterized by a rigid outer framework and separate aerodynamic skin surrounding the gas bags, were produced, the Zeppelins being the largest and most famous. There were still no fixed-wing aircraft or non-rigid balloons large enough to be called airships, so "airship" came to be synonymous with these aircraft. Then several accidents, such as the Hindenburg disaster in 1937, led to the demise of these airships. Nowadays a "balloon" is an unpowered aerostat, whilst an "airship" is a powered one. A powered, steerable aerostat is called a "dirigible". Sometimes this term is applied only to non-rigid balloons, and sometimes "dirigible balloon" is regarded as the definition of an airship (which may then be rigid or non-rigid). Non-rigid dirigibles are characterized by a moderately aerodynamic gasbag with stabilizing fins at the back. These soon became known as "blimps". During the Second World War, this shape was widely adopted for tethered balloons; in windy weather, this both reduces the strain on the tether and stabilizes the balloon. The nickname "blimp" was adopted along with the shape. In modern times any small dirigible or airship is called a blimp, though a blimp may be unpowered as well as powered. Heavier than air – aerodynes. Heavier-than-air aircraft must find some way to push air or gas downwards, so that a reaction occurs (by Newton's laws of motion) to push the aircraft upwards. This dynamic movement through the air is the origin of the term "aerodyne". There are two ways to produce dynamic upthrust: aerodynamic lift, and powered lift in the form of engine thrust. Aerodynamic lift is the most common, with fixed-wing aircraft being kept in the air by the forward movement of wings, and rotorcraft by spinning wing-shaped rotors sometimes called rotary wings. A wing is a flat, horizontal surface, usually shaped in cross-section as an aerofoil. To fly, air must flow over the wing and generate lift. A "flexible wing" is a wing made of fabric or thin sheet material, often stretched over a rigid frame. A "kite" is tethered to the ground and relies on the speed of the wind over its wings, which may be flexible or rigid, fixed or rotary. With powered lift, the aircraft directs its engine thrust vertically downwards. The initialism "VTOL" (vertical take off and landing) is applied to aircraft that can take off and land vertically. Most are rotorcraft. Others, such as the Hawker Siddeley Harrier, take off and land vertically using powered lift and transfer to aerodynamic lift in steady flight. Similarly, "STOL" stands for short take off and landing. Some VTOL aircraft often operate in a short take off/vertical landing mode known as STOVL. A pure rocket is not usually regarded as an aerodyne, because it does not depend on the air for its lift (and can even fly into space); however, many aerodynamic lift vehicles have been powered or assisted by rocket motors. Rocket-powered missiles which obtain aerodynamic lift at very high speed due to airflow over their bodies, are a marginal case.
"Airplanes" or "aeroplanes" are technically called "fixed-wing aircraft". The forerunner of the fixed-wing aircraft is the kite. Whereas a fixed-wing aircraft relies on its forward speed to create airflow over the wings, a kite is tethered to the ground and relies on the wind blowing over its wings to provide lift. Kites were the first kind of aircraft to fly, and were invented in China around 500 BC. Much aerodynamic research was done with kites before test aircraft, wind tunnels and computer modelling programs became available. The first heavier-than-air craft capable of controlled free flight were gliders. A glider designed by Cayley carried out the first true manned, controlled flight in 1853. A variable geometry aircraft can change its wing configuration during flight. A "flying wing" has no fuselage, though it may have small blisters or pods. The opposite of this is a "lifting body" which has no wings, though it may have small stabilising and control surfaces. Most fixed-wing aircraft feature a tail unit or empennage incorporating vertical, and often horizontal, stabilising surfaces. "Seaplanes" are aircraft that land on water, and they fit into two broad classes: Flying boats are supported on the water by their fuselage. A float plane's fuselage remains clear of the water at all times, the aircraft being supported by two or more floats attached to the fuselage and/or wings. Some examples of both flying boats and float planes are amphibious, being able to take off from and alight on both land and water. Some people consider wing-in-ground-effect vehicles to be fixed-wing aircraft, others do not. These craft "fly" close to the surface of the ground or water. An example is the Russian ekranoplan (nicknamed the "Caspian Sea Monster"). Man-powered aircraft also rely on ground effect to remain airborne, but this is only because they are so underpowered—the airframe is theoretically capable of flying much higher.
Rotorcraft, or rotary-wing aircraft, use a spinning rotor with aerofoil section blades (a "rotary wing") to provide lift. Types include helicopters, autogyros and various hybrids such as gyrodynes and compound rotorcraft. "Helicopters" have powered rotors. The rotor is driven (directly or indirectly) by an engine and pushes air downwards to create lift. By tilting the rotor forwards, the downwards flow is tilted backwards, producing thrust for forward flight. "Autogyros" or "gyroplanes" have unpowered rotors, with a separate power plant to provide thrust. The rotor is tilted backwards. As the autogyro moves forward, air blows upwards through it, making it spin.(cf. Autorotation) This spinning dramatically increases the speed of airflow over the rotor, to provide lift. Juan de la Cierva (a Spanish civil engineer) used the product name "autogiro", and Bensen used "gyrocopter". "Rotor kites", such as the Focke Achgelis Fa 330 are unpowered autogyros, which must be towed by a tether to give them forward ground speed or else be tether-anchored to a static anchor in a high-wind situation for kited flight. "Gyrodynes" are a form of helicopter, where forward thrust is obtained from a separate propulsion device rather than from tilting the rotor. The definition of a 'gyrodyne' has changed over the years, sometimes including equivalent autogyro designs. The most important characteristic is that in forward flight air does not flow significantly either up or down through the rotor disc but primarily across it. The "Heliplane" is a similar idea. "Compound rotorcraft" have wings which provide some or all of the lift in forward flight. Compound helicopters and compound autogyros have been built, and some forms of gyroplane may be referred to as compound gyroplanes. "Tiltrotor" aircraft (such as the V-22 Osprey) have their rotors horizontal for vertical flight, and pivot the rotors vertically like a propeller for forward flight. The "Coleopter" had a cylindrical wing forming a duct around the rotor. On the ground it sat on its tail, and took off and landed vertically like a helicopter. The whole aircraft would then have tilted forward to fly as a propeller-driven fixed-wing aircraft using the duct as a wing (though this transition was never achieved in practice.) Some rotorcraft have reaction-powered rotors with gas jets at the tips, but most have one or more lift rotors powered from engine-driven shafts.
Heavier-than-air unpowered aircraft such as gliders (i.e. sailplanes), hang gliders and paragliders, and other gliders usually do not employ propulsion once airborne. Take-off may be by launching forwards and downwards from a high location, or by pulling into the air on a tow-line, by a ground-based winch or vehicle, or by a powered "tug" aircraft. For a glider to maintain its forward air speed and lift, it must descend in relation to the air (but not necessarily in relation to the ground). Some gliders can 'soar'- gain height from updrafts such as thermal currents. The first practical, controllable example was designed and built by the British scientist and pioneer George Cayley, who many recognise as the first aeronautical engineer.
A propeller or airscrew comprises a set of small, wing-like aerofoils set around a central hub which spins on an axis aligned in the direction of travel. Spinning the propeller creates aerodynamic lift, or "thrust", in a forward direction. A "tractor" design mounts the propeller in front of the power source, while a "pusher" design mounts it behind. Although the pusher design allows cleaner airflow over the wing, tractor configuration is more common because it allows cleaner airflow to the propeller and provides a better weight distribution. A "contra-prop" arrangement has a second propeller close behind the first one on the same axis, which rotates in the opposite direction. A variation on the propeller is to use many broad blades to create a fan. Such fans are traditionally surrounded by a ring-shaped fairing or duct, as "ducted fans". Many kinds of power plant have been used to drive propellers. The earliest designs used man power to give dirigible balloons some degree of control, and go back to Jean-Pierre Blanchard in 1784. Attempts to achieve heavier-than-air man-powered flight did not succeed fully until Paul MacCready's Gossamer Condor in 1977. The first powered flight was made in a steam-powered dirigible by Henri Giffard in 1852. Attempts to marry a practical lightweight steam engine to a practical fixed-wing airframe did not succeed until much later, by which time the internal combustion engine was already dominant. From the first controlled powered fixed-wing aircraft flight by the Wright brothers until World War II, propellers turned by the internal combustion piston engine were virtually the only type of propulsion system in use. (See also: Aircraft engine.) The piston engine is still used in the majority of smaller aircraft produced, since it is efficient at the lower altitudes and slower speeds suited to propellers. Turbine engines need not be used as jets (see below), but may be geared to drive a propeller in the form of a turboprop. Modern helicopters also typically use turbine engines to power the rotor. Turbines provide more power for less weight than piston engines, and are better suited to small-to-medium size aircraft or larger, slow-flying types. Some turboprop designs (see below) mount the propeller directly on an engine turbine shaft, and are called propfans. Since the 1940s, propellers and propfans with swept tips or curved "scimitar-shaped" blades have been studied for use in high-speed applications so as to delay the onset of shockwaves, in similar manner to wing sweepback, where the blade tips approach the speed of sound. The Airbus A400M turboprop transport aircraft is expected to provide the first production example: note that it is not a propfan because the propellers are not mounted direct on the engine shaft but are driven through reduction gearing.
Air-breathing jet engines provide thrust by taking in air, burning it with fuel in a combustion chamber, and accelerating the exhaust rearwards so that it ejects at high speed. The reaction against this acceleration provides the engine thrust. Jet engines can provide much higher thrust than propellers, and are naturally efficient at higher altitudes, being able to operate above. They are also much more fuel-efficient at normal flight speeds than rockets. Consequently, nearly all high-speed and high-altitude aircraft use jet engines. The early turbojet and modern turbofan use a spinning turbine to create airflow for takeoff and to provide thrust. Many, mostly in military aviation, use afterburners which inject extra fuel into the exhaust. Use of a turbine is not absolutely necessary: other designs include the crude pulse jet, high-speed ramjet and the still-experimental supersonic-combustion ramjet or scramjet. These designs require an existing airflow to work and cannot work when stationary, so they must be launched by a catapult or rocket booster, or dropped from a mother ship. The bypass turbofan engines of the Lockheed SR-71 were a hybrid design – the aircraft took off and landed in jet turbine configuration, and for high-speed flight the afterburner was lit and the turbine bypassed, to create a ramjet. The motorjet was a very early design which used a piston engine in place of the combustion chamber, similar to a turbocharged piston engine except that the thrust is derived from the turbine instead of the crankshaft. It was soon superseded by the turbojet and remained a curiosity.
The rotor of a helicopter, may, like a propeller, be powered by a variety of methods such as an internal-combustion engine or jet turbine. Tip jets, fed by gases passing along hollow rotor blades from a centrally mounted engine, have been experimented with. Attempts have even been made to mount engines directly on the rotor tips. Helicopters obtain forward propulsion by angling the rotor disc so that a proportion of its lift is directed forwards to provide thrust.
Combat aircraft divide broadly into fighters and bombers, with several in-between types such as fighter-bombers and ground-attack aircraft (including attack helicopters). Other supporting roles are carried out by specialist patrol, search and rescue, reconnaissance, observation, transport, training and Tanker aircraft among others. Many civil aircraft, both fixed-wing and rotary, have been produced in separate models for military use, such as the civil Douglas DC-3 airliner, which became the military C-47/C-53/R4D transport in the U.S. military and the "Dakota" in the UK and the Commonwealth. Even the small fabric-covered two-seater Piper J3 Cub had a military version, the L-4 liaison, observation and trainer aircraft. Gliders and balloons have also been used as military aircraft; for example, balloons were used for observation during the American Civil War and World War I, and military gliders were used during World War II to land troops.
General aviation is a catch-all covering other kinds of private and commercial use, and involving a wide range of aircraft types such as business jets (bizjets), trainers, homebuilt, aerobatic types, racers, gliders, warbirds, firefighters, medical transports, and cargo transports, to name a few. The vast majority of aircraft today are general aviation types. Within general aviation, there is a further distinction between private aviation (where the pilot is not paid for time or expenses) and commercial aviation (where the pilot is paid by a client or employer). The aircraft used in private aviation are usually light passenger, business, or recreational types, and are usually owned or rented by the pilot. The same types may also be used for a wide range of commercial tasks, such as flight training, pipeline surveying, passenger and freight transport, policing, crop dusting, and medical evacuations. However the larger, more complex aircraft are more likely to be found in the commercial sector. For example, piston-powered propeller aircraft (single-engine or twin-engine) are common for both private and commercial general aviation, but for aircraft such as turboprops like the Beechcraft King Air and helicopters like the Bell JetRanger, there are fewer private owners than commercial owners. Conventional business jets are most often flown by paid pilots, whereas the new generation of smaller jets are being produced for private pilots.
Experimental aircraft are one-off specials, built to explore some aspect of aircraft design and with no other useful purpose. The Bell X-1 rocket plane, which first broke the sound barrier in level flight, is a famous example. The formal designation of "experimental aircraft" also includes other types which are "not certified for commercial applications", including one-off modifications of existing aircraft such as the modified Boeing 747 which NASA uses to ferry the space shuttle from landing site to launch site, and aircraft homebuilt by amateurs for their own personal use.
Alfred Bernhard Nobel () (Stockholm, Sweden, 21 October 1833 – Sanremo, Italy, 10 December 1896) was a Swedish chemist, engineer, innovator, armaments manufacturer and the inventor of dynamite. He owned Bofors, a major armaments manufacturer, which he had redirected from its previous role as an iron and steel mill. Nobel held 355 different patents, dynamite being the most famous. In his last will, he used his enormous fortune to institute the Nobel Prizes. The synthetic element nobelium was named after him.
Alfred Nobel was the third son of Immanuel Nobel (1801–1872) and Andriette Ahlsell Nobel (1805–1889). Born in Stockholm on 21 October 1833, he went with his family to Saint Petersburg in 1842, where his father (who had invented modern plywood) started a "torpedo" works. Alfred studied chemistry with Professor Nikolay Nikolaevich Zinin. When Alfred was 18, he went to the United States to study chemistry for four years and worked for a short period under John Ericsson. In 1859, the factory was left to the care of the second son, Ludvig Nobel (1831–1888), who greatly improved the business. Alfred, returning to Sweden with his father after the bankruptcy of their family business, devoted himself to the study of explosives, and especially to the safe manufacture and use of nitroglycerine (discovered in 1847 by Ascanio Sobrero, one of his fellow students under Théophile-Jules Pelouze at the University of Torino). A big explosion occurred on the 3 September 1864 at their factory in Heleneborg in Stockholm, killing five people, among them Alfred's younger brother Emil. The foundations of the Nobel Prize were laid in 1895 when Alfred Nobel wrote his last will, leaving much of his wealth for its establishment. Since 1901, the prize has honored men and women for outstanding achievements in physics, chemistry, medicine, literature, for work in peace and now economics. Though Nobel remained unmarried, his biographers note that he had at least three loves. Nobel's first love was in Russia with a girl named Alexandra, who rejected his proposal. In 1876 Bertha Kinsky became Alfred Nobel's secretary but after only a brief stay left him to marry her old flame, Baron Arthur Gundaccar von Suttner. Though her personal contact with Alfred Nobel had been brief, she corresponded with him until his death in 1896, and it is believed that she was a major influence in his decision to include a peace prize among those prizes provided in his will. Bertha von Suttner was awarded the 1905 Nobel Peace prize, 'for her sincere peace activities'. Nobel's third and long-lasting love was with a flower girl named Sofie Hess from Vienna. This liaison lasted for 18 years and in many of the exchanged letters, Nobel addressed his love as 'Madame Sofie Nobel'. After his death, according to his biographers - Evlanoff and Flour, and Fant - Nobel's letters were locked within the Nobel Institute in Stockholm and became the best-kept secret of the time. They were released only in 1955, to be included with the biographical data of Nobel. Sri Kantha has suggested that ' the one personal trait of Nobel that helped him to sharpen his creativity include his talent for information access, via his multi-lingual skills. Despite the lack of formal secondary and tertiary level education, Nobel gained proficiency in six languages: Swedish, French, Russian, English, German and Italian. He also developed literary skills to write poetry in English.' His "Nemesis", a prose tragedy in four acts about Beatrice Cenci, partly inspired by Percy Bysshe Shelley's "The Cenci", was printed while he was dying. The entire stock except for three copies was destroyed immediately after his death, being regarded as scandalous and blasphemous. The first surviving edition (bilingual Swedish-Esperanto) was published in Sweden in 2003. The play has been translated to Slovenian via the Esperanto version. Nobel was elected a member of the Royal Swedish Academy of Sciences in 1884, the same institution that would later select laureates for two of the Nobel prizes, and he received an honorary doctorate from Uppsala University in 1893. Alfred Nobel is buried in Norra begravningsplatsen in Stockholm.
Nobel found that when nitroglycerin was incorporated in an absorbent inert substance like ["kieselguhr"] (diatomaceous earth) it became safer and more convenient to handle, and this mixture he patented in 1867 as 'dynamite'. Nobel demonstrated his explosive for the first time that year, at a quarry in Redhill, Surrey, England. In order to help reestablish his name and improve the image of his business from the earlier controversies associated with the dangerous explosives, Nobel had also considered naming the highly powerful substance "Nobels Safety Powder", but settled with Dynamite instead, referring to the Greek word for 'power'. Nobel later on combined nitroglycerin with various nitrocellulose compounds, similar to collodion, but settled on a more efficient recipe combining another nitrate explosive, and obtained a transparent, jelly-like substance, which was a more powerful explosive than dynamite. 'Gelignite', or blasting gelatin, as it was named, was patented in 1876; and was followed by a host of similar combinations, modified by the addition of potassium nitrate and various other substances. Gelignite was more stable, transportable and conveniently formed to fit into bored holes, like those used in drilling and mining, than the previously used compounds and was adopted as the standard technology for mining in the Age of Engineering bringing Nobel a great amount of financial success, though at a significant cost to his health.
In 1888 Alfred's brother Ludvig died while visiting Cannes and a French newspaper erroneously published Alfred's obituary. It condemned him for his invention of dynamite and is said to have brought about his decision to leave a better legacy after his death. The obituary stated ' ("The merchant of death is dead") and went on to say, "Dr. Alfred Nobel, who became rich by finding ways to kill more people faster than ever before, died yesterday." On 27 November 1895, at the Swedish-Norwegian Club in Paris, Nobel signed his last will and testament and set aside the bulk of his estate to establish the Nobel Prizes, to be awarded annually without distinction of nationality. He died of a stroke on 10 December 1896 at Sanremo, Italy. After taxes and bequests to individuals, Nobel's will gave 31,225,000 Swedish kronor (equivalent to about 1.8 billion kronor or 250 million US dollars in 2008) to fund the prizes. The first three of these prizes are awarded for eminence in physical science, in chemistry and in medical science or physiology; the fourth is for literary work "in an ideal direction" and the fifth prize is to be given to the person or society that renders the greatest service to the cause of international fraternity, in the suppression or reduction of standing armies, or in the establishment or furtherance of peace congresses. There is no prize awarded for mathematics, but see Abel Prize. The Formulation about the literary prize, "in an ideal direction" (' in Swedish), is cryptic and has caused much confusion. For many years, the Swedish Academy interpreted "ideal" as "idealistic" (') and used it as a reason not to give the prize to important but less Romantic authors, such as Henrik Ibsen and Leo Tolstoy. This interpretation has since been revised, and the prize has been awarded to, for example, Dario Fo and José Saramago, who definitely do not belong to the camp of literary idealism. There was also quite a lot of room for interpretation by the bodies he had named for deciding on the physical sciences and chemistry prizes, given that he had not consulted them before making the will. In his one-page testament, he stipulated that the money go to discoveries or inventions in the physical sciences and to discoveries or improvements in chemistry. He had opened the door to technological awards, but had not left instructions on how to deal with the distinction between science and technology. Since the deciding bodies he had chosen were more concerned with the former, it is not surprising that the prizes went to scientists and not to engineers, technicians or other inventors. In 2001, Alfred Nobel's great-grandnephew, Peter Nobel (b. 1931), asked the Bank of Sweden to differentiate its award to economists given "in Alfred Nobel's memory" from the five other awards. This has caused much controversy whether the Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel is actually a "Nobel Prize" / "Peace Prize".
Alexander Graham Bell (March 3, 1847 – August 2, 1922) was an eminent scientist, inventor, engineer and innovator who is credited with inventing the first practical telephone. Bell's father, grandfather, and brother had all been associated with work on elocution and speech, and both his mother and wife were deaf, profoundly influencing Bell's life's work. His research on hearing and speech further led him to experiment with hearing devices which eventually culminated in Bell being awarded the first U.S. patent for the telephone in 1876. In retrospect, Bell considered his most famous invention an intrusion on his real work as a scientist and refused to have a telephone in his study. Many other inventions marked Bell's later life, including groundbreaking work in optical telecommunications, hydrofoils and aeronautics. In 1888, Alexander Graham Bell became one of the founding members of the National Geographic Society.
Alexander Bell was born in Edinburgh on March 3, 1847. The family home was at 16 South Charlotte Street, and now has a commemorative marker at the doorstep, marking it as Alexander Graham Bell's birthplace. He had two brothers: Melville James Bell (1845–1870) and Edward Charles Bell (1848–1867). Both of his brothers died of tuberculosis. His father was Professor Alexander Melville Bell, and his mother was Eliza Grace (née Symonds). Although he was born "Alexander", at age ten, he made a plea to his father to have a middle name like his two brothers. For his 11th birthday, his father acquiesced and allowed him to adopt the middle name "Graham", chosen out of admiration for Alexander Graham, a Canadian being treated by his father and boarder who had become a family friend. To close relatives and friends he remained "Aleck" which his father continued to call him into later life.
As a child, young Alexander Graham Bell displayed a natural curiosity about his world, resulting in gathering botanical specimens as well as experimenting even at an early age. His best friend was Ben Herdman, a neighbour whose family operated a flour mill, the scene of many forays. Young Aleck asked what needed to be done at the mill. He was told wheat had to be dehusked through a laborious process and at the age of 12, Bell built a homemade device that combined rotating paddles with sets of nail brushes, creating a simple dehusking machine that was put into operation and used steadily for a number of years. In return, John Herdman gave both boys the run of a small workshop within which to "invent". From his early years, Bell showed a sensitive nature and a talent for art, poetry and music that was encouraged by his mother. With no formal training, he mastered the piano and became the family's pianist. Despite being normally quiet and introspective, he reveled in mimicry and "voice tricks" akin to ventriloquism that continually entertained family guests during their occasional visits. Bell was also deeply affected by his mother's gradual deafness, (she began to lose her hearing when he was 12) and learned a manual finger language so he could sit at her side and tap out silently the conversations swirling around the family parlour. He also developed a technique of speaking in clear, modulated tones directly into his mother's forehead wherein she would hear him with reasonable clarity. Bell's preoccupation with his mother's deafness led him to study acoustics. His family was long associated with the teaching of elocution: his grandfather, Alexander Bell, in London, his uncle in Dublin, and his father, in Edinburgh, were all elocutionists. His father published a variety of works on the subject, several of which are still well known, especially his "The Standard Elocutionist" (1860), which appeared in Edinburgh in 1868. "The Standard Elocutionist" appeared in 168 British editions and sold over a quarter of a million copies in the United States alone. In this treatise, his father explains his methods of how to instruct deaf-mutes (as they were then known) to articulate words and read other people's lip movements to decipher meaning. Aleck's father taught him and his brothers not only to write Visible Speech but also to identify any symbol and its accompanying sound. Aleck became so proficient that he became a part of his father's public demonstrations and astounded audiences with his abilities in deciphering Latin, Gaelic and even Sanskrit symbols.
As a young child, Bell, like his brothers, received his early schooling at home from his father. At an early age, however, he was enrolled at the Royal High School, Edinburgh, Scotland, which he left at age 15, completing only the first four forms. His school record was undistinguished, marked by absenteeism and lacklustre grades. His main interest remained in the sciences, especially biology, while he treated other school subjects with indifference, to the dismay of his demanding father. Upon leaving school, Bell travelled to London to live with his grandfather, Alexander Bell. During the year he spent with his grandfather, a love of learning was born, with long hours spent in serious discussion and study. The elder Bell took great efforts to have his young pupil learn to speak clearly and with conviction, the attributes that his pupil would need to become a teacher himself. At age 16, Bell secured a position as a "pupil-teacher" of elocution and music, in Weston House Academy, at Elgin, Moray, Scotland. Although he was enrolled as a student in Latin and Greek, he instructed classes himself in return for board and £10 per session. The following year, he attended the University of Edinburgh; joining his older brother Melville who had enrolled there the previous year.
Bell's father encouraged Aleck's interest in speech and, in 1863, took his sons to see a unique automaton, developed by Sir Charles Wheatstone based on the earlier work of Baron Wolfgang von Kempelen. The rudimentary "mechanical man" simulated a human voice. Aleck was fascinated by the machine and after he obtained a copy of von Kempelen's book, published in German, and had laboriously translated it, he and his older brother Melville built their own automaton head. Their father, highly interested in their project, offered to pay for any supplies and spurred the boys on with the enticement of a "big prize" if they were successful. While his brother constructed the throat and larynx, Aleck tackled the more difficult task of recreating a realistic skull. His efforts resulted in a remarkably lifelike head that could "speak", albeit only a few words. The boys would carefully adjust the "lips" and when a bellows forced air through the windpipe, a very recognizable "Mama" ensued, to the delight of neighbors who came to see the Bell invention. Intrigued by the results of the automaton, Bell continued to experiment with a live subject, the family's Skye Terrier, "Trouve". After he taught it to growl continuously, Aleck would reach into its mouth and manipulate the dog's lips and vocal cords to produce a crude-sounding "Ow ah oo ga ma ma." With little convincing, visitors believed his dog could articulate "How are you grandma?" More indicative of his playful nature, his experiments convinced onlookers that they saw a "talking dog." However, these initial forays into experimentation with sound led Bell to undertake his first serious work on the transmission of sound, using tuning forks to explore resonance. At the age of 19, he wrote a report on his work and sent it to philologist Alexander Ellis, a colleague of his father (who would later be portrayed as Professor Henry Higgins in "Pygmalion"). Ellis immediately wrote back indicating that the experiments were similar to existing work in Germany. Dismayed to find that groundbreaking work had already been undertaken by Hermann von Helmholtz who had conveyed vowel sounds by means of a similar tuning fork "contraption", he pored over the German scientist's book, "Sensations of Tone". Working from his own errant mistranslation of the original German edition, Aleck fortuitously then made a deduction that would be the underpinning of all his future work on transmitting sound, reporting: "Without knowing much about the subject, it seemed to me that if vowel sounds could be produced by electrical means so could consonants, so could articulate speech", and also later remarking: "I thought that Helmhotz had done it... and that my failure was due only to my ignorance of electricity. It was a valuable blunder... If I had been able to read German in those days, I might never have commenced my experiments!"
In 1865, when the Bell family moved to London, Bell returned to Weston House as an assistant master and, in his spare hours, continued experiments on sound using a minimum of laboratory equipment. Bell concentrated on experimenting with electricity to convey sound and later installed a telegraph wire from his room in Somerset College to that of a friend. Throughout late 1867, his health faltered mainly through exhaustion. His younger brother, Edward "Ted," was similarly bed-ridden, suffering from tuberculosis. While Bell recovered (by then referring to himself in correspondence as "A.G. Bell") and served the next year as an instructor at Somerset College, Bath, Somerset, England, his brother's condition deteriorated. Edward would never recover. Upon his brother's death, Bell returned home in 1867. His older brother, "Melly" had married and moved out. With aspirations to obtain a degree at the University College London, Bell considered his next years as preparation for the degree examinations, devoting his spare time at his family's residence to studying. Helping his father in Visible Speech demonstrations and lectures brought Bell to Susanna E. Hull's private school for the deaf in South Kensington, London. His first two pupils were "deaf mute" girls who made remarkable progress under his tutelage. While his older brother seemed to achieve success on many fronts including opening his own elocution school, applying for a patent on an invention, and starting a family, Bell continued as a teacher. However, in May 1870, Melville died from complications due to tuberculosis, causing a family crisis. His father had also suffered a debilitating illness earlier in life and had been restored to health by a convalescence in Newfoundland. Bell's parents embarked upon a long-planned move when they realized that their remaining son was also sickly. Acting decisively, Alexander Melville Bell asked Bell to arrange for the sale of all the family property, conclude all of his brother's affairs (Bell took over his last student, curing a pronounced lisp), and join his father and mother in setting out for the "New World." Reluctantly, Bell also had to conclude a relationship with Marie Eccleston, who, he had surmised, was not prepared to leave England with him.
In 1870, at age 23, Bell, his brother's widow, Caroline (Margaret Ottaway), and his parents travelled on the "SS Nestorian" to Canada. After landing at Quebec City, the Bells boarded a train to Montreal and later to Paris, Ontario to stay with the Reverend Thomas Henderson, a family friend. After a brief stay with the Hendersons, the Bell family purchased a 10-and-a-half acre farm at Tutelo Heights (now called Tutela Heights), near Brantford, Ontario. The property consisted of an orchard, large farm house, stable, pigsty, hen-house and a carriage house, which bordered the Grand River. At the homestead, Bell set up his own workshop in the converted carriage house near to what he called his "dreaming place", a large hollow nestled in trees at the back of the property above the river. Despite his frail condition upon arriving in Canada, Bell found the climate and environs to his liking, and rapidly improved. He continued his interest in the study of the human voice and when he discovered the Six Nations Reserve across the river at Onondaga, he learned the Mohawk language and translated its unwritten vocabulary into Visible Speech symbols. For his work, Bell was awarded the title of Honorary Chief and participated in a ceremony where he donned a Mohawk headdress and danced traditional dances. After setting up his workshop, Bell continued experiments based on Helmholtz's work with electricity and sound. He designed a piano, which, by means of electricity, could transmit its music at a distance. Once the family was settled in, both Bell and his father made plans to establish a teaching practice and in 1871, he accompanied his father to Montreal, where Melville was offered a position to teach his System of Visible Speech.
Subsequently, his father was invited by Sarah Fuller, principal of the Boston School for Deaf Mutes (which continues today as the public Horace Mann School for the Deaf), in Boston, Massachusetts, United States, to introduce the Visible Speech System by providing training for Fuller's instructors, but he declined the post, in favor of his son. Traveling to Boston in April 1871, Bell proved successful in training the school's instructors. He was subsequently asked to repeat the program at the American Asylum for Deaf-mutes in Hartford, Connecticut and the Clarke School for the Deaf in Northampton, Massachusetts. Returning home to Brantford after six months abroad, Bell continued his experiments with his "harmonic telegraph". The basic concept behind his device was that messages could be sent through a single wire if each message was transmitted at a different pitch, but work on both the transmitter and receiver as needed. Unsure of his future, he first contemplated returning to London to complete his studies, but decided to return to Boston as a teacher. His father helped him set up his private practice by contacting Gardiner Greene Hubbard, the president of the Clarke School for the Deaf for a recommendation. Teaching his father's system, in October 1872 Alexander Bell opened his "School of Vocal Physiology and Mechanics of Speech" in Boston, which attracted a large number of deaf pupils with his first class numbering 30 students. While he was working as a private tutor, one of his most famous pupils was Helen Keller, who came to him as a young child unable to see, hear, or speak. She was later to say that Bell dedicated his life to the penetration of that "inhuman silence which separates and estranges." Several influential people of the time, including Bell, viewed deafness as something that ought to be eradicated, and also believed that with resources and effort they could teach the deaf to speak and avoid the use of sign language, thus enabling their integration within the wider society from which many were often being excluded. However in several schools children were mistreated, for example by having their hands tied behind their backs so they could not communicate by signing —the only language they knew— and were therefore forced to attempt oral communication.
In the following year, Bell became professor of Vocal Physiology and Elocution at the Boston University School of Oratory. During this period, he alternated between Boston and Brantford, spending summers in his Canadian home. At Boston University, Bell was "swept up" by the excitement engendered by the many scientists and inventors residing in the city. He continued his research in sound and endeavored to find a way to transmit musical notes and articulate speech, but although absorbed by his experiments, he found it difficult to devote enough time to experimentation. While days and evenings were occupied by his teaching and private classes, Bell began to stay awake late into the night, running experiment after experiment in rented facilities at his boarding house. Keeping up "night owl" hours, he worried that his work would be discovered and took great pains to lock up his notebooks and laboratory equipment. Bell had a specially made table where he could place his notes and equipment inside a locking cover. Worse still, his health deteriorated as he suffered severe headaches. Returning to Boston in fall 1873, Bell made a fateful decision to concentrate on his experiments in sound. Deciding to give up his lucrative private Boston practice, Bell only retained two students, six-year old "Georgie" Sanders, deaf from birth and 15-year old Mabel Hubbard. Each pupil would serve to play an important role in the next developments. George's father, Thomas Sanders, a wealthy businessman, offered Bell a place to stay at nearby Salem with Georgie's grandmother, complete with a room to "experiment". Although the offer was made by George's mother and followed the year-long arrangement in 1872 where her son and his nurse had moved to quarters next to Bell's boarding house, it was clear that Mr. Sanders was backing the proposal. The arrangement was for teacher and student to continue their work together with free room and board thrown in. Mabel was a bright, attractive girl who was ten years his junior but became the object of Bell's affection. Losing her hearing after a bout of scarlet fever at age five, she had learned to read lips but her father, Gardiner Greene Hubbard, Bell's benefactor and personal friend, wanted her to work directly with her teacher.
By 1874, Bell's initial work on the harmonic telegraph had entered a formative stage with progress it made both at his new Boston "laboratory" (a rented facility) as well as at his family home in Canada a big success. While working that summer in Brantford, Bell experimented with a "phonautograph," a pen-like machine that could draw shapes of sound waves on smoked glass by tracing their vibrations. Bell thought it might be possible to generate undulating electrical currents that corresponded to sound waves. Bell also thought that multiple metal reeds tuned to different frequencies like a harp would be able to convert the undulatory currents back into sound. But he had no working model to demonstrate the feasibility of these ideas. In 1874, telegraph message traffic was rapidly expanding and in the words of Western Union President William Orton, had become "the nervous system of commerce". Orton had contracted with inventors Thomas Edison and Elisha Gray to find a way to send multiple telegraph messages on each telegraph line to avoid the great cost of constructing new lines. When Bell mentioned to Gardiner Hubbard and Thomas Sanders that he was working on a method of sending multiple tones on a telegraph wire using a multi-reed device, the two wealthy patrons began to financially support Bell's experiments. Patent matters would be handled by Hubbard's patent attorney, Anthony Pollok. In March 1875, Bell and Pollok visited the famous scientist Joseph Henry, who was then director of the Smithsonian Institution, and asked Henry's advice on the electrical multi-reed apparatus that Bell hoped would transmit the human voice by telegraph. Henry replied that Bell had "the germ of a great invention". When Bell said that he did not have the necessary knowledge, Henry replied, "Get it!" That declaration greatly encouraged Bell to keep trying, even though he did not have the equipment needed to continue his experiments, nor the ability to create a working model of his ideas. However, a chance meeting in 1874 between Bell and Thomas A. Watson, an experienced electrical designer and mechanic at the electrical machine shop of Charles Williams, changed all that. With financial support from Sanders and Hubbard, Bell was able to hire Thomas Watson as his assistant and the two of them experimented with acoustic telegraphy. On 2 June 1875, Watson accidentally plucked one of the reeds and Bell, at the receiving end of the wire, heard the overtones of the reed; overtones that would be necessary for transmitting speech. That demonstrated to Bell that only one reed or armature was necessary, not multiple reeds. This led to the "gallows" sound-powered telephone, which was able to transmit indistinct, voice-like sounds, but not clear speech. The race to the patent office. In 1875, Bell developed an acoustic telegraph and drew up a patent application for it. Since he had agreed to share U.S. profits with his investors Gardiner Hubbard and Thomas Sanders, Bell requested that an associate in Ontario, George Brown, attempt to patent it in Britain, instructing his lawyers to apply for a patent in the U.S. only after they received word from Britain (Britain would issue patents only for discoveries not previously patented elsewhere). Meanwhile, Elisha Gray was also experimenting with acoustic telegraphy and thought of a way to transmit speech using a water transmitter. On February 14, 1876, Gray filed a caveat with the U.S. Patent Office for a telephone design that used a water transmitter. That same morning, Bell's lawyer filed Bell's application with the patent office. There is considerable debate about who arrived first and Gray later challenged the primacy of Bell's patent. Bell was in Boston on February 14, 1876. Bell's patent 174,465, was issued to Bell on March 7, 1876, by the U.S. Patent Office. Bell's patent covered "the method of, and apparatus for, transmitting vocal or other sounds telegraphically... by causing electrical undulations, similar in form to the vibrations of the air accompanying the said vocal or other sound" Bell returned to Boston the same day and the next day resumed work, drawing in his notebook a diagram similar to that in Gray's patent caveat. On March 10, 1876, three days after his patent was issued, Bell succeeded in getting his telephone to work, using a liquid transmitter similar to Gray's design. Vibration of the diaphragm caused a needle to vibrate in the water, varying the electrical resistance in the circuit. When Bell spoke the famous sentence "Mr Watson—Come here—I want to see you" into the liquid transmitter, Watson, listening at the receiving end in an adjoining room, heard the words clearly. Although Bell was accused, and is still accused, of stealing the telephone from Gray, Bell used Gray's water transmitter design only after Bell's patent was granted and only as a proof of concept scientific experiment to prove to his own satisfaction that intelligible "articulate speech" (Bell's words) could be electrically transmitted. After March 1876, Bell focused on improving the electromagnetic telephone and never used Gray's liquid transmitter in public demonstrations or commercial use. The patent examiner, Zenas Fisk Wilber, later stated in a sworn affidavit that he was an alcoholic who was much in debt to Bell's lawyer, Marcellus Bailey, with whom he had served in the Civil War. He claimed he showed Gray's patent caveat to Bailey. Wilber also claimed (after Bell arrived in Washington D.C. from Boston) that he showed Gray's caveat to Bell and that Bell paid him $100. Bell claimed they discussed the patent only in general terms, although in a letter to Gray, Bell admitted that he learned some of the technical details. Bell denied in a sworn affidavit that he ever gave Wilber any money.
Continuing his experiments in Brantford, Bell brought home a working model of his telephone. On August 3, 1876, from the telegraph office in Mount Pleasant five miles (8 km) away from Brantford, Bell sent a tentative telegram indicating that he was ready. With curious onlookers packed into the office as witnesses, faint voices were heard replying. The following night, he amazed guests as well as his family when a message was received at the Bell home from Brantford, four miles (six km) distant along an improvised wire strung up along telegraph lines and fences, and laid through a tunnel. This time, guests at the household distinctly heard people in Brantford reading and singing. These experiments clearly proved that the telephone could work over long distances. Bell and his partners, Hubbard and Sanders, offered to sell the patent outright to Western Union for $100,000. The president of Western Union balked, countering that the telephone was nothing but a toy. Two years later, he told colleagues that if he could get the patent for $25 million he would consider it a bargain. By then, the Bell company no longer wanted to sell the patent. Bell's investors would become millionaires while he fared well from residuals and at one point had assets of nearly one million dollars. Bell began a series of public demonstrations and lectures in order to introduce the new invention to the scientific community as well as the general public. Only one day after, his demonstration of an early telephone prototype at the 1876 Centennial Exposition in Philadelphia made the telephone the featured headline worldwide. Influential visitors to the exhibition included Emperor Pedro II of Brazil, and later Bell had the opportunity to demonstrate the invention personally to William Thomson, a renowned Scottish scientist and even Queen Victoria who had requested a private audience at Osborne House, her Isle of Wight home; she called the demonstration "most extraordinary". The enthusiasm surrounding Bell's public displays laid the groundwork for universal acceptance of the revolutionary device. The Bell Telephone Company was created in 1877, and by 1886, over 150,000 people in the U.S. owned telephones. Bell company engineers made numerous other improvements to the telephone, which emerged as one of the most successful products ever. In 1879, the Bell company acquired Edison's patents for the carbon microphone from Western Union. This made the telephone practical for long distances and it was no longer necessary to shout to be heard at the receiving telephone.
As is sometimes common in scientific discoveries, simultaneous developments can occur, as evidenced by a number of inventors who were at work on the telephone. Over a period of 18 years, the Bell Telephone Company faced over 600 lawsuits posing legal challenges concerning the rights to the telephone, but none was successful in establishing priority over the original Bell patent and the Bell Telephone Company never lost a case that had proceeded to a final trial stage. Bell's laboratory notes and family letters were the key to establishing a long lineage to his experiments. The Bell company lawyers successfully fought off myriad lawsuits generated initially around the challenges by Elisha Gray and Amos Dolbear. In personal correspondence to Bell, both Gray and Dolbear had acknowledged his prior work, which considerably weakened their later claims. On 13 January 1887, the United States Government moved to annul the patent issued to Bell on the grounds of fraud and misrepresentation. After a series of decisions and reversals, the Bell company won a decision in the Supreme Court, though a couple of the original claims from the lower court cases were left undecided. By the time that the trial wound its way through nine years of legal battles, the U.S. prosecuting attorney had died and the two Bell patents (No. 174,465 and dated 7 March 1876 and No. 186,787 dated January 30, 1877) were no longer in effect, although the presiding judges agreed to continue the proceedings due to the case's importance as a "precedent." With a change in administration and charges of conflict of interest (on both sides) arising from the original trial, the U.S. Attorney General dropped the law suit on 30 November 1897 leaving several issues undecided on the merits. During a deposition filed for the 1887 trial, Italian inventor Antonio Meucci also claimed to have created the first working model of a telephone in Italy in 1834. In 1886, in the first of three cases in which he was involved, Meucci took the stand as a witness in the hopes of establishing his invention's priority. Meucci's evidence in this case was disputed due to a lack of material evidence for his inventions as his working models were purportedly lost at the laboratory of American District Telegraph (ADT) of New York, which later, in 1901, was incorporated as a subsidiary of Western Union. Meucci's work, like many other inventors of the period, was based on earlier acoustic principles and despite evidence of earlier experiments, the final case involving Meucci was eventually dropped upon Meucci's death. However, due to the efforts of Congressman Vito Fossella, the U.S. House of Representatives on 11 June 2002 stated that Meucci's "work in the invention of the telephone should be acknowledged", even though this did not put an end to a still contentious issue. Some modern scholars do not agree with the claims that Bell's work on the telephone was influenced by Meucci's inventions. The value of the Bell patent was acknowledged throughout the world, and patent applications were made in most major countries, but when Bell had delayed the German patent application, the electrical firm of Siemens & Halske (S&H) managed to set up a rival manufacturer of Bell telephones under their own patent. The Siemens company produced near-identical copies of the Bell telephone without having to pay royalties. A series of agreements in other countries eventually consolidated a global telephone operation. The strain put on Bell by his constant appearances in court, necessitated by the legal battles, eventually resulted in his resignation from the company.
On July 11, 1877, a few days after the Bell Telephone Company was established, Bell married Mabel Hubbard (1857–1923) at the Hubbard estate in Cambridge. His wedding present to his bride was to turn over 1,487 of his 1,497 shares in the newly created Bell Telephone Company. Shortly thereafter, the newlyweds embarked on a year-long honeymoon in Europe. During that excursion, Alec took a handmade model of his telephone with him, making it a "working holiday". The courtship had begun years earlier, however Alexander waited until he was more financially secure before marrying. Although the telephone appeared to be an "instant" success, it was not initially a profitable venture and Bell's main sources of income were from lectures until after 1897. One unusual request exacted by his fiancée was that he use "Alec" rather than the family's earlier familiar name of "Aleck." From 1876, he would sign his name "Alec Bell." They had four children: Elsie May Bell (1878–1964) who married Gilbert Grosvenor of National Geographic fame, Marian Hubbard Bell (1880–1962) who was referred to as "Daisy", and two sons who died in infancy. The Bell family home was located in Cambridge, Massachusetts until 1880 when Bell's father-in-law bought a house, and later in 1882 the, in Washington, D.C. for the Bell family, so that Alec's family could be with him while he attended to the numerous court cases involving patent disputes. Bell was a British subject throughout his early life in Scotland and later in Canada until 1882, when he became a naturalized citizen of the United States. In 1915, he characterized his status as: "I am not one of those hyphenated Americans who claim allegiance to two countries." Despite this declaration, Bell has been claimed as a "native son" by Canada, Scotland and the United States. By 1885, a new summer retreat was contemplated. That summer, the Bells had a vacation on Cape Breton Island in Nova Scotia, spending time at the small village of Baddeck. Returning in 1886, Bell started building an estate on a point across from Baddeck, overlooking Bras d'Or Lake. By 1889, a large house, christened "The Lodge" was completed and two years later, a larger complex of buildings were begun that the Bells would name Beinn Bhreagh (Gaelic: "beautiful mountain") after Alec's ancestral Scottish highlands. Bell would spend his final, and some of his most productive, years in residence in both Washington, D.C., where he and his family initially resided for most of the year, and Beinn Bhreagh. Until the end of his life, Bell and his family would alternate between the two homes, but "Beinn Bhreagh" would, over the next 30 years, become more than a summer home as Bell became so absorbed in his experiments that annual stays lengthened. Both Mabel and Alec became immersed in the Baddeck community and were accepted by the villagers as "their own". The Bells were still in residence at "Beinn Bhreagh" when the Halifax Explosion occurred on 6 December 1917. Mabel and Alec mobilized the community to help victims in Halifax.
Although Alexander Graham Bell is most often associated with the invention of the telephone, his interests were extremely varied. According to one of his biographers, Charlotte Gray, Bell's work ranged "unfettered across the scientific landscape" and he often went to bed voraciously reading the "Encyclopaedia Britannica", scouring it for new areas of interest. The range of Bell's inventive genius is represented only in part by the 18 patents granted in his name alone and the 12 he shared with his collaborators. These included 14 for the telephone and telegraph, four for the photophone, one for the phonograph, five for aerial vehicles, four for "hydroairplanes" and two for selenium cells. Bell's inventions spanned a wide range of interests and included a metal jacket to assist in breathing, the audiometer to detect minor hearing problems, a device to locate icebergs, investigations on how to separate salt from seawater, and work on finding alternative fuels. Bell worked extensively in medical research and invented techniques for teaching speech to the deaf. During his Volta Laboratory period, Bell and his associates considered impressing a magnetic field on a record as a means of reproducing sound. Although the trio briefly experimented with the concept, they were unable to develop a workable prototype. They abandoned the idea, never realizing they had glimpsed a basic principle which would one day find its application in the tape recorder, the hard disc and floppy disc drive and other magnetic media. Bell's own home used a primitive form of air conditioning, in which fans blew currents of air across great blocks of ice. He also anticipated modern concerns with fuel shortages and industrial pollution. Methane gas, he reasoned, could be produced from the waste of farms and factories. At his Canadian estate in Nova Scotia, he experimented with composting toilets and devices to capture water from the atmosphere. In a magazine interview published shortly before his death, he reflected on the possibility of using solar panels to heat houses.
Bell is also credited with the invention of the metal detector in 1881. The device was quickly put together in an attempt to find the bullet in the body of U.S. President James Garfield. The metal detector worked flawlessly in tests but did not find the assassin's bullet partly because the metal bed frame on which the President was lying disturbed the instrument, resulting in static. The president's surgeons, who were skeptical of the device, ignored Bell's requests to move the president to a bed not fitted with metal springs. Alternatively, although Bell had detected a slight sound on his first test, the bullet may have been lodged too deeply to be detected by the crude apparatus. Bell gave a full account of his experiments in a paper read before the American Association for the Advancement of Science (AAAS) in August 1882.
The March 1906 "Scientific American" article by American hydrofoil pioneer William E. Meacham explained the basic principle of hydrofoils and hydroplanes. Bell considered the invention of the hydroplane as a very significant achievement. Based on information gained from that article he began to sketch concepts of what is now called a hydrofoil boat. Bell and assistant Frederick W. "Casey" Baldwin began hydrofoil experimentation in the summer of 1908 as a possible aid to airplane takeoff from water. Baldwin studied the work of the Italian inventor Enrico Forlanini and began testing models. This led him and Bell to the development of practical hydrofoil watercraft. During his world tour of 1910–1911, Bell and Baldwin met with Forlanini in France. They had rides in the Forlanini hydrofoil boat over Lake Maggiore. Baldwin described it as being as smooth as flying. On returning to Baddeck, a number of initial concepts were built as experimental models, including the "Dhonnas Beag", the first self-propelled Bell-Baldwin hydrofoil. The experimental boats were essentially proof-of-concept prototypes that culminated in the more substantial HD-4, powered by Renault engines. A top speed of 54 miles per hour (87 km/h) was achieved, with the hydrofoil exhibiting rapid acceleration, good stability and steering along with the ability to take waves without difficulty. In 1913, Dr. Bell hired Walter Pinaud, a Sydney yacht designer and builder as well as the proprietor of Pinaud's Yacht Yard in Westmount, Nova Scotia to work on the pontoons of the HD-4. Pinaud soon took over the boatyard at Bell Laboratories on Beinn Bhreagh, Bell's estate near Baddeck, Nova Scotia. Pinaud's experience in boat-building enabled him to make useful design changes to the HD-4. After the First World War, work began again on the HD-4. Bell's report to the U.S. Navy permitted him to obtain two 350 horsepower (260 kW) engines in July 1919. On 9 September 1919, the HD-4 set a world marine speed record of 70.86 miles per hour (114.04 km/h), a record which stood for ten years.
In 1891, Bell had begun experiments to develop motor-powered heavier-than-air aircraft. The AEA was first formed as Bell shared the vision to fly with his wife, who advised him to seek "young" help as Alexander was at the graceful age of 60. In 1898, Bell experimented with tetrahedral box kites and wings constructed of multiple compound tetrahedral kites covered in silk. The tetrahedral wings were named "Cygnet" I, II and III, and were flown both unmanned and manned ("Cygnet I" crashed during a flight carrying Selfridge) in the period from 1907–1912. Some of Bell's kites are on display at the Alexander Graham Bell National Historic Site. Bell was a supporter of aerospace engineering research through the Aerial Experiment Association (AEA), officially formed at Baddeck, Nova Scotia, in October 1907 at the suggestion of Mrs. Mabel Bell and with her financial support. The AEA was headed by Bell and the founding members were four young men: American Glenn H. Curtiss, a motorcycle manufacturer at the time termed the "world's fastest man" having had ridden his self-constructed motor bicycle around in the shortest time, later was awarded the Scientific American Trophy for the first official one-kilometre flight in the Western hemisphere and became a world-renowned airplane manufacturer; Lieutenant Thomas Selfridge, an official observer from the U.S. government and the only person in the army who believed aviation was the future, Frederick W. Baldwin, the first Canadian and first British subject to pilot a public flight in Hammondsport, New York; and J.A.D. McCurdy; both engineering students at University of Toronto. The AEA's work progressed to heavier-than-air machines, applying their knowledge of kites to gliders. Moving to Hammondsport, the group then designed and built the "Red Wing", framed in bamboo and covered in red silk and powered by a small air-cooled engine. On March 12, 1908, over Keuka Lake, the biplane lifted off on the first public flight in North America. The innovations that were incorporated into this design included a cockpit enclosure and tail rudder (later variations on the original design would add ailerons as a means of control). One of the AEA project's inventions, the aileron, is a standard component of aircraft today. (The aileron was also invented independently by Robert Esnault-Pelterie.) The "White Wing" and "June Bug" were to follow and by the end of 1908, over 150 flights without mishap had been accomplished. However, the AEA had depleted its initial reserves and only a $10,000 grant from Mrs. Bell allowed it to continue with experiments. Their final aircraft design, the "Silver Dart" embodied all of the advancements found in the earlier machines. On February 23, 1909, Bell was present as the "Silver Dart" flown by J.A.D. McCurdy from the frozen ice of Bras d'Or, made the first aircraft flight in Canada. Bell had worried that the flight was too dangerous and had arranged for a doctor to be on hand. With the successful flight, the AEA disbanded and the "Silver Dart" would revert to Baldwin and McCurdy who began the Canadian Aerodrome Company and would later demonstrate the aircraft to the Canadian Army.
Along with many very prominent thinkers and scientists of the time, Bell was connected with the eugenics movement in the United States. In his lecture "Memoir upon the formation of a deaf variety of the human race" presented to the National Academy of Sciences on 13 November 1883 he noted that congenitally deaf parents were more likely to produce deaf children and tentatively suggested that couples where both parties were deaf should not marry. However, it was his hobby of livestock breeding which led to his appointment to biologist David Starr Jordan's Committee on Eugenics, under the auspices of the American Breeders Association. The committee unequivocally extended the principle to man. From 1912 until 1918 he was the chairman of the board of scientific advisers to the Eugenics Record Office associated with Cold Spring Harbor Laboratory in New York, and regularly attended meetings. In 1921, he was the honorary president of the Second International Congress of Eugenics held under the auspices of the American Museum of Natural History in New York. Organisations such as these advocated passing laws (with success in some states) that established the compulsory sterilization of people deemed to be, as Bell called them, a "defective variety of the human race". By the late 1930s, about half the states in the U.S. had eugenics laws, and the California laws were used as a model for eugenics laws in Nazi Germany.
Honors and tributes flowed to Bell in increasing numbers as his most famous invention became ubiquitous and his personal fame grew. Bell received numerous honorary degrees from colleges and universities, to the point that the requests almost became burdensome. During his life he also received dozens of major awards, medals and other tributes. These included statuary monuments to both him and the new form of communication his telephone created, notably the erected in his honor in Brantford, Ontario's "Alexander Graham Bell Gardens" in 1917. A large number of Bell's reside at both the United States Library of Congress Manuscript Division (as the "Alexander Graham Bell Family Papers"), and at the, Cape Breton University, Nova Scotia; major portions of which are available for online viewing. In 1880, Bell received the Volta Prize with a purse of 50,000 francs (approximately US$10,000) for the invention of the telephone from the Académie française, representing the French government. Among the luminaries who judged were Victor Hugo and père Alexandre Dumas. The Volta Prize was conceived by Napoleon Bonaparte in 1801, and named in honor of Alessandro Volta, with Bell receiving the third grand prize in its history. Since Bell was becoming increasingly affluent, he used his prize money to create endowment funds (the 'Volta Fund') and institutions in and around the United States capital of Washington, D.C.. These included the prestigious" 'Volta Laboratory Association' "(1880), also known as the" 'Volta Laboratories' "and as the" 'Alexander Graham Bell Laboratory', "as well the Volta Bureau (1887) as a center for studies on deafness. The Volta Laboratory became a permanently funded experimental facility devoted to scientific discovery, and the very next year invented a wax phonograph cylinder that was later used by Thomas Edison; The laboratory was also the site where he and his assistant invented his" 'proudest achievement'," the photophone, the "optical telephone" which presaged fibre optical telecommunications, while the Volta Bureau would later evolve into the Alexander Graham Bell Association for the Deaf and Hard of Hearing (the AG Bell), a leading center for the research and pedagogy of deafness. In partnership with Gardiner Hubbard, Bell helped established the publication Science during the early 1880s. In 1888, Bell was one of the founding members of the National Geographic Society and became its second president (1897–1904), and also became a Regent of the Smithsonian Institution (1898–1922). The French government conferred on him the decoration of the Légion d'honneur (Legion of Honour); the Royal Society of Arts in London awarded him the Albert Medal in 1902; and the University of Würzburg, Bavaria, granted him a Ph.D. He was awarded the AIEE's Edison Medal in 1914 "For meritorious achievement in the invention of the telephone." The "bel" (B) and the smaller "decibel" (dB) are units of measurement of sound intensity invented by Bell Labs and named after him. Since 1976 the IEEE's Alexander Graham Bell Medal has been awarded to honor outstanding contributions in the field of telecommunications. The 150th anniversary of Bell's birth in 1997 was marked by a special issue of commemorative £1 banknotes from the Royal Bank of Scotland. The illustrations on the reverse of the note include Bell's face in profile, his signature, and objects from Bell's life and career: users of the telephone over the ages; an audio wave signal; a diagram of a telephone receiver; geometric shapes from engineering structures; representations of sign language and the phonetic alphabet; the geese which helped him to understand flight; and the sheep which he studied to understand genetics. Additionally, the Government of Canada honoured Bell in 1997 with a $100CAD gold coin, in tribute also to the 150th anniversary of his birth, and with a silver dollar coin in 2009 to honour of the 100th anniversary of flight in Canada. That first flight was made by an airplane designed under Dr. Bell's tutelage, named the Silver Dart Bell's image, and also those of his many inventions have graced paper money, coinage and postal stamps in numerous countries worldwide for many dozens of years. Bell's name is widely known and still used as part of the names of dozens of educational institutes, corporate namesakes, street and place names around the world. Alexander Graham Bell was also ranked 57th among the 100 Greatest Britons (2002) in an official BBC nationwide poll, and among the Top Ten Greatest Canadians (2004), and the 100 Greatest Americans (2005).
Bell died of diabetes on August 2, 1922, at his private estate, Beinn Bhreagh, Nova Scotia, at age 75. Bell had also been afflicted with pernicious anemia. While tending to her husband after a long illness, Mabel whispered, "Don't leave me." By way of reply, Bell traced the sign for "no"—and then he expired. Upon Bell's death, during his funeral, "every phone on the continent of North America was silenced in honor of the man who had given to mankind the means for direct communication at a distance". "[The Government expresses] to you our sense of the world's loss in the death of your distinguished husband. It will ever be a source of pride to our country that the great invention, with which his name is immortally associated, is a part of its history. On the behalf of the citizens of Canada, may I extend to you an expression of our combined gratitude and sympathy. Dr. Alexander Graham Bell was buried atop Beinn Bhreagh mountain, on his estate where he had resided increasingly for the last 35 years of his life, overlooking Bras d'Or Lake. He was survived by his wife and his two daughters, Elisa May and Marion.
Anatolia (, from Greek '; also Asia Minor, from, ') is a geographic and historical term denoting the westernmost protrusion of Asia, comprising about two-thirds of the modern Republic of Turkey. The region is bounded by the Black Sea to the north, Georgia to the northeast, Armenia and the Euphrates river to the east, the Mesopotamian plain and Orontes river to the southeast, the Mediterranean Sea to the south, and the Aegean Sea to the west. Though Anatolia lies entirely within Turkey, the two are not synonymous, as the borders of Turkey extend far to the east of Anatolia. Anatolia has been home to many civilizations throughout history, such as the Hittites, Phrygians, and Lydians, and Achaemenid, Greek, Armenian, Roman, Byzantine, Anatolian Seljuk and Ottoman states. While the coastal regions of Anatolia are generally humid and covered with forests, the central Anatolia mostly consists of a semiarid, high-altitude plateau, with altitude increasing to the east. Steep ranges separate the plateau from the coastline to the north and south, while to the west the plateau slopes down gently to the broad Aegean coastal plain. The Sea of Marmara forms a connection between Black and Aegean seas through the Bosporus and Dardanelles straits, and separates Anatolia from Thrace on the European mainland. The vast majority of the people residing in Anatolia are Turks. Kurds, who constitute a major community in southeastern Anatolia, are the largest ethnic minority. Azerbaijanis, Albanians, Arabs, Armenians, Bosnians, Circassians, Georgians, Greeks, Jews, Lazs and a number of other ethnic groups also live in Anatolia in smaller numbers.
The Anatolian peninsula is bounded by the Black Sea to the north, the Mediterranean Sea to the south, the Aegean Sea to the west, and the sea of Marmara to the northwest, which separates Anatolia from Thrace in Europe. To the east, Anatolia is bounded by Georgia, Armenia and the Euphrates River before that river bends to the southeast to enter Mesopotamia. To the southeast, Anatolia is bounded by the ranges that separate it from the Orontes valley in Syria and the Mesopotamian plain. The norther coast of Anatolia stretches farther east than the central region, reaching all the way to the modern border with Georgia. Anatolia's terrain is structurally complex. A central massif composed of uplifted blocks and downfolded troughs, covered by recent deposits and giving the appearance of a plateau with rough terrain, is wedged between two folded mountain ranges that converge in the east. True lowland is confined to a few narrow coastal strips along the Aegean, Mediterranean, and Black Sea coasts. Flat or gently sloping land is rare and largely confined to the deltas of the Kızıl River, the coastal plains of Çukurova and the valley floors of the Gediz River and the Büyük Menderes River as well as some interior high plains in Anatolia, mainly around "Tuz Gölü" (Salt Lake) and the Konya Basin ("Konya Ovasi").
The Black Sea is characterized by a range of steep mountains that extend along the entire length of the Black Sea coast, separating it from the inland Anatolian plateau. In the west, the mountains tend to be low, with elevations from 1,525 to 1,800 meters, but they rise in the easterly direction to heights greater than 3,000 meters south of Rize, reaching 3,937 m at the Kaçkar Mountains in the Pontic Alps. Lengthy, troughlike valleys and basins characterize the mountains. The southern slopes, facing the Anatolian Plateau, are mostly unwooded, but the northern slopes contain dense growths of both deciduous and evergreen trees. The higher slopes facing northwest tend to be densely forested. The coast is rugged and rocky, with rivers that cascade through the gorges of the coastal ranges. A few larger rivers, those cutting back through the Pontic Alps, have tributaries that flow in broad, elevated basins. Access inland from the coast is limited to a few narrow valleys because mountain ridges. Because of these natural conditions, the Black Sea coast historically has been isolated from Anatolia.
The coast of Anatolia that borders the Sea of Marmara consists mainly of rolling plateau country well suited to agriculture. It receives about 520 millimeters of rainfall annually. Densely populated, this area includes the cities of Istanbul and Bursa, Turkey's fourth largest city. The Bosphorus, which links the Sea of Marmara and the Black Sea, is about twenty-five kilometers long and averages 1.5 kilometers in width but narrows in places to less than 1000 meters. There are two suspension bridges over the Bosphorus, both its Asian and European banks rise steeply from the water and form a succession of cliffs, coves, and nearly landlocked bays. Most of the shores are densely wooded and are marked by numerous small towns and villages. The Dardanelles Strait, which links the Sea of Marmara and the Aegean Sea, is approximately forty kilometers long and increases in width toward the south. Unlike the Bosphorus, the Dardanelles has fewer settlements along its shores. The most important valleys are the Kocaeli Valley, the Bursa Ovasi (Bursa Basin), and the Plains of Troy (historically known as the Troad.) The valley lowlands around Bursa are densely populated.
Located on the west coast of Anatolia, the Aegean region has a fertile soil and a typically Mediterranean climate; with mild, wet winters and hot, dry summers. The broad, cultivated lowland valleys contain about half of the country's richest farmlands. The largest city in the Aegean Region of Turkey is İzmir, which is also the country's third largest city and a major manufacturing center, as well as its second largest port after Istanbul. Olive and olive oil production is particularly important for the economy of the region. The seaside town of Ayvalık and numerous towns in the provinces of Balıkesir, İzmir and Aydın are particularly famous for their olive oil and related products; such as soap and cosmetics. The region also has many important centers of tourism which are known both for their historic monuments and for the beauty of their beaches; such as Assos, Ayvalık, Bergama, Foça, İzmir, Çeşme, Sardis, Ephesus, Kuşadası, Didim, Miletus, Bodrum, Marmaris, Datça and Fethiye.
Beginning in the west of Antalya province, the south-facing mediterranean coast of Turkey is separated from the interior by steep ranges, known as the Taurus mountains, that run along the entire length of the coast. The Taurus Mountains ("Toros Dağları") are Anatolia's second chain of folded mountains. The south facing slopes rise steeply from the mediterranean coastal plain, but slope very gently on the north side towards the Anatolian plateau. In the east, the Taurus mountains arc around the northern side of the Arabian Platform, before turning south and continuing as the ranges that define the Great Rift Valley. Between Adana and Antalya, the Taurus Mountains rise sharply from the coast to high elevations, reaching altitudes of over 3,700 meters north of Adana. The Taurus Mountains are more rugged and less dissected by rivers than the Pontus Mountains and historically have served as a barrier to human movement inland from the Mediterranean coast except where there are mountain passes such as the historic Cilician Gates (Gülek Pass), northwest of Adana. Toward the east, the extensive plains around Adana, Turkey's fourth largest city, consist largely of reclaimed flood lands. In general, rivers have not cut valleys to the sea in the western part of the region. East of Adana, much of the coastal plain has limestone features such as collapsed caverns and sinkholes. Other than Adana, Antalya, and Mersin, the Mediterranean coast has few major cities, although it has numerous farming villages.
Stretching inland from the Aegean coastal plain, the Central Anatolia occupies the area between the two zones of the folded coastal ranges in the north and south, extending east to the point where the two ranges converge. The plateau-like, semiarid highlands of Anatolia are considered the heartland of the country. The region varies in elevation from 600 to 1,200 meters from west to east. The Anatolian plateau is interspersed with extinct volcanoes, the tallest of which is Mt. Erciyes, rising to 3917 m near Kayseri. Frequently interspersed throughout the folded mountains, and also situated on the Anatolian Plateau, are well-defined basins, which the Turks call "ova". Some are no more than a widening of a stream valley; others, such as the Konya Ovasi, are large basins of inland drainage or are the result of limestone erosion. Most of the basins take their names from cities or towns located at their rims. Where a lake has formed within the basin, the water body is usually saline as a result of the internal drainage — the water has no outlet to the sea. The two largest basins on the plateau are the Konya Ovasi and the basin occupied by the large salt lake, Tuz Gölü. Forested areas are confined to the northwest and northeast of the plateau. Rain-fed cultivation is widespread, with wheat being the principal crop. Irrigated agriculture is restricted to the areas surrounding rivers and wherever sufficient underground water is available. Important irrigated crops include barley, corn, cotton, various fruits, grapes, opium poppies, sugar beets, roses, and tobacco. There also is extensive grazing throughout the plateau. Central Anatolia receives little annual rainfall with an average precipitation of 400 millimeters per year. While parts of the northeastern and northwestern of the region receives more than precipitation, the driest semiarid central part of the plateau receives an average yearly precipitation of only 300 millimeters. However, actual rainfall from year to year is irregular and occasionally may be less than 200 millimeters, leading to severe reductions in crop yields for both rain-fed and irrigated agriculture. Overgrazing has contributed to soil erosion on some parts of the plateau. In general, the plateau experiences high temperatures and little rainfall in summer and cold weather with heavy snow in winter.
Anatolia's diverse topography and climate has fostered a similar diversity of plant and animal communities. The mountains and coastal plain of northern Anatolia, with its humid and mild climate, is home to temperate broadleaf, mixed and coniferous forests. The central and eastern plateau, with its drier continental climate, is home to deciduous forests and forest steppes. Western and southern Anatolia, which have a Mediterranean climate, are home to Mediterranean forests, woodlands, and scrub ecoregions.
Eastern Anatolia contains the oldest monumental structures in the world. For example, the monumental structures at Göbekli Tepe were built by hunters and gatherers a thousand years before the development of agriculture. Eastern Anatolia is also a heart region for the Neolithic revolution, one of the earliest areas in which humans domesticated plants and animals. Neolithic sites such as Çatalhöyük, Çayönü, Nevali Cori and Hacilar represent the world's oldest known agricultural villages. The earliest historical records of Anatolia are from the Akkadian Empire under Sargon in the 24th century BC. The region was famous for exporting various raw materials. The Assyrian Empire claimed the resources, notably silver. One of the numerous Assyrian cuneiform records found in Anatolia at Kanesh uses an advanced system of trading computations and credit lines. Unlike the Akkadians and the Assyrians, whose Anatolian possessions were peripheral to their core lands in Mesopotamia, the Hittites were centered at Hattusa in north-central Anatolia. They were speakers of an Indo-European language known as the "language of Nesa". Originating from Nesa, they conquered Hattusa in the 18th century BC, imposing themselves over a Hurrian speaking population. During the Late Bronze Age they created an empire, the Hittite New Kingdom, which reached its height in the 14th century BC. The empire included a large part of Anatolia, north-western Syria and upper Mesopotamia. After 1180 BC, the empire disintegrated into several independent "Neo-Hittite" states. Ancient Anatolia is subdivided by mordern scholars into various regions named after the people that occupied them, such as Lydia, Lycia, Caria, Mysia, Bithynia, Phrygia, Galatia, Lycaonia, Pisidia, Paphlagonia, Cilicia, and Cappadocia. Beginning with the Bronze Age Collapse at the end of the 1st millennium BC, the west coast of Anatolia was settled by Ionian Greeks. Over several centuries numerous Ancient Greek city states were established on the coasts of Anatolia. In the 6th century BC all of Anatolia was conquered by Cyrus the Great,founder of Achaemenid Empire. In the 4th century BC Alexander the Great conquered the peninsula. Following his death and the breakup of his empire, Anatolia was ruled by a series of Hellenistic kingdoms. Two hundred years later western and central Anatolia came under Roman control, but it continued to be strongly influenced by Hellenistic culture. In the first century BC the Armenians established the Armenian Empire under Tigran who reigned throughout much of eastern Anatolia between the Caspian, Black and Mediterranean seas. Anatolia is known as the birthplace of coinage as a medium of exchange (some time in the 7th century BC), which flourished during the Greek and Roman eras.
After the division of the Roman Empire, Anatolia became part of the East Roman, or Byzantine Empire. Byzantine control was challenged by Arab raids starting in the seventh century, but in the 9th and 10th century a resurgent Byzantine Empire regained its lost territories and even expanded beyond its traditional borders, into Armenia and Syria. Following the Battle of Manzikert in 1071, the Seljuk Turks swept across Anatolia and conquered it in its entirety by 1080. The Turkish language and Islamic religion were gradually introduced as a result of the Seljuk conquest, and this period marks the start of Anatolia's slow transition from predominantly Christian and Greek-speaking, to predominantly Muslim and Turkish-speaking. In the following century, the Byzantines managed to reassert their control in Western and Northern Anatolia. Control of Anatolia was then split between the Byzantine Empire and the Seljuk Sultanate of Rûm, with the Byzantine holdings gradually being reduced. In 1255, the Mongols swept through central and eastern Anatolia, and would remain until 1335. The Ilkhanate garrison was stationed near Ankara. By the end of the 14th century, most of Anatolia was controlled by various Anatolian Turkish Beyliks. The Turkmen Beyliks were under the control of the Mongols, at least nominally, through declining Seljuk Sultans. The Beyliks did not mint coins in the names of their own leaders while they remained under the suzerainty of the Ilkhanids. The Osmanli ruler Osman I was the first Turkish ruler who minted coins in his own name in 1320's, for it bears the legend "Minted by Osman son of Ertugul". Since the minting of coins was a prerogative accorded in Islamic practice only to be a sovereign, it can be considered that Osmanli became independent of the Mongol Khans. After the decline of the Ilkhanate from 1335–1353, the Mongol Empire's legacy in the region was the Uyghur Eretna Dynasty that was overthrown by Kadi Burhan al-Din in 1381. Among the Turkmen leaders the Ottomans emerged as great power under Osman and his son Orhan I. Smyrna was conquered in 1330, and the last Byzantine possession, Philadélphia (modern Alaşehir), fell in 1323. The Anatolian Turkish beyliks were in turn absorbed into the rising Ottoman Empire during the 15th century. The Ottomans completed the conquest of the peninsula in 1517 with the taking of Halicarnassus (Bodrum) from the Knights of Saint John.
With the beginning of the slow decline of the Ottoman Empire in the early 19th century, and as a result of the expansionist policies of Czarist Russia in the Caucasus, many Muslim nations and groups in that region, mainly Circassians, Tatars, Azeris, Lezgis, Chechens, and several Turkic groups left their ancestral homelands and settled in Anatolia. As the Ottoman Empire further fragmented during the Balkan Wars, much of the non-Christian populations of its former possessions, mainly the Balkan Muslims, flocked to Anatolia and were resettled in various locations, mostly in formerly Christian villages throughout Anatolia. Anatolia remained multi-ethnic until the early 20th century (see the rise of nationalism under the Ottoman Empire). Following the Greco-Turkish War of 1919-1922, all remaining ethnic Anatolian Greeks were forced out during the 1923 population exchange between Greece and Turkey. Since the foundation of the Republic of Turkey in 1923, most of Anatolia has been part of Turkey, its inhabitants being mainly Turks and Kurds (see demographics of Turkey and history of Turkey).
Apple Inc. is an American multinational corporation that designs and manufactures consumer electronics, computer software, and commercial servers. The company's best-known hardware products include Macintosh computers, the iPod, the iPhone and the iPad. Apple software includes the Mac OS X operating system; the iTunes media browser; the iLife suite of multimedia and creativity software; the iWork suite of productivity software; Aperture, a professional photography package; Final Cut Studio, a suite of professional audio and film-industry software products; and Logic Studio, a suite of audio tools. As of January 2010 the company operates 284 retail stores in ten countries, and an online store where hardware and software products are sold. Established in Cupertino, California on April 1, 1976 and incorporated January 3, 1977, the company was called Apple Computer, Inc. for its first 30 years, but dropped the word "Computer" on January 9, 2007 to reflect the company's ongoing expansion into the consumer electronics market in addition to its traditional focus on personal computers. Apple has about 35,000 employees worldwide and had worldwide annual sales of US$42.91 billion in its fiscal year ending September 26, 2009. For reasons as various as its philosophy of comprehensive aesthetic design to its distinctive advertising campaigns, Apple has established a unique reputation in the consumer electronics industry. This includes a customer base that is devoted to the company and its brand, particularly in the United States. "Fortune" magazine named Apple the most admired company in the United States in 2008 and in the world in 2009.
Apple was established on April 1, 1976 by Steve Jobs, Steve Wozniak, and Ronald Wayne, to sell the Apple I personal computer kit. They were hand-built by Wozniak and first shown to the public at the Homebrew Computer Club. The Apple I was sold as a motherboard (with CPU, RAM, and basic textual-video chips)—less than what is today considered a complete personal computer. The Apple I went on sale in July 1976 and was market-priced at $666.66 ($ in dollars, adjusted for inflation.) Apple was incorporated January 3, 1977 without Wayne, who sold his share of the company back to Jobs and Wozniak for $800. Multi-millionaire Mike Markkula provided essential business expertise and funding of $250,000 during the incorporation of Apple. The Apple II was introduced on April 16, 1977 at the first West Coast Computer Faire. It differed from its major rivals, the TRS-80 and Commodore PET, because it came with color graphics and an open architecture. While early models used ordinary cassette tapes as storage devices, they were superseded by the introduction of a 5 1/4 inch floppy disk drive and interface, the Disk II. The Apple II was chosen to be the desktop platform for the first "killer app" of the business world—the VisiCalc spreadsheet program. VisiCalc created a business market for the Apple II, and gave home users an additional reason to buy an Apple II—compatibility with the office. According to Brian Bagnall, Apple exaggerated its sales figures and was a distant third place to Commodore and Tandy until VisiCalc came along. By the end of the 1970s, Apple had a staff of computer designers and a production line. The company introduced the ill-fated Apple III in May 1980 in an attempt to compete with IBM and Microsoft in the business and corporate computing market. Jobs and several Apple employees including Jef Raskin visited Xerox PARC in December 1979 to see the Xerox Alto. Xerox granted Apple engineers three days of access to the PARC facilities in return for the option to buy 100,000 shares of Apple at the pre-IPO price of $10 a share. Jobs was immediately convinced that all future computers would use a graphical user interface (GUI), and development of a GUI began for the Apple Lisa. In December 1980, Apple launched the initial public offering of its stock to the investing public. When Apple went public, it generated more capital than any IPO since Ford Motor Company in 1956 and instantly created more millionaires (about 300) than any company in history. Several venture capitalists cashed out, reaping billions in long-term capital gains.
Steve Jobs began working on the Apple Lisa in 1978 but in 1982 he was pushed from the Lisa team due to infighting, and took over Jef Raskin's low-cost-computer project, the Macintosh. A turf war broke out between Lisa's "corporate shirts" and Jobs' "pirates" over which product would ship first and save Apple. Lisa won the race in 1983 and became the first personal computer sold to the public with a GUI, but was a commercial failure due to its high price tag and limited software titles. In 1984, Apple next launched the Macintosh. Its debut was announced by the now famous $1.5 million television commercial "1984". It was directed by Ridley Scott, aired during the third quarter of Super Bowl XVIII on January 22, 1984, and is now considered a watershed event for Apple's success and a "masterpiece". The Macintosh initially sold well, but follow-up sales were not strong. This was because of the again high price tag, as well as limited software titles. The machine's fortunes changed with the introduction of the LaserWriter, the first PostScript laser printer to be offered at a reasonable price point, and PageMaker, an early desktop publishing package. The Mac was particularly powerful in this market due to its advanced graphics capabilities, which were already necessarily built-in to create the intuitive Macintosh GUI. It has been suggested that the combination of these three products was responsible for the creation of the desktop publishing market. In 1985, a power struggle developed between Jobs and CEO John Sculley, who had been hired two years prior. Apple's board of directors sided with Sculley and Jobs was removed from his managerial duties. Jobs resigned from Apple and founded NeXT Inc. the same year. Apple's sustained growth during the early 1980s was in great part due to its leadership in the education sector, attributed to an implementation of the LOGO Programming Language by Logo Computer Systems Inc., (LCSI), for the Apple II platform. The success of Apple and LOGO in the education environment provided Apple with a broad base of loyal users around the world. The drive into education was accentuated in California by a momentous agreement concluded between Steve Jobs and Jim Baroux of LCSI, agreeing with the donation of one Apple II and one Apple LOGO software package to each public school in the state. The arrangement, eventually replicated in Texas, established a strong and pervasive presence for Apple in all schools throughout California, that ignited the acquisition of Apple IIs in schools across the country. The conquest of education became critical to Apple's acceptance in the home, as parents supported continued learning experience for children after school.
Having learned several painful lessons after introducing the bulky Macintosh Portable in 1989, Apple introduced the PowerBook in 1991, which established the modern form and ergonomic layout of the laptop computer. The Macintosh Portable was designed to be just as powerful as a desktop Macintosh and turned out 17 pounds with a 12 hour battery life. Apple sold fewer than 100,000 units. The Powerbook was 7 pounds and had a 3 hour battery life, and sold a billion dollars worth within the first year. The same year, Apple introduced System 7, a major upgrade to the operating system, which added color to the interface and introduced new networking capabilities. It remained the architectural basis for Mac OS until 2001. The success of the PowerBook and other products led to increasing revenue. For some time, it appeared that Apple could do no wrong, introducing fresh new products and generating increasing profits in the process. The magazine "MacAddict" has named the period between 1989 and 1991 as the "first golden age" of the Macintosh. Following the success of the Macintosh LC, Apple introduced the Centris line, a low end Quadra offering, and the ill-fated Performa line that was sold in several confusing configurations and software bundles to avoid competing with the various consumer outlets such as Sears, Price Club, and Wal-Mart, the primary dealers for these models. The result was disastrous for Apple as consumers did not understand the difference between models. During this time Apple experimented with a number of other failed consumer targeted products including digital cameras, portable CD audio players, speakers, video consoles, and TV appliances. Enormous resources were also invested in the problem-plagued Newton division based on John Sculley's unrealistic market forecasts. Ultimately, all of this proved too-little-too-late for Apple as their market share and stock prices continued to slide. Apple saw the Apple II series as too expensive to produce, while taking away sales from the low end Macintosh. In 1990 Apple released the Macintosh LC with a single expansion slot for the Apple IIe Card to migrate Apple II users to the Macintosh platform. Apple stopped selling the Apple IIe in 1993. Microsoft continued to gain market share with Windows, focusing on delivering software to cheap commodity personal computers while Apple was delivering a richly engineered, but expensive, experience. Apple relied on high profit margins and never developed a clear response. Instead they sued Microsoft for using a graphical user interface similar to the Apple Lisa in Apple Computer, Inc. v. Microsoft Corporation. The lawsuit dragged on for years before it was thrown out of court. At the same time, a series of major product flops and missed deadlines sullied Apple's reputation, and Sculley was replaced by Michael Spindler.
By the early 1990s, Apple was developing alternative platforms to the Macintosh, such as the UX. The Macintosh platform was becoming outdated since it was not built for multitasking, and several important software routines were programmed directly into the hardware. In addition, Apple was facing competition from 2 and UNIX vendors like Sun Microsystems. The Macintosh would need to be replaced by a new platform, or reworked to run on more powerful hardware. In 1994, Apple allied with IBM and Motorola in the AIM alliance. The goal was to create a new computing platform (the PowerPC Reference Platform), which would use IBM and Motorola hardware coupled with Apple's software. The AIM alliance hoped that PReP's performance and Apple's software would leave the PC far behind, thus countering Microsoft. The same year, Apple introduced the Power Macintosh, the first of many Apple computers to use IBM's PowerPC processor. In 1996, Michael Spindler was replaced by Gil Amelio as CEO. Gil Amelio made many changes at Apple, including massive layoffs. After multiple failed attempts to improve Mac OS, first with the Taligent project, then later with Copland and Gershwin, Amelio chose to purchase NeXT and its NeXTSTEP operating system, bringing Steve Jobs back to Apple as an advisor. On July 9, 1997, Gil Amelio was ousted by the board of directors after overseeing a three-year record-low stock price and crippling financial losses. Jobs became the interim CEO and began restructuring the company's product line. At the 1997 Macworld Expo, Steve Jobs announced that Apple would join Microsoft to release new versions of Microsoft Office for the Macintosh, and that Microsoft made a $150 million investment in non-voting Apple stock. On November 10, 1997, Apple introduced the Apple Store, tied to a new build-to-order manufacturing strategy.
On August 15, 1998, Apple introduced a new all-in-one computer reminiscent of the Macintosh 128K: the iMac. The iMac design team was led by Jonathan Ive, who would later design the iPod and the iPhone. The iMac featured modern technology and a unique design. It sold close to 800,000 units in its first five months and returned Apple to profitability for the first time since 1993. Through this period, Apple purchased several companies to create a portfolio of professional and consumer-oriented digital production software. In 1998, Apple announced the purchase of Macromedia's Final Cut software, signaling its expansion into the digital video editing market. The following year, Apple released two video editing products: iMovie for consumers, and Final Cut Pro for professionals, the latter of which has gone on to be a significant video-editing program, with 800,000 registered users in early 2007. In 2002 Apple purchased Nothing Real for their advanced digital compositing application Shake, as well as Emagic for their music productivity application Logic, which led to the development of their consumer-level GarageBand application. iPhoto's release the same year completed the iLife suite. Mac OS X, based on NeXT's OPENSTEP and BSD Unix was released on March 24, 2001, after several years of development. Aimed at consumers and professionals alike, Mac OS X aimed to combine the stability, reliability and security of Unix with the ease of use afforded by an overhauled user interface. To aid users in migrating from Mac OS 9, the new operating system allowed the use of OS 9 applications through Mac OS X's Classic environment. On May 19, 2001, Apple opened the first official Apple Retail Stores in Virginia and California. The same year, Apple introduced the iPod portable digital audio player. The product was phenomenally successful — over 100 million units were sold within six years. In 2003, Apple's iTunes Store was introduced, offering online music downloads for $0.99 a song and integration with the iPod. The service quickly became the market leader in online music services, with over 5 billion downloads by June 19, 2008. Since 2001 Apple's design team has progressively abandoned the use of translucent colored plastics first used in the iMac G3. This began with the titanium PowerBook and was followed by the white polycarbonate iBook and the flat-panel iMac.
At the Worldwide Developers Conference keynote address on June 6, 2005, Steve Jobs announced that Apple would begin producing Intel-based Mac computers in 2006. On January 10, 2006, the new MacBook Pro and iMac became the first Apple computers to use Intel's Core Duo CPU. By August 7, 2006 Apple had transitioned the entire Mac product line to Intel chips, over 1 year sooner than announced. The Power Mac, iBook, and PowerBook brands were retired during the transition; the Mac Pro, MacBook, and MacBook Pro became their respective successors. On April 29, 2009, The Wall Street Journal reported that Apple was building its own team of engineers to design microchips. Apple also introduced Boot Camp to help users install Windows XP or Windows Vista on their Intel Macs alongside Mac OS X. Apple's success during this period was evident in its stock price. Between early 2003 and 2006, the price of Apple's stock increased more than tenfold, from around $6 per share (split-adjusted) to over $80. In January 2006, Apple's market cap surpassed that of Dell. Nine years prior, Dell's CEO Michael Dell said that if he ran Apple he would "shut it down and give the money back to the shareholders." Although Apple's market share in computers has grown, it remains far behind competitor Microsoft, with only about 8 percent of desktops and laptops in the U.S. Delivering his keynote at the Macworld Expo on January 9, 2007, Jobs announced that Apple Computer, Inc. would from that point on be known as Apple Inc. The event also saw the announcement of the iPhone and the Apple TV. The following day, Apple shares hit $97.80, an all-time high at that point. In May, Apple's share price passed the $100 mark. On February 6, 2007, Apple indicated that it would sell music on the iTunes Store without DRM (which would allow tracks to be played on third-party players) if record labels would agree to drop the technology. On April 2, 2007, Apple and EMI jointly announced the removal of DRM technology from EMI's catalog in the iTunes Store, effective in May. On July 11, 2008, Apple launched the App Store to sell third-party applications for the iPhone and iPod Touch. Within a month, the store sold 60 million applications and brought in $1 million daily on average, with Jobs speculating that the App Store could become a billion-dollar business for Apple. Three months later, it was announced that Apple had become the third-largest mobile handset supplier in the world due to the popularity of the iPhone. On December 16, 2008, Apple announced 2009 would be the last year Apple would be attending the Macworld Expo, and that Phil Schiller would deliver the 2009 keynote in lieu of the expected Jobs. On January 14, 2009, an internal Apple memo from Jobs announced that he would be taking a six-month leave of absence, until the end of June 2009, to allow him to better focus on his health and to allow the company to better focus on its products. Despite Jobs' absence, Apple recorded its best non-holiday quarter (Q1 FY 2009) during the recession with a revenue of $8.16 billion and a profit of $1.21 billion.
On January 27, 2010, Apple introduced their much-anticipated media tablet, the iPad running iPhone OS. It offers multitouch interaction with multimedia formats including newspapers, magazines, ebooks, textbooks, photos, movies, TV shows videos, music, word processing documents, spreadsheets, video games, and all existing iPhone apps. It also includes a mobile version of Safari for internet browsing, as well as access to the App store, iTunes Library, iBooks store, contacts, and notepad. Content is downloadable via WIFI and optional 3G service or synced through the user's computer. AT&T is currently the sole provider of 3G wireless access for the iPad.<ref»
On October 23, 2001, Apple introduced the iPod digital music player. It has evolved to include various models targeting the wants of different users. The iPod is the market leader in portable music players by a significant margin, with more than 220 million units shipped as of September 9, 2009. Apple has partnered with Nike to offer the Nike+iPod Sports Kit enabling runners to synchronize and monitor their runs with iTunes and the. Apple currently sells four variants of the iPod.
At the Macworld Conference & Expo in January 2007, Steve Jobs revealed the long anticipated iPhone, a convergence of an Internet-enabled smartphone and iPod. The original iPhone combined a 2.5G quad band GSM and EDGE cellular phone with features found in hand held devices, running a scaled-down versions of Apple's Mac OS X (dubbed iPhone OS), with various Mac OS X applications such as Safari and Mail. It also includes web-based and Dashboard apps such as Google Maps and Weather. The iPhone features a touch screen display, 4, 8, or 16 GB of memory, Bluetooth, and Wi-Fi (both "b" and "g"). The iPhone first became available on June 29, 2007 for $499 (4 GB) and $599 (8 GB) with an AT&T contract. On February 5, 2008, Apple updated the original iPhone to have 16 GB of memory, in addition to the 8 GB and 4 GB models. On June 9, 2008, at WWDC 2008, Steve Jobs announced that the iPhone 3G would be available on July 11, 2008. This version added support for 3G networking, assisted-GPS navigation, and a price cut to $199 for the 8 GB version, and $299 for the 16 GB version, which was available in both black and white. The new version was visually different from its predecessor in that it eliminated the flat silver back, and large antenna square for a curved glossy black or white back. Following complaints from many people, the headphone jack was changed from a recessed jack to a flush jack to be compatible with more styles of headphones. The software capabilities changed as well, with the release of the new iPhone came the release of Apple's App Store; the store provided applications for download that were compatible with the iPhone. On April 24, 2009, the App Store surpassed one billion downloads. On June 8, 2009, at Apple's annual worldwide developers conference, the iPhone 3GS was announced, providing an incremental update to the device including faster internal components, support for faster 3G speeds, video recording capability, and voice control.
At the 2007 Macworld conference, Jobs demonstrated the Apple TV, (previously known as the iTV), a set-top video device intended to bridge the sale of content from iTunes with high-definition televisions. The device links up to a user's TV and syncs, either via Wi-Fi or a wired network, with one computer's iTunes library and streams from an additional four. The Apple TV originally incorporated a 40 GB hard drive for storage, includes outputs for HDMI and component video, and plays video at a maximum resolution of 720p. On May 31, 2007 a 160 GB drive was released alongside the existing 40 GB model and on January 15, 2008 a software update was released, which allowed media to be purchased directly from the Apple TV. In September 2009, Apple discontinued the original 40GB Apple TV and now continues to produce and sell the 160GB Apple TV.
Apple develops its own operating system to run on Macs, Mac OS X, the latest version being Mac OS X v10.6 Snow Leopard. Apple also independently develops computer software titles for its Mac OS X operating system. Much of the software Apple develops is bundled with its computers. An example of this is the consumer-oriented iLife software package that bundles iDVD, iMovie, iPhoto, iTunes, GarageBand, and iWeb. For presentation, page layout and word processing, iWork is available, which includes Keynote, Pages, and Numbers. iTunes, QuickTime media player, Safari web browser, and Software Update are available as free downloads for both Mac OS X and Windows. Apple also offers a range of professional software titles. Their range of server software includes the operating system Mac OS X Server; Apple Remote Desktop, a remote systems management application; WebObjects, Java EE Web application server; and Xsan, a Storage Area Network file system. For the professional creative market, there is Aperture for professional RAW-format photo processing; Final Cut Studio, a video production suite; Logic, a comprehensive music toolkit and Shake, an advanced effects composition program. Apple also offers online services with MobileMe (formerly.Mac) that bundles personal web pages, email, Groups, iDisk, backup, iSync, and Learning Center online tutorials. MobileMe is a subscription-based internet suite that capitalizes on the ability to store personal data on an online server and thereby keep all web-connected devices in sync. Announced at MacWorld Expo 2009, iWork.com allows iWork users to upload documents for sharing and collaboration.
Apple was one of several highly successful companies founded in the 1970s that bucked the traditional notions of what a corporate culture should look like in organizational hierarchy (flat versus tall, casual versus formal attire, etc.). Other highly successful firms with similar cultural aspects from the same period include Southwest Airlines and Microsoft. Originally, the company stood in opposition to staid competitors like IBM by default, thanks to the influence of its founders; Steve Jobs often walked around the office barefoot even after Apple was a Fortune 500 company. By the time of the "1984" TV ad, this trait had become a key way the company attempts to differentiate itself from its competitors. As the company has grown and been led by a series of chief executives, each with his own idea of what Apple should be, some of its original character has arguably been lost, but Apple still has a reputation for fostering individuality and excellence that reliably draws talented people into its employ, especially after Jobs' return. To recognize the best of its employees, Apple created the Apple Fellows program. Apple Fellows are those who have made extraordinary technical or leadership contributions to personal computing while at the company. The Apple Fellowship has so far been awarded to a few individuals including Bill Atkinson, Steve Capps, Rod Holt, Alan Kay, Guy Kawasaki, Al Alcorn, Don Norman, Rich Page, and Steve Wozniak.
According to surveys by J. D. Power, Apple has the highest brand and repurchase loyalty of any computer manufacturer. While this brand loyalty is considered unusual for any product, Apple appears not to have gone out of its way to create it. At one time, Apple evangelists were actively engaged by the company, but this was after the phenomenon was already firmly established. Apple evangelist Guy Kawasaki has called the brand fanaticism "something that was stumbled upon". Apple has, however, supported the continuing existence of a network of Mac User Groups in most major and many minor centers of population where Mac computers are available. Mac users meet at the European Apple Expo and the San Francisco Macworld Conference & Expo trade shows where Apple traditionally introduced new products each year to the industry and public. Mac developers in turn gather at the annual Apple Worldwide Developers Conference. Apple Store openings can draw crowds of thousands, with some waiting in line as much as a day before the opening or flying in from other countries for the event. The New York City Fifth Avenue "Cube" store had a line as long as half a mile; a few Mac fans took the opportunity of the setting to propose marriage. The Ginza opening in Tokyo was estimated in the thousands with a line exceeding eight city blocks. John Sculley told "The Guardian" newspaper in 1997: "People talk about technology, but Apple was a marketing company. It was the marketing company of the decade." Market research indicates that Apple draws its customer base from an unusually artistic, creative, and well-educated population, which may explain the platform’s visibility within certain youthful, avant-garde subcultures.
Apple has a history of vertical integration in their products, manufacturing the hardware on which they pre-install their software. During the Mac's early history Apple generally refused to adopt prevailing industry standards for hardware, instead creating their own. This trend was largely reversed in the late 1990s beginning with Apple's adoption of the PCI bus in the 7500/8500/9500 Power Macs. Apple has since adopted USB, AGP, HyperTransport, Wi-Fi, and other industry standards in its computers and was in some cases a leader in the adoption of standards such as USB. FireWire is an Apple-originated standard that has seen widespread industry adoption after it was standardized as IEEE 1394. Ever since the first Apple Store opened, Apple has sold third party accessories. This allows, for instance, Nikon and Canon to sell their Mac-compatible digital cameras and camcorders inside the store. Adobe, one of Apple's oldest software partners, also sells its Mac-compatible software, as does Microsoft, who sells Microsoft Office for the Mac. Books from John Wiley & Sons, who publishes the For Dummies series of instructional books, are a notable exception, however. The publisher's line of books were banned from Apple Stores in 2005 because Steve Jobs disagreed with their editorial policy.
Apple Inc.'s world corporate headquarters are located in the middle of Silicon Valley, at 1 Infinite Loop, Cupertino, California. This Apple campus has six buildings that total and was built in 1993 by Sobrato Development Cos. In 2006, Apple announced its intention to build a second campus on assembled from various contiguous plots. The new campus, also in Cupertino, will be about east of the current campus.
Since formation of the Apple Computer Company in 1977, it (as Apple Computer, Inc.) has employed over 75,000 people worldwide. Most of Apple's employees have been located in the United States but Apple has substantial manufacturing, sales, marketing, and support organizations worldwide, with some engineering operations in Paris and Tokyo. Apple employees include employees of companies acquired by Apple as well as subsidiaries such as FileMaker Inc. and Braeburn Capital.
Apple’s first logo, designed by Jobs and Wayne, depicts Sir Isaac Newton sitting under an apple tree. Almost immediately, though, this was replaced by Rob Janoff’s “rainbow Apple”, the now-familiar rainbow-colored silhouette of an apple with a bite taken out of it. Janoff presented Jobs with several different monochromatic themes for the "bitten" logo, and Jobs immediately took a liking to it. While Jobs liked the logo, he insisted it be in color to humanize the company. The Apple logo was designed with a bite so that it would be recognized as an apple rather than a cherry. The colored stripes were conceived to make the logo more accessible, and to represent the fact the monitor could reproduce images in color. In 1998, with the roll-out of the new iMac, Apple discontinued the rainbow theme and began to use monochromatic themes, nearly identical in shape to its previous rainbow incarnation.
Apple's first slogan, "Byte into an Apple", was coined in the late 1970s. From 1997–2002, Apple used the slogan Think Different in advertising campaigns. The slogan had a lasting impact on their image and revived their popularity with the media and customers. Although the slogan has been retired, it is still closely associated with Apple. Apple also has slogans for specific product lines — for example, "iThink, therefore iMac" was used in 1998 to promote the iMac, and "Say hello to iPhone" has been used in iPhone advertisements. "Hello" was also used to introduce the original Macintosh, Newton, iMac ("hello (again)"), and iPod.
Apple’s product commercials are notorious for launching musicians into stardom as a result of their eye-popping graphics and catchy tunes. First, the company popularized Canadian singer Feist’s “1234” song in its ad campaign. Then Apple used the song “New Soul” by French-Israeli singer-songwriter Yael Naim to promote the MacBook Air. The debut single shot to the top of the charts and sold hundreds of thousands of copies in a span of weeks.
Greenpeace, an environmentalist organization, has confronted Apple on various environmental issues, including promoting a global end-of-life take-back plan, non-recyclable hardware components, and toxins within the iPhone hardware. Since 2003 they have campaigned against Apple regarding their chemical policies, in particular the inclusion of PVC and BFRs in their products, both of which have serious negative health effects. On May 2, 2007, Steve Jobs released a report announcing plans to eliminate PVC and BFRs by the end of 2008. Greenpeace runs a "Guide to Greener Electronics", which rates companies on chemical-disposal waste-reduction practices. In the first edition, released in August 2006, Apple scored 2.7/10. In subsequent editions Apple's score has improved steadily. Apple soon improved its score to a 4.1/10, placing it in the 45th percentile among 17 other electronic companies and 10th in the rankings. At the 2007 Macworld Expo, Greenpeace presented a critique of Apple. Rick Hind, the legislative director of Greenpeace's toxics campaign, said, "(The company) is getting greener, but not green enough." Hind commented further, "The Macbook Air has less toxic PVC plastic and less toxic BFRs, but it could have zero and that would make Apple an eco-leader." In May 2008, Climate Counts, a nonprofit organization dedicated to directing consumers toward the greenest companies, gave Apple 11 points out of a possible 100, which placed the company last among electronics companies. Climate Counts also labeled Apple with a "stuck icon," and the environmental group added that Apple was "a choice to avoid for the climate conscious consumer." The Environmental Protection Agency rates Apple highest amongst producers of notebook computers, and fairly well compared to producers of desktop computers and LCD displays. In June 2007, Apple upgraded the MacBook Pro, replacing cold cathode fluorescent lamp (CCFL) backlit LCD displays with mercury-free LED backlit LCD displays and arsenic-free glass, and has since done this for all notebooks. Apple has also phased out BFRs and PVCs from various internal components. Apple also offers detailed information about the emissions, materials, and electrical usage of each product. Apple has also begun to advertise how environmentally friendly their new laptops are with television spots and magazine ads along with stating these facts on their website. In June 2009, Apple's iPhone 3GS was free of PVC, arsenic, BFR's and had an efficient power adapter. In October 2009, Apple upgraded the iMac and MacBook, replacing the cold cathode fluorescent lamp (CCFL) backlit LCD displays with mercury-free LED backlit LCD displays and arsenic-free glass. This means all Apple computers have mercury free LED backlit displays, arsenic-free glass and are without PVC cables. All Apple computers also have EPEAT Gold status.
In 2006, the "Mail on Sunday" alleged that sweatshop conditions existed in factories in China, where the contract manufacturers, Foxconn and Inventec operate the factories that produce the iPod. The article stated that one iPod factory, for instance, had over 200,000 workers that lived and worked in the factory, with workers regularly doing more than 60 hours of labor per week. The article also reported that workers made around $100 per month were required to live on the premises and pay for rent and food from the company. Living expenses (required to keep the job) generally took up a little over half of the worker's earnings. The article also said that workers were given buckets to wash their clothes. Immediately after the allegations, Apple launched a full investigation and worked with their manufacturers to ensure that conditions were acceptable to Apple.
Apple has been criticized from both user and developer perspectives over disabling Google Voice from their online store for iPhone, pressuring journalists to reveal their sources regarding future Apple products, restrictive and long wait in approving or disapproving third party iPhone software, disabling iTunes syncing with third-party devices like Palm Pre, and the iPhone's US exclusivity with AT&T, along with questions and concerns about other app rejections and the general approval process for the iPhone's App Store. Philip W. Schiller, senior vice president of Apple's Worldwide Product Marketing, has tried to address many of the App Store concerns by sending letters to the respective developers.
Aberdeenshire (,) is one of the 32 unitary council areas in Scotland and a lieutenancy area. The present day Aberdeenshire council area does not include the City of Aberdeen, now a separate council area, from which its name derives. Together, the modern council area and the city formed historic Aberdeenshire - one of the counties of Scotland formerly used for local government purposes. Within these borders, the County of Aberdeen remains in existence as a registration county. Aberdeenshire Council is headquartered at Woodhill House, in Aberdeen; the only Scottish council whose headquarters are based outwith its area's border. Aberdeenshire borders Angus and Perth and Kinross to the south, and the Highland council area and Moray to the west.
Aberdeenshire has a rich prehistoric and historic heritage. It is the locus of a large number of Neolithic and Bronze Age archaeological sites, including Longman Hill, Kempstone Hill, Catto Long Barrow and Cairn Lee. Since medieval times there have been a number of crossings of the Mounth (a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven) through present day Aberdeenshire from the Scottish Lowlands to the Highlands. Some of the most well known and historically important trackways are the Causey Mounth and Elsick Mounth. The present council area is named after the historic county of Aberdeen which had different boundaries and was abolished in 1975, under the Local Government (Scotland) Act 1973 to be replaced by Grampian Regional Council, and five district councils; Banff and Buchan, Gordon, Kincardine and Deeside, Moray and the City of Aberdeen, with local government functions shared between the two levels. In 1996, under the Local Government etc (Scotland) Act 1994, the Banff and Buchan district, the Gordon district and the Kincardine and Deeside district were merged to form the present Aberdeenshire council area, with the other two districts becoming autonomous council areas.
The Council's net expenditure is £750.1m a year (2008/09). Education takes the largest share of expenditure (55%), followed by Social Work and Housing (19%), Transportation and Infrastructure (11%), and Joint Services such as Fire and Police (10%). 22% of revenue is raised locally through the Council Tax. Average Band D Council Tax is the eighth lowest in mainland Scotland at £966 (2003/04). The current Chief Executive of the Council is Colin Mackenzie and the elected Head of the Council is Anne Robertson. The council has devolved power to six area committees: Banff and Buchan, Buchan, Formartine, Garioch, Marr and Kincardine and Mearns
There are numerous rivers and burns in Aberdeenshire, including Cowie Water, Carron Water, Burn of Muchalls, River Dee, River Don, River Ury, River Ythan, Water of Feugh, Burn of Myrehouse, Laeca Burn and Luther Water. Numerous bays and estuaries are found along the seacoast of Aberdeenshire, including Banff Bay, Ythan Estuary, Stonehaven Bay and Thornyhive Bay. Summers are mild and winters are typically cold in Aberdeenshire; Coastal temperatures are moderated by the North Sea such that coastal areas are typically cooler in the summer and warmer in winter than inland locations. Coastal areas are also subject to haar, or coastal fog.
Aztlan Underground is a fusion band from Los Angeles. Since early 1989, Aztlan Underground has played Rapcore. Indigenous drums, flutes, and rattles are commonplace in its musical compositions. This unique sound is the backdrop for the band's message of dignity for indigenous people, all of humanity, and Earth. Aztlan Underground has been cultivating a grass roots audience across the country, which has become a large and loyal underground following. Their music includes spoken word pieces and elements of punk, hip hop, rock, funk, jazz, and indigenous music, among others. The artists are Chenek "DJ Bean" (turntables, samples and percussion), Yaotl (vocals, indigenous percussion), Joe "Peps" (bass, rattles), Alonzo Beas (guitars, synth), Caxo (drums, indigenous percussion), and Bulldog (vocals, flute). Aztlan Underground appeared on television on Culture Clash on Fox in 1993, was part of "Breaking Out", a concert on pay per view in 1998, and was featured in the independent films "Algun Dia" and "Frontierlandia". The band has been mentioned or featured in various newspapers and magazines: the Vancouver Sun, Northshore News (Vancouver, Canada newspaper), New Times (Los Angeles weekly entertainment newspaper), BLU Magazine (underground hip hop magazine), BAM Magazine (Southern California), La Banda Elastica Magazine, and the Los Angeles Times Calendar section. It is also the subject of a chapter in "It's Not About A Salary", by Brian Cross. They also opened for Rage Against the Machine in Mexico City. It was nominated in the New Times 1998 "Best Latin Influenced" category, the BAM Magazine 1999 "Best Rock en Español" category, and the LA Weekly 1999 "Best Hip Hop" category. Aztlan Underground were signed to a Basque record label in 1999 which enabled them to tour Spain extensively and perform in France and Portugal. Other parts of the world that Aztlan Underground have performed include Canada, Australia, and Venezuela. The band completed their third album and released it exclusively digital on August 29th 2009. The band is set to begin writing a new record this year.
The American Civil War (1861–1865), also known as the War Between the States as well as several other names, was a civil war in the United States of America. Eleven Southern slave states declared their secession from the United States and formed the Confederate States of America (the Confederacy). Led by Jefferson Davis, they fought against the United States (the Union), which was supported by all the free states and the five border slave states. In the presidential election of 1860, the Republican Party, led by Abraham Lincoln, had campaigned against the expansion of slavery beyond the states in which it already existed. The Republican victory in that election resulted in seven Southern states declaring their secession from the Union even before Lincoln took office on March 4, 1861. Both the outgoing and incoming US administrations rejected the legality of secession, considering it rebellion. Hostilities began on April 12, 1861, when Confederate forces attacked a US military installation at Fort Sumter in South Carolina. Lincoln responded by calling for a volunteer army from each state, leading to declarations of secession by four more Southern slave states. Both sides raised armies as the Union assumed control of the border states early in the war and established a naval blockade. In September 1862, Lincoln's Emancipation Proclamation made ending slavery in the South a war goal, and dissuaded the British from intervening. Confederate commander Robert E. Lee won battles in the east, but in 1863 his northward advance was turned back after the Battle of Gettysburg and, in the west, the Union gained control of the Mississippi River at the Battle of Vicksburg, thereby splitting the Confederacy. Long-term Union advantages in men and materiel were realized in 1864 when Ulysses S. Grant fought battles of attrition against Lee, while Union general William Sherman captured Atlanta, Georgia, and marched to the sea. Confederate resistance collapsed after Lee surrendered to Grant at Appomattox Court House on April 9, 1865. The American Civil War was one of the earliest true industrial wars in human history. Railroads, steamships, mass-produced weapons, and various other military devices were employed extensively. The practices of total war, developed by Sherman in Georgia, and of trench warfare around Petersburg foreshadowed World War I. It remains the deadliest war in American history, resulting in the deaths of 620,000 soldiers and an undetermined number of civilian casualties. Victory for the North meant the end of the Confederacy and of slavery in the United States, and strengthened the role of the federal government. The social, political, economic and racial issues of the war decisively shaped the reconstruction era that lasted to 1877.
The coexistence of a slave-owning South with an increasingly anti-slavery North made conflict likely, if not inevitable. Abraham Lincoln did not propose federal laws against slavery where it already existed, but he had, in his 1858 House Divided Speech, expressed a desire to "arrest the further spread of it, and place it where the public mind shall rest in the belief that it is in the course of ultimate extinction." Much of the political battle in the 1850s focused on the expansion of slavery into the newly created territories. All of the organized territories were likely to become free-soil states, which increased the Southern movement toward secession. Both North and South assumed that if slavery could not expand it would wither and die. Southern fears of losing control of the federal government to antislavery forces, and Northern resentment of the influence that the Slave Power already wielded in government, brought the crisis to a head in the late 1850s. Sectional disagreements over the morality of slavery, the scope of democracy and the economic merits of free labor versus slave plantations caused the Whig and "Know-Nothing" parties to collapse, and new ones to arise (the Free Soil Party in 1848, the Republicans in 1854, the Constitutional Union in 1860). In 1860, the last remaining national political party, the Democratic Party, split along sectional lines. Both North and South were influenced by the ideas of Thomas Jefferson. Southerners used the states' rights ideas mentioned in Jefferson's Kentucky Resolutions to defend slavery. Northerners ranging from the abolitionist William Lloyd Garrison to the moderate Republican leader Lincoln emphasized Jefferson's declaration that all men are created equal. Lincoln mentioned this proposition in his Gettysburg Address. Confederate Vice President Alexander Stephens said that slavery was the chief cause of secession in his Cornerstone Speech shortly before the war. After Confederate defeat, Stephens became one of the most ardent defenders of the Lost Cause. There was a striking contrast between Stephens' post-war states' rights assertion that slavery did "not" cause secession and his pre-war "Cornerstone Speech". Similarly, Confederate President Jefferson Davis also reversed his original position, that the central cause of the war was the issue of slavery, arguing after the war that states' rights was its principal cause. While Southerners often used states' rights arguments to defend slavery, sometimes roles were reversed, as when Southerners demanded national laws to defend their interests with the Gag Rule and the Fugitive Slave Law of 1850. On these issues, Northerners wanted to defend their states' rights. Almost all the inter-regional crises involved slavery, starting with debates on the three-fifths clause and a twenty-year extension of the African slave trade in the Constitutional Convention of 1787. The 1793 invention of the cotton gin by Eli Whitney increased by fiftyfold the quantity of cotton that could be processed in a day and greatly increased the demand for slave labor in the South. There was controversy over adding the slave state of Missouri to the Union that led to the Missouri Compromise of 1820, the Nullification Crisis over the Tariff of 1828 (although the tariff was low after 1846, and even the tariff issue was related to slavery), the gag rule that prevented discussion in Congress of petitions for ending slavery from 1835–1844, the acquisition of Texas as a slave state in 1845 and Manifest Destiny as an argument for gaining new territories where slavery would become an issue after the Mexican–American War (1846–1848), which resulted in the Compromise of 1850. The Wilmot Proviso was an attempt by Northern politicians to exclude slavery from the territories conquered from Mexico. The extremely popular antislavery novel "Uncle Tom’s Cabin" (1852) by Harriet Beecher Stowe greatly increased Northern opposition to the Fugitive Slave Law of 1850. The 1854 Ostend Manifesto was an unsuccessful Southern attempt to annex Cuba as a slave state. The Second Party System broke down after passage of the Kansas-Nebraska Act in 1854, which replaced the Missouri Compromise ban on slavery with popular sovereignty, allowing the people of a territory to vote for or against slavery. The Bleeding Kansas controversy over the status of slavery in the Kansas Territory included massive vote fraud perpetrated by Missouri pro-slavery Border Ruffians. Vote fraud led pro-South Presidents Franklin Pierce and James Buchanan to support the pro-slavery Lecompton Constitution and to attempt to admit Kansas as a slave state. Violence over the status of slavery in Kansas erupted with the Wakarusa War, the Sacking of Lawrence, the caning of Republican Charles Sumner by the Southerner Preston Brooks, the Pottawatomie Massacre, the Battle of Black Jack, the Battle of Osawatomie and the Marais des Cygnes massacre. The 1857 Supreme Court Dred Scott decision allowed slavery in the territories even where the majority opposed slavery, including Kansas. The Lincoln-Douglas debates of 1858 included Northern Democratic leader Stephen A. Douglas' Freeport Doctrine. This doctrine was an argument for thwarting the Dred Scott decision that, along with Douglas' defeat of the Lecompton Constitution, divided the Democratic Party between North and South. Northern abolitionist John Brown's raid at Harpers Ferry Armory was an attempt to incite slave insurrections in 1859. The North-South split in the Democratic Party in 1860 due to the Southern demand for a slave code for the territories completed polarization of the nation between North and South. Other factors include sectionalism, which was caused by the prosperity and growth of slavery in the cotton South while slavery was phased out of Northern states and steadily declined in the border states that lacked cotton. Historians have debated whether economic differences between the industrial Northeast and the agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles Beard and argue that Northern and Southern economies were largely complementary. There was the polarizing effect of slavery that split the largest religious denominations (the Methodist, Baptist and Presbyterian churches) and controversy caused by the worst cruelties of slavery (whippings, mutilations and families split apart). The fact that seven immigrants out of eight settled in the North, plus movement of twice as many whites leaving the South for the North as vice versa, contributed to the South's defensive-aggressive political behavior. The election of Lincoln in 1860 was the final trigger for secession. Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed. Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North.
Support for secession was strongly correlated to the number of plantations in the region. States of the Deep South, which had the greatest concentration of plantations, were the first to secede. The upper South slave states of Virginia, North Carolina, Arkansas, and Tennessee had fewer plantations and rejected secession until the Fort Sumter crisis forced them to choose sides. Border states had fewer plantations still and never seceded. As of 1850 the percentage of Southern whites living in families that owned slaves was 43 percent in the lower South, 36 percent in the upper South and 22 percent in the border states that fought mostly for the Union. 85 percent of slaveowners who owned 100 or more slaves lived in the lower South, as opposed to one percent in the border states. Ninety-five percent of African-Americans lived in the South, comprising one third of the population there as opposed to one percent of the population of the North. Consequently, fears of eventual emancipation were much greater in the South than in the North. The US Supreme Court decision of 1857 in "Dred Scott v. Sandford" added to the controversy. Chief Justice Roger B. Taney's decision said that slaves were "so far inferior that they had no rights which the white man was bound to respect". Taney then overturned the Missouri Compromise, which banned slavery in territory north of the 36°30' parallel. He stated, "[T]he Act of Congress which prohibited a citizen from holding and owning [enslaved persons] in the territory of the United States north of the line therein is not warranted by the Constitution and is therefore void." Democrats praised the "Dred Scott" decision, but Republicans branded it a "willful perversion" of the Constitution. They argued that if Scott could not legally file suit, the Supreme Court had no right to consider the Missouri Compromise's constitutionality. Lincoln warned that "the next "Dred Scott" decision" could threaten Northern states with slavery. Abraham Lincoln said, "This question of Slavery was more important than any other; indeed, so much more important has it become that no other national question can even get a hearing just at present." The slavery issue was related to sectional competition for control of the territories, and the Southern demand for a slave code for the territories was the issue used by Southern politicians to split the Democratic Party in two, which all but guaranteed the election of Lincoln and secession. When secession was an issue, South Carolina planter and state Senator John Townsend said that, "our enemies are about to take possession of the Government, that they intend to rule us according to the caprices of their fanatical theories, and according to the declared purposes of abolishing slavery." Similar opinions were expressed throughout the South in editorials, political speeches and declarations of reasons for secession. Even though Lincoln had no plans to outlaw slavery where it existed, whites throughout the South expressed fears for the future of slavery. Southern concerns included not only economic loss but also fears of racial equality. The Texas Declaration of Causes for Secession said that the non-slave-holding states were "proclaiming the debasing doctrine of equality of all men, irrespective of race or color", and that the African race "were rightfully held and regarded as an inferior and dependent race". Alabama secessionist E. S. Dargan warned that whites and free blacks could not live together; if slaves were emancipated and remained in the South, "we ourselves would become the executioners of our own slaves. To this extent would the policy of our Northern enemies drive us; and thus would we not only be reduced to poverty, but what is still worse, we should be driven to crime, to the commission of sin." Beginning in the 1830s, the US Postmaster General refused to allow mail which carried abolition pamphlets to the South. Northern teachers suspected of any tinge of abolitionism were expelled from the South, and abolitionist literature was banned. Southerners rejected the denials of Republicans that they were abolitionists. The North felt threatened as well, for as Eric Foner concludes, "Northerners came to view slavery as the very antithesis of the good society, as well as a threat to their own fundamental values and interests." During the 1850s, slaves left the border states through sale, manumission and escape, and border states also had more free African-Americans and European immigrants than the lower South, which increased Southern fears that slavery was threatened with rapid extinction in this area. Such fears greatly increased Southern efforts to make Kansas a slave state. By 1860, the number of white border state families owning slaves plunged to only 16 percent of the total. Slaves sold to lower South states were owned by a smaller number of wealthy slave owners as the price of slaves increased. Even though Lincoln agreed to the Corwin Amendment, which would have protected slavery in existing states, secessionists claimed that such guarantees were meaningless. Besides the loss of Kansas to free soil Northerners, secessionists feared that the loss of slaves in the border states would lead to emancipation, and that upper South slave states might be the next dominoes to fall. They feared that Republicans would use patronage to incite slaves and antislavery Southern whites such as Hinton Rowan Helper. Then slavery in the lower South, like a "scorpion encircled by fire, would sting itself to death." A few secessionists mentioned the tariff issue along with slavery, but these were rare. Among other reasons, slavery represented much more money than the tariff. However, a few libertarian economists place more importance on the tariff issue. There were non-slavery related causes of secession, but they had little to do with tariffs or states' rights.
South Carolina did more to advance nullification and secession than any other Southern state. South Carolina adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union" on December 24, 1860. It argued for states' rights for slave owners in the South, but contained a complaint about states' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. All the alleged violations of the rights of Southern states were related to slavery.
Before Lincoln took office, seven states had declared their secession from the Union. They established a Southern government, the Confederate States of America on February 4, 1861. They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "was intended to be perpetual", but that "the power by force of arms to compel a State to remain in the Union" was not among the "enumerated powers granted to Congress". One quarter of the U.S. Army—the entire garrison in Texas—was surrendered to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy. As Southerners resigned their seats in the Senate and the House, secession later enabled Republicans to pass bills for projects that had been blocked by Southern Senators before the war, including the Morrill Tariff, land grant colleges (the Morill Act), a Homestead Act, a trans-continental railroad (the Pacific Railway Acts), the National Banking Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.
Seven Deep South cotton states seceded by February 1861, starting with South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas. These seven states formed the Confederate States of America (February 4, 1861), with Jefferson Davis as president, and a governmental structure closely modeled on the U.S. Constitution. Following the attack on Fort Sumter, President Lincoln called for a volunteer army from each state. Within two months, four more Southern slave states declared their secession and joined the Confederacy: Virginia, Arkansas, North Carolina and Tennessee. The northwestern portion of Virginia subsequently seceded from Virginia, joining the Union as the new state of West Virginia on June 20, 1863. By the end of 1861, Missouri and Kentucky were effectively under Union control, with Confederate state governments in exile.
Twenty-three states remained loyal to the Union: California, Connecticut, Delaware, Illinois, Indiana, Iowa, Kansas, Kentucky, Maine, Maryland, Massachusetts, Michigan, Minnesota, Missouri, New Hampshire, New Jersey, New York, Ohio, Oregon, Pennsylvania, Rhode Island, Vermont, and Wisconsin. During the war, Nevada and West Virginia joined as new states of the Union. Tennessee and Louisiana were returned to Union military control early in the war. The territories of Colorado, Dakota, Nebraska, Nevada, New Mexico, Utah, and Washington fought on the Union side. Several slave-holding Native American tribes supported the Confederacy, giving the Indian Territory (now Oklahoma) a small bloody civil war.
The border states in the Union were West Virginia (which was separated from Virginia and became a new state), and four of the five northernmost slave states (Maryland, Delaware, Missouri, and Kentucky). Maryland had numerous pro-Confederate officials who tolerated anti-Union rioting in Baltimore and the burning of bridges. Lincoln responded with martial law and called for troops. Militia units that had been drilling in the North rushed toward Washington, DC and Baltimore. Before the Confederate government realized what was happening, Lincoln had seized firm control of Maryland and the District of Columbia, by arresting all the Maryland government members and holding them without trial. In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon. After the Camp Jackson Affair Lyon chased the governor and the rest of the State Guard to the southwestern corner of the state. ("See also: Missouri secession"). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri. Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces, Confederate sympathizers organized a secession convention, inaugurated a governor, and gained recognition from the Confederacy. The rebel government soon went into exile and never controlled Kentucky. After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. Returns were received from 41 of the 48 counties, and a minority turnout voted heavily in favor of the new state, at first called Kanawha but later renamed West Virginia, which was admitted to the Union on June 20, 1863. Jefferson and Berkeley counties were annexed to the new state in late 1863. The western counties of Virginia had voted nearly 2 to 1 against secession, though 24 of the 50 counties had voted for secession. Soldier numbers from West Virginia were about evenly divided between the Confederacy and the Union. A similar Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy. Jefferson Davis arrested over 3000 men suspected of being loyal to the Union and held them without trial.
Over 10,000 military engagements took place during the war, 40% of them in Virginia and Tennessee. Since separate articles deal with every major battle and many minor ones, this article only gives the broadest outline. For more information see List of American Civil War battles and Military leadership in the American Civil War. The Beginning of the War, 1861. Lincoln's victory in the presidential election of 1860 triggered South Carolina's declaration of secession from the Union. By February 1861, six more Southern states made similar declarations. On February 7, the seven states adopted a provisional constitution for the Confederate States of America and established their temporary capital at Montgomery, Alabama. A pre-war February Peace Conference of 1861 met in Washington in a failed attempt at resolving the crisis. The remaining eight slave states rejected pleas to join the Confederacy. Confederate forces seized most of the federal forts within their boundaries. President Buchanan protested but made no military response apart from a failed attempt to resupply Fort Sumter using the ship "Star of the West", which was fired upon by South Carolina forces and turned back before it reached the fort. However, governors in Massachusetts, New York, and Pennsylvania quietly began buying weapons and training militia units. On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a "more perfect union" than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void". He stated he had no intent to invade Southern states, nor did he intend to end slavery where it existed, but that he would use force to maintain possession of federal property. His speech closed with a plea for restoration of the bonds of union. The South sent delegations to Washington and offered to pay for the federal properties and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government. However, Secretary of State William Seward engaged in unauthorized and indirect negotiations that failed. Fort Sumter in Charleston, South Carolina, Fort Pickens and Fort Taylor were the remaining Union-held forts in the Confederacy, and Lincoln was determined to hold Fort Sumter. Under orders from Confederate President Jefferson Davis, troops controlled by the Confederate government under P. G. T. Beauregard bombarded the fort with artillery on April 12, forcing the fort's capitulation. Northerners rallied behind Lincoln's call for all the states to send troops to recapture the forts and to preserve the Union. With the scale of the rebellion apparently small so far, Lincoln called for 75,000 volunteers for 90 days. For months before that, several Northern governors had discreetly readied their state militias; they began to move forces the next day. Liberty Arsenal in Liberty, Missouri was seized eight days after Fort Sumter. Four states in the upper South (Tennessee, Arkansas, North Carolina, and Virginia), which had repeatedly rejected Confederate overtures, now refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond. The city was the symbol of the Confederacy. Richmond was in a highly vulnerable location at the end of a tortuous Confederate supply line. Although Richmond was heavily fortified, supplies for the city would be reduced by Sherman's capture of Atlanta and cut off almost entirely when Grant besieged Petersburg and its railroads that supplied the Southern capital. Anaconda Plan and blockade, 1861. Winfield Scott, the commanding general of the U.S. Army, devised the Anaconda Plan to win the war with as little bloodshed as possible. His idea was that a Union blockade of the main ports would weaken the Confederate economy; then the capture of the Mississippi River would split the South. Lincoln adopted the plan, but overruled Scott's warnings against an immediate attack on Richmond. In May 1861, Lincoln enacted the Union blockade of all Southern ports, ending regular international shipments to the Confederacy. When violators' ships and cargoes were seized, they were sold and the proceeds given to Union sailors, but the British crews were released. By late 1861, the blockade stopped most local port-to-port traffic. The blockade shut down King Cotton, ruining the Southern economy. British investors built small, fast "blockade runners" that traded arms and luxuries brought in from Bermuda, Cuba and the Bahamas in return for high-priced cotton and tobacco. Shortages of food and other goods triggered by the blockade, foraging by Northern armies, and the impressment of crops by Confederate armies combined to cause hyperinflation and bread riots in the South. On March 8, 1862, the Confederate Navy waged a fight against the Union Navy when the ironclad CSS "Virginia" attacked the blockade. Against wooden ships, she seemed unstoppable. The next day, however, she had to fight the new Union warship USS "Monitor" in the Battle of the Ironclads. The battle ended in a draw, which was a strategic victory for the Union in that the blockade was sustained. The Confederacy lost the "Virginia" when the ship was scuttled to prevent capture, and the Union built many copies of "Monitor". Lacking the technology to build effective warships, the Confederacy attempted to obtain warships from Britain. The Union victory at the Second Battle of Fort Fisher in January 1865 closed the last useful Southern port and virtually ended blockade running.
Because of the fierce resistance of a few initial Confederate forces at Manassas, Virginia, in July 1861, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces there was halted in the First Battle of Bull Run, or "First Manassas", McDowell's troops were forced back to Washington, D.C., by the Confederates under the command of Generals Joseph E. Johnston and P. G. T. Beauregard. It was in this battle that Confederate General Thomas Jackson received the nickname of "Stonewall" because he stood like a stone wall against Union troops. Alarmed at the loss, and in an attempt to prevent more slave states from leaving the Union, the U.S. Congress passed the Crittenden-Johnson Resolution on July 25 of that year, which stated that the war was being fought to preserve the Union and not to end slavery. Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. Although McClellan's army reached the gates of Richmond in the Peninsula Campaign, Johnston halted his advance at the Battle of Seven Pines, then General Robert E. Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat. The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South. McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops. Emboldened by Second Bull Run, the Confederacy made its first invasion of the North. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history. Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation. When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg on December 13, 1862, when over twelve thousand Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker. Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, he was humiliated in the Battle of Chancellorsville in May 1863. He was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to July 3, 1863), the bloodiest battle of the war, which is sometimes considered the war's turning point. Pickett's Charge on July 3 is often recalled as the high-water mark of the Confederacy, not just because it signaled the end of Lee's plan to pressure Washington from the north, but also because Vicksburg, Mississippi, the key stronghold to control of the Mississippi, fell the following day. Lee's army suffered 28,000 casualties (versus Meade's 23,000). However, Lincoln was angry that Meade failed to intercept Lee's retreat, and after Meade's inconclusive fall campaign, Lincoln decided to turn to the Western Theater for new leadership.
While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West. They were driven from Missouri early in the war as a result of the Battle of Pea Ridge. Leonidas Polk's invasion of Columbus, Kentucky ended Kentucky's policy of neutrality and turned that state against the Confederacy. Nashville and central Tennessee fell to the Union early in 1862, leading to attrition of local food supplies and livestock and a breakdown in social organization. Most of the Mississippi was opened to Union traffic with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee. In May 1862, the Union Navy captured New Orleans without a major fight, which allowed Union forces to begin moving up the Mississippi. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river. General Braxton Bragg's second Confederate invasion of Kentucky ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville, although Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of support for the Confederacy in that state. Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee. The one clear Confederate victory in the West was the Battle of Chickamauga. Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas. Rosecrans retreated to Chattanooga, which Bragg then besieged. The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry and Donelson (by which the Union seized control of the Tennessee and Cumberland Rivers); the Battle of Shiloh; and the Battle of Vicksburg, which cemented Union control of the Mississippi River and is considered one of the turning points of the war. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga, driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy.
Guerrilla activity turned much of Missouri into a battleground. Missouri had, in total, the third-most battles of any state during the war. The other states of the west, though geographically isolated from the battles to the east, saw numerous small-scale military actions. Battles in the region served to secure Missouri, Indian Territory, and New Mexico Territory for the Union. Confederate incursions into New Mexico territory were repulsed in 1862 and a Union campaign to secure Indian Territory succeeded in 1863. Late in the war, the Union's Red River Campaign was a failure. Texas remained in Confederate hands throughout the war, but was cut off from the rest of the Confederacy after the capture of Vicksburg in 1863 gave the Union control of the Mississippi River. Conquest of Virginia and End of War: 1864–1865. At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would bring an end to the war. This was total war not in terms of killing civilians but rather in terms of destroying homes, farms, and railroads. Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama. Union forces in the East attempted to maneuver past Lee and fought several battles during that phase ("Grant's Overland Campaign") of the Eastern campaign. Grant's battles of attrition at the Wilderness, Spotsylvania, and Cold Harbor resulted in heavy Union losses, but forced Lee's Confederates to fall back repeatedly. An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Grant was tenacious and, despite astonishing losses (over 65,000 casualties in seven weeks), kept pressing Lee's Army of Northern Virginia back to Richmond. He pinned down the Confederate army in the Siege of Petersburg, where the two armies engaged in trench warfare for over nine months. Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia. Meanwhile, Sherman marched from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, was a significant factor in the reelection of Lincoln as president. Hood left the Atlanta area to menace Sherman's supply lines and invade Tennessee in the Franklin-Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army. Leaving Atlanta, and his base of supplies, Sherman's army marched with an unknown destination, laying waste to about 20% of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia in December 1864. Sherman's army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee's army. Lee's army, thinned by desertion and casualties, was now much smaller than Grant's. Union forces won a decisive victory at the Battle of Five Forks on April 1, forcing Lee to evacuate Petersburg and Richmond. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west and after a defeat at Sayler's Creek, it became clear to Robert E. Lee that continued fighting against the United States was both tactically and logistically impossible.
Lee surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House in the village of Appomattox Court House. In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully folding the Confederacy back into the Union, Lee was permitted to keep his officer's saber and his horse, Traveller. On April 14, 1865, President Lincoln was shot. Lincoln died early the next morning, and Andrew Johnson became President. Events leading to Lee's surrender began with the capture of key Confederate officers Richard S. Ewell and Richard H. Anderson on April 6, following Confederate defeat at the battle of Sayler's Creek. On April 8, Union cavalry under Major General George Armstrong Custer destroyed three trains of Confederate supplies at Appomattox Station, leading to the surrender of General Lee the next day. General St. John Richardson Liddell's army surrendered after the loss of the Confederate fortifications at the Battle of Spanish Fort in Alabama, also on April 9. Unaware of the surrender of Lee, on April 16 the last major battles of the war were fought at the Battle of Columbus, Georgia and the Battle of West Point. Both towns surrendered to Wilson's Raiders. On April 21, John S. Mosby’s raiders of the 43rd Battalion Virginia Cavalry was disbanded, and on April 26, General Joseph E. Johnston surrendered his troops to Sherman at Bennett Place in Durham, North Carolina. Surrendering on May 4 and 5 were the Confederate departments of Alabama, Mississippi and East Louisiana regiments and the District of the Gulf. The Confederate President was captured on May 10 and the surrender of the Department of Florida and South Georgia happened the same day. Confederate Brigadier General "Jeff" Meriwether Thompson surrendered his brigade the next day and the day following saw the surrender of the Confederate forces of North Georgia. On June 23, 1865, at Fort Towson in the Choctaw Nations' area of the Oklahoma Territory, Stand Watie signed a cease-fire agreement with Union representatives, becoming the last Confederate general in the field to stand down. The last Confederate ship to surrender was the CSS "Shenandoah", whose officers did not know of the end of the war until August 2. Not wanting to surrender to Federal authorities, the ship's commander plotted a course for the country of his ship's birth, so that they surrendered on November 6, 1865, in Liverpool, England. These surrenders marked the conclusion of the American Civil War.
At the beginning of the war, some Union commanders thought they were supposed to return escaped slaves to their masters. By 1862, when it became clear that this would be a long war, the question of what to do about slavery became more general. The Southern economy and military effort depended on slave labor. It began to seem unreasonable to protect slavery while blockading Southern commerce and destroying Southern production. As one Congressman put it, the slaves "...cannot be neutral. As laborers, if not as soldiers, they will be allies of the rebels, or of the Union." The same Congressman—and his fellow Radical Republicans—put pressure on Lincoln to rapidly emancipate the slaves, whereas moderate Republicans came to accept gradual, compensated emancipation and colonization. Copperheads, the border states and War Democrats opposed emancipation, although the border states and War Democrats eventually accepted it as part of total war needed to save the Union. In 1861, Lincoln expressed the fear that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game." At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Frémont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected. Only the District of Columbia accepted Lincoln's gradual plan, and Lincoln mentioned his Emancipation Proclamation to members of his cabinet on July 21, 1862. Secretary of State William H. Seward told Lincoln to wait for a victory before issuing the proclamation, as to do otherwise would seem like "our last shriek on the retreat". In September 1862 the Battle of Antietam provided this opportunity, and the subsequent War Governors' Conference added support for the proclamation. Lincoln had already published a letter encouraging the border states especially to accept emancipation as necessary to save the Union. Lincoln later said that slavery was "somehow the cause of the war". Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling... I claim not to have controlled events, but confess plainly that events have controlled me." Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty. Lincoln also played a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent. Enslaved African Americans did not wait for Lincoln's action before escaping and seeking freedom behind Union lines. From early years of the war, hundreds of thousands of African Americans escaped to Union lines, especially in occupied areas like Nashville, Norfolk and the Hampton Roads region in 1862, Tennessee from 1862 on, the line of Sherman's march, etc. So many African Americans fled to Union lines that commanders created camps and schools for them, where both adults and children learned to read and write. The American Missionary Association entered the war effort by sending teachers south to such contraband camps, for instance establishing schools in Norfolk and on nearby plantations. In addition, nearly 200,000 African-American men served as soldiers and sailors with Union troops. Most of those were escaped slaves. Confederates enslaved captured black Union soldiers, and black soldiers especially were shot when trying to surrender at the Fort Pillow Massacre. This led to a breakdown of the prisoner exchange program and the growth of prison camps such as Andersonville prison in Georgia, where almost 13,000 Union prisoners of war died of starvation and disease. In spite of the South's shortage of soldiers, most Southern leaders — until 1865 — opposed enlisting slaves. They used them as laborers to support the war effort. As Howell Cobb said, "If slaves will make good soldiers our whole theory of slavery is wrong." Confederate generals Patrick Cleburne and Robert E. Lee argued in favor of arming blacks late in the war, and Jefferson Davis was eventually persuaded to support plans for arming slaves to avoid military defeat. The Confederacy surrendered at Appomattox before this plan could be implemented. The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France. Lincoln's moderate approach succeeded in getting border states, War Democrats and emancipated slaves fighting on the same side for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware. The great majority of the 4 million slaves were freed by the Emancipation Proclamation, as Union armies moved South. The 13th amendment, ratified December 6, 1865, finally made slavery illegal everywhere in the United States, thus freeing the remaining slaves--65,000 in Kentucky (as of 1865), 1,800 in Delaware, and 18 in New Jersey as of 1860. Historian Stephen Oates said that many myths surround Lincoln: "man of the people", "true Christian", "arch villain" and racist. The belief that Lincoln was racist was caused by an incomplete picture of Lincoln, such as focusing on only selective quoting of statements Lincoln made to gain the support of the border states and Northern Democrats, and ignoring the many things he said against slavery, and the military and political context within which such statements were made. Oates said that Lincoln's letter to Horace Greeley has been "persistently misunderstood and misrepresented" for such reasons.
The Confederacy's best hope was military intervention into the war by Britain and France against the Union. The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America (none ever did). In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war in order to get cotton. Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860–62 crop failures in Europe made the North's grain exports of critical importance. It was said that "King Corn was more powerful than King Cotton", as US grain went from a quarter of the British import trade to almost half. When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, iron workers, and British ships to transport weapons. Charles Francis Adams proved particularly adept as minister to Britain for the U.S. and Britain was reluctant to boldly challenge the blockade. The Confederacy purchased several warships from commercial ship builders in Britain. The most famous, the CSS "Alabama", did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for European politicians, especially in Britain. War loomed in late 1861 between the U.S. and Britain over the Trent Affair, involving the U.S. Navy's boarding of a British mail steamer to seize two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation—though even such an offer would have risked war with the U.S. Lord Palmerston reportedly read "Uncle Tom’s Cabin" three times when deciding on this. The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation further reinforced the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's own seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris.
Historians have debated whether the Confederacy could have won the war. Most scholars emphasize that the Union held an insurmountable long-term advantage over the Confederacy in terms of industrial strength and population. Confederate actions, they argue, only delayed defeat. Southern historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back...If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don't think the South ever had a chance to win that War." The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had succeeded in getting the support of the border states, War Democrats, emancipated slaves and Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform. Lincoln had found military leaders like Grant and Sherman who would press the Union's numerical advantage in battle over the Confederate Armies. Generals who did not shy from bloodshed won the war, and from the end of 1864 onward there was no hope for the South. On the other hand, James McPherson has argued that the North’s advantage in population and resources made Northern victory likely, but not inevitable. Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win. Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. Although Lincoln's approach to emancipation was slow, the Emancipation Proclamation was an effective use of the President's war powers. The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly England and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95% effective at stopping trade goods, as a result, imports and exports to the South declined significantly. The abundance of European cotton and England's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either England or France would enter the war. The more industrialized economy of the North aided in the production of arms, munitions and supplies, as well as finances, and transportation. The table shows the relative advantage of the Union over the Confederate States of America (CSA) at the start of the war. The advantages widened rapidly during the war, as the Northern economy grew, and Confederate territory shrank and its economy weakened. The Union population was 22 million and the South 9 million in 1861. The Southern population included more than 3.5 million slaves and about 5.5 million whites, thus leaving the South's white population outnumbered by a ratio of more than four to one. The disparity grew as the Union controlled an increasing amount of southern territory with garrisons, and cut off the trans-Mississippi part of the Confederacy. The Union at the start controlled over 80% of the shipyards, steamships, riverboats, and the Navy. It augmented these by a massive shipbuilding program. This enabled the Union to control the river systems and to blockade the entire southern coastline. Excellent railroad links between Union cities allowed for the quick and cheap movement of troops and supplies. Transportation was much slower and more difficult in the South which was unable to augment its much smaller rail system, repair damage, or even perform routine maintenance. The failure of Davis to maintain positive and productive relationships with state governors (especially governor Joseph E. Brown of Georgia and governor Zebulon Baird Vance of North Carolina) damaged his ability to draw on regional resources. The Confederacy's "King Cotton" misperception of the world economy led to bad diplomacy, such as the refusal to ship cotton before the blockade started. The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army. About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery. Emancipated slaves mostly handled garrison duties, and fought numerous battles in 1864–65. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.
Northern leaders agreed that victory would require more than the end of fighting. It had to encompass the two war goals: secession had to be repudiated and all forms of slavery had to be eliminated. They disagreed sharply on the criteria for these goals. They also disagreed on the degree of federal control that should be imposed on the South, and the process by which Southern states should be reintegrated into the Union. Reconstruction, which began early in the war and ended in 1877, involved a complex and rapidly changing series of federal and state policies. The long-term result came in the three Reconstruction Amendments to the Constitution: the Thirteenth Amendment, which abolished slavery; the Fourteenth Amendment, which extended federal legal protections equally to citizens regardless of race; and the Fifteenth Amendment, which abolished racial restrictions on voting. Reconstruction ended in the different states at different times, the last three by the Compromise of 1877.
Slavery effectively ended in the U.S. in the spring of 1865 when the Confederate armies surrendered. All slaves in the Confederacy were freed by the Emancipation Proclamation, which stipulated that slaves in Confederate-held areas were free. Slaves in the border states and Union-controlled parts of the South were freed by state action or (on December 6, 1865) by the Thirteenth Amendment. The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction. The war produced about 1,030,000 casualties (3% of the population), including about 620,000 soldier deaths—two-thirds by disease. The war accounted for roughly as many American deaths as all American deaths in other U.S. wars combined. The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. About 4 million black slaves were freed in 1861–65. Based on 1860 census figures, 8% of all white males aged 13 to 43 died in the war, including 6% in the North and an extraordinary 18% in the South. About 56,000 soldiers died in prisons during the Civil War. One reason for the high number of battle deaths during the war was the use of Napoleonic tactics such as charges. With the advent of more accurate rifled barrels, Minié balls and (near the end of the war for the Union army) repeating firearms such as the Spencer repeating rifle and a few experimental Gatling guns, soldiers were devastated when standing in lines in the open. This gave birth to trench warfare, a tactic heavily used during World War I.
Andrew Warhola (August 6, 1928 – February 22, 1987), more commonly known as Andy Warhol, was an American painter, printmaker, and filmmaker who was a leading figure in the visual art movement known as pop art. After a successful career as a commercial illustrator, Warhol became famous worldwide for his work as a painter, avant-garde filmmaker, record producer, author, and public figure known for his membership in wildly diverse social circles that included bohemian street people, distinguished intellectuals, Hollywood celebrities and wealthy aristocrats. Warhol has been the subject of numerous retrospective exhibitions, books, and feature and documentary films. He coined the widely used expression "15 minutes of fame." In his hometown of Pittsburgh, Pennsylvania, The Andy Warhol Museum exists in memory of his life and artwork. The highest price ever paid for a Warhol painting is $100 million for a 1963 canvas titled "Eight Elvises." The private transaction was reported in an article in "The Economist", which described Warhol as the "bellwether of the art market." $100 million is a benchmark price that only Jackson Pollock, Pablo Picasso, Gustav Klimt and Willem de Kooning have achieved.
Andy Warhol was born in Pittsburgh, Pennsylvania. He was the fourth child of Ondrej Warhola and Ulja, whose first child was born in their homeland and died before their migration to the U.S. His parents were working-class immigrants from Mikó (now called Miková), in northeastern Slovakia, then part of Austro-Hungarian Empire. Warhol's father immigrated to the US in 1914, and his mother joined him in 1921, after the death of Andy Warhol's grandparents. Warhol's father worked in a coal mine. The family lived at 55 Beelen Street and later at 3252 Dawson Street in the Oakland neighborhood of Pittsburgh. The family was Byzantine Catholic and attended St. John Chrysostom Byzantine Catholic Church. Andy Warhol had two older brothers, Ján and Pavol, who were born in today's Slovakia. Pavol's son, James Warhola, became a successful children's book illustrator. In third grade, Warhol had chorea, a nervous system disease that causes involuntary movements of the extremities, which is believed to be a complication of scarlet fever and causes skin pigmentation blotchiness. He became a hypochondriac, developing a fear of hospitals and doctors. Often bed-ridden as a child, he became an outcast among his school-mates and bonded strongly with his mother. At times when he was confined to bed, he drew, listened to the radio and collected pictures of movie stars around his bed. Warhol later described this period as very important in the development of his personality, skill-set and preferences.
Warhol showed early artistic talent and studied commercial art at the School of Fine Arts at Carnegie Institute of Technology in Pittsburgh, Pennsylvania (now Carnegie Mellon University). In 1949, he moved to New York City and began a successful career in magazine illustration and advertising. During the 1950s, he gained fame for his whimsical ink drawings of shoe advertisements. These were done in a loose, blotted-ink style, and figured in some of his earliest showings at the Bodley Gallery in New York. With the concurrent rapid expansion of the record industry and the introduction of the vinyl record, Hi-Fi, and stereophonic recordings, RCA Records hired Warhol, along with another freelance artist, Sid Maurer, to design album covers and promotional materials.
His first one-man art-gallery exhibition as a fine artist was on July 9, 1962, in the Ferus Gallery of Los Angeles. The exhibition marked the West Coast debut of pop art. Andy Warhol's first New York solo Pop exhibit was hosted at Eleanor Ward's Stable Gallery November 6-24, 1962. The exhibit included the works "Marilyn Diptych", "100 Soup Cans", "100 Coke Bottles" and "100 Dollar Bills". At the Stable Gallery exhibit, the artist met for the first time John Giorno who would star in Warhol's first film, "Sleep", in 1963. It was during the 1960s that Warhol began to make paintings of iconic American products such as Campbell's Soup Cans and Coca-Cola bottles, as well as paintings of celebrities such as Marilyn Monroe, Elvis Presley, Troy Donahue, Muhammad Ali and Elizabeth Taylor. He founded "The Factory", his studio during these years, and gathered around himself a wide range of artists, writers, musicians, and underground celebrities. He began producing prints using the silkscreen method. His work became popular and controversial. New York's Museum of Modern Art hosted a Symposium on pop art in December 1962 during which artists like Warhol were attacked for "capitulating" to consumerism. Critics were scandalized by Warhol's open embrace of market culture. This symposium set the tone for Warhol's reception. Throughout the decade it became more and more clear that there had been a profound change in the culture of the art world, and that Warhol was at the center of that shift. A pivotal event was the 1964 exhibit "The American Supermarket", a show held in Paul Bianchini's Upper East Side gallery. The show was presented as a typical U.S. small supermarket environment, except that everything in it – from the produce, canned goods, meat, posters on the wall, etc. – was created by six prominent pop artists of the time, among them the controversial (and like-minded) Billy Apple, Mary Inman, and Robert Watts. Warhol's painting of a can of Campbell's soup cost $1,500 while each autographed can sold for $6. The exhibit was one of the first mass events that directly confronted the general public with both pop art and the perennial question of what art is (or of what is art and what is not). As an advertisement illustrator in the 1950s, Warhol used assistants to increase his productivity. Collaboration would remain a defining (and controversial) aspect of his working methods throughout his career; in the 1960s, however, this was particularly true. One of the most important collaborators during this period was Gerard Malanga. Malanga assisted the artist with producing silkscreens, films, sculpture, and other works at "The Factory", Warhol's aluminum foil-and-silver-paint-lined studio on 47th Street (later moved to Broadway). Other members of Warhol's Factory crowd included Freddie Herko, Ondine, Ronald Tavel, Mary Woronov, Billy Name, and Brigid Berlin (from whom he apparently got the idea to tape-record his phone conversations). During the '60s, Warhol also groomed a retinue of bohemian eccentrics upon whom he bestowed the designation "Superstars", including Edie Sedgwick, Viva, Ultra Violet, and Candy Darling. These people all participated in the Factory films, and some – like Berlin – remained friends with Warhol until his death. Important figures in the New York underground art/cinema world, such as writer John Giorno and film-maker Jack Smith, also appear in Warhol films of the 1960s, revealing Warhol's connections to a diverse range of artistic scenes during this period.
On June 3, 1968, Valerie Solanas shot Warhol and art critic and curator Mario Amaya at Warhol's studio. Before the shooting, Solanas had been a marginal figure in the Factory scene. She founded a "group" called S.C.U.M. (Society for Cutting Up Men) and authored the "S.C.U.M. Manifesto", a separatist feminist attack on patriarchy. Over the years, Solanas' manifesto has found a following. Solanas appears in the 1968 Warhol film "I, A Man". Earlier on the day of the attack, Solanas had been turned away from the Factory after asking for the return of a script she had given to Warhol. The script, apparently, had been misplaced. Amaya received only minor injuries and was released from the hospital later the same day. Warhol however, was seriously wounded by the attack and barely survived (surgeons opened his chest and massaged his heart to help stimulate its movement again). He suffered physical effects for the rest of his life. The shooting had a profound effect on Warhol's life and art. Solanas was arrested the day after the assault. By way of explanation, she said that "He had too much control over my life," following which she was eventually sentenced to three years under the control of the department of corrections. After the shooting, the Factory scene became much more tightly controlled, and for many this event brought the "Factory 60s" to an end. The shooting was mostly overshadowed in the media due to the assassination of Robert F. Kennedy two days later. Warhol had this to say about the attack: "Before I was shot, I always thought that I was more half-there than all-there – I always suspected that I was watching TV instead of living life. People sometimes say that the way things happen in movies is unreal, but actually it's the way things happen in life that's unreal. The movies make emotions look so strong and real, whereas when things really do happen to you, it's like watching television – you don't feel anything. Right when I was being shot and ever since, I knew that I was watching television. The channels switch, but it's all television."
Compared to the success and scandal of Warhol's work in the 1960s, the 1970s proved a much quieter decade, as Warhol became more entrepreneurial. According to Bob Colacello, Warhol devoted much of his time to rounding up new, rich patrons for portrait commissions– including Shah of Iran Mohammad Reza Pahlavi, his wife Empress Farah Pahlavi, his sister Princess Ashraf Pahlavi, Mick Jagger, Liza Minnelli, John Lennon, Diana Ross, Brigitte Bardot, and Michael Jackson. Warhol's famous portrait of Chinese Communist leader Mao Zedong was created in 1973. He also founded, with Gerard Malanga, "Interview" magazine, and published "The Philosophy of Andy Warhol" (1975). An idea expressed in the book: "Making money is art, and working is art and good business is the best art." Warhol used to socialize at various nightspots in New York City, including Max's Kansas City; Serendipity 3; and, later in the '70s, Studio 54. He was generally regarded as quiet, shy, and a meticulous observer. Art critic Robert Hughes called him "the white mole of Union Square."
Warhol had a re-emergence of critical and financial success in the 1980s, partially due to his affiliation and friendships with a number of prolific younger artists, who were dominating the "bull market" of '80s New York art: Jean-Michel Basquiat, Julian Schnabel, David Salle and other so-called Neo-Expressionists, as well as members of the Transavantgarde movement in Europe, including Francesco Clemente and Enzo Cucchi. By this period, Warhol was being criticized for becoming merely a "business artist". In 1979, unfavorable reviews met his exhibits of portraits of 1970s personalities and celebrities, calling them superficial, facile and commercial, with no depth or indication of the significance of the subjects. This criticism was echoed for his 1980 exhibit of ten portraits at the Jewish Museum in New York, entitled "Jewish Geniuses", which Warhol – who exhibited no interest in Judaism or matters of interest to Jews – had described in his diary as "They're going to sell." In hindsight, however, some critics have come to view Warhol's superficiality and commerciality as "the most brilliant mirror of our times," contending that "Warhol had captured something irresistible about the zeitgeist of American culture in the 1970s." Warhol also had an appreciation for intense Hollywood glamour. He once said: "I love Los Angeles. I love Hollywood. They're so beautiful. Everything's plastic, but I love plastic. I want to be plastic."
Many people think of Warhol as asexual and merely a "voyeur"; however, it is now well established that he was gay (see biographers such as Victor Bockris, Bob Colacello, and art historian Richard Meyer). The question of how Warhol's sexuality influenced his work and shaped his relationship to the art world is a major subject of scholarship on the artist, and is an issue that Warhol himself addressed in interviews, in conversation with his contemporaries, and in his publications ("e.g." "Popism: The Warhol Sixties"). Throughout his career, Warhol produced erotic photography and drawings of male nudes. Many of his most famous works (portraits of Liza Minnelli, Judy Garland, and Elizabeth Taylor, and films like "Blow Job", "My Hustler", and "Lonesome Cowboys") draw from gay underground culture and/or openly explore the complexity of sexuality and desire. Many of his films premiered in gay porn theaters. That said, some stories about Warhol's development as an artist revolved around the obstacle his sexuality initially presented as he tried to launch his career. The first works that he submitted to a gallery in the pursuit of a career as an artist were homoerotic drawings of male nudes. They were rejected for being too openly gay. In "Popism", furthermore, the artist recalls a conversation with the film maker Emile de Antonio about the difficulty Warhol had being accepted socially by the then more famous (but closeted) gay artists Jasper Johns and Robert Rauschenberg. De Antonio explained that Warhol was "too swish and that upsets them." In response to this, Warhol writes, "There was nothing I could say to that. It was all too true. So I decided I just wasn't going to care, because those were all the things that I didn't want to change anyway, that I didn't think I 'should' want to change... Other people could change their attitudes but not me". In exploring Warhol's biography, many turn to this period – the late 1950s and early 1960s – as a key moment in the development of his persona. Some have suggested that his frequent refusal to comment on his work, to speak about himself (confining himself in interviews to responses like "Um, No" and "Um, Yes", and often allowing others to speak for him) – and even the evolution of his Pop style –can be traced to the years when Warhol was first dismissed by the inner circles of the New York art world.
Warhol was a practicing Byzantine Catholic. He regularly volunteered at homeless shelters in New York, particularly during the busier times of the year, and described himself as a religious person. Several of Warhol's later works depicted religious subjects, including two series, "Details of Renaissance Paintings" (1984) and "The Last Supper" (1986). In addition, a body of religious-themed works was found posthumously in his estate. During his life, Warhol regularly attended Mass, and the priest at Warhol's church, Saint Vincent's, said that the artist went there almost daily. His art is noticeably influenced by the eastern Christian iconographic tradition which was so evident in his places of worship. Warhol's brother has described the artist as "really religious, but he didn't want people to know about that because [it was] private." Despite the private nature of his faith, in Warhol's eulogy John Richardson depicted it as devout: "To my certain knowledge, he was responsible for at least one conversion. He took considerable pride in financing his nephew's studies for the priesthood"
Warhol died in New York City at 6:32 a.m. on February 22, 1987. According to news reports, he had been making good recovery from a routine gallbladder surgery at New York Hospital before dying in his sleep from a sudden post-operative cardiac arrhythmia. Prior to his diagnosis and operation, Warhol delayed having his recurring gallbladder problems checked, as he was afraid to enter hospitals and see doctors. His family sued the hospital for inadequate care, saying that the arrhythmia was caused by improper care and water intoxication. Warhol's body was taken back to Pittsburgh by his brothers for burial. The wake was at Thomas P. Kunsak Funeral Home and was an open-coffin ceremony. The coffin was a solid bronze casket with gold plated rails and white upholstery. Warhol was dressed in a black cashmere suit, a paisley tie, a platinum wig, and sunglasses. He was posed holding a small prayer book and a red rose. The funeral liturgy was held at the Holy Ghost Byzantine Catholic Church on Pittsburgh's North Side. The eulogy was given by Monsignor Peter Tay. Yoko Ono also made an appearance. The coffin was covered with white roses and asparagus ferns. After the liturgy, the coffin was driven to St. John the Baptist Byzantine Catholic Cemetery in Bethel Park, a south suburb of Pittsburgh. At the grave, the priest said a brief prayer and sprinkled holy water on the casket. Before the coffin was lowered, Paige Powell dropped a copy of "Interview" magazine, an "Interview" t-shirt, and a bottle of the Estee Lauder perfume "Beautiful" into the grave. Warhol was buried next to his mother and father. Weeks later a memorial service was held in Manhattan for Warhol on April 1, 1987, at St. Patrick's Cathedral, New York. Warhol's will dictated that his entire estate – with the exception of a few modest legacies to family members – would go to create a foundation dedicated to the "advancement of the visual arts". Warhol had so many possessions that it took Sotheby's nine days to auction his estate after his death; the auction grossed more than US$20 million. His total estate was worth considerably more, due in no small part to shrewd investments over the years. In 1987, in accordance with Warhol's will, the Andy Warhol Foundation for the Visual Arts was founded. The Foundation not only serves as the official Estate of Andy Warhol, but also has a mission "to foster innovative artistic expression and the creative process" and is "focused primarily on supporting work of a challenging and often experimental nature." The Artists Rights Society is the U.S. copyright representative for the Andy Warhol Foundation for the Visual Arts for all Warhol works with the exception of Warhol film stills. The U.S. copyright representative for Warhol film stills is the Warhol Museum in Pittsburgh. Additionally, the Andy Warhol Foundation for the Visual Arts has agreements in place for its image archive. All digital images of Warhol are exclusively managed by Corbis, while all transparency images of Warhol are managed by Art Resource. The Andy Warhol Foundation released its 20th Anniversary Annual Report as a three-volume set in 2007: Vol. I, 1987–2007; Vol. II, Grants & Exhibitions; and Vol. III, Legacy Program. The Foundation remains one of the largest grant-giving organizations for the visual arts in the U.S.
By the beginning of the 1960s, Warhol was a very successful commercial illustrator. His detailed and elegant drawings for I. Miller shoes were particularly popular. These illustrations consisted mainly of "blotted ink" drawings (or monoprints), a technique which he applied in much of his early art. Although many artists of this period worked in commercial art, most did so discreetly. Warhol was so successful, however, that his profile as an illustrator seemed to undermine his efforts to be taken seriously as an artist. Pop Art was an experimental form that several artists were independently adopting; some of these pioneers, such as Roy Lichtenstein, would later become synonymous with the movement. Warhol, who would become famous as the "Pope of Pop", turned to this new style, where popular subjects could be part of the artist's palette. His early paintings show images taken from cartoons and advertisements, hand-painted with paint drips. Those drips emulated the style of successful abstract expressionists (such as Willem de Kooning). Warhol's first Pop Art paintings were displayed in April 1961, serving as the backdrop for New York Department Store Bronwit Teller's window display. This was the same stage his Pop Art contemporaries Jasper Johns, James Rosenquist and Robert Rauschenberg had also once graced. Eventually, Warhol pared his image vocabulary down to the icon itself – to brand names, celebrities, dollar signs – and removed all traces of the artist's "hand" in the production of his paintings. To him, part of defining a niche was defining his subject matter. Cartoons were already being used by Lichtenstein, typography by Jasper Johns, and so on; Warhol wanted a distinguishing subject. His friends suggested he should paint the things he loved the most. In his signature way of taking things literally, for his first major exhibition he painted his famous cans of Campbell's Soup, which he claimed to have had for lunch for most of his life. The work sold for $10,000 at an auction on November 17, 1971, at Sotheby's New York – a minimal amount for the artist whose paintings sell for over $6 million more recently. He loved celebrities, so he painted them as well. From these beginnings he developed his later style and subjects. Instead of working on a signature subject matter, as he started out to do, he worked more and more on a signature style, slowly eliminating the hand-made from the artistic process. Warhol frequently used silk-screening; his later drawings were traced from slide projections. At the height of his fame as a painter, Warhol had several assistants who produced his silk-screen multiples, following his directions to make different versions and variations. In 1979, Warhol was commissioned by BMW to paint a Group 4 race version of the then elite supercar BMW M1 for the fourth installment in the BMW Art Car Project. Unlike the three artists before him, Warhol declined the use of a small scale practice model, instead opting to immediately paint directly onto the full scale automobile. It was indicated that Warhol spent only a total of 23 minutes to paint the entire car. Warhol produced both comic and serious works; his subject could be a soup can or an electric chair. Warhol used the same techniques– silkscreens, reproduced serially, and often painted with bright colors – whether he painted celebrities, everyday objects, or images of suicide, car crashes, and disasters, as in the 1962–63 "Death and Disaster" series. The "Death and Disaster" paintings (such as "Red Car Crash", "Purple Jumping Man", and "Orange Disaster") transform personal tragedies into public spectacles, and signal the use of images of disaster in the then evolving mass media. The unifying element in Warhol's work is his deadpan Keatonesque style – artistically and personally affectless. This was mirrored by Warhol's own demeanor, as he often played "dumb" to the media, and refused to explain his work. The artist was famous for having said that all you need to know about him and his works is already there, "Just look at the surface of my paintings and films and me, and there I am. There's nothing behind it." Warhol's first portrait of "Basquiat" (1982) is a black photosilkscreen over an oxidized copper "piss painting". After many years of silkscreen, oxidation, photography, etc., Warhol returned to painting with a brush in hand in a series of over 50 large collaborative works done with Jean-Michel Basquiat between 1984 and 1986. These were influential for his later work. Warhol's "The Last Supper" cycle was his last series, possibly his largest and seen by some as "arguably his greatest". It is also the largest series of religious works by any U.S. artist.
Warhol worked across a wide range of media – painting, photography, drawing, and sculpture. In addition, he was a highly prolific filmmaker. Between 1963 and 1968, he made more than 60 films, plus some 500 short black-and-white "screen test" portraits of Factory visitors. One of his most famous films, "Sleep", monitors poet John Giorno sleeping for six hours. The 35-minute film "Blow Job" is one continuous shot of the face of DeVeren Bookwalter supposedly receiving oral sex from filmmaker Willard Maas, although the camera never tilts down to see this. Another, "Empire" (1964), consists of eight hours of footage of the Empire State Building in New York City at dusk. The film "Eat" consists of a man eating a mushroom for 45 minutes. Warhol attended the 1962 premiere of the static composition by LaMonte Young called "Trio for Strings" and subsequently created his famous series of static films including "Kiss", "Eat", and "Sleep" (for which Young initially was commissioned to provide music). Uwe Husslein cites filmmaker Jonas Mekas, who accompanied Warhol to the Trio premiere, and who claims Warhol's static films were directly inspired by the performance. "Batman Dracula" is a 1964 film that was produced and directed by Warhol, without the permission of DC Comics. It was screened only at his art exhibits. A fan of the Batman series, Warhol's movie was an "homage" to the series, and is considered the first appearance of a blatantly campy Batman. The film was until recently thought to have been lost, until scenes from the picture were shown at some length in the 2006 documentary "Jack Smith and the Destruction of Atlantis". Warhol's 1965 film "Vinyl" is an adaptation of Anthony Burgess' popular dystopian novel "A Clockwork Orange". Others record improvised encounters between Factory regulars such as Brigid Berlin, Viva, Edie Sedgwick, Candy Darling, Holly Woodlawn, Ondine, Nico, and Jackie Curtis. Legendary underground artist Jack Smith appears in the film "Camp". His most popular and critically successful film was "Chelsea Girls" (1966). The film was highly innovative in that it consisted of two 16 mm-films being projected simultaneously, with two different stories being shown in tandem. From the projection booth, the sound would be raised for one film to elucidate that "story" while it was lowered for the other. The multiplication of images evoked Warhol's seminal silk-screen works of the early 1960s. Other important films include "Bike Boy", "My Hustler", and "Lonesome Cowboys", a raunchy pseudo-western. These and other titles document gay underground and camp culture, and continue to feature prominently in scholarship about sexuality and art. "Blue Movie" – a film in which Warhol superstar Viva makes love and fools around in bed with a man for 33 minutes of the film's playing-time – was Warhol's last film as director. The film was at the time scandalous for its frank approach to a sexual encounter. For many years Viva refused to allow it to be screened. It was publicly screened in New York in 2005 for the first time in over thirty years. After his June 3, 1968, shooting, a reclusive Warhol relinquished his personal involvement in filmmaking. His acolyte and assistant director, Paul Morrissey, took over the film-making chores for the Factory collective, steering Warhol-branded cinema towards more mainstream, narrative-based, B-movie exploitation fare with "Flesh", "Trash", and "Heat". All of these films, including the later "Andy Warhol's Dracula" and "Andy Warhol's Frankenstein", were far more mainstream than anything Warhol as a director had attempted. These latter "Warhol" films starred Joe Dallesandro – more of a Morrissey star than a true Warhol superstar. In the early '70s, most of the films directed by Warhol were pulled out of circulation by Warhol and the people around him who ran his business. After Warhol's death, the films were slowly restored by the Whitney Museum and are occasionally projected at museums and film festivals. Few of the Warhol-directed films are available on video or DVD.
In the mid 1960s, Warhol adopted the band The Velvet Underground, making them a crucial element of the Exploding Plastic Inevitable multimedia performance art show. Warhol, with Paul Morrissey, acted as the band's manager, introducing them to Nico (who would perform with the band at Warhol's request). In 1966 he "produced" their first album "The Velvet Underground & Nico", as well as providing its album art. His actual participation in the album's production amounted to simply paying for the studio time. After the band's first album, Warhol and band leader Lou Reed started to disagree more about the direction the band should take, and their artistic friendship ended. Warhol designed many album covers for various artists starting with the photographic cover of John Wallowitch's debut album, "This Is John Wallowitch!!!" (1964). Warhol designed the cover art for The Rolling Stones albums "Sticky Fingers" (1971) and "Love You Live" (1977), and the John Cale album "Honi Soit" in 1981. In 1975, Warhol was commissioned to do several portraits of the band's frontman Mick Jagger while in 1982, he designed the album cover for the Diana Ross album Silk Electric. One of his last works was a portrait of Aretha Franklin for the cover of her 1986 gold album "Aretha", which was done in the style of the "Reigning Queens" series he had completed the year before. Warhol was also friendly with many recording artists, including Deborah Harry, Grace Jones, Diana Ross and John Lennon - he designed the cover to Lennon's 1986 posthumously released "Menlove Ave.". Warhol also appeared as a bartender in The Cars' music video for their single "Hello Again", and Curiosity Killed The Cat's video for their "Misfit" single (both videos, and others, were produced by Warhol's video production company). Warhol featured in Grace Jones' music video for "I'm Not Perfect (But I'm Perfect for You)". Warhol strongly influenced the New Wave/punk rock band Devo, as well as David Bowie. Bowie recorded a song called "Andy Warhol" for his 1971 album "Hunky Dory". Lou Reed wrote the song "Andy's Chest", about Valerie Solanas, the woman who shot Warhol, in 1968. He recorded it with the Velvet Underground, but this version wasn't officially released until the VU album appeared in 1985. He recorded a new version for his 1972 solo album "Transformer", produced by Bowie and Mick Ronson. "Sam and One Blue Pussy" by Andy Warhol given in 1954 to Edgar de Evia and Robert Denning when the author was a guest in their home in the Rhinelander Mansion.
Beginning in the early 1950s, Warhol produced several unbound portfolios of his work. The first of several bound self-published books by Warhol was "25 Cats Name Sam and One Blue Pussy", printed in 1954 by Seymour Berlin on Arches brand watermarked paper using his blotted line technique for the lithographs. The original edition was limited to 190 numbered, hand colored copies, using Dr. Martin's ink washes. Most of these were given by Warhol as gifts to clients and friends. Copy #4, inscribed "Jerry" on the front cover and given to Geraldine Stutz, was used for a facsimile printing in 1987 and the original was auctioned in May 2006 for US $35,000 by Doyle New York. Warhol created the fashion magazine "Interview" that is still published today. The loopy title script on the cover is thought to be either his own handwriting or that of his mother, Julia Warhola, who would often do text work for his early commercial pieces.
Warhol had assistance in producing his paintings. This is also true of his film-making and commercial enterprises. He founded the gossip magazine "Interview", a stage for celebrities he "endorsed" and a business staffed by his friends. He collaborated with others on all of his books (some of which were written with Pat Hackett.) He adopted the young painter Jean-Michel Basquiat, and the band The Velvet Underground, presenting them to the public as his latest interest, and collaborating with them. One might even say that he produced people (as in the Warholian "Superstar" and the Warholian portrait). He endorsed products, appeared in commercials, and made frequent celebrity guest appearances on television shows and in films (he appeared in everything from "Love Boat" to "Saturday Night Live" and the Richard Pryor movie, "Dynamite Chicken"). In this respect Warhol was a fan of "Art Business" and "Business Art"– he, in fact, wrote about his interest in thinking about art as business in "The Philosophy of Andy Warhol from A to B and Back Again".
Two museums are dedicated to Andy Warhol. The Andy Warhol Museum, one of the Carnegie Museums of Pittsburgh, is located at 117 Sandusky Street in Pittsburgh, Pennsylvania. It is the largest American art museum dedicated to a single artist, holding more than 12,000 works by the artist. The other museum is the Andy Warhol Museum of Modern Art, established in 1991 by Andy's brother John Warhola, the Slovak Ministry of Culture, and the Warhol Foundation in New York. It is located in the small town of Medzilaborce, Slovakia. Andy's parents and his two brothers were born 15 kilometres away in the village of Miková. The museum houses several originals donated mainly by the Andy Warhol Foundation in New York and also personal items donated by Warhol's relatives.
In 1979, Warhol appeared as himself in the film "Cocaine Cowboys". After his passing, Warhol was portrayed by Crispin Glover in Oliver Stone's film "The Doors" (1991), by David Bowie in "Basquiat", a film by Julian Schnabel, and by Jared Harris in the film "I Shot Andy Warhol" directed by Mary Harron (1996). Warhol appears as a character in Michael Daugherty's 1997 opera Jackie O. Actor Mark Bringleson makes a brief cameo as Warhol in (1997). Many films by avant-garde cineast Jonas Mekas have caught the moments of Andy's life. Sean Gregory Sullivan depicted Warhol in the 1998 film "54". Guy Pearce portrayed Warhol in the 2007 film, "Factory Girl", about Edie Sedgwick's life. Actor Greg Travis portrays Warhol in a brief scene from the 2009 film "Watchmen". Gus Van Sant was planning a version of Warhol's life with River Phoenix in the lead role just before Phoenix's death in 1993.
He succeeded his father Chagri Begh as governor of Khorasan in 1059. When his uncle Toğrül died he was succeeded by Suleiman, Alp Arslan's brother. Alp Arslan and his uncle Kutalmish both contested this succession. Alp Arslan defeated Kutalmish for the throne and succeeded on 27 April 1064 as sultan of Great Seljuk, and thus became sole monarch of Persia from the river Oxus to the Tigris. In consolidating his empire and subduing contending factions he was ably assisted by Nizam ul-Mulk, his Persian vizier, and one of the most eminent statesmen in early Muslim history. With peace and security established in his dominions, he convoked an assembly of the states and declared his son Malik Shah I his heir and successor. With the hope of acquiring immense booty in the rich church of St. Basil in Caesarea Mazaca, the capital of Cappadocia, he placed himself at the head of the Turkish cavalry, crossed the Euphrates and entered and plundered that city. He then marched into Armenia and Georgia, which he conquered in 1064.
In 1068, en route to Syria, Alp Arslan Oush invaded the Byzantine Empire. The Emperor Romanos IV Diogenes, assuming the command in person, met the invaders in Cilicia. In three arduous campaigns, the first two of which were conducted by the emperor himself while the third was directed by Manuel Comnenos (great-uncle of Emperor Manuel Comnenos), the Turks were defeated in detail in 1070 and driven across the Euphrates. In 1071 Romanos again took the field and advanced with 40,000 men, including a contingent of the Cuman Turks as well as contingents of Franks and Normans, under Ursel de Baieul, into Armenia. At Manzikert, on the Murad Tchai, north of Lake Van, Diogenes was met by Alp Arslan. The sultan proposed terms of peace, which were rejected by the emperor, and the two forces met in the Battle of Manzikert. The Cuman mercenaries among the Byzantine forces immediately defected to the Turkish side; and, seeing this, "the Western mercenaries rode off and took no part in the battle." The Byzantines were totally routed. Alp Arslan's victories changed the balance in near Asia completely in favour of the Seljuk Turks and Sunni Muslims. While the Byzantine Empire was to continue for nearly another four centuries, and the Crusades would contest the issue for some time, the victory at Manzikert signalled the beginning of Turkish ascendancy in Anatolia. Most historians, including Edward Gibbon, date the defeat at Manzikert as the beginning of the end of the Eastern Roman Empire. Certainly the entry of Turkic farmers following their horsemen ended the themes in Anatolia which had furnished the Empire with men and treasure.
Alp Arslan's strength lay in the military realm. Domestic affairs were handled by his able vizier, Nizam al-Mulk, the founder of the administrative organization which characterized and strengthened the sultanate during the reigns of Alp Arslan and his son, Malik Shah. Military fiefs, governed by Seljuk princes, were established to provide support for the soldiery and to accommodate the nomadic Turks to the established Persian agricultural scene. This type of military fiefdom enabled the nomadic Turks to draw on the resources of the sedentary Persians and other established cultures within the Seljuk realm, and allowed Alp Arslan to field a huge standing army, without depending on tribute from conquest to pay his soldiery. He not only had enough food from his subjects to maintain his military, but the taxes collected from traders and merchants added to his coffers sufficiently to fund his continuous wars. Suleiman ibn Kutalmish was the son of the contender for Arslan's throne; he was appointed governor of the north-western provinces. An explanation for this choice can only be conjectured from Ibn al-Athir’s account of the battle between Alp-Arslan and Kutalmish, in which he writes that Alp-Arslan wept for the latter's death and greatly mourned the loss of his kinsman.
As he lay dying, Alp Arslan whispered to his son that his vanity had killed him. "Alas," he is recorded to have said, "surrounded by great warriors devoted to my cause, guarded night and day by them, I should have allowed them to do their job. I had been warned against trying to protect myself, and against letting my courage get in the way of my good sense. I forgot those warnings, and here I lie, dying in agony. Remember well the lessons learned, and do not allow your vanity to overreach your good sense..."
The American Film Institute is an independent non-profit organization created by the National Endowment for the Arts, which was established in 1967 when President Lyndon B. Johnson signed the National Foundation on the Arts and the Humanities Act. The organization describes itself as "a national institute providing leadership in screen education and the recognition and celebration of excellence in the art of film, television and digital media." The AFI Conservatory focuses on training through hands-on experience with established figures. AFI also produces film education and appreciation materials for middle and high school students. The American Film Institute re-opened the AFI Silver theatre in Silver Spring, Maryland, near Washington, D.C., in April 2003.
The American Film Institute was founded in 1967 as a national arts organization to preserve the legacy of America’s film heritage, educate the next generation of filmmakers and honor the artists and their work. The National Endowment for the Arts and Humanities recommended creating AFI “to enrich and nurture the art of film in America” with initial funding from the National Endowment for the Arts, the Motion Picture Association of America and the Ford Foundation. The original 22-member Board of Trustees included Chair Gregory Peck and Vice Chair Sidney Poitier as well as Francis Ford Coppola, Arthur Schlesinger, Jr., Jack Valenti and other representatives from the arts and academia. George Stevens, Jr., was the founding director. Jean Picker Firstenberg was President and CEO from 1980 to 2007. Bob Gazzale was named President and CEO in 2007. As a national nonprofit organization, the institute funds its efforts through contributions and sponsorships from large corporations and small companies, donations from individuals and its AFI membership program.
The AFI Conservatory describes itself as a “world-renowned Conservatory where a dedicated group of working professionals from the film and television communities serve as mentors in a hands-on, production-based environment nurturing the talents of tomorrow's storytellers.” In a two-year program that emphasizes narrative storytelling and grants an MFA, Fellows specialize in one of six disciplines: Cinematography, Directing, Editing, Production Design, Producing and Screenwriting. In 1969, the institute established the Center for Advanced Film Studies at Greystone, the Doheny Mansion in Beverly Hills, CA. The first class included filmmakers Terrence Malick, David Lynch, Caleb Deschanel and Paul Schrader. That program grew into the AFI Conservatory, a fully accredited graduate film school, located in the hills above Hollywood, CA. In addition to the Conservatory, AFI has a tuition-free program called the AFI Directing Workshop for Women that operates each spring and summer from the Los Angeles campus.
Several AFI Alums have received both national and international recognition. Among the notable alumni of AFI are: Darren Aronofsky, Jon Avnet, Stuart Cornfeld, Bill Duke, Edward James Olmos, Todd Field, Rodrigo Garcia, Anne Garefino, Steve Golin, Amy Heckerling, Marshall Herskovitz, Janusz Kaminski, Mimi Leder, David Lynch, Terrence Malick, John McTiernan, Paul Schrader, Frank Spotnitz, Mark Waters, Gary Winick, Ed Zwick, and Susannah Grant. AFI Catalog Of Feature Films. The AFI Catalog of Feature Films, started in 1968, is an online database that preserves the history of American film in authoritative, encyclopedic detail. A prime research tool for film historians, the catalog consists of entries on more than 50,000 films, from 1893 to the mid-1970s, documenting casts, crews, synopses and production notes. New catalog entries of the remaining 15,000 American feature films produced between 1974 and present day are incorporated every year.
The annual AFI AWARDS honors the creative ensembles of the 10 outstanding movies and television shows of the year. Two 13-person juries composed of artists, academics, critics and AFI Trustees deliberate, discuss and determine the honored ensembles, who are then feted at a private event in January. In addition, Ten Moments of Significance, documenting the year’s media milestones, are entered into an ongoing almanac.
The popular AFI 100 Years… series, which ran from 1998 to 2008, and created jury-selected lists of America’s best movies in categories including Musicals, Laughs and Thrills, drove new generations to experience classic American films. The juries consisted of over 1,500 artists, scholars, critics and historians, with movies selected based on the film’s popularity over time, historical significance and cultural impact. "Citizen Kane" was voted the greatest American film twice.
AFI operates two film festivals: AFI FEST in Los Angeles, CA, and AFI-Discovery Channel SILVERDOCS documentary festival in Silver Spring, MD. AFI FEST is the only film festival in the US to hold FIAPF (Fédération Internationale des Associations de Producteurs de Films) accreditation. AFI Silver Theatre and Cultural Center. As the largest nonprofit exhibitor in the United States, AFI screens films regularly at the AFI Silver Theatre and Cultural Center in Silver Spring, MD, and the ArcLight Cinemas and Skirball Cultural Center in Los Angeles, CA. Programming at the AFI Silver Theatre consists of an eclectic mix of retrospectives, festivals and first-run features as well as community events and educational activities.
AFI Project: 20/20 is a global exchange program that promotes cultural understanding through film. The program sends filmmakers to festivals and cultural institutions around the world to screen their films and engage audiences in discussions that challenge stereotypes and encourage mutual respect. The program is a collaboration between AFI and America’s four government cultural agencies—the National Endowment for the Arts, the National Endowment for the Humanities, the Institute of Museum and Library Services and the President’s Committee on the Arts and Humanities.
was a Japanese film director, producer, screenwriter and editor. In a career that spanned 50 years, Kurosawa directed 30 films. He is widely regarded as one of the most important and influential filmmakers in film history. In 1989, he was awarded the Academy Award for Lifetime Achievement "for cinematic accomplishments that have inspired, delighted, enriched and entertained worldwide audiences and influenced filmmakers throughout the world."
Akira Kurosawa was born to Isamu and Shima Kurosawa on 23 March 1910. He was the youngest of eight children born to the Kurosawas in a suburb of Tokyo. Shima Kurosawa was 40 years old at the time of Akira's birth and his father Isamu was 45. Akira Kurosawa grew up in a household with three older brothers and four older sisters. Of his three older brothers, one died before Akira was born and one was already grown and out of the household. One of his four older sisters had also left the home to begin her own family before Kurosawa was born. Kurosawa's next-oldest sibling, a sister he called "Little Big Sister," also died suddenly after a short illness when he was 10 years old. Kurosawa's father worked as the director of a junior high school operated by the Japanese military and the Kurosawas descended from a line of former samurai. Financially, the family was above average. Isamu Kurosawa embraced western culture both in the athletic programs that he directed and by taking the family to see films, which were then just beginning to appear in Japanese theaters. Later, when Japanese culture turned away from western films, Isamu Kurosawa continued to believe that films were a positive educational experience. In primary school, Kurosawa was encouraged to draw by Tachikawa, a teacher who took an interest in mentoring his talents. Both Tachikawa and his brother Heigo had a profound impact on him. Heigo was very intelligent and won several academic competitions but also had what was later called a cynical or dark side. In 1923, the Great Kantō earthquake destroyed Tokyo and left 100,000 people dead. In the wake of this event, Heigo, 17, and Akira, 13, made a walking tour of the devastation. Corpses of humans and animals were piled everywhere. When Akira would attempt to turn his head away, Heigo urged him not to. According to Akira, this experience would later instruct him that to look at a frightening thing head-on is to defeat its ability to cause fear. Heigo eventually began a career as a benshi in Tokyo film theaters. Benshi narrated silent films for the audience and were a uniquely Japanese addition to the theater experience. In the transition to talking pictures, later in Japan than elsewhere, benshi lost work all over the country. Heigo organized a benshi strike that failed. Akira was likewise involved in labor-management struggles, writing several articles for a radical newspaper while improving and expanding his skills as a painter and reading literature. When Akira Kurosawa was in his early 20s, his older brother Heigo committed suicide. Four months later, the oldest of Kurosawa's brothers also died, leaving Akira, at age 23, as the only surviving son of an original four.
In 1936, Kurosawa learned of an apprenticeship program for directors through a major film studio, PCL (later Toho). He was hired and worked as an assistant director to Kajiro Yamamoto. After his directorial debut with "Sanshiro Sugata" (1943), his next few films were made under the watchful eye of the wartime Japanese government and sometimes contained nationalistic themes. For instance, "The Most Beautiful" (1944) is a propaganda film about Japanese women working in a military optics factory. "Judo Saga 2" (1945) portrays Japanese judo as superior to western (American) boxing. His first post-war film "No Regrets for Our Youth" (1946), by contrast, is critical of the old Japanese regime and is about the wife of a left-wing dissident who is arrested for his political leanings. Kurosawa made several more films dealing with contemporary Japan, most notably "Drunken Angel" (1948) and "Stray Dog" (1949). However, it was the period film "Rashomon" (1950) which led to him being known internationally and won him the Golden Lion at the Venice Film Festival.
Kurosawa had a distinctive cinematic technique, which he had developed by the 1950s. He liked using telephoto lenses for the way they flattened the frame. He believed that placing cameras farther away from his actors produced better performances as they would not be conscious of the camera. He also liked using multiple cameras, which allowed him to shoot an action scene from different angles. As with the use of telephoto lenses, the multiple-camera technique also prevented Kurosawa's actors from "figuring out which one is shooting him [and invariably turning] one-third to halfway in its direction." Another Kurosawa trademark was the use of weather elements to heighten mood; for example, the heavy rain in the opening scene of "Rashomon" and the final battle in "Seven Samurai" (1954); the intense heat in "Stray Dog"; the cold wind in "Yojimbo" (1961); the snow in "Ikiru" (1952); and the fog in "Throne of Blood" (1957). Kurosawa also liked using frame wipes, sometimes cleverly hidden by motion within the frame, as a transition device. He was known as "Tenno", literally "Emperor", for his dictatorial directing style. He was a perfectionist who spent enormous amounts of time and effort to achieve the desired visual effects. For the rainstorm scenes in "Rashomon," because the falling water (provided by fire trucks) did not show up on film against the sky, he dyed the water black with calligraphy ink in order to achieve the effect of heavy rain. In the final scene of "Throne of Blood", in which Mifune is shot by arrows, Kurosawa used real arrows shot by expert archers from a short range, landing within centimetres of Mifune's body. In "Ran" (1985), an entire castle set was constructed on the slopes of Mt. Fuji only to be burned to the ground in a climactic scene. Other stories include demanding a stream be made to run in the opposite direction in order to get a better visual effect, and having the roof of a house removed, later to be replaced, because he felt the roof's presence to be unattractive in a short sequence filmed from a train. His perfectionism also showed in his approach to costumes: he felt that giving an actor a brand new costume made the character look less than authentic. To resolve this, he often gave his cast their costumes weeks before shooting was to begin and required them to wear them on a daily basis and "bond with them." In some cases, such as with "Seven Samurai", where most of the cast portrayed poor farmers, the actors were told to make sure the costumes were worn down and tattered by the time shooting started. Kurosawa did not believe that "finished" music went well with film. When choosing a musical piece to accompany his scenes, he usually had it stripped down to one element (e.g., trumpets only). Only towards the end of his films are more finished pieces heard. Unusual among directors, Kurosawa edited his films himself during production. After each day's shooting he would go to the cutting room and cut the dailies.
A notable feature of Kurosawa's films is the breadth of his artistic influences. Some of his plots are based on William Shakespeare's works: "Ran" is loosely based on "King Lear", "Throne of Blood" is based on "Macbeth", while "The Bad Sleep Well" (1960) parallels "Hamlet", but is not affirmed to be based on it. Kurosawa also directed film adaptations of Russian literary works, including "The Idiot" (1951) by Dostoevsky (his favorite author) and "The Lower Depths" (1957), from the play by Maxim Gorky. "Ikiru" was inspired by Leo Tolstoy's "The Death of Ivan Ilyich". "Dersu Uzala" (1975) was based on the 1923 memoir of the same title by Russian explorer Vladimir Arsenyev. Story lines in "Red Beard" (1965) can be found in "The Insulted and Humiliated" by Dostoevsky. "High and Low" (1963) was based on "King's Ransom" by American crime writer Ed McBain. "Yojimbo" may have been based on Dashiell Hammett's "Red Harvest" and also borrows from American Westerns. Kurosawa was very fond of Georges Simenon and "Stray Dog" was a product of Kurosawa's desire to make a film in Simenon's manner. Cinematic influences include Frank Capra, William Wyler, Howard Hawks, his mentor Kajiro Yamamoto, and his favorite director John Ford, whose habit of wearing dark glasses Kurosawa emulated. When Kurosawa met Ford, the American simply said, "You really like rain." Kurosawa responded, "You've really been paying attention to my films." He would later instruct Yoshio Tsuchiya, one of the actors in "Seven Samurai", to retrieve the same hat Ford wore during that meeting. Despite criticism by some Japanese critics that Kurosawa was "too Western," he was deeply influenced by Japanese culture as well, such as the Noh theaters and the Jidaigeki (period drama) genre of Japanese cinema.
"Seven Samurai" was remade as "The Magnificent Seven" (1960). "Seven Samurai" is also considered the progenitor of the "men on a mission" film, popularized by films such as "The Dirty Dozen" (1967) and "The Guns of Navarone" (1961). The film is also recognized for popularizing the use of slow-motion in action films/sequences. "Rashomon" was remade by Martin Ritt in 1964's "The Outrage". Several films and television programs have also come to use what is known as the Rashomon effect, wherein various people give opposing or contrasting accounts of an event; these films include, but are not limited to "Vantage Point", "Courage Under Fire", "Hero", "Hoodwinked", and "The Usual Suspects". "Tajomaru", a film that centers on the eponymous character from "Rashomon", was released in 2009. "Yojimbo" was unofficially remade as the Sergio Leone western "A Fistful of Dollars" (1964) (resulting in a successful lawsuit by Kurosawa) and was remade as the prohibition-era film "Last Man Standing" (1996). "Sanjuro" was remade in 2007 as "Tsubaki Sanjuro", directed by Yoshimitsu Morita. "The Hidden Fortress" (1957) was remade as "The Last Princess" (2008) and is an acknowledged influence on George Lucas's "Star Wars" films, in particular Episodes IV and VI and most notably in the characters of R2-D2 and C-3PO. As well as using a modified version of Kurosawa's signature wipe transition, it has been observed that specific scenes from various Kurosawa films have been emulated throughout George Lucas's "Star Wars" saga. Remakes for "Ikiru" and "High and Low" are in progress. Second remakes for "Rashomon" and "Seven Samurai" are also on the way.
During his most productive period, from the late 40s to the mid-60s, Kurosawa often worked with the same group of collaborators. Fumio Hayasaka composed music for seven of his films — notably "Rashomon", "Ikiru" and "Seven Samurai". When Hayasaka died, he collaborated with composer Masaru Satō, who scored most of his later films. Kurosawa worked with the same five scriptwriters during his career: Eijiro Hisaita, Ryuzo Kikushima, Shinobu Hashimoto, Hideo Oguni, and Masato Ide. Yoshiro Muraki was Kurosawa's production designer or art director for most of his films after "Stray Dog" in 1949, and Asakazu Nakai was his cinematographer on 11 films including "Ikiru", "Seven Samurai" and "Ran". Kurosawa also liked working with the same group of actors, especially Takashi Shimura, Tatsuya Nakadai, and Toshirō Mifune. His collaboration with the latter, which began with 1948's "Drunken Angel" and ended with 1965's "Red Beard", is one of the most famous director-actor collaborations in cinema history.
The film "Red Beard" marked a turning point in Kurosawa's career in more ways than one. In addition to being his last film with Mifune, it was his last in black-and-white. It was also his last as a major director within the Japanese studio system making roughly a film a year. Kurosawa was signed to direct a Hollywood project, "Tora! Tora! Tora!" (1970) but 20th Century Fox replaced him with Toshio Masuda and Kinji Fukasaku before it was completed. His next few films were to be significantly more difficult to finance and were made at intervals of five years. The first, "Dodesukaden" (1970), about a group of poor people living around a rubbish dump, was not a commercial or financial success. After an attempted suicide, Kurosawa went on to make several more films, although he had great difficulty in obtaining domestic financing despite his international reputation. "Dersu Uzala", made in the Soviet Union and set in Siberia in the early 20th century, was the only Kurosawa film made outside of Japan and not in the Japanese language. It is about the friendship of a Russian explorer and a nomadic hunter, and won the Oscar for Best Foreign Language Film. "Kagemusha" (1980), financed with the help of the director's most famous admirers, George Lucas and Francis Ford Coppola, is the story of a man who is the body double of a medieval Japanese lord and takes over his identity after the lord's death. The film was awarded the Palme d'Or (Golden Palm) at the 1980 Cannes Film Festival (shared with Bob Fosse's "All That Jazz"). "Ran" was the director's version of Shakespeare's "King Lear", set in medieval Japan (and the only film of Kurosawa's career that he received a "Best Director" Academy Award nomination for). It was by far the largest project of Kurosawa's late career, and he spent a decade planning it and trying to obtain funding, which he was finally able to do with the help of the French producer Serge Silberman. The film was an international success and is generally considered Kurosawa's last masterpiece. In an interview, Kurosawa said that he considered it to be the best film he ever made. Kurosawa made three more films during the 1990s which were more personal than his earlier works. "Dreams" (1990) is a series of vignettes based on his own dreams. "Rhapsody in August" (1991) is about memories of the Nagasaki atomic bomb and his final film, "Madadayo" (1993), is about a retired teacher and his former students. Kurosawa died of a stroke in Setagaya, Tokyo, at age 88. "After the Rain" is a 1998 posthumous film directed by Kurosawa's closest collaborator, Takashi Koizumi, co-produced by Kurosawa Production (Hisao Kurosawa) and starring Tatsuya Nakadai and Shiro Mifune, son of Toshirō Mifune. The film's screenplay was written by Kurosawa. The story is based on a short novel by Shugoro Yamamoto, "Ame Agaru". To coincide with the 100th anniversary of Kurosawa's birth, his unfinished documentary "Gendai no Noh" will be completed and released in 2010. While filming his masterpiece "Ran" in 1983, Kurosawa experienced a number of problems during production, including financial troubles, and temporarily postponed filming to work on a non-fiction project. The documentary was to be about classic Japanese Noh theater, whose style had a substantial influence on "Ran", as well as "Throne of Blood" and "Kagemusha". Only about 50 minutes of footage exist, but to finish the film, an additional hour will be shot using Kurosawa's original screenplay.
The Akira Kurosawa Foundation was established in December 2003. In commemoration of the 100th anniversary of Kurosawa's birth, the AK100 Project was created. The AK100 Project aims to "expose young people who are the representatives of the next generation, and all people everywhere, to the light and spirit of Akira Kurosawa and the wonderful world he created." Anaheim University launched the Anaheim University Akira Kurosawa School of Film at the Beverly Hills Hotel on March 23, 2009, which would have been Kurosawa's 99th birthday. Kurosawa's son, Hisao Kurosawa, attended as Guest of Honor and a special memorial tribute video was played at the event featuring video presentations from Steven Spielberg, George Lucas, Martin Scorsese, Kurosawa's Assistant Director Teruyo Nogami and "Dreams" Producer/Nephew of Akira Kurosawa, Mike Inoue. Two awards have been named in Kurosawa's honor, the "Akira Kurosawa Award for Lifetime Achievement in Film Directing", awarded during the San Francisco International Film Festival, and the "Akira Kurosawa Award", awarded during the Tokyo International Film Festival.
Unlike his other noted contemporaries, such as Yasujiro Ozu or Kenji Mizoguchi, the majority of Kurosawa's work is commercially available throughout the U.S. and Europe. The Criterion Collection is the primary distributor of Kurosawa's films on DVD in the U.S., issuing 16 of his films as individual DVDs, with frequent contributions from Kurosawa historians Stephen Price and Donald Richie, and a further 5 issued in the "Post-War Kurosawa" Boxset as part of the label's Eclipse brand. Combined, these cover all the directors films from 1946's No Regrets for Our Youth to 1985's Ran except 1949's The Quiet Duel and 1975's Dersu Uzala. Kurosawa's first four films, Sanshiro Sugata, The Most Beautiful, Sanshiro Sugata Part II and The Men Who Tread on the Tiger's Tail, never previously available on DVD, are included with Criterion's other Kurosawa releases in the "AK 100: 25 Films by Akira Kurosawa Boxset" released to mark the 100th anniversary of the director's birth on December 8, 2009. The director's remaining work, The Quiet Duel, Dersu Uzala, Dreams, Rhapsody in August and Madadayo, have all been commercially available on DVD in the U.S., though some are currently out of print. The first Kurosawa film to be released in the U.S. on Blu-ray was Kagemusha in August 2009.
Ancient Egypt was an ancient civilization of eastern North Africa, concentrated along the lower reaches of the Nile River in what is now the modern country of Egypt. The civilization coalesced around 3150 BC with the political unification of Upper and Lower Egypt under the first pharaoh, and it developed over the next three millennia. Its history occurred in a series of stable "Kingdoms", separated by periods of relative instability known as Intermediate Periods. Ancient Egypt reached its pinnacle during the New Kingdom, after which it entered a period of slow decline. Egypt was conquered by a succession of foreign powers in this late period, and the rule of the pharaohs officially ended in 31 BC when the early Roman Empire conquered Egypt and made it a province. The success of ancient Egyptian civilization stemmed partly from its ability to adapt to the conditions of the Nile River Valley. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which fueled social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to defeat foreign enemies and assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs. The many achievements of the ancient Egyptians include the quarrying, surveying and construction techniques that facilitated the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems and agricultural production techniques, the first known ships, Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty. Egypt left a lasting legacy. Its art and architecture were widely copied, and its antiquities carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travellers and writers for centuries. A newfound respect for antiquities and excavations in the early modern period led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy, for Egypt and the world.
By the late Paleolithic period, the arid climate of Northern Africa became increasingly hot and dry, forcing the populations of the area to concentrate along the Nile valley, and since nomadic modern human hunter-gatherers began living in the region through the end of the Middle Pleistocene some 120 thousand years ago, the Nile has been the lifeline of Egypt. The fertile floodplain of the Nile gave humans the opportunity to develop a settled agricultural economy and a more sophisticated, centralized society that became a cornerstone in the history of human civilization.
In Predynastic and Early Dynastic times, the Egyptian climate was much less arid than it is today. Large regions of Egypt were covered in treed savanna and traversed by herds of grazing ungulates. Foliage and fauna were far more prolific in all environs and the Nile region supported large populations of waterfowl. Hunting would have been common for Egyptians and this is also the period during which many animals would have been first domesticated. By about 5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper Egypt, the Badari, was known for its high quality ceramics, stone tools, and its use of copper. In Northern Egypt, the Badari was followed by Amratian and Gerzian cultures which showed a number of technological improvements. In Gerzian times, early evidence exists of contact with Canaan and the Byblos coast. In southern Egypt, the Naqada culture, similar to the Badari, began to expand along the Nile by about 4000 BC. As early as the Naqada I Period, predynastic Egyptians imported obsidian from Ethiopia, used to shape blades and other objects from flakes. Over a period of about 1000 years, the Naqada culture developed from a few small farming communities into a powerful civilization whose leaders were in complete control of the people and resources of the Nile valley. Establishing a power center at Hierakonpolis, and later at Abydos, Naqada III leaders expanded their control of Egypt northwards along the Nile. They also traded with Nubia to the south, the oases of the western desert to the west, and the cultures of the eastern Mediterranean to the east. The Naqada culture manufactured a diverse array of material goods, reflective of the increasing power and wealth of the elite, which included painted pottery, high quality decorative stone vases, cosmetic palettes, and jewelry made of gold, lapis, and ivory. They also developed a ceramic glaze known as faience which was used well into the Roman Period to decorate cups, amulets, and figurines. During the last predynastic phase, the Naqada culture began using written symbols which would eventually evolve into a full system of hieroglyphs for writing the ancient Egyptian language.
The third century BC Egyptian priest Manetho grouped the long line of pharaohs from Menes to his own time into 30 dynasties, a system still in use today. He chose to begin his official history with the king named "Meni" (or Menes in Greek) who was then believed to have united the two kingdoms of Upper and Lower Egypt (around 3200BC). The transition to a unified state actually happened more gradually than the ancient Egyptian writers would have us believe, and there is no contemporary record of Menes. Some scholars now believe, however, that the mythical Menes may have actually been the pharaoh Narmer, who is depicted wearing royal regalia on the ceremonial Narmer Palette in a symbolic act of unification. In the Early Dynastic Period about 3150 BC, the first of the Dynastic pharaohs solidified their control over lower Egypt by establishing a capital at Memphis, from which they could control the labor force and agriculture of the fertile delta region as well as the lucrative and critical trade routes to the Levant. The increasing power and wealth of the pharaohs during the early dynastic period was reflected in their elaborate mastaba tombs and mortuary cult structures at Abydos, which were used to celebrate the deified pharaoh after his death. The strong institution of kingship developed by the pharaohs served to legitimize state control over the land, labor, and resources that were essential to the survival and growth of ancient Egyptian civilization.
Stunning advances in architecture, art, and technology were made during the Old Kingdom, fueled by the increased agricultural productivity made possible by a well developed central administration. Under the direction of the vizier, state officials collected taxes, coordinated irrigation projects to improve crop yield, drafted peasants to work on construction projects, and established a justice system to maintain peace and order. With the surplus resources made available by a productive and stable economy, the state was able to sponsor construction of colossal monuments and to commission exceptional works of art from the royal workshops. The pyramids built by Djoser, Khufu, and their descendants are the most memorable symbols of ancient Egyptian civilization, and the power of the pharaohs that controlled it. Along with the rising importance of a central administration arose a new class of educated scribes and officials who were granted estates by the pharaoh in payment for their services. Pharaohs also made land grants to their mortuary cults and local temples to ensure that these institutions would have the necessary resources to worship the pharaoh after his death. By the end of the Old Kingdom, five centuries of these feudal practices had slowly eroded the economic power of the pharaoh, who could no longer afford to support a large centralized administration. As the power of the pharaoh diminished, regional governors called nomarchs began to challenge the supremacy of the pharaoh. This, coupled with severe droughts between 2200 and 2150 BC, ultimately caused the country to enter a 140-year period of famine and strife known as the First Intermediate Period.
After Egypt's central government collapsed at the end of the Old Kingdom, the administration could no longer support or stabilize the country's economy. Regional governors could not rely on the king for help in times of crisis, and the ensuing food shortages and political disputes escalated into famines and small-scale civil wars. Yet despite difficult problems, local leaders, owing no tribute to the pharaoh, used their newfound independence to establish a thriving culture in the provinces. Once in control of their own resources, the provinces became economically richer — a fact demonstrated by larger and better burials among all social classes. In bursts of creativity, provincial artisans adopted and adapted cultural motifs formerly restricted to the royalty of the Old Kingdom, and scribes developed literary styles that expressed the optimism and originality of the period. Free from their loyalties to the pharaoh, local rulers began competing with each other for territorial control and political power. By 2160 BC, rulers in Herakleopolis controlled Lower Egypt, while a rival clan based in Thebes, the Intef family, took control of Upper Egypt. As the Intefs grew in power and expanded their control northward, a clash between the two rival dynasties became inevitable. Around 2055 BC the Theban forces under Nebhepetre Mentuhotep II finally defeated the Herakleopolitan rulers, reuniting the Two Lands and inaugurating a period of economic and cultural renaissance known as the Middle Kingdom.
The pharaohs of the Middle Kingdom restored the country's prosperity and stability, thereby stimulating a resurgence of art, literature, and monumental building projects. Mentuhotep II and his 11th Dynasty successors ruled from Thebes, but the vizier Amenemhat I, upon assuming kingship at the beginning of the 12th Dynasty around 1985 BC, shifted the nation's capital to the city of Itjtawy located in Faiyum. From Itjtawy, the pharaohs of the 12th Dynasty undertook a far-sighted land reclamation and irrigation scheme to increase agricultural output in the region. Moreover, the military reconquered territory in Nubia rich in quarries and gold mines, while laborers built a defensive structure in the Eastern Delta, called the "Walls-of-the-Ruler", to defend against foreign attack. Having secured military and political security and vast agricultural and mineral wealth, the nation's population, arts, and religion flourished. In contrast to elitist Old Kingdom attitudes towards the gods, the Middle Kingdom experienced an increase in expressions of personal piety and what could be called a democratization of the afterlife, in which all people possessed a soul and could be welcomed into the company of the gods after death. Middle Kingdom literature featured sophisticated themes and characters written in a confident, eloquent style, and the relief and portrait sculpture of the period captured subtle, individual details that reached new heights of technical perfection. The last great ruler of the Middle Kingdom, Amenemhat III, allowed Asiatic settlers into the delta region to provide a sufficient labor force for his especially active mining and building campaigns. These ambitious building and mining activities, however, combined with inadequate Nile floods later in his reign, strained the economy and precipitated the slow decline into the Second Intermediate Period during the later 13th and 14th dynasties. During this decline, the foreign Asiatic settlers began to seize control of the delta region, eventually coming to power in Egypt as the Hyksos. Second Intermediate Period and the Hyksos. Around 1650 BC, as the power of the Middle Kingdom pharaohs weakened, Asiatic immigrants living in the Eastern Delta town of Avaris seized control of the region and forced the central government to retreat to Thebes, where the pharaoh was treated as a vassal and expected to pay tribute. The Hyksos ("foreign rulers") imitated Egyptian models of government and portrayed themselves as pharaohs, thus integrating Egyptian elements into their Middle Bronze Age culture. After their retreat, the Theban kings found themselves trapped between the Hyksos to the north and the Hyksos' Nubian allies, the Kushites, to the south. Nearly 100 years of tenuous inaction followed, and it was not until 1555 BC that the Theban forces gathered enough strength to challenge the Hyksos in a conflict that would last more than 30 years. The pharaohs Seqenenre Tao II and Kamose were ultimately able to defeat the Nubians, but it was Kamose's successor, Ahmose I, who successfully waged a series of campaigns that permanently eradicated the Hyksos' presence in Egypt. In the New Kingdom that followed, the military became a central priority for the pharaohs seeking to expand Egypt’s borders and secure her complete dominance of the Near East.
The New Kingdom pharaohs established a period of unprecedented prosperity by securing their borders and strengthening diplomatic ties with their neighbors. Military campaigns waged under Tuthmosis I and his grandson Tuthmosis III extended the influence of the pharaohs into Syria and Nubia, cementing loyalties and opening access to critical imports such as bronze and wood. The New Kingdom pharaohs began a large-scale building campaign to promote the god Amun, whose growing cult was based in Karnak. They also constructed monuments to glorify their own achievements, both real and imagined. The female pharaoh Hatshepsut used such propaganda to legitimize her claim to the throne. Her successful reign was marked by trading expeditions to Punt, an elegant mortuary temple, a colossal pair of obelisks and a chapel at Karnak. Despite her achievements, Hatshepsut's nephew-stepson Tuthmosis III sought to erase her legacy near the end of his reign, possibly in retaliation for usurping his throne. Around 1350 BC, the stability of the New Kingdom was threatened when Amenhotep IV ascended the throne and instituted a series of radical and chaotic reforms. Changing his name to Akhenaten, he touted the previously obscure sun god Aten as the supreme deity, suppressed the worship of other deities, and attacked the power of the priestly establishment. Moving the capital to the new city of Akhetaten (modern-day Amarna), Akhenaten turned a deaf ear to foreign affairs and absorbed himself in his new religion and artistic style. After his death, the cult of the Aten was quickly abandoned, and the subsequent pharaohs Tutankhamun, Ay, and Horemheb erased all mention of Akhenaten's heresy, now known as the Amarna Period. Around 1279 BC, Ramesses II, also known as Ramesses the Great, ascended the throne, and went on to build more temples, erect more statues and obelisks, and sire more children than any other pharaoh in history. A bold military leader, Ramesses II led his army against the Hittites in the Battle of Kadesh and, after fighting to a stalemate, finally agreed to the first recorded peace treaty around 1258 BC. Egypt's wealth, however, made it a tempting target for invasion, particularly by the Libyans and the Sea Peoples. Initially, the military was able to repel these invasions, but Egypt eventually lost control of Syria and Palestine. The impact of external threats was exacerbated by internal problems such as corruption, tomb robbery and civil unrest. The high priests at the temple of Amun in Thebes accumulated vast tracts of land and wealth, and their growing power splintered the country during the Third Intermediate Period.
Following the death of Ramesses XI in 1078 BC, Smendes assumed authority over the northern part of Egypt, ruling from the city of Tanis. The south was effectively controlled by the High Priests of Amun at Thebes, who recognized Smendes in name only. During this time, Libyans had been settling in the western delta, and chieftains of these settlers began increasing their autonomy. Libyan princes took control of the delta under Shoshenq I in 945 BC, founding the so-called Libyan or Bubastite dynasty that would rule for some 200 years. Shoshenq also gained control of southern Egypt by placing his family members in important priestly positions. Libyan control began to erode as a rival dynasty in the delta arose in Leontopolis, and Kushites threatened from the south. Around 727 BC the Kushite king Piye invaded northward, seizing control of Thebes and eventually the Delta. Egypt's far-reaching prestige declined considerably toward the end of the Third Intermediate Period. Its foreign allies had fallen under the Assyrian sphere of influence, and by 700 BC war between the two states became inevitable. Between 671 and 667 BC the Assyrians began their attack on Egypt. The reigns of both Kushite kings Taharqa and his successor, Tanutamun, were filled with constant conflict with the Assyrians, against whom the Nubian rulers enjoyed several victories. Ultimately, the Assyrians pushed the Kushites back into Nubia, occupied Memphis, and sacked the temples of Thebes.
With no permanent plans for conquest, the Assyrians left control of Egypt to a series of vassals who became known as the Saite kings of the Twenty-Sixth Dynasty. By 653 BC, the Saite king Psamtik I was able to oust the Assyrians with the help of Greek mercenaries, who were recruited to form Egypt's first navy. Greek influence expanded greatly as the city of Naukratis became the home of Greeks in the delta. The Saite kings based in the new capital of Sais witnessed a brief but spirited resurgence in the economy and culture, but in 525 BC, the powerful Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from his home of Susa, leaving Egypt under the control of a satrapy. A few successful revolts against the Persians marked the 5th century BC, but Egypt was never able to permanently overthrow the Persians. Following its annexation by Persia, Egypt was joined with Cyprus and Phoenicia in the sixth satrapy of the Achaemenid Persian Empire. This first period of Persian rule over Egypt, also known as the Twenty-Seventh dynasty, ended in 402 BC, and from 380–343 BC the Thirtieth Dynasty ruled as the last native royal house of dynastic Egypt, which ended with the kingship of Nectanebo II. A brief restoration of Persian rule, sometimes known as the Thirty-First Dynasty, began in 343 BC, but shortly after, in 332 BC, the Persian ruler Mazaces handed Egypt over to Alexander the Great without a fight.
In 332 BC, Alexander the Great conquered Egypt with little resistance from the Persians and was welcomed by the Egyptians as a deliverer. The administration established by Alexander's successors, the Ptolemies, was based on an Egyptian model and based in the new capital city of Alexandria. The city was to showcase the power and prestige of Greek rule, and became a seat of learning and culture, centered at the famous Library of Alexandria. The Lighthouse of Alexandria lit the way for the many ships which kept trade flowing through the city, as the Ptolemies made commerce and revenue-generating enterprises, such as papyrus manufacturing, their top priority. Greek culture did not supplant native Egyptian culture, as the Ptolemies supported time-honored traditions in an effort to secure the loyalty of the populace. They built new temples in Egyptian style, supported traditional cults, and portrayed themselves as pharaohs. Some traditions merged, as Greek and Egyptian gods were syncretized into composite deities, such as Serapis, and classical Greek forms of sculpture influenced traditional Egyptian motifs. Despite their efforts to appease the Egyptians, the Ptolemies were challenged by native rebellion, bitter family rivalries, and the powerful mob of Alexandria which had formed following the death of Ptolemy IV. In addition, as Rome relied more heavily on imports of grain from Egypt, the Romans took great interest in the political situation in the country. Continued Egyptian revolts, ambitious politicians, and powerful Syrian opponents made this situation unstable, leading Rome to send forces to secure the country as a province of its empire.
Egypt became a province of the Roman Empire in 30 BC, following the defeat of Marc Antony and Ptolemaic Queen Cleopatra VII by Octavian (later Emperor Augustus) in the Battle of Actium. The Romans relied heavily on grain shipments from Egypt, and the Roman army, under the control of a prefect appointed by the Emperor, quelled rebellions, strictly enforced the collection of heavy taxes, and prevented attacks by bandits, which had become a notorious problem during the period. Alexandria became an increasingly important center on the trade route with the orient, as exotic luxuries were in high demand in Rome. Although the Romans had a more hostile attitude than the Greeks towards the Egyptians, some traditions such as mummification and worship of the traditional gods continued. The art of mummy portraiture flourished, and some of the Roman emperors had themselves depicted as pharaohs, though not to the extent that the Ptolemies had. The former lived outside Egypt and did not perform the ceremonial functions of Egyptian kingship. Local administration became Roman in style and closed to native Egyptians. From the mid-first century AD, Christianity took root in Alexandria as it was seen as another cult that could be accepted. However, it was an uncompromising religion that sought to win converts from paganism and threatened the popular religious traditions. This led to persecution of converts to Christianity, culminating in the great purges of Diocletian starting in 303 AD, but eventually Christianity won out. In 391 AD the Christian Emperor Theodosius introduced legislation that banned pagan rites and closed temples. Alexandria became the scene of great anti-pagan riots with public and private religious imagery destroyed. As a consequence, Egypt's pagan culture was continually in decline. While the native population continued to speak their language, the ability to read hieroglyphic writing slowly disappeared as the role of the Egyptian temple priests and priestesses diminished. The temples themselves were sometimes converted to churches or abandoned to the desert.
The pharaoh was the absolute monarch of the country and, at least in theory, wielded complete control of the land and its resources. The king was the supreme military commander and head of the government, who relied on a bureaucracy of officials to manage his affairs. In charge of the administration was his second in command, the vizier, who acted as the king's representative and coordinated land surveys, the treasury, building projects, the legal system, and the archives. At a regional level, the country was divided into as many as 42 administrative regions called nomes each governed by a nomarch, who was accountable to the vizier for his jurisdiction. The temples formed the backbone of the economy. Not only were they houses of worship, but were also responsible for collecting and storing the nation's wealth in a system of granaries and treasuries administered by overseers, who redistributed grain and goods. Much of the economy was centrally organized and strictly controlled. Although the ancient Egyptians did not use coinage until the Late period, they did use a type of money-barter system, with standard sacks of grain and the "deben", a weight of roughly of copper or silver, forming a common denominator. Workers were paid in grain; a simple laborer might earn 5½ sacks (200 kg or 400 lb) of grain per month, while a foreman might earn 7½ sacks (250 kg or 550 lb). Prices were fixed across the country and recorded in lists to facilitate trading; for example a shirt cost five copper deben, while a cow cost 140 deben. Grain could be traded for other goods, according to the fixed price list. During the 5th century BC coined money was introduced into Egypt from abroad. At first the coins were used as standardized pieces of precious metal rather than true money, but in the following centuries international traders came to rely on coinage.
Egyptian society was highly stratified, and social status was expressly displayed. Farmers made up the bulk of the population, but agricultural produce was owned directly by the state, temple, or noble family that owned the land. Farmers were also subject to a labor tax and were required to work on irrigation or construction projects in a corvée system. Artists and craftsmen were of higher status than farmers, but they were also under state control, working in the shops attached to the temples and paid directly from the state treasury. Scribes and officials formed the upper class in ancient Egypt, the so-called "white kilt class" in reference to the bleached linen garments that served as a mark of their rank. The upper class prominently displayed their social status in art and literature. Below the nobility were the priests, physicians, and engineers with specialized training in their field. Slavery was known in ancient Egypt, but the extent and prevalence of its practice are unclear. The ancient Egyptians viewed men and women, including people from all social classes except slaves, as essentially equal under the law, and even the lowliest peasant was entitled to petition the vizier and his court for redress. Both men and women had the right to own and sell property, make contracts, marry and divorce, receive inheritance, and pursue legal disputes in court. Married couples could own property jointly and protect themselves from divorce by agreeing to marriage contracts, which stipulated the financial obligations of the husband to his wife and children should the marriage end. Compared with their counterparts in ancient Greece, Rome, and even more modern places around the world, ancient Egyptian women had a greater range of personal choices and opportunities for achievement. Women such as Hatshepsut and Cleopatra even became pharaohs, while others wielded power as Divine Wives of Amun. Despite these freedoms, ancient Egyptian women did not take part in official roles in the administration, served only secondary roles in the temples, and were not as likely to be as educated as men.
The head of the legal system was officially the pharaoh, who was responsible for enacting laws, delivering justice, and maintaining law and order, a concept the ancient Egyptians referred to as Ma'at. Although no legal codes from ancient Egypt survive, court documents show that Egyptian law was based on a common-sense view of right and wrong that emphasized reaching agreements and resolving conflicts rather than strictly adhering to a complicated set of statutes. Local councils of elders, known as "Kenbet" in the New Kingdom, were responsible for ruling in court cases involving small claims and minor disputes. More serious cases involving murder, major land transactions, and tomb robbery were referred to the "Great Kenbet", over which the vizier or pharaoh presided. Plaintiffs and defendants were expected to represent themselves and were required to swear an oath that they had told the truth. In some cases, the state took on both the role of prosecutor and judge, and it could torture the accused with beatings to obtain a confession and the names of any co-conspirators. Whether the charges were trivial or serious, court scribes documented the complaint, testimony, and verdict of the case for future reference. Punishment for minor crimes involved either imposition of fines, beatings, facial mutilation, or exile, depending on the severity of the offense. Serious crimes such as murder and tomb robbery were punished by execution, carried out by decapitation, drowning, or impaling the criminal on a stake. Punishment could also be extended to the criminal's family. Beginning in the New Kingdom, oracles played a major role in the legal system, dispensing justice in both civil and criminal cases. The procedure was to ask the god a "yes" or "no" question concerning the right or wrong of an issue. The god, carried by a number of priests, rendered judgment by choosing one or the other, moving forward or backward, or pointing to one of the answers written on a piece of papyrus or an ostracon.
A combination of favorable geographical features contributed to the success of ancient Egyptian culture, the most important of which was the rich fertile soil resulting from annual inundations of the Nile River. The ancient Egyptians were thus able to produce an abundance of food, allowing the population to devote more time and resources to cultural, technological, and artistic pursuits. Land management was crucial in ancient Egypt because taxes were assessed based on the amount of land a person owned. Farming in Egypt was dependent on the cycle of the Nile River. The Egyptians recognized three seasons: "Akhet" (flooding), "Peret" (planting), and "Shemu" (harvesting). The flooding season lasted from June to September, depositing on the river's banks a layer of mineral-rich silt ideal for growing crops. After the floodwaters had receded, the growing season lasted from October to February. Farmers plowed and planted seeds in the fields, which were irrigated with ditches and canals. Egypt received little rainfall, so farmers relied on the Nile to water their crops. From March to May, farmers used sickles to harvest their crops, which were then threshed with a flail to separate the straw from the grain. Winnowing removed the chaff from the grain, and the grain was then ground into flour, brewed to make beer, or stored for later use. The ancient Egyptians cultivated emmer and barley, and several other cereal grains, all of which were used to make the two main food staples of bread and beer. Flax plants, uprooted before they started flowering, were grown for the fibers of their stems. These fibers were split along their length and spun into thread, which was used to weave sheets of linen and to make clothing. Papyrus growing on the banks of the Nile River was used to make paper. Vegetables and fruits were grown in garden plots, close to habitations and on higher ground, and had to be watered by hand. Vegetables included leeks, garlic, melons, squashes, pulses, lettuce, and other crops, in addition to grapes that were made into wine.
The Egyptians believed that a balanced relationship between people and animals was an essential element of the cosmic order; thus humans, animals and plants were believed to be members of a single whole. Animals, both domesticated and wild, were therefore a critical source of spirituality, companionship, and sustenance to the ancient Egyptians. Cattle were the most important livestock; the administration collected taxes on livestock in regular censuses, and the size of a herd reflected the prestige and importance of the estate or temple that owned them. In addition to cattle, the ancient Egyptians kept sheep, goats, and pigs. Poultry such as ducks, geese, and pigeons were captured in nets and bred on farms, where they were force-fed with dough to fatten them. The Nile provided a plentiful source of fish. Bees were also domesticated from at least the Old Kingdom, and they provided both honey and wax. The ancient Egyptians used donkeys and oxen as beasts of burden, and they were responsible for plowing the fields and trampling seed into the soil. The slaughter of a fattened ox was also a central part of an offering ritual. Horses were introduced by the Hyksos in the Second Intermediate Period, and the camel, although known from the New Kingdom, was not used as a beast of burden until the Late Period. There is also evidence to suggest that elephants were briefly utilized in the Late Period, but largely abandoned due to lack of grazing land. Dogs, cats and monkeys were common family pets, while more exotic pets imported from the heart of Africa, such as lions, were reserved for royalty. Herodotus observed that the Egyptians were the only people to keep their animals with them in their houses. During the Predynastic and Late periods, the worship of the gods in their animal form was extremely popular, such as the cat goddess Bastet and the ibis god Thoth, and these animals were bred in large numbers on farms for the purpose of ritual sacrifice.
Egypt is rich in building and decorative stone, copper and lead ores, gold, and semiprecious stones. These natural resources allowed the ancient Egyptians to build monuments, sculpt statues, make tools, and fashion jewelry. Embalmers used salts from the Wadi Natrun for mummification, which also provided the gypsum needed to make plaster. Ore-bearing rock formations were found in distant, inhospitable wadis in the eastern desert and the Sinai, requiring large, state-controlled expeditions to obtain natural resources found there. There were extensive gold mines in Nubia, and one of the first maps known is of a gold mine in this region. The Wadi Hammamat was a notable source of granite, greywacke, and gold. Flint was the first mineral collected and used to make tools, and flint handaxes are the earliest pieces of evidence of habitation in the Nile valley. Nodules of the mineral were carefully flaked to make blades and arrowheads of moderate hardness and durability even after copper was adopted for this purpose. The Egyptians worked deposits of the lead ore galena at Gebel Rosas to make net sinkers, plumb bobs, and small figurines. Copper was the most important metal for toolmaking in ancient Egypt and was smelted in furnaces from malachite ore mined in the Sinai. Workers collected gold by washing the nuggets out of sediment in alluvial deposits, or by the more labor-intensive process of grinding and washing gold-bearing quartzite. Iron deposits found in upper Egypt were utilized in the Late Period. High-quality building stones were abundant in Egypt; the ancient Egyptians quarried limestone all along the Nile valley, granite from Aswan, and basalt and sandstone from the wadis of the eastern desert. Deposits of decorative stones such as porphyry, greywacke, alabaster, and carnelian dotted the eastern desert and were collected even before the First Dynasty. In the Ptolemaic and Roman Periods, miners worked deposits of emeralds in Wadi Sikait and amethyst in Wadi el-Hudi.
The ancient Egyptians engaged in trade with their foreign neighbors to obtain rare, exotic goods not found in Egypt. In the Predynastic Period, they established trade with Nubia to obtain gold and incense. They also established trade with Palestine, as evidenced by Palestinian-style oil jugs found in the burials of the First Dynasty pharaohs. An Egyptian colony stationed in southern Canaan dates to slightly before the First Dynasty. Narmer had Egyptian pottery produced in Canaan and exported back to Egypt. By the Second Dynasty at latest, ancient Egyptian trade with Byblos yielded a critical source of quality timber not found in Egypt. By the Fifth Dynasty, trade with Punt provided gold, aromatic resins, ebony, ivory, and wild animals such as monkeys and baboons. Egypt relied on trade with Anatolia for essential quantities of tin as well as supplementary supplies of copper, both metals being necessary for the manufacture of bronze. The ancient Egyptians prized the blue stone lapis lazuli, which had to be imported from far-away Afghanistan. Egypt's Mediterranean trade partners also included Greece and Crete, which provided, among other goods, supplies of olive oil. In exchange for its luxury imports and raw materials, Egypt mainly exported grain, gold, linen, and papyrus, in addition to other finished goods including glass and stone objects.
The Egyptian language is a northern Afro-Asiatic language closely related to the Berber and Semitic languages. It has the longest history of any language, having been written from c. 3200 BC to the Middle Ages and remaining as a spoken language for longer. The phases of Ancient Egyptian are Old Egyptian, Middle Egyptian (Classical Egyptian), Late Egyptian, Demotic and Coptic. Egyptian writings do not show dialect differences before Coptic, but it was probably spoken in regional dialects around Memphis and later Thebes. Ancient Egyptian was a synthetic language, but it became more analytic later on. Late Egyptian develops prefixal definite and indefinite articles, which replace the older inflectional suffixes. There is a change from the older Verb Subject Object word order to Subject Verb Object. The Egyptian hieroglyphic, hieratic, and demotic scripts were eventually replaced by the more phonetic Coptic alphabet. Coptic is still used in the liturgy of the Egyptian Orthodox Church, and traces of it are found in modern Egyptian Arabic.
Ancient Egyptian has 25 consonants similar to those of other Afro-Asiatic languages. These include pharyngeal and emphatic consonants, voiced and voiceless stops, voiceless fricatives and voiced and voiceless affricates. It has three long and three short vowels, which expanded in Later Egyptian to about nine. The basic word in Egyptian, similar to Semitic and Berber, is a triliteral or biliteral root of consonants and semiconsonants. Suffixes are added to form words. The verb conjugation corresponds to the person. For example, the triconsonantal skeleton S--M is the semantic core of the word 'hear'; its basic conjugation is "sm=f" 'he hears'. If the subject is a noun, suffixes are not added to the verb: ' 'the woman hears'. Adjectives are derived from nouns through a process that Egyptologists call "nisbation" because of its similarity with Arabic. The word order is PREDICATE-SUBJECT in verbal and adjectival sentences, and SUBJECT-PREDICATE in nominal and adverbial sentences. The subject can be moved to the beginning of sentences if it is long and is followed by a resumptive pronoun. Verbs and nouns are negated by the particle "n", but "nn" is used for adverbial and adjectival sentences. Stress falls on the ultimate or penultimate syllable, which can be open (CV) or closed (CVC).
Hieroglyphic writing dates to c. 3200 BC, and is composed of some 500 symbols. A hieroglyph can represent a word, a sound, or a silent determinative; and the same symbol can serve different purposes in different contexts. Hieroglyphs were a formal script, used on stone monuments and in tombs, that could be as detailed as individual works of art. In day-to-day writing, scribes used a cursive form of writing, called hieratic, which was quicker and easier. While formal hieroglyphs may be read in rows or columns in either direction (though typically written from right to left), hieratic was always written from right to left, usually in horizontal rows. A new form of writing, Demotic, became the prevalent writing style, and it is this form of writing — along with formal hieroglyphs — that accompany the Greek text on the Rosetta Stone. Around the 1st century AD, the Coptic alphabet started to be used alongside the Demotic script. Coptic is a modified Greek alphabet with the addition of some Demotic signs. Although formal hieroglyphs were used in a ceremonial role until the 4th century AD, towards the end only a small handful of priests could still read them. As the traditional religious establishments were disbanded, knowledge of hieroglyphic writing was mostly lost. Attempts to decipher them date to the Byzantine and Islamic periods in Egypt, but only in 1822, after the discovery of the Rosetta stone and years of research by Thomas Young and Jean-François Champollion, were hieroglyphs almost fully deciphered.
Writing first appeared in association with kingship on labels and tags for items found in royal tombs. It was primarily an occupation of the scribes, who worked out of the "Per Ankh" institution or the House of Life. The latter comprised offices, libraries (called House of Books), laboratories and observatories. Some of the best-known pieces of ancient Egyptian literature, such as the Pyramid and Coffin Texts, were written in Classical Egyptian, which continued to be the language of writing until about 1300 BC. Later Egyptian was spoken from the New Kingdom onward and is represented in Ramesside administrative documents, love poetry and tales, as well as in Demotic and Coptic texts. During this period, the tradition of writing had evolved into the tomb autobiography, such as those of Harkhuf and Weni. The genre known as "Sebayt" ("Instructions") was developed to communicate teachings and guidance from famous nobles; the Ipuwer papyrus, a poem of lamentations describing natural disasters and social upheaval, is a famous example. The Story of Sinuhe, written in Middle Egyptian, might be the classic of Egyptian literature. Also written at this time was the Westcar Papyrus, a set of stories told to Khufu by his sons relating the marvels performed by priests. The Instruction of Amenemope is considered a masterpiece of near-eastern literature. Towards the end of the New Kingdom, the vernacular language was more often employed to write popular pieces like the Story of Wenamun and the Instruction of Any. The former tells the story of a noble who is robbed on his way to buy cedar from Lebanon and of his struggle to return to Egypt. From about 700 BC, narrative stories and instructions, such as the popular Instructions of Onchsheshonqy, as well as personal and business documents were written in the demotic script and phase of Egyptian. Many stories written in demotic during the Graeco-Roman period were set in previous historical eras, when Egypt was an independent nation ruled by great pharaohs such as Ramesses II.
Most ancient Egyptians were farmers tied to the land. Their dwellings were restricted to immediate family members, and were constructed of mud-brick designed to remain cool in the heat of the day. Each home had a kitchen with an open roof, which contained a grindstone for milling flour and a small oven for baking bread. Walls were painted white and could be covered with dyed linen wall hangings. Floors were covered with reed mats, while wooden stools, beds raised from the floor and individual tables comprised the furniture. The ancient Egyptians placed a great value on hygiene and appearance. Most bathed in the Nile and used a pasty soap made from animal fat and chalk. Men shaved their entire bodies for cleanliness, and aromatic perfumes and ointments covered bad odors and soothed skin. Clothing was made from simple linen sheets that were bleached white, and both men and women of the upper classes wore wigs, jewelry, and cosmetics. Children went without clothing until maturity, at about age 12, and at this age males were circumcised and had their heads shaved. Mothers were responsible for taking care of the children, while the father provided the family's income. The staple diet consisted of bread and beer, supplemented with vegetables such as onions and garlic, and fruit such as dates and figs. Wine and meat were enjoyed by all on feast days while the upper classes indulged on a more regular basis. Fish, meat, and fowl could be salted or dried, and could be cooked in stews or roasted on a grill. Music and dance were popular entertainments for those who could afford them. Early instruments included flutes and harps, while instruments similar to trumpets, oboes, and pipes developed later and became popular. In the New Kingdom, the Egyptians played on bells, cymbals, tambourines, and drums and imported lutes and lyres from Asia. The sistrum was a rattle-like musical instrument that was especially important in religious ceremonies. The ancient Egyptians enjoyed a variety of leisure activities, including games and music. Senet, a board game where pieces moved according to random chance, was particularly popular from the earliest times; another similar game was mehen, which had a circular gaming board. Juggling and ball games were popular with children, and wrestling is also documented in a tomb at Beni Hasan. The wealthy members of ancient Egyptian society enjoyed hunting and boating as well. The excavation of the workers village of Deir el-Madinah has resulted in one of the most thoroughly documented accounts of community life in the ancient world that spans almost four hundred years. There is no comparable site in which the organisation, social interactions, working and living conditions of a community can be studied in such detail.
The architecture of ancient Egypt includes some of the most famous structures in the world: the Great Pyramids of Giza and the temples at Thebes. Building projects were organized and funded by the state for religious and commemorative purposes, but also to reinforce the power of the pharaoh. The ancient Egyptians were skilled builders; using simple but effective tools and sighting instruments, architects could build large stone structures with accuracy and precision. The domestic dwellings of elite and ordinary Egyptians alike were constructed from perishable materials such as mud bricks and wood, and have not survived. Peasants lived in simple homes, while the palaces of the elite were more elaborate structures. A few surviving New Kingdom palaces, such as those in Malkata and Amarna, show richly decorated walls and floors with scenes of people, birds, water pools, deities and geometric designs. Important structures such as temples and tombs that were intended to last forever were constructed of stone instead of bricks. The architectural elements used in the world's first large-scale stone building, Djoser's mortuary complex, include post and lintel supports in the papyrus and lotus motif. The earliest preserved ancient Egyptian temples, such as those at Giza, consist of single, enclosed halls with roof slabs supported by columns. In the New Kingdom, architects added the pylon, the open courtyard, and the enclosed hypostyle hall to the front of the temple's sanctuary, a style that was standard until the Graeco-Roman period. The earliest and most popular tomb architecture in the Old Kingdom was the mastaba, a flat-roofed rectangular structure of mudbrick or stone built over an underground burial chamber. The step pyramid of Djoser is a series of stone mastabas stacked on top of each other. Pyramids were built during the Old and Middle Kingdoms, but later rulers abandoned them in favor of less conspicuous rock-cut tombs.
The ancient Egyptians produced art to serve functional purposes. For over 3500 years, artists adhered to artistic forms and iconography that were developed during the Old Kingdom, following a strict set of principles that resisted foreign influence and internal change. These artistic standards — simple lines, shapes, and flat areas of color combined with the characteristic flat projection of figures with no indication of spatial depth — created a sense of order and balance within a composition. Images and text were intimately interwoven on tomb and temple walls, coffins, stelae, and even statues. The Narmer Palette, for example, displays figures which may also be read as hieroglyphs. Because of the rigid rules that governed its highly stylized and symbolic appearance, ancient Egyptian art served its political and religious purposes with precision and clarity. Ancient Egyptian artisans used stone to carve statues and fine reliefs, but used wood as a cheap and easily carved substitute. Paints were obtained from minerals such as iron ores (red and yellow ochres), copper ores (blue and green), soot or charcoal (black), and limestone (white). Paints could be mixed with gum arabic as a binder and pressed into cakes, which could be moistened with water when needed. Pharaohs used reliefs to record victories in battle, royal decrees, and religious scenes. Common citizens had access to pieces of funerary art, such as shabti statues and books of the dead, which they believed would protect them in the afterlife. During the Middle Kingdom, wooden or clay models depicting scenes from everyday life became popular additions to the tomb. In an attempt to duplicate the activities of the living in the afterlife, these models show laborers, houses, boats, and even military formations that are scale representations of the ideal ancient Egyptian afterlife. Despite the homogeneity of ancient Egyptian art, the styles of particular times and places sometimes reflected changing cultural or political attitudes. After the invasion of the Hyksos in the Second Intermediate Period, Minoan-style frescoes were found in Avaris. The most striking example of a politically driven change in artistic forms comes from the Amarna period, where figures were radically altered to conform to Akhenaten's revolutionary religious ideas. This style, known as Amarna art, was quickly and thoroughly erased after Akhenaten's death and replaced by the traditional forms.
Beliefs in the divine and in the afterlife were ingrained in ancient Egyptian civilization from its inception; pharaonic rule was based on the divine right of kings. The Egyptian pantheon was populated by gods who had supernatural powers and were called on for help or protection. However, the gods were not always viewed as benevolent, and Egyptians believed they had to be appeased with offerings and prayers. The structure of this pantheon changed continually as new deities were promoted in the hierarchy, but priests made no effort to organize the diverse and sometimes conflicting creation myths and stories into a coherent system. These various conceptions of divinity were not considered contradictory but rather layers in the multiple facets of reality. Gods were worshiped in cult temples administered by priests acting on the king's behalf. At the center of the temple was the cult statue in a shrine. Temples were not places of public worship or congregation, and only on select feast days and celebrations was a shrine carrying the statue of the god brought out for public worship. Normally, the god's domain was sealed off from the outside world and was only accessible to temple officials. Common citizens could worship private statues in their homes, and amulets offered protection against the forces of chaos. After the New Kingdom, the pharaoh's role as a spiritual intermediary was de-emphasized as religious customs shifted to direct worship of the gods. As a result, priests developed a system of oracles to communicate the will of the gods directly to the people. The Egyptians believed that every human being was composed of physical and spiritual parts or "aspects". In addition to the body, each person had a "šwt" (shadow), a "ba" (personality or soul), a "ka" (life-force), and a "name". The heart, rather than the brain, was considered the seat of thoughts and emotions. After death, the spiritual aspects were released from the body and could move at will, but they required the physical remains (or a substitute, such as a statue) as a permanent home. The ultimate goal of the deceased was to rejoin his "ka" and "ba" and become one of the "blessed dead", living on as an "akh", or "effective one". In order for this to happen, the deceased had to be judged worthy in a trial, in which the heart was weighed against a "feather of truth". If deemed worthy, the deceased could continue their existence on earth in spiritual form.
The ancient Egyptians maintained an elaborate set of burial customs that they believed were necessary to ensure immortality after death. These customs involved preserving the body by mummification, performing burial ceremonies, and interring, along with the body, goods to be used by the deceased in the afterlife. Before the Old Kingdom, bodies buried in desert pits were naturally preserved by desiccation. The arid, desert conditions continued to be a boon throughout the history of ancient Egypt for the burials of the poor, who could not afford the elaborate burial preparations available to the elite. Wealthier Egyptians began to bury their dead in stone tombs and, as a result, they made use of artificial mummification, which involved removing the internal organs, wrapping the body in linen, and burying it in a rectangular stone sarcophagus or wooden coffin. Beginning in the Fourth Dynasty, some parts were preserved separately in canopic jars. By the New Kingdom, the ancient Egyptians had perfected the art of mummification; the best technique took 70 days and involved removing the internal organs, removing the brain through the nose, and desiccating the body in a mixture of salts called natron. The body was then wrapped in linen with protective amulets inserted between layers and placed in a decorated anthropoid coffin. Mummies of the Late Period were also placed in painted cartonnage mummy cases. Actual preservation practices declined during the Ptolemaic and Roman eras, while greater emphasis was placed on the outer appearance of the mummy, which was decorated. Wealthy Egyptians were buried with larger quantities of luxury items, but all burials, regardless of social status, included goods for the deceased. Beginning in the New Kingdom, books of the dead were included in the grave, along with shabti statues that were believed to perform manual labor for them in the afterlife. Rituals in which the deceased was magically re-animated accompanied burials. After burial, living relatives were expected to occasionally bring food to the tomb and recite prayers on behalf of the deceased.
The ancient Egyptian military was responsible for defending Egypt against foreign invasion, and for maintaining Egypt's domination in the ancient Near East. The military protected mining expeditions to the Sinai during the Old Kingdom and fought civil wars during the First and Second Intermediate Periods. The military was responsible for maintaining fortifications along important trade routes, such as those found at the city of Buhen on the way to Nubia. Forts also were constructed to serve as military bases, such as the fortress at Sile, which was a base of operations for expeditions to the Levant. In the New Kingdom, a series of pharaohs used the standing Egyptian army to attack and conquer Kush and parts of the Levant. Typical military equipment included bows and arrows, spears, and round-topped shields made by stretching animal skin over a wooden frame. In the New Kingdom, the military began using chariots that had earlier been introduced by the Hyksos invaders. Weapons and armor continued to improve after the adoption of bronze: shields were now made from solid wood with a bronze buckle, spears were tipped with a bronze point, and the Khopesh was adopted from Asiatic soldiers. The pharaoh was usually depicted in art and literature riding at the head of the army, and there is evidence that at least a few pharaohs, such as Seqenenre Tao II and his sons, did do so. Soldiers were recruited from the general population, but during, and especially after, the New Kingdom, mercenaries from Nubia, Kush, and Libya were hired to fight for Egypt.
Even before the Old Kingdom, the ancient Egyptians had developed a glassy material known as faience, which they treated as a type of artificial semi-precious stone. Faience is a non-clay ceramic made of silica, small amounts of lime and soda, and a colorant, typically copper. The material was used to make beads, tiles, figurines, and small wares. Several methods can be used to create faience, but typically production involved application of the powdered materials in the form of a paste over a clay core, which was then fired. By a related technique, the ancient Egyptians produced a pigment known as Egyptian Blue, also called blue frit, which is produced by fusing (or sintering) silica, copper, lime, and an alkali such as natron. The product can be ground up and used as a pigment. The ancient Egyptians could fabricate a wide variety of objects from glass with great skill, but it is not clear whether they developed the process independently. It is also unclear whether they made their own raw glass or merely imported pre-made ingots, which they melted and finished. However, they did have technical expertise in making objects, as well as adding trace elements to control the color of the finished glass. A range of colors could be produced, including yellow, red, green, blue, purple, and white, and the glass could be made either transparent or opaque.
The medical problems of the ancient Egyptians stemmed directly from their environment. Living and working close to the Nile brought hazards from malaria and debilitating schistosomiasis parasites, which caused liver and intestinal damage. Dangerous wildlife such as crocodiles and hippos were also a common threat. The life-long labors of farming and building put stress on the spine and joints, and traumatic injuries from construction and warfare all took a significant toll on the body. The grit and sand from stone-ground flour abraded teeth, leaving them susceptible to abscesses (though caries were rare). The diets of the wealthy were rich in sugars, which promoted periodontal disease. Despite the flattering physiques portrayed on tomb walls, the overweight mummies of many of the upper class show the effects of a life of overindulgence. Adult life expectancy was about 35 for men and 30 for women, but reaching adulthood was difficult as about one-third of the population died in infancy. Ancient Egyptian physicians were renowned in the ancient Near East for their healing skills, and some, like Imhotep, remained famous long after their deaths. Herodotus remarked that there was a high degree of specialization among Egyptian physicians, with some treating only the head or the stomach, while others were eye-doctors and dentists. Training of physicians took place at the "Per Ankh" or "House of Life" institution, most notably those headquartered in Per-Bastet during the New Kingdom and at Abydos and Saïs in the Late period. Medical papyri show empirical knowledge of anatomy, injuries, and practical treatments. Wounds were treated by bandaging with raw meat, white linen, sutures, nets, pads and swabs soaked with honey to prevent infection, while opium was used to relieve pain. Garlic and onions were used regularly to promote good health and were thought to relieve asthma symptoms. Ancient Egyptian surgeons stitched wounds, set broken bones, and amputated diseased limbs, but they recognized that some injuries were so serious that they could only make the patient comfortable until he died.
Early Egyptians knew how to assemble planks of wood into a ship hull as early as 3000 BC. The Archaeological Institute of America reports that the oldest ships yet unearthed, a group of 14 discovered in Abydos, were constructed of wooden planks which were "sewn" together. Discovered by Egyptologist David O'Connor of New York University, woven straps were found to have been used to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. Because the ships are all buried together and near a mortuary belonging to Pharaoh Khasekhemwy, originally they were all thought to have belonged to him, but one of the 14 ships dates to 3000 BC, and the associated pottery jars buried with the vessels also suggest earlier dating. The ship dating to 3000 BC was 75 feet long and is now thought to perhaps have belonged to an earlier pharaoh. According to professor O'Connor, the 5,000-year-old ship may have even belonged to Pharaoh Aha. Early Egyptians also knew how to assemble planks of wood with treenails to fasten them together, using pitch for caulking the seams. The "Khufu ship", a 43.6-meter vessel sealed into a pit in the Giza pyramid complex at the foot of the Great Pyramid of Giza in the Fourth Dynasty around 2500 BC, is a full-size surviving example which may have fulfilled the symbolic function of a solar barque. Early Egyptians also knew how to fasten the planks of this ship together with mortise and tenon joints. Despite the ancient Egyptian's ability to construct very large boats to sail along the easily navigable Nile, they were not known as good sailors and did not engage in widespread sailing or shipping in the Mediterranean or Red Seas.
The earliest attested examples of mathematical calculations date to the predynastic Naqada period, and show a fully developed numeral system. The importance of mathematics to an educated Egyptian is suggested by a New Kingdom fictional letter in which the writer proposes a scholarly competition between himself and another scribe regarding everyday calculation tasks such as accounting of land, labor and grain. Texts such as the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus show that the ancient Egyptians could perform the four basic mathematical operations — addition, subtraction, multiplication, and division — use fractions, compute the volumes of boxes and pyramids, and calculate the surface areas of rectangles, triangles, circles and even spheres. They understood basic concepts of algebra and geometry, and could solve simple sets of simultaneous equations. Mathematical notation was decimal, and based on hieroglyphic signs for each power of ten up to one million. Each of these could be written as many times as necessary to add up to the desired number; so to write the number eighty or eight hundred, the symbol for ten or one hundred was written eight times respectively. Because their methods of calculation could not handle most fractions with a numerator greater than one, ancient Egyptian fractions had to be written as the sum of several fractions. For example, the fraction two-fifths was resolved into the sum of one-third + one-fifteenth; this was facilitated by standard tables of values. Some common fractions, however, were written with a special glyph; the equivalent of the modern two-thirds is shown on the right. a reasonable approximation of the formula π"r" 2. The golden ratio seems to be reflected in many Egyptian constructions, including the pyramids, but its use may have been an unintended consequence of the ancient Egyptian practice of combining the use of knotted ropes with an intuitive sense of proportion and harmony.
The culture and monuments of ancient Egypt have left a lasting legacy on the world. The cult of the goddess Isis, for example, became popular in the Roman Empire, as obelisks and other relics were transported back to Rome. The Romans also imported building materials from Egypt to erect structures in Egyptian style. Early historians such as Herodotus, Strabo and Diodorus Siculus studied and wrote about the land which became viewed as a place of mystery. During the Middle Ages and the Renaissance, Egyptian pagan culture was in decline after the rise of Christianity and later Islam, but interest in Egyptian antiquity continued in the writings of medieval scholars such as Dhul-Nun al-Misri and al-Maqrizi. In the 17th and 18th centuries, European travelers and tourists brought back antiquities and wrote stories of their journeys, leading to a wave of Egyptomania across Europe. This renewed interest sent collectors to Egypt, who took, purchased, or were given many important antiquities. Although the European colonial occupation of Egypt destroyed a significant portion of the country's historical legacy, some foreigners had more positive results. Napoleon, for example, arranged the first studies in Egyptology when he brought some 150 scientists and artists to study and document Egypt's natural history, which was published in the "Description de l'Ėgypte". In the 19th century, the Egyptian Government and archaeologists alike recognized the importance of cultural respect and integrity in excavations. The Supreme Council of Antiquities now approves and oversees all excavations, which are aimed at finding information rather than treasure. The council also supervises museums and monument reconstruction programs designed to preserve the historical legacy of Egypt.
Analog Brothers is an experimental hip-hop crew featuring Ice Oscillator also known as Ice-T (keyboards, drums, vocals), Keith Korg also known as Kool Keith (bass, strings, vocals), Mark Moog also known as Marc Live (drums, "violyns" and vocals), Silver Synth also known as Black Silver (synthesizer, lazar bell and vocals), and Rex Roland also known as Pimp Rex (keyboards, vocals, production). Its album "Pimp to Eat" featured guest appearances by various members of Rhyme Syndicate, Odd Oberheim, Jacky jasper (who appears as Jacky Jasper on the song "We Sleep Days" and H-Bomb on "War"), D.J. Cisco from S.M., the Synth-a-Size Sisters and Teflon. While the group only recorded one album together as the Analog Brothers, a few bootlegs of its live concert performances, including freestyles with original lyrics, have occasionally surfaced online. After "Pimp to Eat", the Analog Brothers continued performing together in various line ups. Kool Keith and Marc Live joined with Jacky jasper to release two albums as KHM. Marc Live rapped with Ice T's group SMG. Marc also formed a group with Black Silver called Live Black, but while five of their tracks were released on a demo CD sold at concerts, Live Black's first album has yet to be released. Recently, Ice-T and Black Silver toured together as Black Ice, and released an album together called Urban Legends. In addition to all this, the Analog Brothers continue to make frequent appearances on each other's solo albums.
In this article, "MND" refers to a group of diseases that affect motor neurones. In the United States, MND is more commonly called "amyotrophic lateral sclerosis (ALS)", or "Lou Gehrig's disease", after the baseball player. In France the disease is sometimes known as "maladie de Charcot" (Charcot's disease), although it may also be referred to by the direct translation of ALS, "sclerose laterale amyotrophique (SLA)". To avoid confusion, the annual scientific research conference dedicated to the study of MND is called the International ALS/MND Symposium. It should be noted that ALS/MND refers to a specific subset of pathologically-identical diseases. There are numerous other afflictions of motor neurons that are pathologically distinct from ALS/MND and have a different clinical course. Examples of other diseases of the motor neuron that should not be confused with ALS/MND include spinobulbar muscular atrophy, spinal muscular atrophy, Charcot-Marie-Tooth disease, and many others.
About 90% of cases of MND are "sporadic", meaning that the patient has no family history of ALS and the case appears to have occurred with no known cause. Genetic factors are suspected to be important in determining an individual's susceptibility to disease, and there is some weak evidence to suggest that onset can be "triggered" by as yet unknown environmental factors (see 'Epidemiology' below). Approximately 10% of cases are "familial MND", defined either by a family history of MND or by testing positive for a known genetic mutation associated with the disease. The following genes are known to be linked to ALS: Cu/Zn superoxide dismutase, (a small number of cases), senataxin () and vesicle associated protein B (). Of these, SOD1 mutations account for some 20% of familial MND cases. The "SOD1" gene codes for the enzyme superoxide dismutase, a free radical scavenger that reduces the oxidative stress of cells throughout the body. So far over 100 different mutations in the SOD1 gene have been found, all of which cause some form of ALS(). In North America, the most commonly occurring mutation is known as A4V and occurs in up to 50% of SOD1 cases. In people of Scandinavian extraction there is a relatively benign mutation called D90A which is associated with a slow progression. In Japan, the H46R mutation is most common. G93A, the mutation used to generate the first animal model (and by far the most widely studied), is present only in a few families worldwide. Future research is concentrating on identifying new genetic mutations and the clinical syndrome associated with them. Familial MND may also confer a higher risk of developing cognitive changes such as frontotemporal dementia or executive dysfunction (see 'extra-motor change in MND' below). It is thought that "SOD1" mutations confer a toxic gain, rather than a loss, of function to the enzyme. SOD1 mutations may increase the propensity for the enzyme to form protein aggregates which are toxic to nerve cells.
Skeletal muscles are innervated by a group of neurones ("lower motor neurones") located in the ventral horns of the spinal cord which project out the ventral roots to the muscle cells. These nerve cells are themselves innervated by the corticospinal tract or "upper motor neurones" that project from the motor cortex of the brain. On macroscopic pathology, there is a degeneration of the ventral horns of the spinal cord, as well as atrophy of the ventral roots. In the brain, atrophy may be present in the frontal and temporal lobes. On microscopic examination, neurones may show spongiosis, the presence of astrocytes, and a number of inclusions including characteristic "skein-like" inclusions, bunina bodies, and vacuolisation. The availability of mouse models has led to extensive research into the causes of SOD1-mutant linked familial ALS. The most commonly used mouse model is G93A, although many others have since been generated. At the gross physiological level, the mouse models faithfully recapitulate the features of human ALS (motorneuron death, muscle atrophy, respiratory failure). Although there is no consensus as to the exact mechanism by which mutated SOD1 causes the disease (in either mice or patients), studies based largely on mouse models suggest a role for excitotoxicity and more controversially, oxidative stress, presumably secondary to mitochondrial dysfunction. Death by apoptosis has also been suggested.
Symptoms usually present themselves between the ages of 50-70, and include progressive weakness, muscle wasting, and muscle fasciculations, spasticity or stiffness in the arms and legs, and overactive tendon reflexes. Patients may present with symptoms as diverse as a dragging foot, unilateral muscle wasting in the hands, or slurred speech. Neurological examination presents specific signs associated with upper and lower motor neurone degeneration. Signs of upper motor neurone damage include spasticity, brisk reflexes and the Babinski sign. Signs of lower motor neurone damage include weakness and muscle atrophy. Note that every muscle group in the body requires both upper and lower motor neurones to function. The signs described above can occur in any muscle group, including the arms, legs, torso, and bulbar region. The symptoms described above may resemble a number of other rare diseases, known as "MND Mimic Disorders". These include, but are not limited to, multifocal motor neuropathy, Kennedy's disease, hereditary spastic paraplegia, spinal muscular atrophy and monomelic amyotrophy. A small subset of familial MND cases occur in children, such as "juvenile ALS", Madras syndrome, and individuals who have inherited the ALS2 gene. However, these are not typically referred to as MND, but by their specific names.
The diagnosis of MND is a clinical one, established by a neurologist on the basis of history and neurological examination. There is no diagnostic test for MND. Investigations such as blood tests, electromyography (EMG), magnetic resonance imaging (MRI), and sometimes genetic testing are useful to rule out other disorders that may mimic MND. However, the diagnosis of MND remains a clinical one. Having excluded other diseases, a relatively rapid progression of symptoms is a strong diagnostic factor. Although an individual's progression may sometimes "plateau", it will not improve. A set of diagnostic criteria called the El Escorial criteria have been defined by the World Federation of Neurologists for use in research, particularly as inclusion/exclusion criteria for clinical trials. Owing to a lack of clinical diagnostic criteria, some neurologists use the El Escorial criteria during the diagnostic process, although strictly speaking this is functionality creep, and some have questioned the appropriateness of the criteria in a clinical setting. The "bulbar region" in the table above refers to the mouth, face, and throat. It is possible that Transcranial magnetic stimulation can be used to diagnose MND.
Currently there is no cure for ALS. The only drug that affects the course of the disease is riluzole. The drug functions by blocking the effects of the neurotransmitter glutamate, and is thought to extend the lifespan of an ALS patient by only a few months. The lack of effective medications to slow the progression of ALS does not mean that patients with ALS cannot be medically cared for. Instead, treatment of patients with ALS focuses on the relief of symptoms associated with the disease. This involves a variety of health professionals including neurologists, speech-language pathologists, physical therapists, occupational therapists, dieticians, respiratory therapists, social workers, palliative care specialists, specialist nurses and psychologists.
Most cases of MND progress quite quickly, with noticeable decline occurring over the course of months. Although symptoms may present in one region, they will typically spread. If restricted to one side of the body they are more likely to progress to the same region on the other side of the body before progressing to a new region. After several years, most patients require help to carry out activities of daily living such as self care, feeding, and transportation. MND is typically fatal within 2–5 years. Around 50% die within 14 months of diagnosis. The remaining 50% will not necessarily die within the next 14 months as the distribution is significantly skewed. As a rough estimate, 1 in 5 patients survive for 5 years, and 1 in 10 patients survive 10 years. Professor Stephen Hawking is a well-known example of a person with MND, and has lived for more than 40 years with the disease. Mortality normally results when control of the diaphragm is impaired and the ability to breathe is lost. One exception is PLS, which may last for upwards of 25 years. Given the typical age of onset, this effectively leaves most PLS patients with a normal life span. PLS can progress to ALS, decades later.
Cognitive change occurs in between 33–50% of patients. A small proportion exhibit a form of frontotemporal dementia characterised by behavioural abnormalities such as disinhibition, apathy, and personality changes. A small proportion of patients may also suffer from an aphasia, which causes difficulty in naming specific objects. A larger proportion (up to 50%) suffer from a milder version of cognitive change which primarily affects what is known as executive function. Briefly, this is the ability of an individual to initiate, inhibit, sustain, and switch attention and is involved in the organisation of complex tasks down to smaller components. Often patients with such changes find themselves unable to do the family finances or drive a car. Depression is surprisingly rare in MND (around 5–20%) relative to the frequency with which it is found in other, less severe, neurological disorders e.g. ~50% in multiple sclerosis and Parkinson's disease, ~20% in Epilepsy. Depression does not necessarily increase as the symptoms progress, and in fact many patients report being happy with their quality of life despite profound disability. This may reflect the use of coping strategies such as reevaluating what is important in life. Although traditionally thought only to affect the motor system, sensory abnormalities are not necessarily absent, with some patients finding altered sensation to touch and heat, found in around 10% of patients. Patients with a predominantly upper motor neurone syndrome, and particularly PLS, often report an enhanced startle reflex to loud noises. Neuroimaging and neuropathology has demonstrated extra-motor changes in the frontal lobes including the inferior frontal gyrus, superior frontal gyrus, anterior cingulate cortex, and superior temporal gyrus. The degree of pathology in these areas has been directly related to the degree of cognitive change experienced by the patient, if any. Patients with MND and dementia have been shown to exhibit marked frontotemporal lobe atrophy as revealed by MRI or SPECT neuroimaging.
The incidence of MND is approximately 1–5 out of 100,000 people. Men have a slightly higher incidence rate than women. Approximately 5,600 cases are diagnosed in the U.S. every year. By far the greatest risk factor is age, with symptoms typically presenting between the ages of 50-70. Cases under the age of 50 years are called "young onset MND", whilst incidence rates appear to tail off after the age of 85. Tentative environmental risk factors identified so far include: exposure to severe electrical shock leading to coma, having served in the first Gulf War, and playing Association football (soccer). However, these findings have not been firmly identified and more research is needed. There are three "hot spots" of MND in the world. One is in the Kii peninsula of Japan, one amongst a tribal population in Papua New Guinea. Chamorro inhabitants from the island of Guam in the Pacific Ocean have an increased risk of developing a form of MND known as Guamanian ALS-PD-dementia complex or "lytico bodig", although the incidence rate has declined over the last 50 years and the average age of onset has increased. Putative theories involve neurotoxins in the traditional diet including cycad nut flour and bats that have eaten cycad nuts.
Terminology around the motor neurone diseases can be confusing; in the UK "motor neurone disease" refers to both ALS specifically (the most common form of disease) and to the broader spectrum of motor neurone diseases including progressive muscular atrophy, primary lateral sclerosis, and progressive bulbar palsy. In the United States the most common terms used are ALS (both specifically for ALS and as a blanket term) or "Lou Gehrig's disease". "Amyotrophic" comes from the Greek language: "A-" means "no", "myo" refers to "muscle", and "trophic" means "nourishment"; "amyotrophic" therefore means "no muscle nourishment," which describes the characteristic atrophication of the sufferer's disused muscle tissue. "Lateral" identifies the areas in a person's spinal cord where portions of the nerve cells that are affected are located. As this area degenerates it leads to scarring or hardening ("sclerosis") in the region.
An abjad is a type of writing system in which each symbol always or usually stands for a consonant; the reader must supply the appropriate vowel. It is a term suggested by Peter T. Daniels to replace the common terms consonantary or consonantal alphabet or syllabary to refer to the family of scripts called West Semitic. In popular usage, abjads often contain the word "alphabet" in their names, such as "Phoenician alphabet" and "Arabic alphabet".
According to the formulations of Daniels, abjads differ from alphabets in that only consonants, not vowels, are represented among the basic graphemes. Abjads differ from abugidas, another category invented by Daniels, in that in abjads the vowel sound is "implied" by phonology, and where vowel marks exist for the system, such as nikkud for Hebrew and harakāt for Arabic, their use is optional and not the dominant (or literate) form. In an abugida, the vowels (other than the "inherent" vowel) are always marked, either with a diacritic, a minor attachment to the letter or a standalone glyph. Some abugidas use a special symbol to "suppress" the inherent vowel so that the consonant alone can be properly represented. In a syllabary, a grapheme denotes a complete syllable, that is, either a lone vowel sound or a combination of a vowel sound with one or more consonant sounds. Florian Coulmas, a critic of Daniels and the abjad terminology, argues that this terminology can confuse alphabets with "transcription systems", and that there is no reason to relegate the Hebrew, Aramaic or Phoenician alphabets to second class status as an "incomplete alphabet". However, Daniels's terminology has found acceptance in the linguistic community. The system takes its name from the Arabic word for alphabet, which is derived from the first four letters of the Arabic alphabet in the older abjadi order, just as the English word "alphabet" is made up of the names of the first two letters of the Greek alphabet (alpha and beta).
All known abjads belong to the Semitic family of scripts. These scripts are thought to derive from the Proto-Sinaitic alphabet (dated to about 1500 BC) which is thought to derive from Egyptian hieroglyphs. The abjad was significantly simpler than the earlier hieroglyphs. The number of distinct glyphs was reduced tremendously, at the cost of increased ambiguity. The first abjad to gain widespread usage was the Phoenician abjad. Unlike other contemporary scripts, such as Cuneiform and Egyptian hieroglyphs, the Phoenician script consisted of only about two dozen symbols. This made the script easy to learn, and Phoenician seafaring merchants took the script wherever they went. Phoenician gave way to a number of new writing systems, including the Greek alphabet, and Aramaic, a widely used abjad. Greek evolved into the modern western alphabets, such as Latin and Cyrillic, while Aramaic became the ancestor of many modern abjads and abugidas of Asia. Aramaic spread across Asia, reaching as far as India and becoming Brahmi, the ancestral abugida to most modern Indian and Southeast Asian scripts. In the Middle East, Aramaic gave rise to the Hebrew and Nabataean abjads, which retained many of the Aramaic letter forms. The Syriac script was a cursive variation of Aramaic. It is unclear whether the Arabic abjad was derived from Nabatean or Syriac.
Modern abjads have also been used for isopsephy, a system of assigning numeric values to individual letters. Before the development of the positional number system, this was one of the regular systems for writing numbers. In some languages, the relationship between words and numbers created by this system has led to poetic and mystical usages, such as the Kaballah being a poetical and numerical deviation from the Hebrew Bible.
"Impure" abjads have characters for some vowels, optional vowel diacritics, or both. The term "pure" abjad refers to scripts entirely lacking in vowel indicators. However, most modern abjads, such as Arabic, Modern Hebrew, Aramaic and Avestan, are "impure" abjads, that is, they also contain symbols for some of the vowel phonemes. An example of a pure abjad is ancient Phoenician. Urdu and Persian are written in a form of the same script as Arabic, but almost always without the optional vowel diacritics.
In the 9th century BC, the Greeks adapted the Phoenician script for use in their own language. The phonetic structure of the Greek language created too many ambiguities when the vowels went unrepresented, so the script was modified. They did not need letters for the guttural sounds represented by aleph, he, heth or ayin, so these symbols were assigned vocalic values. The letters waw and yod were also used. The Greek alphabet thus became the world's first "true" alphabet. Abugidas developed along a slightly different route. The basic consonantal symbol was considered to have an inherent "a" vowel sound. Hooks or short lines attached to various parts of the basic letter modify the vowel. In this way, the South Arabian alphabet evolved into the Ge'ez alphabet between the 5th century BC and the 5th century AD. Similarly, around the 3rd century BC, the Brāhmī script developed (from the Aramaic abjad, it has been hypothesised). Abjads and the structure of Semitic languages. The abjad form of writing is well-adapted to the morphological structure of the Semitic languages it was developed to write. This is because words in Semitic languages are formed from a root consisting of (usually) three consonants, the vowels being used to indicate inflectional or derived forms. For instance, from the Arabic root ذبح "Ḏ-B-Ḥ" (to sacrifice) can be derived the forms ذَبَح "ḏabaḥa" (he sacrificed), ذَبَحْتَ "ḏabaḥta" (you (masculine singular) sacrificed), ذَبَّحَ"ḏabbaḥa" (he slaughtered), يُذَبَّح "yuḏabbiḥ" (he slaughters), and مَذْبَح "maḏbaḥ" (slaughterhouse). In each case, the absence of full glyphs for vowels makes the common root clearer, meaning that readers with knowledge of one word of the same root would have an easier time picking up on the similarity of meaning.
Many non-Semitic languages such as English could, theoretically, be written without vowels. It would be more difficult to read, but many words could be interpreted in context. This fact can be used to semi-bowdlerise offensive language, a practice known as disemvoweling. Many European languages, however, would lose grammatical information such as gender, case, and/or number. However, in English text the few inflections contain distinct consonants. It can be written without vowels and might still be clear, as in the first sentence of this paragraph: "Mny nn-Smtc lnggs sch s Nglsh, cld, thrtclly, b wrttn wtht vwls." This is a poor example, but it can be understood with effort. In general, non-Semitic texts are often puzzling and ambiguous without their vowels, more so if the context and subject are unknown, because the consonant sequences for many word roots are not unique. For example, in English, "abate", "abet", "about", "abut", "bat", "bait", "bet", "beat", "beet", "bit", "bite", "bot", "boat", "boot", "bout", "but", and "byte" all reduce to "bt". Nevertheless, Indo-European languages can be written adequately using the "impure" abjads of Arabic and Hebrew, as with Persian and Yiddish orthography.
An abugida (, from Ge‘ez አቡጊዳ "’äbugida"), also called an alphasyllabary, is a segmental writing system which is based on consonants, and in which vowel notation is obligatory but secondary. This contrasts with an alphabet proper, in which vowels have status equal to consonants, and with an abjad, in which vowel marking is absent or optional. (In less formal treatments, all three are commonly called alphabets.) About half the writing systems in the world are abugidas, including the extensive Brahmic family of scripts used in South and Southeast Asia. The term "abugida" was suggested by Peter T. Daniels in his 1990 typology of writing systems. It is an Ethiopian name of the Ge‘ez script, "’ä bu gi da," taken from four letters of that script the way "abecedary" derives from Latin "a be ce de." As Daniels used the word, an abugida contrasts with a syllabary, where letters with shared consonants or vowels show no particular resemblance to each another, and with an alphabet proper, where independent letters are used to denote both consonants and vowels. The term "alphasyllabary" was suggested for the Indic scripts in 1997 by William Bright, following South Asian linguistic usage, to convey the idea that "they share features of both alphabet and syllabary". Abugidas were long considered to be syllabaries or intermediate between syllabaries and alphabets, and the term "syllabics" is retained in the name of Canadian Aboriginal Syllabics. Other terms that have been used include "neosyllabary" (Février 1959), "pseudo-alphabet" (Householder 1959), "semisyllabary" (Diringer 1968; a word which has other uses) and "syllabic alphabet" (Coulmas 1996; this term is also a synonym for syllabary).
In general, a letter of an abugida transcribes a consonant. Letters are written as a linear sequence, in most cases left to right. Vowels are written through modification of these consonant letters, either by means of diacritics (which may not follow the direction of writing the letters) or by changes in the form of the letter itself. There are three principal families of abugidas, depending on whether vowels are indicated by modifying consonants by "diacritics, distortion," or "orientation." Tāna of the Maldives has dependent vowels and a zero vowel sign, but no inherent vowel.
Indic scripts originated in South Asia and spread to Southeast Asia. All surviving Indic scripts are descendants of the Brahmi alphabet. Today they are used in most languages of South Asia (although replaced by Perso-Arabic in Urdu, Kashmiri and other languages of Pakistan and India) and mainland Southeast Asia (Burma, Thailand, Laos, Cambodia; but not Malaysia or Vietnam). The primary division is into North Indic scripts used in North India, Nepal, Tibet, Bhutan and South Indic scripts used in South India, Sri Lanka, and Southeast Asia. South Indic letter forms are very rounded; North Indic less so, though Oriya, Golmol and Litumol of Nepal script are rounded. Most North Indic scripts' full letters incorporate a horizontal line at the top, with Gujarati script an exception; South Indic scripts do not. Indic scripts indicate vowels through dependent vowel signs (diacritics) around the consonants, often including a sign that explicitly indicates the lack of a vowel. If a consonant has no vowel sign, this indicates a default vowel. Vowel diacritics may appear above, below, to the left, to the right, or around the consonant. The most populous Indic script is Devanagari, used for Hindi, Bhojpuri, Marathi, Nepali, and often Sanskrit. A basic letter such as क represents a syllable with the default vowel, in this case "ka" (), or, in final position, a final consonant, in this case "k." This inherent vowel may be changed by adding vowel marks (diacritics), producing syllables such as कि "ki," कु "ku," के "ke," को "ko." The mora a consonant letter represents, either with or without a marked vowel, is called an akshara. In many of the Brahmic scripts, a syllable beginning with a cluster is treated as a single character for purposes of vowel marking, so a vowel marker like "-i," falling before the character it modifies, may appear several positions before the place where it is pronounced. For example, the game cricket in Hindi is क्रिकेट "krikeţ;" the diacritic for /i/ appears before the consonant cluster /kr/, not before the /r/. A more unusual example is seen in the Batak alphabet: Here the syllable "bim" is written "ba-ma-i-(virama)". That is, the vowel diacritic and virama are both written after the consonants for the whole syllable. In many abugidas, there is also a diacritic to suppress the inherent vowel, yielding the bare consonant. In Devanagari, क् is "k," and ल् is "l". This is called the "virama" in Sanskrit, or "halant" in Hindi. It may be used to form consonant clusters, or to indicate that a consonant occurs at the end of a word. For text information processing on computer, other means of expressing these functions include special conjunct forms in which two or more consonant characters are merged to express a cluster, such as Devanagari: क्ल "kla." (Note that on some fonts display this as क् followed by ल, rather than forming a conjunct. This expedient is used by ISCII and South Asian scripts of Unicode.) Thus a closed syllable such as "kal" requires two akshara to write. The Róng script used for the Lepcha language goes further than other Indic abugidas, in that a single akshara can represent a closed syllable: Not only the vowel, but any final consonant is indicated by a diacritic. For example, the syllable [sok] would be written as something like, here with an underring representing /o/ and an overcross representing the diacritic for final /k/. Most other Indic abugidas can only indicate a very limited set of final consonants with diacritics, such as /ŋ/ or /r/, if they can indicate any at all.
In Ethiopic, from which the term "abugida" originates, the diacritics have fused to the consonants to the point that they must be considered modifications of the form of the letters. Children learn each modification separately, as in a syllabary; nonetheless, the graphic similarities between syllables with the same consonant is readily apparent, unlike the case in a true syllabary. Though now an abugida, the Ge'ez alphabet was actually an abjad until the advent of Christianity "ca" 350 CE. In the Ge'ez abugida, the form of the letter itself may be altered. For example, ሀ "hä" (base form), ሁ "hu" (with a right-side diacritic that does not alter the letter), ሂ "hi" (with a subdiacritic that compresses the letter, so that the whole fidel occupies the same amount of space), ህ "hə" or (where the letter is modified with a kink in the left arm).
In the family known as Canadian Aboriginal syllabics, vowels are indicated by changing the orientation of the akshara. Each vowel has a consistent orientation; for example, Inuktitut ᐱ "pi," ᐳ "pu," ᐸ "pa;" ᑎ "ti," ᑐ "tu," ᑕ "ta". Although there is a vowel inherent in each, all rotations have equal status and none can be identified as basic. Bare consonants are indicated either by separate diacritics, or by superscript versions of the aksharas; there is no vowel-killer mark.
Consonantal scripts ("abjads") are normally written without indication of many vowels. However in some contexts like teaching materials or scriptures, Arabic and Hebrew are written with full indication of vowels via diacritic marks (harakat, niqqud) making them effectively abugidas. The Brahmic and Ethiopic families are thought to have originated from the Semitic abjads by the addition of vowel marks. The Arabic-alphabet scripts used for Kurdish in Iraq and for Uighur in Xinjiang, China are fully voweled, but since the vowels are full letters rather than diacritics, and there are no inherent vowels, these are considered alphabets rather than abugidas.
Pahawh Hmong is a non-segmental script that indicates syllable onsets and rimes, such as consonant clusters and vowels with final consonants. Thus it is not segmental and cannot be considered an abugida. However, it superficially resembles an abugida with the roles of consonant and vowel reversed. Most syllables are written with two letters in the order rime–onset (typically vowel-consonant), even though they are pronounced as onset-rime (consonant-vowel), rather like the position of the /i/ vowel in Devanagari, which is written before the consonant. Pahawh is also unusual in that, while an inherent rime /āu/ (with mid tone) is unwritten, it also has an inherent onset /k/. For the syllable /kau/, which requires one or the other of the inherent sounds to be overt, it is /au/ that is written. Thus it is the rime (vowel) which is basic to the system.
It is difficult to draw a dividing line between abugidas and other segmental scripts. For example, the Meroitic script of ancient Sudan did not indicate an inherent "a" (one symbol stood for both "m" and "ma," for example), and is thus similar to Brahmic family abugidas. However, the other vowels were indicated with full letters, not diacritics or modification, so the system was essentially an alphabet that did not bother to write the most common vowel.
Several systems of shorthand use diacritics for vowels, but they do not have an inherent vowel, and are thus more similar to Thaana and Kurdish script than to the Brahmic scripts. The Gabelsberger shorthand system and its derivatives modify the "following" consonant to represent vowels. The Pollard script, which was based on shorthand, also uses diacritics for vowels; the placements of the vowel relative to the consonant indicates tone.
As the term "alphasyllabary" suggests, abugidas have been considered an intermediate step between alphabets and syllabaries. Historically, abugidas appear to have evolved from abjads (vowelless alphabets). They contrast with syllabaries, where there is a distinct symbol for each syllable or consonant-vowel combination, and where these have no systematic similarity to each other, and typically develop directly from logographic scripts. Compare the Devanagari examples above to sets of syllables in the Japanese hiragana syllabary: か "ka", き "ki", く "ku", け "ke", こ "ko" have nothing in common to indicate "k;" while ら "ra", り "ri", る "ru", れ "re", ろ "ro" have neither anything in common for "r", nor anything to indicate that they have the same vowels as the "k" set. Most Indian and Indochinese abugidas appear to have first been developed from abjads with the and Brāhmī scripts; the abjad in question is usually considered to be the Aramaic one, but while the link between Aramaic and Kharosthi is more or less undisputed, this is not the case with Brahmi. The Kharosthi family does not survive today, but Brahmi's descendants include most of the modern scripts of South and Southeast Asia. Ge'ez derived from a different abjad, the Sabean script of Yemen; the advent of vowels coincided with the introduction of Christianity about 350 CE.
ABBA was a Swedish pop music group formed in Stockholm in 1972, consisting of Anni-Frid Lyngstad (Frida), Björn Ulvaeus, Benny Andersson and Agnetha Fältskog (Anna). Fältskog and Ulvaeus were a married couple, as were Lyngstad and Andersson during their career, although both couples later divorced. They became one of the most commercially successful acts in the history of popular music, and they topped the charts worldwide from 1972 to 1982. ABBA gained international popularity employing catchy song hooks, simple lyrics, sound effects (reverb, phasing) and a Wall of Sound achieved by overdubbing the female singers' voices in multiple harmonies. As their popularity grew, they were sought after to tour Europe, Australia, and North America, drawing crowds of ardent fans, notably in Australia. Touring became a contentious issue, being particularly cumbersome for Fältskog, but they continued to release studio albums to widespread commercial success. At the height of their popularity, however, both relationships began suffering strain that led ultimately to the collapse of first the Ulvaeus-Fältskog marriage (in 1979) and then of the Andersson-Lyngstad marriage in 1981. In the late 1970s and early 1980s these relationship changes began manifesting in the group's music, as they produced more thoughtful, introspective lyrics with different compositions. ABBA remains a fixture of radio playlists and is one of the world's best-selling bands, having sold over 375 million records worldwide, making them the second best-selling popular music group in the history of recorded music. They still sell two to three million records a year. ABBA was also the first pop group from mainland Europe to enjoy consistent success in the charts of English-speaking countries, including the United Kingdom, the United States, Canada, Ireland, South Africa/Rhodesia, Australia and New Zealand. After ABBA split, Andersson and Ulvaeus achieved success writing music for the stage while Lyngstad and Fältskog pursued individual solo careers with varying success. ABBA's music remained steadily popular until several films, notably "Muriel's Wedding" and "The Adventures of Priscilla, Queen of the Desert", revived interest in the group, spawning several tribute bands. In 1999, ABBA's music was adapted into the successful musical "Mamma Mia!" that toured worldwide and had a movie version released in 2008, becoming the highest grossing film in the UK ever. The group will be inducted into The Rock and Roll Hall of Fame on 15 March 2010. Only Benny Andersson and Anni-Frid Lyngstad will be present to accept ABBA's induction into the Rock and Roll Hall of Fame. The group will be inducted into the museum by Bee Gees members Barry Gibb and Robin Gibb.
Benny Andersson (born 16 December 1946 in Stockholm, Sweden) became (at age 18) member of a popular Swedish pop-rock group, The Hep Stars, that performed covers of international hits. The Hep Stars were known as "The Swedish Beatles"; they even set up Hep House, their equivalent of the Apple Corps. Andersson played keyboard and eventually started writing original compositions for his band, many of which became major hits including "No Response" that hit #3 in 1965, "Sunny Girl", "Wedding", "Consolation", all of which hit #1 in 1966. Andersson also had a fruitful songwriting collaboration with Lasse Berghagen, with whom he composed his first Svensktoppen entry "Sagan om lilla Sofi" ("The Story of Little Sophie") in 1968. Björn Ulvaeus (born 25 April 1945 in Gothenburg, Sweden) also began his musical career at 18 (as a singer and guitarist), when he fronted The Hootenanny Singers, a popular Swedish folk-skiffle group. Ulvaeus started writing English-language songs for his group, and even had a brief solo career alongside. The Hootenanny Singers and The Hep Stars sometimes crossed paths while touring, and in June 1966 Ulvaeus and Andersson decided to write a song together. Their first attempt was "Isn't It Easy to Say", a song later recorded by The Hep Stars. Stig Anderson was the manager of The Hootenanny Singers and founder of the Polar Music label. He saw potential in the collaboration, and encouraged them to compose more. Both also began playing occasionally with the other's bands on stage and on record, although not until 1969 did the pair write and produce some of their first real hits together: "Ljuva sextiotal" ('Merry Sixties'), recorded by Brita Borg and The Hep Stars' 1969 hit "Speleman" ("Fiddler"). Björn Ulvaeus and Agnetha Fältskog were married on 6 July 1971. Andersson wrote and submitted the song "Hej, Clown" for the 1969 Melodifestivalen, the Swedish Eurovision Song Contest finals. The song tied for first, but re-voting relegated Andersson's song to second place. On that occasion Andersson briefly met his future spouse, singer Anni-Frid Lyngstad, who also participated in the contest. A month later, the two had become a couple. As their respective bands began to break up during 1969, Andersson and Ulvaeus teamed up and recorded their first album together in 1970, called "Lycka" ("Happiness"), which included original compositions sung by both men. Their spouses were often present in the recording studio, and sometimes added backing vocals; Fältskog even co-wrote a song with the two. Ulvaeus still occasionally recorded and performed with The Hootenanny Singers until the summer of 1974, and Andersson took part in producing their records. Agnetha Fältskog (born 5 April 1950 in Jönköping, Sweden) had a #1 record in Sweden when she was only 17, and was soon noted by the critics and songwriters as a talented singer/songwriter of "schlager" style songs. Fältskog's main inspiration in her early years were singers like Connie Francis. Along with her own compositions, she recorded covers of foreign hits and performed them on tours in Swedish folkparks. In 1967 she submitted an original song ("Försonade" ("Redeemed")) for Melodifestivalen, but it was rejected. She briefly met Anni-Frid Lyngstad for the first time during a TV show in January 1968, and met Björn Ulvaeus at a concert venue a few months later. During filming of a Swedish TV special in May 1969, Fältskog met Ulvaeus again, and they were married in 1971. Fältskog and Ulvaeus eventually got involved in each other's recording sessions, and soon even Andersson and Lyngstad added backing vocals to her 1970 album "Som jag är" ("As I Am"). In 1973, Fältskog starred as Mary Magdalene in the original Swedish production of "Jesus Christ Superstar" and attracted favourable reviews. Between 1967 and 1975, Fältskog released five studio albums. Anni-Frid "Frida" Lyngstad (born 15 November 1945 in Bjørkåsen in Ballangen, Norway) sang from the age of thirteen with various dance bands, and worked mainly in a jazz-oriented cabaret style. She also formed her own band, the Anni-Frid Four. In the summer of 1967, she won a national talent competition with the song "En ledig dag" ("A Day Off"), included in the EMI compilation "Frida 1967-1972". The first prize was a recording contract with EMI Sweden and to perform live on the most popular TV show in Sweden. This first TV performance, amongst many others, is included in the 3½ hour documentary "Frida - The DVD". Lyngstad released several singles on EMI and had many hits in the Swedish charts. When Benny Andersson started to produce her recordings in 1971, she got her first #1 single, "Min egen stad" ("My Own Town"), for which all the future ABBA members sang the backup vocals. Lyngstad toured and performed regularly in the folkpark circuit and made appearances on radio and TV. She met Björn Ulvaeus briefly in 1963 during a talent contest, and Agnetha Fältskog during a TV show in early 1968. Lyngstad finally linked up with her future bandmates in 1969. On 1 March 1969, she participated in the Melodifestivalen, where she met Andersson for the first time. A few weeks later they met again during a concert tour in southern Sweden and they soon became a couple. Andersson produced her single "Peter Pan" in September 1969 – the first collaboration between her and Benny & Björn, as they had written the song. Later Andersson produced Lyngstad's debut album, "Frida", which was released in March 1971 and praised by critics. Lyngstad also played in several revues and cabaret shows in Stockholm between 1969 and 1973. After ABBA formed, she recorded another successful album in 1975, "Frida Ensam", which included the original Swedish rendition of "Fernando", which became a huge hit in Scandinavia before the English version was recorded. First live performance and the start of "Festfolk". An attempt at combining their talents occurred in April 1970 when the two couples went on holiday together to the island of Cyprus. What started as singing for fun on the beach ended up as an improvised live performance in front of the United Nations soldiers stationed on the island. Andersson and Ulvaeus were at this time recording their first album together, "Lycka", which was to be released in September 1970. Fältskog and Lyngstad added backing vocals on several tracks during June, and the idea of them all working together saw them launch a stage act, "Festfolk", which translates from Swedish to mean both "Party People" and "Engaged Couples", on 1 November 1970 in Gothenburg. The cabaret show attracted positive reviews. The foursome performed the Andersson and Ulvaeus hit "Hej, gamle man" ("Hi, Old Man"); the first recording credited to all four – and solo numbers from respective albums, but the foursome did not feel like working together, and soon concentrated on individual projects again... First record together "Hej, gamle man". "Hej, gamle man", a song about an old Salvation Army soldier, became the quartet's first hit. The record was credited to Björn & Benny and reached number 5 on the sales charts and number 1 on Svensktoppen, staying there for 15 weeks. In the first half of 1971, the four artists worked more together, adding vocals to the others' recordings. Fältskog, Andersson and Ulvaeus toured together in May, while Lyngstad toured on her own. Frequent recording sessions brought the foursome tighter together during the summer.
After the 1970 release of Andersson's and Ulvaeus' album "Lycka", two more singles credited to 'Björn & Benny' were released in Sweden, "Det kan ingen doktor hjälpa" ("No doctor can help with that") and "Tänk om jorden vore ung" ("Imagine if the Earth were young"), but clearly with more prominent vocals by Fältskog and Lyngstad – and with moderate chart success. Fältskog released her fourth album in 1971 and married Ulvaeus on 6 July 1971. Andersson, Ulvaeus, and Fältskog started performing together on a regular basis during the summer of 1971. Stig Anderson, founder and owner of Polar, was determined to break into the mainstream international market with music by Andersson and Ulvaeus. "One day the pair of you will write a song that becomes a worldwide hit", he predicted. Stig encouraged Ulvaeus and Andersson to write a song for Melodifestivalen, and after two rejected entries in 1971, Andersson and Ulvaeus submitted their new song "Säg det med en sång" ("Say It With A Song") for the 1972 contest, and they chose newcomer Lena Anderson to perform. The song won third place, encouraging Stig, and became a huge hit in Sweden. The first signs of foreign success came as a surprise, as the Andersson and Ulvaeus single "She's My Kind of Girl" was released by Epic in Japan in March 1972, giving the duo a Top 10 hit. Two more singles were released in Japan, "En Carousel" (earlier version of "Merry-Go-Round") and "Love Has Its Ways" (a song they wrote with Koichi Morita). First hit as 'Björn, Benny, Agnetha & Anni-Frid'. Ulvaeus and Andersson persevered with their songwriting and experimented with new sounds and vocal arrangements. "People Need Love" was released in June 1972, featuring guest vocals by the women, who were now given much greater prominence. Stig Anderson released it as a single, credited to Björn & Benny, Agnetha & Anni-Frid. The song reached #17 in the Swedish combined single and album charts, enough to convince them they were on to something. The single also became the first record to chart for the quartet in the United States, where it peaked at #114 on the Cashbox singles chart and #117 on Record World's singles chart. Billed as Björn & Benny (with Svenska Flicka), it was released there on Playboy Records. However, according to Stig Anderson, "People Need Love" could have been a much bigger American hit, but a small label like Playboy Records did not have the distribution resources to meet the demand for the single from retailers and radio programmers. The foursome decided to record their first album together in the autumn of 1972, and sessions began on 26 September 1972. The two women shared lead vocal on "Nina, Pretty Ballerina", on this day, and the two women's voices combined in harmonies for the first time gave the foursome an idea of the qualities of their combined talents.
For 1973, the band and their manager Stig Anderson decided to have another try at the Melodifestivalen, this time with the song "Ring Ring." The studio sessions were handled by Michael B. Tretow, who experimented with a "wall of sound" production technique that became the wholly new ABBA sound. Anderson arranged an English translation of the lyrics by Neil Sedaka and Phil Cody and they thought this would be a surefire winner, but in the Melodifestivalen, on 10 February 1973, it placed third, and thus never reached the international contest. Nevertheless the proto-group put out their first album, called "Ring Ring". The album did well and the "Ring Ring" single was a hit in many parts of Europe and also in South Africa, but Stig Anderson felt the true breakthrough could only come with a UK or US hit.
In early 1973, Stig Anderson, tired of unwieldy names, started to refer to the group privately and publicly as ABBA. At first, this was a play on words, as Abba is also the name of a well-known fish-canning company in Sweden, and itself an acronym. However, since the fish-canners were unknown outside Sweden, Anderson came to believe the name would work in international markets. Coincidentally, "abba" exists as a word in the Aramaic and Hebrew languages. It means "father", but in the more informal familiar sense that saying "dad" might connote. A competition to find a suitable name for the group was held in a Gothenburg newspaper. The group was impressed with the names "Alibaba," "FABB," and "Baba", but in the end all the entries were ignored and it was announced in the summer that the name "ABBA" was official. Later the group negotiated with the canners for the right to the name. "ABBA" is an acronym formed from the first letters of each group member's first name: Agnetha, Björn, Benny and Anni-Frid (Frida). The first 'B' in the logo version of the name was "mirror-image" reversed on the band's promotional material from 1976 onwards and became the group's registered trademark. The first time the name is found written on paper is on a recording session sheet from the Metronome Studio in Stockholm, dated 16 October 1973. This was first written as "Björn, Benny, Agnetha & Frida", but was subsequently crossed out with "ABBA" written in large letters on top. The official logo, using the bold version of the News Gothic typeface, was designed by Rune Söderqvist, and appeared for the first time on the Dancing Queen single in August 1976, and subsequently on all later original albums and singles. But the idea for the official logo was made by the German photographer Wolfgang Heilemann on a "Dancing Queen" shoot for the teenage magazine BRAVO. On the photo, every ABBA-member held a giant initial letter of his/her name. After the pictures were made, Heilemann found out that one of the men held his letter upside down. They discussed it and the members of ABBA liked it. In 1992 Polygram redesigned the logo for the ABBA Gold compilation, having a different font along with a crown emblem. Still, the classic logo is more commonly seen, for instance being used on the official ABBA website.
For their first Eurovision, ABBA entered with "Ring Ring" but failed to qualify as the 1973 Swedish entry; it came third in the preliminary round. Stig immediately started planning for the 1974 contest. Ulvaeus, Andersson, and manager Stig Anderson believed in the possibilities of using Melodifestivalen and Eurovision TV contests as a way to make the music business aware of the band and Andersson, Ulvaeus and Stig as composers. In late 1973, they were invited by Swedish television to contribute a song for the 1974 contest, and from a number of newly written compositions, the foursome chose the upbeat "Waterloo"; the group was now inspired by the growing glam rock scene in England. "Waterloo" was an unashamedly glam-style pop track produced with Michael B. Tretow's wall-of-sound approach. ABBA won their national heats on Swedish TV on 9 February 1974, and with this third attempt were far more experienced and better prepared for the international contest. Winning the Eurovision Song Contest gave ABBA the chance to tour Europe and perform on major TV shows; thus the band saw the "Waterloo" single climb the charts in many European countries. "Waterloo" was ABBA's first number one single in big markets such as the UK, Germany and Australia. In the US, it reached #6 on the Billboard Hot 100 chart, paving the way for their first album and their first trip as a group there. Albeit a short promotional visit, it included their first performance on American TV, The Mike Douglas Show. The "Waterloo" album only peaked at #145 on the Billboard 200 album chart, but received unanimous high praise from the US critics: Los Angeles Times called it "a compelling and fascinating debut album that captures the spirit of mainstream pop quite effectively...an immensely enjoyable and pleasant project", while Creem characterized it as "a perfect blend of exceptional lovable compositions". However the win was by the smallest percentage of the votes ever recorded in the Eurovision history. A record still unbroken over 35 years. ABBA's follow-up single, "Honey, Honey", reached #27 on the US Billboard Hot 100, and was #2 in Germany. However, in the UK, a cover version of the song by the act Sweet Dreams made #10 because ABBA's British record company, Epic, decided to re-release a remixed version of "Ring Ring" instead. It failed to reach the Top 30, increasing growing speculation that the group were simply Eurovision one-hit wonders.
In November 1974, ABBA embarked on their first European tour, playing dates in Denmark, West Germany, and Austria. It was not as successful as the band had hoped, since most of the venues did not sell out. Due to a lack of demand, they were even forced to cancel a few shows, including a sole concert scheduled in Switzerland. The second leg of the tour, which took them through Scandinavia in January 1975, was different. They played to full houses everywhere and finally got the reception they aimed for. Live performances continued during the summer of 1975 when ABBA embarked on a sixteen open-air date tour of Sweden and Finland, attracting huge crowds. Their Stockholm show at the Gröna Lund amusement park was seen by an estimated audience of 19,200. In 1974 "So Long" was released as a single in the UK but it received no airplay from Radio 1 and failed to chart. In summer 1975 they released "I Do I Do I Do I Do I Do", which again received little airplay on radio 1 but managed to climb the charts, to #38. Later in 1975 the release of their next album "ABBA" and single "SOS" brought back their chart presence in the UK, where the single hit #6 and the album reached #13. SOS also became ABBA's second number 1 single in both Germany and Australia. Success was further solidified with "Mamma Mia" reaching the #1 spot in UK, Germany and Australia in January 1976. In the US, "SOS" reached #10 on the Record World Top 100 singles chart and #15 on the Billboard Hot 100 chart, picking up the BMI Award along the way as one of the most played songs on American radio in 1975. The success of the group in the United States had been so far limited to single releases. By early 1976, the group already had four Top 30 singles on US charts, but the album market proved to be tough to crack. The eponymous "ABBA " album generated three American hits, but it only peaked at #165 on the "Cashbox" album chart and #174 on the Billboard 200 chart. Opinions were voiced, by "Creem" in particular, that in the US ABBA had endured "a very sloppy promotional campaign". The group, however, enjoyed warm reviews from American press. "Cashbox" went as far as saying that "there is a recurrent thread of taste and artistry inherent in Abba's marketing, creativity and presentation that makes it almost embarrassing to critique their efforts", while "Creem" wrote: "SOS is surrounded on this LP by so many good tunes that the mind boggles". In Australia, the airing of the videos for "I Do, I Do, I Do, I Do, I Do" and "Mamma Mia" on nationwide TV in August 1975 started an immense interest for ABBA, resulting in #1 positions on both the single and album charts for months.
In March 1976, the band released the compilation "Greatest Hits", despite having had only six Top 40 hits in the UK and the US. Nevertheless, it became their first UK #1 album, and also took ABBA into the Top 50 on the US album charts for the first time, eventually selling more than a million copies there. At the same time, Germany released a compilation named "The Very Best of ABBA", also becoming a #1 album there whereas the "Greatest hits" LP followed a few months later to #2 on the German charts, despite all similarities with "The Very Best" album. Also included on "Greatest Hits" was a new single, "Fernando", which had first been written by Ulvaeus and Andersson in Swedish for Lyngstad's #1 1975 solo album "Frida ensam" ("Frida alone"). After Lyngstad's major success with the song in Scandinavia, the group decided to record an English version. "Fernando" took the world by storm, hitting #1 in at least thirteen countries worldwide, including the UK, Germany and Australia and the single went to sell over 10 million copies worldwide. In Australia, the song occupied the top position during 14 weeks, and stayed in the Australian charts for 40 weeks, tying The Beatles for longest number one for "Hey Jude", making "Fernando" one of the best-selling singles of all time in Australia. That same year, the group received its first international prize, with "Fernando" being chosen as the "Best Studio Recording of 1975". In the US, "Fernando" reached the Top 10 of the Cashbox Top 100 singles chart and #13 on the Billboard Hot 100. It also topped the Billboard Adult Contemporary chart, ABBA's first American number one single of any kind. The group's next album, "Arrival", a #1 bestseller all over Europe and Australia, represented a new level of accomplishment in both songwriting and studio work, prompting rave reviews from more rock-oriented UK music weeklies such as "Melody Maker" and "New Musical Express", and mostly appreciative notices from American critics. Hit after hit flowed from "Arrival": "Money, Money, Money", another #1 in Germany and Australia, and "Knowing Me, Knowing You", ABBA's sixth consecutive German #1 as well as another UK #1. The real sensation of all was "Dancing Queen", not only topping the charts in the loyal markets UK, Germany and Australia, but also reaching #1 in the US. In South Africa ABBA had astounding success with "Fernando", "Dancing Queen" and "Knowing Me, Knowing You" being among the top 20 best selling singles for 1976-7. In 1977 "Arrival" was nominated for the inaugural BRIT Award in the category "Best International Album of the Year". By this time ABBA were popular in the UK, most of Western Europe, Australia and New Zealand. In Frida the dvd, Lyngstad explains how she and Fältskog developed as singers, as ABBA's recordings grew more complex over the years. Their popularity in the US would remain on a comparatively smaller scale, and "Dancing Queen" became the only Billboard Hot 100 #1 single ABBA had there (they did, however, get three more singles to the #1 position on other "Billboard" charts, including Billboard Adult Contemporary and Hot Dance Club Play). Nevertheless, "Arrival" finally became a true breakthrough release for ABBA on the US album market where it peaked at #20 on the Billboard 200 album chart and was certified gold by RIAA.
In January 1977, ABBA hit the road. The group's status had changed dramatically and they were clearly regarded as superstars. They opened their much anticipated tour in Oslo, Norway, on 28 January, and mounted a lavishly produced spectacle that included a few scenes from their self-penned mini-operetta "The Girl With The Golden Hair." The concert attracted immense media attention from across Europe and Australia. They continued the tour through Western Europe visiting Gothenburg, Copenhagen, Berlin, Cologne, Amsterdam, Antwerp, Essen, Hanover, Hamburg, and ended it with shows in the UK in Manchester, Birmingham, Glasgow and two sold-out concerts at London's Royal Albert Hall. Tickets for these two shows were available only by mail application and it was later revealed that the box-office received 3.5 million requests for tickets, enough to fill the venue 580 times. Along with praise ("Abba turn out to be amazingly successful at reproducing their records", wrote "Creem"), there were complaints that "Abba performed slickly...but with a zero personality coming across from a total of 16 people on stage" ("Melody Maker"). One of the Royal Albert Hall concerts was filmed as a reference for the filming of the Australian tour for what became ', though it is not known exactly how much of the concert was filmed. After the European leg of the tour, in March 1977, ABBA played eleven dates in Australia before a total of 160,000 people. The opening concert in Sydney at the Sydney Showground on 3 March before over 20,000 was marred by torrential rain and Frida slipped on the wet stage during the concert. However, all four members would later recall this concert to be the most memorable of their career. Upon their arrival in Melbourne, a civic reception was held at the Town Hall and ABBA appeared on the balcony to greet an enthusiastic crowd of 6,000 people. In Melbourne, ABBA played three concerts at the Melbourne Myer Music Bowl with 14,500 at each including the Australian Prime Minister Malcolm Fraser and his family. At the first Melbourne concert, an additional 16,000 people gathered outside the fenced-off area to listen to the concert. In Adelaide, the group performed one concert at West Lakes Football Stadium before 20,000 people with another 10,000 listening outside. During the first five concerts in Perth, there was a bomb scare with everyone having to evacuate the Entertainment Centre. The trip was accompanied by mass hysteria and unprecedented media attention ("Swedish ABBA stirs box-office in Down Under tour...and the media coverage of the quartet rivals that set to cover the upcoming Royal tour of Australia", wrote "Variety"), and is vividly captured on film in ', directed by Lasse Hallström. The Australian tour and its subsequent "ABBA: The Movie" produced some ABBA lore, as well. Agnetha Fältskog's blonde good looks had long made her the band's 'pin-up girl', a role she disdained. Many fans thought that this unfair focus on Agnetha detracted from the band's wholesome image, and also was unfair on the other members; especially Anni-Frid, who the fans viewed as equal to Agnetha, and some saw it as a vendetta against Anni-Frid. During the Australian tour, she performed in a skin-tight white jumpsuit, causing one Australian newspaper to use the headline "Agnetha's bottom tops dull show". When asked about this at a news conference, she replied: "Don't they have bottoms in Australia?" Frida is also pictured in this scene, looking particularly unamused; unsurprising, as the focus and comments made to Agnetha in this particular part of the movie are particularly embarrassing. In December 1977, ABBA followed up "Arrival" with the more musically and lyrically ambitious fifth album "The Album", which was released to coincide with "ABBA: The Movie". Although the album was less well-received by the critics in the UK, it did spawn more worldwide hits: "The Name of the Game" and "Take a Chance on Me", both of which topped the UK charts, and reached #12 and #3, respectively, on the Billboard Hot 100 chart in the US. Although "Take a Chance on Me" did not top the American charts, it has actually proved to be ABBA's biggest hit single in the United States, selling more copies than "Dancing Queen". "The Album" also included the ABBA signature tune, "Thank You for the Music", released as a single in the UK in 1983, and had been the B-side of "Eagle" in countries where the latter had been released.
By 1978, ABBA was a megagroup. They converted a vacant movie theatre into the Polar Music Studio, a state-of-the-art studio in Stockholm. The studio was used by several other bands; notably, Genesis' "Duke" and Led Zeppelin's "In Through the Out Door" were recorded there. During May, the group went to the US for a huge promotional campaign, and performed on Olivia Newton-John's TV show. However, a lot of effort was put into the new recording studio in Stockholm. The recording sessions for "Summer Night City" were an uphill struggle, but upon release the song became another significant hit for the group. The track would also set the stage for ABBA's foray into disco with their upcoming album. Several years ago, the original Polar Music Studios (by that time renamed Polar Studios) were closed because the landlord of the building had increased the rent. The site is now a Fitness First gymnasium, and there is a display in its foyer acknowledging its history as Polar (Music) Studios. On 9 January 1979, the group performed "Chiquitita" at the Music for UNICEF Concert held at the United Nations General Assembly to celebrate UNICEF's Year of the Child. ABBA donated the copyright of this worldwide hit to the UNICEF; see Music for UNICEF Concert. The single was released the following week, and reached #1 in ten countries. North American and European tours. In mid-January 1979, Ulvaeus and Fältskog announced they were getting divorced. The news caused massive interest from the media, and led to speculation about the band's future. ABBA assured the press and their fan base they were continuing their work as a group, and that the divorce would not affect them. Nonetheless, the media continued to confront them with this in interviews. The group's sixth album, "Voulez-Vous", was released in April 1979, the title track of which was recorded at the famous Criteria Studios in Miami, U.S. with the assistance, among others, of recording engineer Tom Dowd. The album topped the charts across Europe and in Japan and Mexico, hit the Top 10 in Canada and Australia and the Top 20 in the US None of the singles from the album reached #1 on the UK charts, but "Chiquitita", "Does Your Mother Know", "Angeleyes" and "Voulez-Vous" all charted no lower than #4. "I Have a Dream" was the exception, when the single reached #2 in UK and #1 on Eurochart Hot 100 singles. In Canada, "I Have a Dream" became ABBA's second #1 on the RPM Adult Contemporary chart, after "Fernando" hit the top previously. Later that year, the group released their second compilation album, "Greatest Hits Vol. 2", which featured a brand new track: "Gimme! Gimme! Gimme! (A Man After Midnight)", another number 3 hit in both, the UK and Germany. In Russia during the late 1970s, they were paid in oil commodities because of an embargo on the ruble. On 13 September 1979, ABBA began their first (and only) North American Tour at the Northlands Coliseum in Edmonton, Canada, with a full house of 14,000. During the next four weeks, they played a total of seventeen sold-out dates, thirteen in the U.S. and four in Canada. The last scheduled ABBA concert on U.S. soil, in Washington, DC, was canceled due to Agnetha Fältskog's emotional distress suffered during the flight from New York to Boston, when the private plane the group was on was subjected to extreme weather conditions and was unable to land for an extended period. They appeared on the Boston Music Hall stage for the performance ninety minutes late. The tour ended with a show in Toronto, Canada at Maple Leaf Gardens before a capacity crowd of 18,000. The shows also generated the same type of complaints that were expressed during the group's 1977 tour: many fans regarded ABBA as more of a studio group than a live band. On 19 October 1979, the tour resumed in Western Europe where the band played 23 sold-out gigs, including an unprecedented six sold-out nights at London's Wembley Arena.
In March 1980, ABBA travelled to Japan where upon their arrival at Narita International Airport, they were besieged by thousands of fans. The group played eleven concerts to full houses, including six shows at Tokyo's Budokan. This tour was the last "on the road" adventure of their career. The same year saw the release of ABBA's seventh album "Super Trouper", which reflected a certain change in ABBA's style with more prominent use of synthesizers and increasingly more personal lyrics. It set a record for the most pre-orders ever received for a UK album after one million copies were ordered before release. Anticipation for the album had been built up by "The Winner Takes It All", the group's eighth UK chart topper (and their first since 1978). In the US, the single reached #8 on the Billboard Hot 100 chart and became ABBA's second Billboard Adult Contemporary #1. The song was allegedly written about Ulvaeus and Fältskog's marital tribulations. It was also re-recorded by Andersson and Ulvaeus with a slightly different backing track, by French chanteuse Mirielle Mathieu at the end of 1980 – as "Bravo Tu As Gagne", with French lyrics by Alain Boublil. The next single from the album, "Super Trouper", also hit #1 in the UK as well as in Germany, becoming the group's ninth and final UK chart-topper. Another track from "Super Trouper", "Lay All Your Love on Me", released in 1981 as a 12-inch single only in selected territories, managed to top the Billboard Hot Dance Club Play chart and peaked at #7 on the UK singles chart becoming at the time the highest ever charting 12-inch release in UK chart history. Also in 1980, ABBA recorded a compilation of Spanish-language versions of their hits called "Gracias Por La Música". It was released in Spanish-speaking countries as well as Japan and Australia. The album became a major success, and along with the Spanish version of "Chiquitita", this signalled the group's breakthrough in Latin America Final album and performances (1981–1982). In January 1981, Ulvaeus married Lena Källersjö, and manager Stig Anderson celebrated his 50th birthday with a huge party. For this occasion, ABBA recorded the track 'Hovas Vittne' (a pun on the Swedish name for Jehovah's Witness and Anderson's birthplace, Hova) as a tribute to him, and released it only on 200 red vinyl copies, to be distributed to the guests attending the party. This single has become a sought-after collectible. In mid-February 1981, Andersson and Lyngstad announced they were filing for divorce. Information surfaced that their marriage had been an uphill struggle for years, and Benny had already met another woman, Mona Nörklit, whom he married in November 1981. Andersson and Ulvaeus had songwriting sessions during the first months of 1981, and recording sessions began in mid-March. At the end of April, the group recorded a TV special, "Dick Cavett meets ABBA" with the US talk show host Dick Cavett. "The Visitors", ABBA's eighth and final studio album, showed a songwriting maturity and depth of feeling distinctly lacking from their earlier recordings but still placing the band squarely in the pop genre, with catchy tunes and harmonies. Although not revealed at the time of its release, the album's title track, according to Ulvaeus, refers to the secret meetings held against the approval of totalitarian governments in Soviet-dominated states, while other tracks address topics like failed relationships, the threat of war, ageing, loss of innocence, and a parent watching a child grow up. This change of content led to the release of the album "The Visitors" including the UK #3 single "One of Us", proving the last of ABBA's nine number 1 singles in Germany in December 1981; and the swansong of their sixteen top 5 singles on the charts in South Africa. Although it topped the album charts across most of Europe, including the UK and Germany, "The Visitors" was not as commercially successful as its predecessors, showing a commercial decline in previously loyal markets such as France, Australia or Japan. A track from "The Visitors", "When All Is Said and Done", was released as a single in North America, Australia and New Zealand, and fittingly became ABBA's final Top 40 hit in the US, while also reaching the US Adult Contemporary Top 10 and #4 on the RPM Adult Contemporary chart in Canada. The song's lyrics, as with "The Winner Takes It All" and "One of Us", dealt with the painful experience of separating from a long-term partner, though it looked at the trauma more optimistically. With the now publicised story of Andersson and Lyngstad's divorce, speculation increased of tension within the band. Also released in the US was the title track of "The Visitors", which hit the Top Ten on the Billboard Hot Dance Club Play chart.
In the spring of 1982, songwriting sessions had started and the group came together for more recordings. Plans were not completely clear, but a new album was discussed and the prospect of a small tour suggested. The recording sessions in May and June were a struggle, and only three songs were eventually recorded: "You Owe Me One", "I Am the City", and "Just Like That". Andersson and Ulvaeus were not satisfied with the outcome, so the tapes were shelved and the group took a break for the summer. Back in the studio again in early August, the group had changed plans for the rest of the year: they settled for a Christmas release of a double album compilation of all their past single releases to be named '. New songwriting and recording sessions took place, and during October and November, they released the singles "The Day Before You Came"/"Cassandra" and "Under Attack"/"You Owe Me One", the A-sides of which were included on the compilation album. Neither single made the top 20 in the UK, though "The Day Before You Came" became a Top 5 hit in many European countries such as Germany, the Netherlands and Belgium. The album went to #1 in the UK and Belgium, Top 5 in the Netherlands and West Germany and Top 20 in many other countries. The last single, "Under Attack," hit the top 10 in about 3 European countries. "I Am the City" and "Just Like That" were left unreleased on "The Singles: The First Ten Years" for possible inclusion on the next projected studio album from ABBA, though this never came to fruition. "I Am the City" was eventually released as a bonus track on the compilation album "More ABBA Gold" in 1993, while "Just Like That" has been recycled in new songs with other artists produced by Andersson and Ulvaeus. A reworked version of the verses ended up in the musical "Chess". The chorus section of "Just Like That" was eventually released on a retrospective box set in 1994. Despite numerous requests from fans, Ulvaeus and Andersson are still refusing to release ABBA's version of "Just Like That" in its entirety, even though the complete version surfaced on bootlegs. The group travelled to London to promote "The Singles: The First Ten Years" in the first week of November 1982, appearing on "Saturday Superstore" and "The Late, Late Breakfast Show", and also to West Germany in the second week, to perform on Show Express. On 19 November 1982, ABBA appeared for the last time in Sweden on the TV programme Nöjesmaskinen, and on 11 December 1982, they made their last performance ever, transmitted to the UK on Noel Edmonds' "The Late, Late Breakfast Show", via a live link from a TV studio in Stockholm.
Andersson and Ulvaeus began collaborating with Tim Rice in early 1983 on writing songs for the musical project "Chess", while Fältskog and Lyngstad both concentrated on international solo careers. While Andersson and Ulvaeus were working on the musical, a further cooperation between three of them came with the musical "Abbacadabra" that was produced in France for television. It was a children's musical utilising 14 ABBA songs. Alain and Daniel Boublil, who wrote "Les Misérables", had been in touch with Stig Anderson about the project, and the TV musical was aired over Christmas 1983 on the British channel ITV. Boublil previously also wrote the French lyrics for Mirielle Mathieu's version of "The Winner Takes It All". Lyngstad, who had recently moved to Paris, participated in the French version, and recorded a single, "Belle", a duet with French singer Daniel Balavoine. The song was a cover of ABBA's instrumental 1976 track "Arrival". As the single "Belle" sold well in France, Cameron Mackintosh wanted to stage an English language version of the show in London, with the French lyrics translated by David Wood and Don Black; Andersson and Ulvaeus got involved in the project, and contributed with one new song, "The Seeker". "Abbacadabra" premièred 8 December 1983 at The Lyric Hammersmith Theatre in London, to mixed reviews and full houses for eight weeks, closing on 21 January 1984. Lyngstad was involved in this production as well, recording 'Belle' in English as "Time"; a duet with actor and singer B. A. Robertson: the single sold well, this time produced and recorded by Andersson and Ulvaeus. All four members made their last public appearance, as four friends more than as ABBA, in January 1986, when they recorded a video of themselves performing an acoustic version of "Tivedshambo", which was the first song written by their manager, Stig Anderson, for a Swedish TV show honouring Anderson on his 55th birthday. The four had not seen each other for more than two years. That same year they also performed privately at another friend's 40th birthday: their old tour manager, Claes af Geijerstam. They sang a self-composed song titled "Der Kleine Franz" that later was to surface in "Chess". The same year "ABBA Live" was released, featuring selections of live performances from the group's 1977 and 1979 tours. They were guests on the 50th birthday of Görel Hanser in 1999. Hanser was a long-time friend of all four, and also former secretary of Stig Anderson. Honouring Görel, ABBA performed a Swedish birthday song "Med En Enkel Tulipan" a cappella. Benny Andersson has on several occasions performed old ABBA songs. In June 1992, he and Björn Ulvaeus appeared with U2 at a Stockholm concert, singing the chorus of "Dancing Queen", and a few years later during the final performance of the B & B in Concert in Stockholm, Andersson joined the cast for an encore at the piano. Andersson frequently adds an ABBA song to the playlist when he performs with his BAO band. He also played the piano during new recordings of the ABBA songs "Like an Angel Passing Through My Room" with opera singer Anne Sofie von Otter, and "When All Is Said And Done" with Swede Viktoria Tolstoy. Andersson and Ulvaeus both did an a cappella rendition of the first verse of "Fernando" as they accepted their Ivor Novello award in London in 2002. Frida Lyngstad performed and recorded an a cappella version of "Dancing Queen" with the Swedish group The Real Group in 1993, and has also re-recorded "I Have a Dream" with Swiss singer Dan Daniell in 2003.
ABBA has never officially announced the end of the group, but the group has long been considered dissolved. Their last public performance together "as" ABBA was on the British TV programme "The Late, Late Breakfast Show" (live from Stockholm) 11 December 1982. In January 1983, Agnetha started recording sessions for a solo album, as Frida had released her album "Something's Going On" some months earlier to great success. Björn and Benny started songwriting sessions for the musical "Chess", and ABBA was shelved in the meantime. In interviews, Björn and Benny denied the split of ABBA ("Who are we without our ladies? Initials of Brigitte Bardot?") and Frida and Agnetha kept claiming in interviews that ABBA would come together for a new album repeatedly during 1983 and 1984. Internal strife between the group and their manager escalated and the group sold their shares in Polar Music during 1983. With this, the foursome did not come together publicly until all four members were reunited at the Swedish premiere of "Mamma Mia!" on 4 July 2008. In an interview with the "Sunday Telegraph", following the premiere, Björn Ulvaeus and Benny Andersson confirmed that there was nothing that could entice them back on stage again. "We will never appear on stage again", Ulvaeus said. "There is simply no motivation to re-group. Money is not a factor and we would like people to remember us as we were. Young, exuberant, full of energy and ambition. I remember Robert Plant saying Led Zeppelin were a cover band now because they cover all their own stuff. I think that hit the nail on the head."
In October 1984, Ulvaeus and Andersson together with lyricist Tim Rice released the musical concept double album "Chess". The singles "One Night in Bangkok" (with vocals by Murray Head) and "I Know Him So Well" (a duet by Barbara Dickson and Elaine Paige and later also recorded by both Barbra Streisand and Whitney Houston) were both huge successes. In May 1986, the musical premièred in the West End of London, and ran for almost three years. On Broadway it opened in April 1988, but closed within two months due to bad reviews. The musical has since been staged regularly on a smaller scale to great success, and even the concert version is popular. In Stockholm, the composers staged "Chess på svenska" ('Chess in Swedish') in 2003, with some new material including the musical numbers "Han är en man, han är ett barn" ("He's a man, he's a child") and "Glöm mig om du kan" ("Forget me if you can"). What is considered to be Andersson's and Ulvaeus' masterpiece, however, is "Kristina från Duvemåla", an epic Swedish musical which the composers premiered in Malmö in southern Sweden in October 1995. It was directed for the stage by Lars Rudolfsson and based on the "The Emigrants" tetralogy by Swedish novelist Vilhelm Moberg. The musical ran for five years in Stockholm, and an English version has been in development for some considerable time. It has been reported that a Broadway production is in its earliest stages of pre-production. Since 1983, besides "Chess" and "Kristina från Duvemåla", Benny Andersson has continued writing songs with Björn Ulvaeus. The pair produced two English-language pop albums with Swedish duo Gemini in 1985 and 1987. In 1987, Andersson also released his first solo album on his own label, Mono Music, called "Klinga mina klockor" ("Ring my bells"), all new material inspired by Swedish folk music – and followed it with his 2nd album titled "November 1989". In the 1990s, Andersson wrote music for the popular Swedish cabaret quartet Ainbusk Singers, giving them two hits: "Lassie" and "Älska mig" ("Love me"), and later produced "Shapes", an English-language album by the group's Josefin Nilsson with all-new material by Andersson and Ulvaeus. Andersson has also regularly written music for films (most notably to Roy Andersson's "Songs from the Second Floor"). In 2001, Andersson formed his own band, BAO!, which released three successful albums in 2001, 2004 and 2007. Andersson has the distinction of remaining the longest in the Swedish Radio Svensktoppen charts; the song "Du är min man", "You Are My Man", sung by Helen Sjöholm, spent "278" weeks there between 2004 and 2009. Andersson released his third album BAO 3 in October 2007 with new material with his band BAO! and vocalists Helen Sjöholm and Tommy Körberg, as well as playing to full houses at two of Sweden's largest concert venues in October and November 2007 with an audience of 14,000. Björn Ulvaeus has not appeared on stage performing music since ABBA, but had a reunion with his co-members of The Hootenanny Singers on 16 July 2005 at a music festival in his hometown of Västervik, singing their 1966 hit "Marianne". Andersson and Ulvaeus are highly involved in the worldwide productions of the musical Mamma Mia!, alongside Lyngstad who attends premieres. They were also involved in the production of the successful film version of the musical, which opened in July 2008. Andersson produced the soundtrack utilising many of the musicians ABBA used on their albums and tours. Andersson made a cameo appearance in the movie as a 'fisherman' piano player in the 'Dancing Queen' scene, while Ulvaeus is seen as a Greek god playing a lyre during the closing credits. Andersson and Ulvaeus are continuously composing new material; most recently the two wrote the title track for Andersson's first international release, "The Benny Andersson Band – The Story Of a Heart", released in July 2009. The album is a compilation of 14 tracks from Andersson's five Swedish-language releases, including five songs now recorded in English, and the new title song premiered on BBC2's Ken Bruce Show Monday on 25 May. A Swedish-language version, "Sommaren Du Fick" ('The Summer You Got'), was released in Sweden prior to the English version, with vocals by Helen Sjöholm. In the spring of 2009, Andersson also released a single recorded by the staff at his privately owned Stockholm hotel "Hotel Rival", titled "2nd Best to None", accompanied by a video showing the staff at work. In 2008 the two wrote a song for Swedish singer Sissela Kyle titled "Jag vill bli gammal" ("I Wanna Grow Old"); for her Stockholm stage show "Your Days Are Numbered", in 2007 they wrote "Han som har vunnit allt" ("He Who's Won It All") for actor/singer Anders Ekborg and "I Walk With You Mama" and "After the Rain" for opera singer Anne Sofie Von Otter for her Andersson tribute album "I Let The Music Speak".
Both female members of ABBA pursued solo careers on the international scene after their work with ABBA. In 1982, Lyngstad chose Genesis drummer and singer Phil Collins to produce the album "Something's Going On" and unveiled the hit single and video " I Know There's Something Going On" in autumn of that year. The single became a #1 hit in France, where it spent five weeks at the top, Belgium, Switzerland and Costa Rica. The track reached #3 in Austria, the Netherlands, Norway, Sweden and Poland, and was also a Top 10 hit in Germany, Italy, South Africa and Finland. In the United States, the single reached #13. In all, "I Know There's Something Going On" sold 3.5 million copies worldwide and is the biggest selling single any of the four members have had outside ABBA. Lyngstad's album sold 1.5 million copies internationally. Sveriges Television, documented this historical event, by filming the whole recording process. The result became a one-hour TV documentary, including interviews with Frida, Phil, Björn, and Benny as well as all the musicians. This documentary and the promotion videos from the album are included in "Frida - The DVD". Frida's second solo album after ABBA was the experimental "Shine", produced by Steve Lillywhite. "Shine" was recorded in Paris and released in 1984. "Shine" reached the Top 10 on the album charts in Sweden, Norway and Belgium and the Top 20 in the Netherlands. The leadsingle was the title track "Shine". This album was Frida's final studio album release for twelve years. It featured "Slowly", the last known Andersson-Ulvaeus composition to have been recorded by one of the former female ABBA vocalists. The promotion videos and clips for "Shine" are included in Frida - The DVD. Agnetha Fältskog followed in 1983 with the album "Wrap Your Arms Around Me". This included the hit single "The Heat Is On", which was a hit in Europe and Scandinavia, while the title track went to number 2 in South Africa. In the US, Fältskog scored a Billboard Top 30 hit with "Can't Shake Loose". In Europe, the single "Wrap Your Arms Around Me" was another successful hit, topping the charts in Belgium and Denmark, reaching the Top 5 in Sweden and the Netherlands and the Top 20 in Germany and France. Her album sold 1.2 million copies worldwide. Fältskog's second post-ABBA solo album was "Eyes of a Woman", released in March 1985, which reached #2 in Sweden and performed reasonably well in Europe. The first single from the album was her self-penned "I Won't Let You Go". In November 1987, Fältskog released her third post-ABBA solo album, the Peter Cetera-produced "I Stand Alone", (which also included the Billboard adult contemporary duet with Cetera, "I Wasn't the One (Who Said Goodbye)" as well as the European charting singles "The Last Time" and "Let It Shine". The album sold very well in Sweden, where it spent eight weeks at #1. Later that year, however, Fältskog withdrew from public life and halted her music career for a while. In 1996, she released her autobiography, "As I Am", and a compilation album featuring her solo hits alongside some ABBA classics. In 2004, she made a successful comeback, releasing the critically acclaimed album "My Colouring Book", which debuted at #1 in Sweden (achieving triple-platinum status), #6 in Germany, and #12 in the UK, winning a silver award, and achieving gold status in Finland. The single "If I Thought You'd Ever Change Your Mind" (a cover of the Cilla Black 60's song) became Fältskog's biggest solo hit in the UK, reaching the #11 position. The single achieved the #2 spot in Sweden and was a hit throughout Scandinavia and Europe. A further single "When You Walk in the Room" was released and peaked at no. 34 in the UK. In January 2007, she sang a live duet on stage with Swedish singer Tommy Körberg at the after party for the final showing of the musical, "Mamma Mia!", in Stockholm, at which Benny Andersson and Björn Ulvaeus were also present. In 1992 Frida was asked and chosen to be the chairperson for the environmental organisation "Artister för miljön" (Artists for the Environment) in Sweden. Frida accepted and became chairwoman for this organisation from 1992 to 1995. To mark her interests for the environment, she recorded the Julian Lennon song "Saltwater" and performed it live in Stockholm. She arranged and financed summer camps for poor children in Sweden, focusing on environmental and ecological issues. Her environmental work for this organisation led up to the decision to record again. "Djupa andetag" ("Deep Breaths") was released towards the end of 1996 and became a huge success in Sweden, where it reached #1 and Scandinavia. The lyrics for the single from this album, "Även en blomma" (Even a Flower), deal with environmental issues.In 2004, Lyngstad recorded a song called "The Sun Will Shine Again", written especially for her and released with former Deep Purple member Jon Lord. The couple made several TV performances with this song in Germany. Lyngstad lives a low-profile life but occasionally appears at a party or charity function. On 26 August 1992, she married Prince Heinrich Ruzzo Reuss von Plauen, of the German Reuss family. Von Plauen died of lymphoma at the age of 49. In addition to losing her husband, Lyngstad had also lost her daughter in a car crash a year earlier. On 15 November 2005, due to Anni-Frid Lyngstad's 60th birthday, Universal released the Frida Box Set, consisting of the solo albums she recorded for the Polar Label. Included is also the 3 1/2 hour documentary Frida - The DVD. On this DVD, which covers Lyngstad's entire singing career, the viewer is guided by Frida herself through the years. From her TV debut in Sweden 1967 to the TV performances she made in Germany 2004, singing "The Sun Will Shine Again" together with Jon Lord of rock group Deep Purple. Many rare clips are included in the set and each performance is explained by Lyngstad herself. The interview with Lyngstad was filmed in the Swiss Alps, summer 2005.
The same year the members of ABBA went their separate ways, the French production of a 'tribute' show; a children's TV musical named "Abbacadabra", using 14 of ABBA's songs, spawned new interest in the group's music. The London staging of the musical had stars such as Elaine Paige and Finola Hughes singing new lyrics to the old ABBA hits. After receiving little attention during the mid 1980s, ABBA's music experienced a resurgence in the early 1990s due to the UK synth-pop duo Erasure who released a cover EP featuring versions of ABBA's songs which topped the charts in the spring of 1992. As U2 arrived in Stockholm for a concert in June of that year, the band paid homage to ABBA by inviting Björn Ulvaeus and Benny Andersson to join them on stage for a rendition of "Dancing Queen", playing guitar and keyboards. September 1992 saw the release of ', a new compilation album, which became a massive worldwide seller. The album became the most popular ABBA release ever, selling more than twenty-eight million copies to date and setting chart longevity records. The enormous interest in the "Gold" compilation saw the release of ' in 1993. This collection also contained the bonus track "I Am the City", one of the unreleased songs from the 1982 recording sessions. In 1994, two Australian movies caught the attention of the world's media, both focussing on admiration for ABBA: "The Adventures of Priscilla, Queen of the Desert" and "Muriel's Wedding". The same year, "Thank You for the Music", a four-disc box set comprising all the group's hits and stand-out album tracks, was released with the involvement of all four members. For this release, several demo versions and odd tracks were discovered in the Polar vaults. ABBA were soon recognised and embraced by other acts: Evan Dando of The Lemonheads recorded a cover version of "Knowing Me, Knowing You", Sinéad O'Connor and Boyzone's Stephen Gately have recorded "Chiquitita", Tanita Tikaram, and Blancmange paid tribute to "The Day Before You Came", Cliff Richard covered "Lay All Your Love On Me", while Dionne Warwick and Peter Cetera recorded their versions of "SOS". U.S. alternative-rock musician Marshall Crenshaw has also been known to play a version of "Knowing Me, Knowing You" in concert appearances, while legendary English Latin pop songwriter Richard Daniel Roman has recognized ABBA as a major influence. Swedish metal guitarist Yngwie Malmsteen covered "Gimme! Gimme! Gimme! (A Man After Midnight)" with slightly altered lyrics. Tribute albums were released both in Sweden and the UK. Two different compilation albums of ABBA songs have been released. "ABBA: A Tribute" coincided with the 25th anniversary celebration and featured 17 songs, some of which were recorded especially for this release. Notable tracks include Go West's "One Of Us", Army Of Lovers "Hasta Manana", Information Society's "Lay All Your Love On Me", Erasure's "Take A Chance On Me" (with MC Kinky), and Frida's a cappella duet with The Real Group on "Dancing Queen". A second 12-track album was released in 1999, entitled "ABBAMANIA", with proceeds going to the Youth Music charity in England. It featured all new cover versions, notable tracks were by Madness ("Money, Money, Money"), Culture Club ("Voulez-Vous"), The Corrs ("The Winner Takes It All"), Steps ("Lay All Your Love On Me", "I Know Him So Well") and a medley entitled "Thank ABBA For The Music" performed by several artists and as featured on the Brits Awards that same year. This compilation was envisioned and executive produced by Pete Waterman of Stock, Aiken, & Waterman fame. In Sweden, the growing recognition of the legacy of Andersson and Ulvaeus resulted in the 1998 "B & B Concerts": a tribute concert (with Swedish singers who had worked with the composers through the years) showcasing not only their ABBA years, but even hits from the 1960s and after ABBA. The concert was a huge success, released on CD, and later toured Scandinavia and even went to Beijing in the People's Republic of China for two concerts. In 1999, Sweden saw the birth of ABBA Teens, later re-named A*Teens, recording techno-pop versions of ABBA songs to huge success worldwide: not only the English original versions, but ABBA's Spanish versions also. In April 1999, the "Mamma Mia!" musical opened in London, and soon premièred in cities worldwide to huge success. In 2000, ABBA were reported to have turned down an offer of approximately US$1,000,000,000 (one billion US dollars) to do a reunion tour consisting of 100 concerts. For the 2004 semi-final of the Eurovision Song Contest, staged in Istanbul thirty years after ABBA had won the contest in Brighton, all four members of ABBA appeared briefly in a special comedy video made for the interval act, entitled "Our Last Video Ever". Each of the four members of the group made a brief cameo role, as did others such as Cher and Rik Mayall. The video was not included in the official DVD release of the Eurovision Contest, but was issued as a separate DVD release, retitled "The Last Video" at the request of the former ABBA members. Although it was billed as the first time the four had worked together since the group split, each member was filmed separately. In 2005, all four members of ABBA appeared at the Stockholm premiere of the musical Mamma Mia. With "Mamma Mia!s huge success worldwide, and the 2008 film starring Meryl Streep and Pierce Brosnan, there is a huge interest in ABBA's music. However, in a November 2004 interview with the German magazine "Bunte", Ulvaeus said a reunion would not satisfy ABBA's many fans, even though there are legions of them around the world often clamouring for one. On 4 July 2008, all four ABBA members were reunited at the Swedish premiere of the film "Mamma Mia!". It was only the second time all of them had appeared together in public since 1986. During the appearance, they re-emphasized that they intended never to officially reunite, citing the opinion of Robert Plant that the re-formed Led Zeppelin was more like a cover band of itself than the original band. Ulvaeus stated that he wanted the band to be remembered as they were during the peak years of their success. The compilation album ', originally released in 1992, returned to number one in the UK album charts for the fifth time on 3 August 2008. On 14 August 2008, the Mamma Mia! The Movie film soundtrack went to number 1 on the USA Billboard Charts. While ABBA were together, the highest album chart position they ever achieved in America was No. 14. The year 2008 was the first time an "ABBA" album went to the top of the American record charts. Most recently all eight studio albums, together with a ninth of rare tracks, have been released as ABBA The Albums It hit several charts, peaking at #4 in Sweden and reaching the top 10 in several other European territories. UK release was Monday, 24 November 2008. In 2008, Sony Computer Entertainment Europe, in collaboration with Universal Music Group Sweden AB, released SingStar ABBA on both PlayStation 2 and PlayStation 3 video game systems as part of the SingStar music video games. The game features 20 ABBA songs on PS2 and 25 on PS3, most of them #1 hits. The game was released worldwide and as a stand-alone game. On 22 January 2009, Agnetha Fältskog and Anni-Frid Lyngstad showed up together to receive the Swedish music award "Rockbjörnen" (for "lifetime achievement") and gave an interview onstage; the two wanted to express gratitude for the honorary award and to thank their fans. They also commented with concern on the old rumour that the two weren't friends. On 25 November 2009, PRS for Music announced that the British public voted ABBA as the band they would most like to reform. On episode of The Suite Life on Deck, "The Swede Life", which aired on December 4, 2009, Mr. Moseby and Marcus wanted to check out the museum honoring the band. On 27 January 2010, ABBAWORLD, a 25-room touring exhibition featuring interactive and audiovisual activities is scheduled to debut at Earl's Court Exhibition Centre in London, England. According to the exhibitions website ABBA world is "approved and fully supported" by the group. On 15 March 2010, ABBA will be officially inducted into the Rock & Roll Hall Of Fame.
ABBA were widely noted for the colourful and trend-setting costumes its members wore. The videos that accompanied some of their biggest hits are often cited as being among the earliest examples of the genre. Most of ABBA's videos (and "ABBA: The Movie") were directed by Lasse Hallström, who would later direct the films "My Life as a Dog", "The Cider House Rules" and "Chocolat". ABBA made videos because their songs were hits in many different countries and personal appearances were not always possible. This was also done in an effort to minimize traveling, particularly to countries that would have required extremely long flights. Fältskog and Ulvaeus had two young children and Fältskog, who was also afraid of flying, was very reluctant to leave her children for such a long time. ABBA's manager, Stig Anderson, realized the potential of showing a simple video clip on television to publicize a single or album, thereby allowing easier and quicker exposure than a concert tour. Some of these videos became classics because of the 1970s-era costumes and early video effects, such as the grouping of the band members in different combinations of pairs, overlapping one singer's profile with the other's full face, and the contrasting of one member against another. The director somehow managed to produce the videos with acts that seem to be integrated with the music. In 1976, ABBA participated in a high-profile advertising campaign by the Matsushita Electric Industrial Co., Ltd., which was designed to promote the brand "National". This campaign was designed initially for Australia, where "National" was still the primary brand used by Matsushita, who had not introduced the "Panasonic" brand to Australia yet despite its widespread use in other parts of the world such as the United States. However, the campaign was also aired in Japan. Five commercials, each approximately one minute long, were produced, each using the "National Song" sung by ABBA, which used the melody and instrumental arrangement of Fernando, adapted with new lyrics promoting National, and working in several slogans used by National in their advertising. In 2008, United States Senator John McCain wanted to use the group's music in his 2008 presidential campaign but opted against it, citing licensing and other concerns.
The term "allegiance" was traditionally often used by English legal commentators in a larger sense, divided by them into natural and local, the latter applying to the deference which even a foreigner must pay to the institutions of the country in which he happens to live. However it is in its proper sense, in which it indicates national character and the subjection due to that character, that the word is more important. In that sense it represents the feudal liege homage, which could be due only to one lord, while simple homage might be due to every lord under whom the person in question held land.
The English doctrine, which was at one time adopted in the United States, asserted that allegiance was indelible: "Nemo potest exuere patriam". Accordingly, as the law stood before 1870, every person who by birth or naturalization satisfied the conditions set forth, though he should be removed in infancy to another country where his family resided, owed an allegiance to the British crown which he could never resign or lose, except by act of parliament or by the recognition of the independence or the cession of the portion of British territory in which he resided. Allegiance is the tie which binds the subject to the Sovereign in return for that protection which the Sovereign affords the subject. It was the mutual bond and obligation between monarch and subjects, whereby subjects are called his liege subjects, because they are bound to obey and serve him; and he is called their liege lord, because he should maintain and defend them ("Ex parte Anderson" (1861) 3 El & El 487; 121 ER 525; "China Navigation Co v Attorney-General" (1932) 48 TLR 375; "Attorney-General v Nissan" [1969] 1 All ER 629; "Oppenheimer v Cattermole" [1972] 3 All ER 1106). The duty of the Crown towards its subjects is to govern and protect. The reciprocal duty of the subject towards the Crown is that of allegiance. Natural allegiance and obedience is an incident inseparable to every subject, for as soon as the Sovereign is born, they owe allegiance and obedience ("Ex parte Anderson" (1861) 3 El & El 487; 121 ER 525). Natural-born subjects owe allegiance wherever they may be. Where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)). Allegiance is owed both to the Sovereign as a natural person and to the Sovereign in the political capacity ("Re Stepney Election Petition, Isaacson v Durant" (1886) 17 QBD 54 (per Lord Coleridge CJ)). Attachment to the person of the reigning Sovereign is not sufficient. Loyalty requires affection also to the office of the Sovereign, attachment to royalty, attachment to the law and to the constitution of the realm, and he who would, by force or by fraud, endeavour to prostrate that law and constitution, though he may retain his affection for its head, can boast but an imperfect and spurious species of loyalty ("R v O'Connell" (1844) 7 ILR 261). There were four kinds of allegiances ("Rittson v Stordy" (1855) 3 Sm & G 230; "De Geer v Stone" (1882) 22 Ch D 243; "Isaacson v Durant" (1886) 54 LT 684; "Gibson, Gavin v Gibson" [1913] 3 KB 379; "Joyce v DPP" [1946] AC 347; "Collingwood v Pace" (1661) O Bridg 410; "Lane v Bennett" (1836) 1 M & W 70; "Lyons Corp v East India Co" (1836) 1 Moo PCC 175; "Birtwhistle v Vardill" (1840) 7 Cl & Fin 895; "R v Lopez, R v Sattler" (1858) Dears & B 525; Ex p Brown (1864) 5 B & S 280); (a) "Ligeantia naturalis, absoluta, pura et indefinita", and this originally is due by nature and birthright, and is called "alta ligeantia", and those that owe this are called "subditus natus"; (b) "Ligeantia acquisita", not by nature but by acquisition or denization, being called a denizen, or rather denizon, because they are "subditus datus"; (c) "Ligeantia localis", by operation of law, when a friendly alien enters the country, because so long as they are in the country they are within the Sovereign's protection, therefore they owe the Sovereign a local obedience or allegiance ("R v Cowle" (1759) 2 Burr 834; "Low v Routledge" (1865) 1 Ch App 42; "Re Johnson, Roberts v Attorney-General" [1903] 1 Ch 821; "Tingley v Muller" [1917] 2 Ch 144; "Rodriguez v Speyer" [1919] AC 59; "Johnstone v Pedlar" [1921] 2 AC 262; "R v Tucker" (1694) Show Parl Cas 186; "R v Keyn" (1876) 2 Ex D 63; "Re Stepney Election Petn, Isaacson v Durant" (1886) 17 QBD 54); (d) A legal obedience, where a particular law requires the taking of an oath of allegiance by subject or alien alike. Natural allegiance was acquired by birth within the Sovereign's dominions (except for the issue of diplomats or of invading forces or of an alien in enemy occupied territory). The natural allegiance and obedience is an incident inseparable to every subject, for as soon as they are born they owe by birthright allegiance and obedience to the Sovereign ("Ex p. Anderson" (1861) 3 E & E 487). A natural-born subject owes allegiance wherever they may be, so that where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)). Acquired allegiance was acquired by naturalisation or denization. Denization, or "ligeantia acquisita", appears to be threefold ("Thomas v Sorrel" (1673) 3 Keb 143); Local allegiance was due by an alien while in the protection of the Crown. All friendly resident aliens incurred all the obligations of subjects ("The Angelique" (1801) 3 Ch Rob App 7). An alien, coming into a colony also became, temporarily a subject of the Crown, and acquired rights both within and beyond the colony, and these latter rights could not be affected by the laws of that colony ("Routledge v Low" (1868) LR 3 HL 100; 37 LJ Ch 454; 18 LT 874; 16 WR 1081, HL; "Reid v Maxwell" (1886) 2 TLR 790; "Falcon v Famous Players Film Co" [1926] 2 KB 474). A resident alien owed allegiance even when the protection of the Crown was withdrawn owing to the occupation of an enemy, because the absence of the Crown's protection was temporary and involuntary ("de Jager v Attorney-Geneneral of Natal" [1907] AC 326). Legal allegiance was due when an alien took an oath of allegiance required for a particular office under the Crown. By the Naturalization Act 1870, it was made possible for British subjects to renounce their nationality and allegiance, and the ways in which that nationality is lost are defined. So British subjects voluntarily naturalized in a foreign state are deemed aliens from the time of such naturalization, unless, in the case of persons naturalized before the passing of the act, they have declared their desire to remain British subjects within two years from the passing of the act. Persons who from having been born within British territory are British subjects, but who at birth became under the law of any foreign state subjects of such state, and also persons who though born abroad are British subjects by reason of parentage, may by declarations of alienage get rid of British nationality. Emigration to an uncivilized country leaves British nationality unaffected: indeed the right claimed by all states to follow with their authority their subjects so emigrating is one of the usual and recognized means of colonial expansion.
The doctrine that no man can cast off his native allegiance without the consent of his sovereign was early abandoned in the United States, and on July 27, 1868, the day before the Fourteenth Amendment was adopted, U.S. Congress declared in the preamble of the Expatriation Act that "the right of expatriation is a natural and inherent right of all people, indispensable to the enjoyment of the rights of life, liberty and the pursuit of happiness," and (Section I) one of "the fundamental principles of this government" (United States Revised Statutes, sec. 1999). Every natural-born citizen of a foreign state who is also an American citizen and every natural-born American citizen who is a citizen of a foreign land owes a double allegiance, one to the United States, and one to his homeland (in the event of an immigrant becoming a citizen of the US), or to his adopted land (in the event of an emigrant natural born citizen of the US becoming a citizen of another nation). If these allegiances come into conflict, he or she may be guilty of treason against one or both. If the demands of these two sovereigns upon his duty of allegiance come into conflict, those of the United States have the paramount authority in American law; likewise, those of the foreign land have paramount authority in their legal system. In such a situation, it may be incumbent on the individual to abjure one of his citizenships to avoid possibly being forced into situations where countervailing duties are required of him, such as might occur in the event of war.
The oath of allegiance is an oath of fidelity to the sovereign taken by all persons holding important public office and as a condition of naturalization. By ancient common law it might be required of all persons above the age of twelve, and it was repeatedly used as a test for the disaffected. In England it was first imposed by statute in the reign of Elizabeth I of England (1558) and its form has more than once been altered since. Up to the time of the revolution the promise was, "to be true and faithful to the king and his heirs, and truth and faith to bear of life and limb and terrene honour, and not to know or hear of any ill or damage intended him without defending him therefrom." This was thought to favour the doctrine of absolute non-resistance, and accordingly the convention parliament enacted the form that has been in use since that time - "I do sincerely promise and swear that I will be faithful and bear true allegiance to His Majesty..."
An absolute majority or majority of the entire membership (in American English, a supermajority voting requirement) is a voting basis which usually requires that more than half of "all" the members of a group (including those absent and those present but not voting) must vote in favour of a proposition in order for it to be passed. In practical terms, it may mean that abstention from voting could be equivalent to a "no" vote. Absolute majority can be contrasted with simple majority which only requires a majority of those actually voting to approve a proposition for it to be enacted. Absolute majority voting is most often used to pass significant changes to constitutions or to by-laws in order to ensure that there is substantial support for a proposal.
The MessagePad was the first series of personal digital assistant devices developed by Apple Computer (now Apple Inc.) for the Newton platform in 1993. Some electronic engineering and the manufacture of Apple's MessagePad devices was done in Japan by the Sharp Corporation. The devices were based on the ARM 610 RISC processor and all featured handwriting recognition software and were developed and marketed by Apple. The devices ran the Newton OS.
With the MessagePad 120 with Newton OS 2.0, the Newton Keyboard by Apple became available, which can also be used via the dongle on Newton devices with a Newton InterConnect port, most notably the Apple MessagePad 2000/2100 series, as well as the Apple eMate 300. Newton devices featuring Newton OS 2.1 or higher can be used with the screen turned horizontally ("landscape") as well as vertically ("portrait"). A change of a setting instantly rotates the contents of the display by 90, 180 or 270 degrees. Handwriting recognition still works properly with the display rotated, although display calibration is needed when rotation in any direction is used for the first time or when the Newton device is reset.
In initial versions (Newton OS 1.x) the handwriting recognition gave extremely mixed results for users and was sometimes inaccurate. The original handwriting recognition engine was called Calligrapher, and was licensed from a Russian company called Paragraph International. Calligrapher's design was quite sophisticated; it attempted to learn the user's natural handwriting, using a database of known words to make guesses as to what the user was writing, and could interpret writing anywhere on the screen, whether hand-printed, in cursive, or a mix of the two. By contrast, Palm Pilot's Graffiti had a less sophisticated design than Calligrapher, but was sometimes found to be more accurate and precise due to its reliance on a fixed, predefined stroke alphabet. The stroke alphabet used letter shapes which resembled standard handwriting, but which were modified to be both simple and very easy to differentiate. Palm Computing also released two versions of Graffiti for Newton devices. Ironically, the Newton version sometimes performed better and could also show strokes as they were being written as input was done on the display itself, rather than on a silkscreen area. For editing text, Newton had a very intuitive system for handwritten editing, such as scratching out words to be deleted, circling text to be selected, or using written carets to mark inserts. Later releases of the Newton operating system retained the original recognizer for compatibility, but added a hand-printed-text-only (not cursive) recognizer, called "Rosetta", which was developed by Apple, included in version 2.0 of the Newton operating system, and refined in Newton 2.1. Rosetta is generally considered a significant improvement and many reviewers, testers, and most users consider the Newton 2.1 handwriting recognition software better than any of the alternatives even 10 years after it was introduced. Recognition and computation of handwritten horizontal and vertical formulas such as "1 + 2 =" was also under development but never released. However, users wrote similar programs which could evaluate mathematical formulas using the Newton OS Intelligent Assistant, a unique part of every Newton device. A vital feature of the Newton handwriting recognition system is the modeless error correction. That is, correction done in situ without using a separate window or widget, using a minimum of gestures. If a word is recognized improperly, the user could double-tap the word and a list of alternatives would pop up in a menu under the stylus. Most of the time, the correct word will be in the list. If not, a button at the bottom of the list allows the user to edit individual characters in that word. Other pen gestures could do such things as transpose letters (also in situ). The correction popup also allowed the user to revert to the original, un-recognized letter shapes - this would be useful in note-taking scenarios if there was insufficient time to make corrections immediately. To conserve memory and storage space, alternative recognition hypotheses would not be saved indefinitely. If the user returned to a note a week later, for example, they would only see the best match. Error correction in many current handwriting systems provides such functionality but adds more steps to the process, greatly increasing the interruption to a user's workflow that a given correction requires.
Text could also be entered by tapping with the stylus on a small on-screen pop-up QWERTY virtual keyboard, although more layouts were developed by users. Newton devices could also accept free-hand "Sketches", "Shapes", and "Ink Text", much like a desktop computer graphics tablet. With "Shapes", Newton could recognize that the user was attempting to draw a circle, a line, a polygon, etc, and it would clean them up into perfect vector representations (with modifiable control points and defined vertices) of what the user was attempting to draw. "Shapes" and "Sketches" could be scaled or deformed once drawn. "Ink text" captured the user's free-hand writing but allowed it to be treated somewhat like recognized text when manipulating for later editing purposes ("ink text" supported word wrap, could be formatted to be bold, italic, etc). At any time a user could also direct their Newton device to recognize selected "ink text" and turn it into recognized text (deferred recognition). A Newton note (or the notes attached to each contact in Names and each Dates calendar or to-do event) could contain any mix of interleaved text, Ink Text, Shapes, and Sketches.
The MessagePad 100 series of devices used the Macintosh-standard serial ports—round Mini-DIN 8 connectors. The MessagePad 2000/2100 models (as well as the eMate 300) have a small, proprietary "Newton InterConnect" port. However, the development of the software platform was canceled by Steve Jobs on February 27, 1998, so the InterConnect port, while itself very advanced, can only be used to connect a serial dongle. A prototype multi-purpose InterConnect device containing serial, audio in, audio out, and other ports was also discovered. In addition, all Newton devices have infrared connectivity, initially only the Sharp ASK protocol, but later also IrDA, though the Sharp ASK protocol was kept in for compatibility reasons. Unlike the Palm Pilot, all Newton devices are equipped with a standard PC Card expansion slot (two on the 2000/2100). This allows native modem and even Ethernet connectivity; Newton users have also written drivers for 802.11b wireless networking cards and ATA-type flash memory cards (including the popular CompactFlash format), as well as for Bluetooth cards. Newton can also dial a phone number through the built-in speaker of the Newton device by simply holding a telephone handset up to the speaker and transmitting the appropriate tones. Fax and printing support is also built in at the operating system level, although it requires peripherals such as parallel adapters, PCMCIA cards, or serial modems, the most notable of which is the lightweight Newton Fax Modem released by Apple in 1993. It is powered by 2 AA batteries, and can also be used with a power adapter. It provides data transfer at 2400 bps, and can also send and receive fax messages at 9600 and 4800 bps respectively.
The original Apple MessagePad and MessagePad 100 used four AAA batteries. They were eventually replaced by AA batteries with the release of the Apple MessagePad 110. The use of 4 AA NiCd (MessagePad 110, 120 and 130) and 4x AA NiMH cells (MP2x00 series, eMate 300) give a runtime of up to 30 hours (MP2100 with two 20 MB Linear Flash memory PC Cards, no backlight usage) and up to 24 hours with backlight on. While adding more weight to the handheld Newton devices than AAA batteries or custom battery packs, the choice of an easily replaceable/rechargeable cell format gives the user a still unsurpassed runtime and flexibility of power supply. This, together with the flash memory used as internal storage starting with the Apple MessagePad 120 (if all cells lost their power, no data was lost due to the non-volatility of this storage), gave birth to the slogan "Newton never dies, it only gets new batteries".
The original Apple MessagePad and MessagePad 100 were limited by the very short lifetime of their inadequate AAA batteries. Critics also panned the handwriting recognition that was available in the debut models, which had been trumpeted in the Newton's marketing campaign. It was this problem that was skewered in the Doonesbury comic strips and the animated television series The Simpsons. Not even the word 'freckles' was in the dictionary, though the user could add it themselves. The real case was that the advanced Calligrapher handwriting recognition software needed to adjust to the user's handwriting; this process could take anywhere from two weeks to two months. Another factor which limited the early Newton devices' appeal was that desktop connectivity was not included in the basic retail package, a problem that was later solved with 2.x Newton devices - these were bundled with a serial cable and the appropriate Newton Connection Utilities software. Later versions of Newton OS offered improved handwriting recognition, quite possibly a leading reason for the continued popularity of the devices among Newton users. Even given the age of the hardware and software, Newtons still demand a sale price on the used market far greater than that of comparatively aged PDAs produced by other companies. In 2006 CNET compared an Apple MessagePad 2000 to a Samsung Q1, and the Newton was declared better. In 2009, CNET compared an Apple MessagePad 2000 to an iPhone, and the Newton was still declared better.
If one removes all patches to the eMate 300 (by replacing the ROM chip, and then putting in the original one again, as the eMate and the MessagePad 2000/2100 devices erase their memory completely after replacing the chip), the result will be the Newton OS saying that this is version 2.2.00. Also, the Original MessagePad and the MessagePad 100 share the same model number, as they only differ in the ROM chip version. (The OMP has OS versions 1.0 to 1.05, or 1.10 to 1.11, while the MP100 has 1.3 that can be upgraded with various patches.)
There were a number of projects that used the Newton as a portable information device in cultural settings such as museums. For example, Visible Interactive created a walking tour in San Francisco's Chinatown but the most significant effort took place in Malaysia at the Petronas Discovery Center, known as Petrosains. In 1995, an exhibit design firm, DMCD Inc., was awarded the contract to design a new 100,000 square foot (9300 m²) science museum in the Petronas Towers in Kuala Lumpur. A major factor in the award was the concept that visitors would use a Newton device to access additional information, find out where they were in the museum, listen to audio, see animations, control robots and other media, and to bookmark information for printout at the end of the exhibit. The device became known as the ARIF, a Malay word for "wise man" or "seer" and it was also an acronym for A Resourceful Informative Friend. Some 400 ARIFS were installed and over 300 are still in use today. The development of the ARIF system was extremely complex and required a team of hardware and software engineers, designers, and writers. ARIF is an ancestor of the PDA systems used in museums today and it boasted features that have not been attempted since.
Born on a farm in Edenburg, a Russian Mennonite community east of Gretna, Manitoba, Canada, Van Vogt is one of the most popular and highly esteemed writers of the Golden Age of Science Fiction. After starting his writing career by writing for 'true confession' style pulp magazines like "True Story", Van Vogt decided to switch to writing something he enjoyed, science fiction. Van Vogt's first published SF story, "Black Destroyer" ("Astounding Science Fiction", July 1939), was inspired by "On the Origin of Species" by Charles Darwin. The story depicted a fierce, carnivorous alien stalking the crew of an exploration spaceship. It was the cover story of this issue of "Astounding", the issue often described as having ushered in the Golden Age of science fiction. The story became an instant classic and eventually served as the inspiration for a number of science fiction movies. In 1950 it was combined with "War of Nerves" (1950), "Discord in Scarlet" (1939) and "M33 in Andromeda" (1943) to form the novel "The Voyage of the Space Beagle" (1950). In 1941 Van Vogt decided to become a full time writer, quitting his job at the Canadian Department of National Defence. Extremely prolific for a few years, Van Vogt wrote a large number of short stories. In the 1950s, many of them were retrospectively patched together into novels, or "fixups" as he called them, a term which entered the vocabulary of science fiction criticism. When the original stories were related (e.g. "The War against the Rull") this was often successful. When not (e.g. "Quest for the Future") the disparate stories thrown together generally made for a less coherent plot. One of Van Vogt's best-known novels of this period is "Slan", which was originally serialised in "Astounding Science Fiction" (September - December 1940). Using what became one of Van Vogt's recurring themes, it told the story of a 9-year-old superman living in a world in which his kind are slain by "Homo sapiens". A sequel, "Slan Hunter", was prepared by his widow, Lydia Van Vogt, and Kevin J. Anderson, starting from an incomplete draft and outline left by the late van Vogt. It was published in July 2007 (ISBN 978-0765316752). Lydia Van Vogt had already given permission to publish her online.
In 1944, Van Vogt moved to Hollywood, California, where his writing took on new dimensions after World War II. Van Vogt was always interested in the idea of all-encompassing systems of knowledge (akin to modern meta-systems) -- the characters in his very first story used a system called 'Nexialism' to analyze the alien's behaviour, and he became interested in the General Semantics of Alfred Korzybski. He subsequently wrote three novels merging these overarching themes, "The World of Null-A" and "The Pawns of Null-A" in the late 1940s, and "Null-A Three" in the early 1980s. "Null-A", or non-Aristotelian logic, refers to the capacity for, and practice of, using intuitive, inductive reasoning (compare fuzzy logic), rather than reflexive, or conditioned, deductive reasoning. Van Vogt was also profoundly affected by revelations of totalitarian police states that emerged after World War II. He wrote a mainstream novel that was set in Communist China, "The Violent Man" (1962); he said that to research this book he had read 100 books about China. Into this book he incorporated his view of "the violent male type", which he described as a "man who had to be right", a man who "instantly attracts women" and who he said were the men who "run the world". Van Vogt systematized his writing method, using scenes of 800 words or so where a new complication was added or something resolved. Several of his stories hinge upon temporal conundra, a favorite theme. He stated that he acquired many of his writing techniques from three books, "Narrative Technique" by Thomas Uzzell, and "The Only Two Ways to Write a Story" plus "Twenty Problems of the Short-Story Writer", both by John Gallishaw. He said many of his ideas came from dreams; throughout his writing life he arranged to be awakened every 90 minutes during his sleep period so he could write down his dreams. In the 1950s, Van Vogt briefly became involved in L. Ron Hubbard's projects. Van Vogt operated a storefront for Dianetics, the secular precursor to Hubbard's Church of Scientology, in the Los Angeles area for a time, before winding up at odds with Hubbard and his methods. His writing more or less stopped for some years, a period in which he bitterly claimed to have been harassed and intimidated by Hubbard's followers. In this period he was limited to collecting old short stories to form notable fixups like: "The Mixed Men" (1952), "The War Against the Rull" (1959), "The Beast" (1963) and the two novels of the "Linn" cyle, which were inspired (like Asimov's Foundation series) by the fall of the Roman Empire. He resumed writing again in the 1960s, mainly at Frederik Pohl's invitation, while remaining in Hollywood with his second wife, Lydia Bereginsky, who cared for him through his declining years. In this later period, his novels were conceived and written as unitary works. On 26 January 2000, van Vogt died in Los Angeles, USA from Alzheimer's disease.
In 1946, Van Vogt and his first wife, Edna Mayne Hull, were co-Guests of Honor at the fourth World Science Fiction Convention. In 1980, Van Vogt received a "Casper Award" (precursor to the Canadian Aurora Awards) for Lifetime Achievement. In 1995 he was awarded the Damon Knight Memorial Grand Master Award by the Science Fiction Writers of America. In 1996, Van Vogt was recognized on two occasions: the World Science Fiction Convention presented him with a Special Award "for six decades of golden age science fiction", and the Science Fiction and Fantasy Hall of Fame included him among its initial four inductees.
Fellow science fiction author Philip K. Dick has said that Van Vogt's stories spurred his interest in science fiction with their strange sense of the unexplained, that something more was going on than the protagonists realized. In "The John W. Campbell Letters", Campbell says, "The son-of-a-gun gets hold of you in the first paragraph, ties a knot around you, and keeps it tied in every paragraph thereafterincluding the ultimate last one." Harlan Ellison (who began reading van Vogt as a teenager) wrote, "Van was the first writer to shine light on the restricted ways in which I had been taught to view the universe and the human condition."
Anna Sergeyevna Kournikova (; born 7 June 1981) is a Russian professional tennis player and model. Her celebrity status made her one of the best known tennis players worldwide. At the peak of her fame, fans looking for images of Kournikova made her name one of the most common search strings on the Internet search engine Google. Although also successful in singles, reaching No. 8 in the world in 2000, Kournikova's specialty has been doubles, where she has at times been the World No. 1 player. With Martina Hingis as her partner, she won Grand Slam titles in Australia in 1999 and 2002. Kournikova's professional tennis career has been curtailed for the past several years, and possibly ended, by serious back and spinal problems. She currently resides in Miami Beach, Florida, and plays in occasional exhibitions and in doubles for the St. Louis Aces of World Team Tennis.
Anna was born in Moscow, Soviet Union, on 7 June 1981. Her father, Sergei Kournikov was 20 at the time. Sergei, a former Greco-Roman wrestling champion, had earned a Ph.D and was a professor at the University of Physical Culture and Sport in Moscow. As of 2001, he was still a part-time martial arts instructor there. Her mother Alla, a sturdily built blonde who was 18 when Anna was born, had been a 400-metre runner. Sergei said: "We were young and we liked the clean, physical life, so Anna was in a good environment for sport from the beginning." The family name is spelled in Russian without an "o", so a direct translation would be "Kurnikova", and it is sometimes written that way. But it is pronounced "Kournikova", so the family chose that as their English spelling. Anna received her first tennis racquet as a New Year gift in 1986 at age 5. Anna says: "I played two times a week from age five. It was a children's program. And it was just for fun; my parents didn't know I was going to play professionally, they just wanted me to do something because I had lots of energy. It was only when I started playing well at seven that I went to a professional academy. I would go to school, and then my parents would take me to the club, and I'd spend the rest of the day there just having fun with the kids." In 1986, Anna became a member of the prestigious Spartak Tennis Club, coached by Larissa Preobrazhenskaya. In 1989, at the age of eight, Anna began appearing in junior tournaments, and by the following year, was attracting attention from tennis scouts across the world. Anna signed a management deal at age ten and went to Bradenton, Florida to train at Nick Bollettieri's celebrated tennis academy. 1989–1997: Early years and breakthrough. Following her arrival in the United States, Anna exploded onto the tennis scene, making her the internationally recognized tennis star she is today. At the age of 14, she went on to win the European Championships and the Italian Open Junior tournament. Anna also beat out the competition to win the prestigious Junior Orange Bowl, becoming the youngest player ever to win the 18 and under division at that tournament. By the end of the year, Anna was crowned the ITF Junior World Champion U-18 and Junior European Champion U-18. In 1994, Anna Kournikova received a wild card into ITF tournament in Moscow qualifications, but lost to the third seed Sabine Appelmans. She debuted in professional tennis at age 14 in the Fed Cup for Russia, the youngest player ever to participate and win. In 1995, she turned pro, and won two ITF titles, in Midland, Michigan and Rockford, Illinois. The same year Kournikova reached her first WTA Tour doubles final at the Kremlin Cup. Partnering with 1995 Wimbledon girls' champion in both singles and doubles Aleksandra Olsza, she lost to Meredith McGrath and Larisa Neiland with 6–0, 6–1. At age 15, she made her grand slam debut, when she reached the fourth round of the 1996 U.S. Open, only to be stopped by then-top ranked player, Steffi Graf, eventual champion. After this tournament, her ranking jumped from No. 144 to debut in Top 100 at No. 69. Kournikova was a member of the Russian delegation to the 1996 Olympic Games in Atlanta, Georgia. In 1996, she was named WTA Newcomer of the Year, and she was ranked No. 57 in the end of the season. Kournikova entered the 1997 Australian Open as World No. 67. However, she lost in the first round to World No. 12 Amanda Coetzer 6–2, 6–2. She also partnered with Russian fellow Elena Likhovtseva at women's doubles event, but also lost in the first round, to eight seeds Chanda Rubin and Brenda Schultz-McCarthy 6–2, 6–3. Kournikova reached the second round at the Pacific Life Open; after beating Patricia Hy-Boulais in the first round 1–6, 6–1, 6–4, she lost to World No. 3 Anke Huber in the second round 3–6, 6–2, 6–2. In doubles, Kournikova and Likhovtseva beat the second seeds Larisa Neiland and Helena Suková in the second 7–5, 4–6, 6–3, before losing to Mary Joe Fernández and Chanda Rubin in the quartefinals 2–6, 6–4, 7–5/ At Miami Open Kournikova defeated No. 12 Amanda Coetzer 6–1, 3–6, 6–3 in the second round, and No. 29 Katarina Studenikova 1–6, 6–4, 6–0 in the third, and then lost to No. 3 Jana Novotná 6–3, 6–4 in the fourth. She and Likhovtseva were beaten 6–4, 6–3 by Dominique Monami and Barbara Rittner in the first round of Miami doubles. After beating Shi-Ting Wang in the first round of Italian Open with 6–3, 6–4, Kournikova lost to Amanda Coetzer 6–2, 4–6, 6–1 in the second. However, she reached the semifinals partnering with Likhovtseva, after they defeated the first seeds Neiland and Suková 6(4)–7, 6–2, 7–5 in the second round, and Barbara Schett and Patty Schnyder 7–6(2), 6–4 in the third, before losing to the sixth seeds Mary Joe Fernández and Patricia Tarabini 7–6(5), 6–3. Kournikova was then defeated in the quarterfinals of German Open by Mary Joe Fernández 6–1, 6–4, after she defeated former World No. 1 and current World No. 5 Arantxa Sánchez Vicario 3–6, 6–0, 6–3 in the third round. Partnering with Likhovtseva, she also reached the doubles quarterfinals, beating the sixth seeds Alexandra Fusai and Nathalie Tauziat 6–4, 7–6(2) in the second round and losing to the first seeds Gigi Fernández and Natasha Zvereva 6–2, 7–5. At the 1997 French Open Kournikova defeated Radka Zrubáková 6–3, 6–2 in the first round and Sandra Cecchini 6–2, 6–2 in the second and then lost to World No. 1 Martina Hingis 6–1, 6–3 in the third. She also reached the third round in doubles with Likhovtseva, losing to domestic team and the eight seeds Fusai and Tauziat. At 1997 Wimbledon Championships Anna Kournikova became the only second woman in the open era to reach the semifinals, which was also her first WTA Tour semifinals, in her Wimbledon debut, the first being Chris Evert in 1972. She defeated Chanda Rubin 6–1, 6–1 in the first round, Barbara Rittner 4–6, 7–6(7), 6–3 in the second, the seventh seed Anke Huber 3–6, 6–4, 6–4 in the third, Helena Suková 2–6, 6–2, 6–3 in the fourth, No. 4 and the French Open champion Iva Majoli 7–6(1), 6–4 in the quarterfinals and then lost to eventual champion Martina Hingis by a score of 6–3, 6–2. Kournikova then lost to Anke Huber 6–0, 6–1 in the first round of the Los Angeles Open, also reaching the doubles semifinals with Ai Sugiyama. At the 1997 US Open she lost in the second round to the eleventh seed Irina Spîrlea 6–1, 3–6, 6–3. Partnering with Likhovtseva, she reached the third round of women's doubles event, losing 6–4, 6–4 to the second seeds Hingis and Sánchez Vicario. Kournikova played her last WTA Tour event in 1997 at Porsche Tennis Grand Prix in Filderstadt, losing to Amanda Coetzer 3–6 6–3 6–4 in the second round of singles, and 6–2, 6–4 in the first round of doubles to Lindsay Davenport and Jana Novotná with Likhovtseva. She broke into the top 50 on 19 May, and was ranked No. 32 in singles and No. 41 in doubles at the end of the season.
In 1998 Kournikova broke into the WTA's top 20 rankings for the first time, when she was ranked No. 16. She also scored impressive victories over Martina Hingis, Lindsay Davenport, Steffi Graf and Monica Seles. Kournikova began her 1998 season in Hannover, where she lost to the first seed Jana Novotná in the semifinals with 6–3, 6–3. She also partnered Larisa Neiland in doubles, and lost to Elena Likhovtseva and Caroline Vis in the quarterfinals 3–6, 6–2, 7–5. She then reached the second round of both singles and doubles at the Medibank International in Sidney, losing to Lindsay Davenport in the second round of singles with 6–2, 6(4)–7, 6–3. At the 1998 Australian Open Kournikova lost in the third round to World No. 1 player Martina Hingis 6–4, 4–6, 6–4. She also partnered with Larisa Neiland in women's doubles, and they lost to eventual champions Hingis and Mirjana Lučić 7–5, 6–2 in the second round. Although she lost in the second round of Paris Open to Anke Huber in singles, Kournikova reached the her second doubles WTA Tour final partnering with Larisa Neiland. They lost to Sabine Appelmans and Miriam Oremans 1–6, 6–3, 7–6(3). Kournikova and Neiland reached their second consecutive final at Linz Open, losing to Alexandra Fusai and Nathalie Tauziat 6–3, 3–6, 6–4. In singles, Kournikova reached the third round. At the Pacific Life Open, she reached the third round and lost to 1994 Wimbledon champion Conchita Martínez 6–3, 6–4, and also reached the doubles quarterfinals with Neiland. Although she reached the doubles quarterfinals with Neiland, Kournikova made a greater success in singles at the Miami Open, reaching her first WTA Tour singles final. After beating Mirjana Lučić 6–4, 6–2 in the first round, former World No. 1 Monica Seles 7–5, 6–4 in the second, Conchita Martínez 6–3, 6–0 in the third, Lindsay Davenport 6–4, 2–6, 6–2 in the quarterfinals, and former No. 1 Arantxa Sánchez Vicario 3–6, 6–1, 6–3 in the semifinals, and then lost to Venus Williams in the final with 2–6, 6–4, 6–1. Kournikova then reached two consecutive quarterfinals, at Amelia Island and Italian Open, losing, respectively, 7–5, 6–3 to Lindsay Davenport, and 6–2, 6–4 to Martina Hingis. At the German Open, she reached the semifinals in both singles and doubles, with Larisa Neiland, losing 6–0, 6–1 to Conchita Martínez and 6–1, 6–4 to Alexandra Fusai and Nathalie Tauziat, respectively. At the 1998 French Open Kournikova reached her best result at this tournament, losing to Jana Novotná 6(2)–7, 6–3, 6–3 in the fourth round. She also reached her first grand slam doubles semifinals, losing with Neiland to Lindsay Davenport and Natasha Zvereva 6–3, 6–2. During her quarterfinals match at the Eastbourne Open versus Steffi Graf, Kournikova injured her thumb, which would eventually force her to withdraw from the 1998 Wimbledon Championships. However, she won that match 6(4)–7, 6–3, 6–4, but then withdraw from her semifinals match against Arantxa Sánchez Vicario. Kournikova returned for the Du Maurier Open, defeating Alexandra Fusai and Ruxandra Dragomir before losing to Conchita Martínez in the third round 6–0, 6–3. At the Pilot Pen Tennis in New Haven she lost to Amanda Coetzer 1–6, 6–4, 7–5 in the second round. At the 1998 US Open Kournikova reached the fourth round and lost to Arantxa Sánchez Vicario. She then made a series of low results at Toyota Princess Cup, Porsche Tennis Grand Prix, Zürich Open and Kremlin Cup, but due to good results during the year, she qualified for the 1998 WTA Tour Championships. She lost to Monica Seles in the first round 6–4, 6–3. However, with Seles, she won her first doubles title, in Tokyo, beating Mary Joe Fernández and Arantxa Sánchez Vicario 6–4, 6–4 in the final. At the end of the season, she was ranked No. 10 in doubles. Kournikova began her 1999 season at Adidas Open, where she lost to Dominique Van Roost in the second round. She then played at the Australian Open, losing to Mary Pierce 6–0, 6–4 in the fourth round. However, Kournikova won her first doubles grand slam title, partnering Martina Hingis. The two defeated Lindsay Davenport and Natasha Zvereva in the final. Kournikova then lost in the quarterfinals of Toray Pan Pacific Open to Monica Seles 7–5, 6–3. In Oklahoma City she was defeated by Amanda Coetzer 6–4, 6–2 in the semifinals, by Silvia Farina Elia in the first round of Evert Cup, and by Barbara Schett at Lipton Championships. At Tier I Family Circle Cup, Kournikova reached her second WTA Tour final, but lost to Martina Hingis 6–4, 6–3. She then defeated Jennifer Capriati, Lindsay Davenport and Patty Schnyder on her route to the Bausch & Lomb Championships semifinals, losing to Ruxandra Dragomir 6–3, 7–5. After round robin results at Italian Open and German Open, Kournikova reached the fourth round of 1999 French Open, losing to eventual champion Steffi Graf 6–3, 7–6. She then lost to Nathalie Tauziat 6–4, 4–6, 8–6 in the semifinals in Eastbourne. At 1999 Wimbledon Championships, Kournikova lost to Venus Williams in the fourth round 3–6, 6–3, 6–2. She also reached the 1999 Wimbledon final in mixed doubles, partnering with Jonas Björkman, but they lost to Leander Paes and Lisa Raymond 6–4, 3–6, 6–3. Kournikova qualified for 1999 WTA Tour Championships, but lost to Mary Pierce 6(3)–7, 7–6(5), 6–0 in the first round, and ended the season as World No. 12. Also at times during 1999, she was the most searched athlete in the world on Yahoo!, the premier search engine of the day. Kournikova was more successful in doubles that season. After their victory at the Australian Open, she and Martina Hingis won tournaments in Indian Wells, Rome, Eastbourne and 1999 WTA Tour Championships, and reached the final of 1999 French Open, where they lost to Serena and Venus Williams 3–6, 7–6(2), 6–8. Partnering with Elena Likhovtseva, Kournikova also reached the final in Stanford. On 22 November 1999 she reached World No. 1 ranking in doubles, and ended the season at this ranking. Anna Kournikova and Martina Hingis were presented with the WTA Award for Doubles Team of the Year. Kournikova opened her 2000 season winning the Gold Coast Open partnering with Julie Halard. She then reached the singles semifinals at Medibank International Sydney, losing to Lindsay Davenport 6–3, 6–2. At the 2000 Australian Open, she reached the fourth round in singles and the semifinals in doubles. Partnering with Barbara Schett, they lost to Lisa Raymond and Rennae Stubbs. That season, Kournikova reached eight semifinals (Sydney, Scottsdale, Stanford, San Diego, Luxembourg, Leipzig and 2000 WTA Tour Championships), seven quarterfinals (Gold Coast, Tokyo, Amelia Island, Hamburg, Eastbourne, Zürich and Philadelphia) and one final. Despite being a domestic player, Kournikova lost to Martina Hingis 6–3, 6–1 in the final of Kremlin Cup. On 20 November 2000 she finally broke into top 10 for the first time, reaching No. 8. She was also ranked #4 in doubles at the end of the season. Kournikova was, once again, more successful in doubles. She reached the final of the 2000 US Open in mixed doubles, partnering with Max Mirnyi, but they lost to Jared Palmer and Arantxa Sánchez Vicario 6–4, 6–3. She also won six doubles titles — Gold Coast (with Julie Halard), Hamburg (with Natasha Zvereva), Filderstadt, Zürich, Philadelphia and the 2000 WTA Tour Championships (with Martina Hingis). 2001–2003: Injuries and final years. This season was dominated by injury, including a left foot stress fracture which forced her withdrawal from twelve tournaments, including the French Open and Wimbledon. She underwent surgery in April. She reached her second career grand slam quarterfinals, at the Australian Open. Kournikova then withdrew from several events due to continuing problems with her left foot and did not return until Leipzig. With Barbara Schett, she won the doubles title in Sydney. She then lost in the finals in Tokyo, partnering with Iroda Tulyaganova, and at San Diego, partnering with Martina Hingis. Hingis and Kournikova also won the Kremlin Cup. At the end of the 2001 season, she was ranked #74 in singles and #26 in doubles. Kournikova was quite successful in 2002. She reached the semifinals of Auckland, Tokyo, Acapulco and San Diego, and the finals of China Open, losing 6–2, 6–3 to Anna Smashnova. This was Kournikova's last singles finals and the last chance to win a single title. With Martina Hingis, Anna Kournikova lost in the finals of Sydney, but they won their second grand slam title together, Australian Open in women's doubles. They also lost in the quarterfinals of U.S. Open. With Chanda Rubin, Anna Kournikova played the semifinals of Wimbledon, but they lost to Serena and Venus Williams. Partnering Janet Lee, she won the Shangai title. At the end of 2002 season, she was ranked #35 in singles and #11 in doubles. In 2003, Anna Kournikova collected her first grand slam match victory in two years at the Australian Open. She defeated Henrieta Nagyová in the 1st round, and then lost to Justine Henin-Hardenne in the 2nd round. She withdrew from Tokyo due to a sprained back suffered at Australian Open and did not return to Tour until Miami. Kournikova retired in the 1st round of the Charleston due to a left adductor strain. She reached the semifinals at the ITF tournament in Sea Island, before withdrawing from a match versus Maria Sharapova due to the adductor injury. She lost in the 1st round of the ITF tournament in Charlottesville. She did not compete for the rest of the season due to a continuing back injury At the end of the 2003 season and her professional career, she was ranked #305 in singles and #176 in doubles. Kournikova's two Grand Slam doubles titles came in 1999 and 2002, both at the Australian Open in the Women's Doubles event with partner Martina Hingis, with whom she played frequently starting in 1999. Kournikova proved a successful doubles player on the professional circuit, winning 16 tournament doubles titles, including two Australian Opens and being a finalist in mixed doubles at the U.S. Open and at Wimbledon, and reaching the No.1 ranking in doubles in the Women's Tennis Association tour rankings. Her pro career doubles record was 200–71. However, her singles career plateaued after 1999. For the most part, she managed to retain her ranking between 10 and 15 (her career high singles ranking was No.8), but her expected finals breakthrough failed to occur; she only reached four finals out of 130 singles tournaments, never in a Grand Slam event, and never won one. Her singles record is 209–129. Her final playing years were marred by a string of injuries, especially back injuries, which caused her ranking to erode gradually. As a personality Kournikova was among the most common search strings for both articles and images in her prime. She continues to be the most searched athlete in the world. 2004–present: Exhibitions and World Team Tennis. Kournikova has not played on the WTA Tour since 2003, but still plays exhibition matches for charitable causes. In late 2004, she participated in three events organized by Elton John and by fellow tennis players Serena Williams and Andy Roddick. In January 2005, she played in a doubles charity event for the Indian Ocean tsunami with John McEnroe, Andy Roddick, and Chris Evert. In November 2005, she teamed up with Martina Hingis, playing against Lisa Raymond and Samantha Stosur in the WTT finals for charity. Kournikova is also a member of the St. Louis Aces in the World Team Tennis (WTT), playing doubles only. In September 2008, Kournikova showed up for the 2008 Nautica Malibu Triathlon held at Zuma Beach in Malibu, California. The Race raised funds for children's Hospital Los Angeles. She won that race for women's K-Swiss team. On 27 September 2008, Kournikova played exhibition matches in Charlotte, North Carolina; she played two mixed doubles matches. She partnered Tim Wilkison and Karel Novacek. Kournikova and Wilkinson defeated Jimmy Arias and Chanda Rubin, and then Kournikova and Novacek defeated Chanda Rubin and Tim Wilkison. On 12 October 2008, Anna Kournikova played one exhibitional match for the annual charity event, hosted by Billie Jean King and Sir Elton John, raised more than $400,000 for the Elton John AIDS Foundation and Atlanta AIDS Partnership Fund. She played doubles with Andy Roddick (they were coached by Sir Elton John) versus Martina Navratilova and Jesse Levine (coached by Billie Jean King); Kournikova and Roddick won 5–4(3). Kournikova competed alongside John McEnroe, Tracy Austin and Jim Courier at the "Legendary Night", which was held on 2 May 2009, at the Turning Stone Event Center, Verona, NY. The legendary night of tennis consisted of a grudge match between McEnroe and Courier in singles followed by a mixed doubles match of McEnroe and Austin against Courier and Kournikova. She is the current K-Swiss spokesperson. In a feature for "ELLE" magazine's July 2005 issue, Kournikova stated that if she were 100% fit, she would like to come back and compete again.
As a player, Kournikova was noted for her footspeed and aggressive baseline play, and excellent angles and dropshots; however, her relatively flat, high-risk groundstrokes tended to produce frequent errors, and her serve was sometimes unreliable in singles. Kournikova holds her racket in her right hand but uses both hands when she plays backhand shots. She is a good player at the net. She can hit forceful groundstrokes and also drop shots. Her playing style fits the profile for a doubles player, and is complemented by her height. She has been compared to such doubles specialists as Pam Shriver and Peter Fleming.
Kournikova's marital status has been an issue on several occasions. There were conflicting rumors about whether she was engaged to ice hockey player Pavel Bure. There were reports that she married NHL ice hockey star Sergei Fedorov in 2001. Kournikova's representatives have denied this, but Fedorov stated in 2003 that the couple had married and since divorced. Kournikova started dating pop star Enrique Iglesias in late 2001 (she appeared in his video, "Escape"), and rumors that the couple had secretly married circulated in 2003 and again in 2005. Kournikova herself has consistently refused to directly confirm or deny the status of her personal relationships. But in May 2007, Enrique Iglesias was (mistakenly, as he would clarify later) quoted in the "New York Sun" that he had no intention of marrying Kournikova and settling down because they had split up. The singer would later deny these rumors of "divorce" or simply separation. In June 2008, Iglesias told the "Daily Star" that he had married Kournikova the previous year and that they are currently separated. Enrique has stated in interviews after that it was simply a joke, and they are still very much together. Kournikova has a younger brother, Allan. She became an American citizen in late 2009.
Most of Kournikova's fame has come from the publicity surrounding her personal life, as well as numerous modeling shoots. During Kournikova's debut at the 1996 U.S. Open at the age of 15, the world noticed her beauty, and soon pictures of her appeared in numerous magazines worldwide. In 2000, Kournikova became the new face for Berlei's shock absorber sports bras, and appeared in the highly successful "only the ball should bounce" billboard campaign. Photographs of her scantily-clad form have appeared in various men's magazines, including one in the much-publicized 2004 "Sports Illustrated Swimsuit Issue", where she posed in bikinis and swimsuits, and in other popular men's publications such as "FHM" and "Maxim". Kournikova was named one of "People's" 50 Most Beautiful People in 1998, 2000, 2002, and 2003 and was voted "hottest female athlete" and "hottest couple" (with Iglesias) on ESPN.com. In 2002 she also placed first in "FHM's 100 Sexiest Women in the World" in U.S. and UK editions. By contrast, ESPN—citing the degree of hype as compared to actual accomplishments as a singles player—ranked Kournikova 18th in its "25 Biggest Sports Flops of the Past 25 Years". Kournikova was also ranked #1 in the ESPN Classic series "Who's number 1?" when the series featured sport's most overrated athletes. Anna's popularity has extended into Texas Hold 'em lingo, where the hole cards Ace-King are sometimes referred to as an "Anna Kournikova," not only because the hand shares the AK initials with the tennis star, but also because the hand has the reputation of not playing well. It is said that a Kournikova hand "looks really good, but rarely wins."
Alfons Maria Jakob (2 July 1884, Aschaffenburg/Bavaria–17 October 1931, Hamburg) was a German neurologist with important contributions on neuropathology. Alfons Maria Jakob was the son of a shopkeeper. He studied medicine in Munich, Berlin, and Strasbourg, where obtained his doctorate in 1908. In 1909 he commenced clinical work under the psychiatrist Emil Kraepelin and did laboratory work with Franz Nissl and Alois Alzheimer in Munich. In 1911 he went to Hamburg to work with Theodor Kaes and became head of the laboratory of anatomical pathology at the psychiatric State Hospital Hamburg-Friedrichsberg. Following the death of Kaes in 1913, Jakob succeeded him as prosector. After serving in the German army in World War I, he returned to Hamburg and climbed the academic ladder. He was habilitated in neurology in 1919 and in 1924 became professor of neurology. Under Jakob's guidance the department grew rapidly. He made notable contributions to knowledge on concussion and secondary nerve degeneration and became a doyen of neuropathology. Jakob published five monographs and more than 75 papers. His neuropathological studies contributed greatly to the delineation of several diseases, including multiple sclerosis and Friedreich's ataxia. He first recognised and described Alper's disease and Creutzfeldt-Jakob disease (the latter with Hans Gerhard Creutzfeldt). He accumulated immense experience in neurosyphilis, having a 200-bedded ward devoted exclusively to that disorder. Jakob made a lecture tour of the United States and South America where he wrote a paper on the neuropathology of yellow fever. He suffered from chronic osteomyelitis for the last 7 years of his life. This eventually caused a retroperitoneal abscess and paralytic ileus from which he died following operation.
Agnosticism is the view that the truth value of certain claims—especially claims about the existence of any deity, but also other religious and metaphysical claims—is unknown or unknowable. Agnosticism can be defined in various ways, and is sometimes used to indicate doubt or a skeptical approach to questions. In some senses, agnosticism is a stance about the differences between belief and knowledge, rather than about any specific claim or belief. Thomas Henry Huxley, an English biologist, coined the word "agnostic" in 1860. However, earlier thinkers and written works have promoted agnostic points of view. They include Protagoras, a 5th-century BCE Greek philosopher, and a Nasadiya Sukta creation myth in the Rigveda, an ancient Hindu religious text. Since Huxley coined the term, many other thinkers have written extensively about agnosticism.
Demographic research services normally list agnostics in the same category as atheists and/or non-religious people. Some sources use "agnostic" in the sense of "noncommittal". Agnosticism often overlaps with other belief systems. Agnostic theists identify themselves both as agnostics and as followers of particular religions, viewing agnosticism as a framework for thinking about the nature of belief and their relation to revealed truths. Some nonreligious people, such as author Philip Pullman, identify as both agnostic and atheist.
"Agnostic" (Greek: ἀ- a-, without + γνῶσις gnōsis, knowledge) was used by Thomas Henry Huxley in a speech at a meeting of the Metaphysical Society in 1876 to describe his philosophy which rejects all claims of spiritual or mystical knowledge. Early Christian church leaders used the Greek word "gnosis" (knowledge) to describe "spiritual knowledge." Agnosticism is not to be confused with religious views opposing the ancient religious movement of Gnosticism in particular; Huxley used the term in a broader, more abstract sense. Huxley identified agnosticism not as a creed but rather as a method of skeptical, evidence-based inquiry. In recent years, scientific literature dealing with neuroscience and psychology has used the word to mean "not knowable". In technical and marketing literature, "agnostic" often has a meaning close to "independent"—for example, "platform agnostic" or "hardware agnostic."
Scottish Enlightenment philosopher David Hume contended that meaningful statements about the universe are always qualified by some degree of doubt.. He asserted that the fallibility of human beings means that they cannot obtain absolute certainty except in trivial cases where a statement is true by definition (i.e. tautologies such as "all bachelors are unmarried" or "all triangles have three corners"). All rational statements that assert a factual claim about the universe that begin "I believe that..." are simply shorthand for, "Based on my knowledge, understanding, and interpretation of the prevailing evidence, I tentatively believe that..." For instance, when one says, "I believe that Lee Harvey Oswald shot John F. Kennedy," one is not asserting an absolute truth but a tentative belief based on interpretation of the assembled evidence. Even though one may set an alarm clock prior to the following day, believing that waking up will be possible, that belief is tentative, tempered by a small but finite degree of doubt (the alarm might break, or one might die before the alarm goes off). The Catholic Church sees merit in examining what it calls Partial Agnosticism, specifically those systems that "do not aim at constructing a complete philosophy of the Unknowable, but at excluding special kinds of truth, notably religious, from the domain of knowledge." However, the Church is historically opposed to a full denial of the ability of human reason to know God. The Council of the Vatican, relying on biblical scripture, declares that "God, the beginning and end of all, can, by the natural light of human reason, be known with certainty from the works of creation" (Const. De Fide, II, De Rev.)
Huxley's agnosticism is believed to be a natural consequence of the intellectual and philosophical conditions of the 1860s, when clerical intolerance was trying to suppress scientific discoveries which appeared to clash with a literal reading of the Book of Genesis and other established Jewish and Christian doctrines. Agnosticism should not, however, be confused with natural theology, deism, pantheism, or other science positive forms of theism. By way of clarification, Huxley states, "In matters of the intellect, follow your reason as far as it will take you, without regard to any other consideration. And negatively: In matters of the intellect, do not pretend that conclusions are certain which are not demonstrated or demonstrable" (Huxley, "Agnosticism", 1889). Although A. W. Momerie has noted that this is nothing but a definition of honesty, Huxley's usual definition goes beyond mere honesty to insist that these metaphysical issues are fundamentally unknowable.
Bertrand Russell's pamphlet, "Why I Am Not a Christian", based on a speech delivered in 1927 and later included in a book of the same title, is considered a classic statement of agnosticism. The essay briefly lays out Russell’s objections to some of the arguments for the existence of God before discussing his moral objections to Christian teachings. He then calls upon his readers to "stand on their own two feet and look fair and square at the world," with a "fearless attitude and a free intelligence."
Many theistic thinkers repudiate the validity of agnosticism, or certain forms of agnosticism. Religious scholars in the three Abrahamic religions affirm the possibility of knowledge, even of metaphysical realities such as God and the soul, because human intelligence, they assert, has a non-material, spiritual element. They affirm that “not being able to see or hold some specific thing does not necessarily negate its existence,” as in the case of gravity, entropy, or reason and thought. However, this argument has two main flaws. Firstly, it only asserts that the non-material is "possible"—this is precisely what agnostics believe (that you cannot "prove" the existence of God). Secondly, simply because one non-material idea exists as a force does not mean that another idea necessarily exists. For instance, just because gravity exists does not entail that that wizardry, unicorns, or fate exists. Additionally gravity and entropy, while elusive to the "human" senses, can be detected or measured using scientific methods; god cannot. Religious scholars, such as Brown, Tacelli, and Kreeft, argue that agnosticism does not take into account the numerous evidence of his existence that God has placed in his creation. And for this, Peter Kreeft and Ronald Tacelli cite 20 arguments for God’s existence. They assert that agnosticism's demand for scientific evidence through laboratory testing is in effect asking God, the supreme being, to become man’s servant. They argue that the question of God should be treated differently from other knowable objects in that "this question regards not that which is below us, but that which is above us." Christian Philosopher Blaise Pascal argued that, even if there were truly no evidence for God, agnostics should consider what is now known as Pascal’s Wager: the infinite expected value of acknowledging God is always greater than the finite expected value of not acknowledging his existence, and thus it is a safer “bet” to choose God. According to Joseph Ratzinger, later Pope Benedict XVI, agnosticism, more specifically strong agnosticism, is reasoning that limits and contradicts itself in claiming the power of reason to know scientific truth, but not religious or philosophical truths. He blames the exclusion of reasoning from religion and ethics for the dangerous pathologies of religion and science such as human and ecological disasters. “Agnosticism,” said Ratzinger, “is always the fruit of a refusal of that knowledge which is in fact offered to man [...] The knowledge of God has always existed.” He asserted that agnosticism is a choice of comfort, pride, dominion, and utility over truth, and is opposed by the following attitudes: the keenest self-criticism, humble listening to the whole of existence, the persistent patience and self-correction of the scientific method, a readiness to be purified by the truth. According to some theistic scholars, agnosticism is impossible in actual practice, since a person can live only either as if God did not exist ("etsi Deus non daretur"), or as if God did exist ("etsi Deus daretur"). These scholars believe that each day in a person’s life is an unavoidable step towards death, and thus not to decide for or against God, whom they view as the all-encompassing foundation, purpose, and meaning of life, is to decide in favor of atheism.
Argon () is a chemical element designated by the symbol Ar. Argon has atomic number 18 and is the third element in group 18 of the periodic table (noble gases). Argon is the third most common gas in the Earth's atmosphere, at 0.93% -- making it more common than carbon dioxide. It is the third most abundant gas and the most frequently used of the noble gases. The complete octet in the outer atomic shell makes it stable and resistant to bonding with other elements. Its triple point temperature of 83.8058 K is a defining fixed point in the International Temperature Scale of 1990.
Argon has approximately the same solubility in water as oxygen gas and is 2.5 times more soluble in water than nitrogen gas. Argon is colorless, odorless, and nontoxic as a solid, liquid, and gas. Argon is inert under most conditions and forms no confirmed stable compounds at room temperature. Although argon is a noble gas, it has been found to have the capability of forming some compounds. For example, the creation of argon fluorohydride (HArF), a marginally stable compound of argon with fluorine and hydrogen, was reported by researchers at the University of Helsinki in 2000. Although the neutral ground-state chemical compounds of argon are presently limited to HArF, argon can form clathrates with water when atoms of it are trapped in a lattice of the water molecules. Also argon-containing ions and excited state complexes, such as and ArF, respectively, are known to exist. Theoretical calculations have predicted several argon compounds that should be stable, but for which no synthesis routes are currently known.
"Argon" (αργος, Greek meaning "inactive", in reference to its chemical inactivity) was suspected to be present in air by Henry Cavendish in 1785 but was not isolated until 1894 by Lord Rayleigh and Sir William Ramsay in Scotland in an experiment in which they removed all of the oxygen, carbon dioxide, water and nitrogen from a sample of clean air. They had determined that nitrogen produced from chemical compounds was one-half percent lighter than nitrogen from the atmosphere. The difference seemed insignificant, but it was important enough to attract their attention for many months. They concluded that there was another gas in the air mixed in with the nitrogen. Argon was also encountered in 1882 through independent research of H. F. Newall and W.N. Hartley. Each observed new lines in the color spectrum of air but were unable to identify the element responsible for the lines. Argon became the first member of the noble gases to be discovered. The symbol for argon is now Ar, but up until 1957 it was A.
The main isotopes of argon found on Earth are (99.6%), 36Ar (0.34%), and 38Ar (0.06%). Naturally occurring with a half-life of 1.25 years, decays to stable (11.2%) by electron capture and positron emission, and also to stable (88.8%) via beta decay. These properties and ratios are used to determine the age of rocks. In the Earth's atmosphere, is made by cosmic ray activity, primarily with. In the subsurface environment, it is also produced through neutron capture by or alpha emission by calcium. 37Ar is created from the neutron spallation of as a result of subsurface nuclear explosions. It has a half-life of 35 days. Argon is notable in that its isotopic composition varies greatly between different locations in the solar system. Where the major source of argon is the decay of potassium-40 in rocks, Argon-40 will be the dominant isotope, as it is on earth. Argon produced directly by stellar nucleosynthesis, in contrast, is dominated by the alpha process nuclide, argon-36. Correspondingly, solar argon contains 84.6% argon-36 based on solar wind measurements. The predominance of radiogenic argon-40 is responsible for the fact that the standard atomic weight of terrestrial argon is greater than that of the next element, potassium. This was puzzling at the time when argon was discovered, since Mendeleev had placed the elements in his periodic table in order of atomic weight, although the inertness of argon implies that it must be placed before the reactive alkali metal potassium. Henry Moseley later solved this problem by showing that the periodic table is actually arranged in order of atomic number. (See History of the periodic table). The Martian atmosphere contains 1.6% of argon-40 and 5 ppm of argon-36. The Mariner space probe fly-by of the planet Mercury in 1973 found that Mercury has a very thin atmosphere with 70% argon, believed to result from releases of the gas as a decay product from radioactive materials on the planet. In 2005, the "Huygens" probe also discovered the presence of argon-40 on Titan, the largest moon of Saturn.
Argon’s complete octet of electrons indicates full s and p subshells. This full outer energy level makes argon very stable and extremely resistant to bonding with other elements. Before 1962, argon and the other noble gases were considered to be chemically inert and unable to form compounds; however, compounds of the heavier noble gases have since been synthesized. In August 2000, the first argon compounds were formed by researchers at the University of Helsinki. By shining ultraviolet light onto frozen argon containing a small amount of hydrogen fluoride, argon fluorohydride (HArF) was formed. It is stable up to 40 kelvin (−233 °C).
Other noble gases would probably work as well in most of these applications, but argon is by far the cheapest. Argon is inexpensive since it is a byproduct of the production of liquid oxygen and liquid nitrogen, both of which are used on a large industrial scale. The other noble gases (except helium) are produced this way as well, but argon is the most plentiful since it has the highest concentration in the atmosphere. The bulk of argon applications arise simply because it is inert and relatively cheap.
Argon is used in some high-temperature industrial processes, where ordinarily non-reactive substances become reactive. For example, an argon atmosphere is used in graphite electric furnaces to prevent the graphite from burning. For some of these processes, the presence of nitrogen or oxygen gases might cause defects within the material. Argon is used in various types of metal inert gas welding such as tungsten inert gas welding, as well as in the processing of titanium and other reactive elements. An argon atmosphere is also used for growing crystals of silicon and germanium. Argon is an asphyxiant in the poultry industry, either for mass culling following disease outbreaks, or as a means of slaughter more humane than the electric bath. Argon's relatively high density causes it to remain close to the ground during gassing. Its non-reactive nature makes it suitable in a food product, and since it replaces oxygen within the dead bird, argon also enhances shelf life. Argon is sometimes used for extinguishing fires where damage to equipment is to be avoided (see photo).
Argon is used to displace oxygen- and moisture-containing air in packaging material to extend the shelf-lives of the contents. Aerial oxidation, hydrolysis, and other chemical reactions which degrade the products are retarded or prevented entirely. Bottles of high-purity chemicals and certain pharmaceutical products are available in sealed bottles or ampoules packed in argon. In wine making, argon is used to top-off barrels to avoid the aerial oxidation of ethanol to acetic acid during the aging process. Argon is also available in aerosol-type cans, which may be used to preserve compounds such as varnish, polyurethane, paint, etc. for storage after opening. Since 2001 the American National Archives stores important national documents such as the Declaration of Independence and the Constitution within argon-filled cases to retard their degradation. Using argon reduces gas leakage, compared with the helium used in the preceding five decades.
Argon may be used as the inert gas within Schlenk lines and gloveboxes. The use of argon over comparatively less expensive dinitrogen is preferred where nitrogen may react. Argon may be used as the carrier gas in gas chromatography and in electrospray ionization mass spectrometry; it is the gas of choice for the plasma used in ICP spectroscopy. Argon is preferred for the sputter coating of specimens for scanning electron microscopy. Argon ions are also used for sputtering in microelectronics.
Cryosurgery procedures such as cryoablation use liquefied argon to destroy cancer cells. In surgery it is used in a procedure called "argon enhanced coagulation" which is a form of argon plasma beam electrosurgery. The procedure carries a risk of producing gas embolism in the patient and has resulted in the death of one person via this type of accident. Blue argon lasers are used in surgery to weld arteries, destroy tumors, and to correct eye defects. It has also used experimentally to replace nitrogen in the breathing or decompression mix, to speed the elimination of dissolved nitrogen from the blood. See Argox (breathing gas).
It is used for thermal insulation in energy efficient windows. Argon is also used in technical scuba diving to inflate a dry suit, because it is inert and has low thermal conductivity. Compressed argon is allowed to expand, to cool the seeker heads of the AIM-9 Sidewinder missile, and other missiles that use cooled thermal seeker heads. The gas is stored at high pressure. Argon-39, with a half-life of 269 years, has been used for a number of applications, primarily ice core and ground water dating. Also, potassium-argon dating is used in dating igneous rocks.
Although argon is non-toxic, it does not satisfy the body's need for oxygen and is thus an asphyxiant. Argon is 25% more dense than air and is considered highly dangerous in closed areas. It is also difficult to detect because it is colorless, odorless, and tasteless. In confined spaces, it is known to result in death due to asphyxiation. A 1994 incident in Alaska that resulted in one fatality highlights the dangers of argon tank leakage in confined spaces, and emphasizes the need for proper use, storage and handling.
Arsenic (,; also, when attributive) is the chemical element that has the symbol "As", atomic number 33 and atomic mass 74.92. Arsenic was first documented by Albertus Magnus in 1250. Arsenic is a notoriously poisonous metalloid with many allotropic forms, including a yellow (molecular non-metallic) and several black and grey forms (metalloids). Three metalloidal forms of arsenic, each with a different crystal structure, are found free in nature (the minerals arsenic "sensu stricto" and the much rarer arsenolamprite and pararsenolamprite). However, it is more commonly found as arsenide and in arsenate compounds, several hundred of which are known. Arsenic and its compounds are used as pesticides, herbicides, insecticides and in various alloys.
The word "arsenic" was borrowed from the Syriac word ܠܐ ܙܐܦܢܝܐ "(al) zarniqa" and the Persian word "Zarnikh", meaning "yellow orpiment", into Greek as "arsenikon" (Αρσενικόν). It is also related to the similar Greek word "arsenikos" (Αρσενικός), meaning "masculine" or "potent". The word was adopted in Latin "arsenicum" and Old French "arsenic," from which the English word "arsenic" is derived. Arsenic sulfides (orpiment, realgar) and oxides have been known and used since ancient times. Zosimos (circa 300 AD) describes roasting "sandarach" (realgar) to obtain "cloud of arsenic" (arsenious oxide) which he then reduces to metallic arsenic. As the symptoms of arsenic poisoning were somewhat ill-defined, it was frequently used for murder until the advent of the Marsh test, a sensitive chemical test for its presence. (Another less sensitive but more general test is the Reinsch test.) Owing to its use by the ruling class to murder one another and its potency and discreetness, arsenic has been called the "Poison of Kings" and the "King of Poisons". During the Bronze Age, arsenic was often included in bronze, which made the alloy harder (so-called "arsenical bronze"). Albertus Magnus (Albert the Great, 1193–1280) is believed to have been the first to isolate the element in 1250 by heating soap together with arsenic trisulfide. In 1649, Johann Schröder published two ways of preparing arsenic. Cadet's fuming liquid (impure cacodyl), the first organometallic compound, was synthesized in 1760 by Louis Claude Cadet de Gassicourt by the reaction of potassium acetate with arsenic trioxide. In the Victorian era, "arsenic" (colourless, crystalline, soluble "white arsenic" trioxide) was mixed with vinegar and chalk and eaten by women to improve the complexion of their faces, making their skin paler to show they did not work in the fields. Arsenic was also rubbed into the faces and arms of women to "improve their complexion". The accidental use of arsenic in the adulteration of foodstuffs led to the Bradford sweet poisoning in 1858, which resulted in approximately 20 deaths and 200 people taken ill with arsenic poisoning.
Naturally occurring arsenic is composed of one stable isotope, 75As. As of 2003, at least 33 radioisotopes have also been synthesized, ranging in atomic mass from 60 to 92. The most stable of these is 73As with a half-life of 80.3 days. Isotopes that are lighter than the stable 75As tend to decay by β+ decay, and those that are heavier tend to decay by β- decay, with some exceptions. At least 10 nuclear isomers have been described, ranging in atomic mass from 66 to 84. The most stable of arsenic's isomers is 68mAs with a half-life of 111 seconds.
Like phosphorus, arsenic is an excellent example of an element that exhibits allotropy, as its various allotropes have strikingly different properties. The three most common allotropes are "metallic grey", "yellow" and "black arsenic". The most common allotrope of arsenic is "grey arsenic". It has a similar structure to black phosphorus (β-metallic phosphorus) and has a layered crystal structure somewhat resembling that of graphite. It consists of many six-membered rings which are interlinked. Each atom is bound to three other atoms in the layer and is coordinated by each 3 arsenic atoms in the upper and lower layer. This relatively close packing leads to a high density of 5.73 g/cm3. "Yellow arsenic" () is soft and waxy, not unlike. Both have four atoms arranged in a tetrahedral structure in which each atom is bound to the other three atoms by a single bond, resulting in very high ring strain and instability. This form of arsenic is the least stable, most reactive, most volatile, least dense, and most toxic of all the allotropes. Yellow arsenic is produced by rapid cooling of arsenic vapour with liquid nitrogen. It is rapidly transformed into the grey arsenic by light. The yellow form has a density of 1.97 g/cm3. "Black arsenic" is similar in structure to red phosphorus.
The most common oxidation states for arsenic are −3 (arsenides: usually alloy-like intermetallic compounds), +3 (arsenates(III) or arsenites, and most organoarsenic compounds), and +5 (arsenates: the most stable inorganic arsenic oxycompounds). Arsenic also bonds readily to itself, forming square As ions in the arsenide skutterudite. In the +3 oxidation state, the stereochemistry of arsenic is affected by the presence of a lone pair of electrons. Arsenic is very similar chemically to its predecessor in the Periodic Table, phosphorus. Like phosphorus, it forms colourless, odourless, crystalline oxides As2O3 and As2O5 which are hygroscopic and readily soluble in water to form acidic solutions. Arsenic(V) acid is a weak acid. Like phosphorus, arsenic forms an unstable, gaseous hydride: arsine (AsH3). The similarity is so great that arsenic will partly substitute for phosphorus in biochemical reactions and is thus poisonous. However, in subtoxic doses, soluble arsenic compounds act as stimulants, and were once popular in small doses as medicine by people in the mid 18th century. When heated in air, arsenic oxidizes to arsenic trioxide; the fumes from this reaction have an odour resembling garlic. This odour can be detected on striking arsenide minerals such as arsenopyrite with a hammer. Arsenic (and some arsenic compounds) sublimes upon heating at atmospheric pressure, converting directly to a gaseous form without an intervening liquid state. The liquid state appears at 20 atmospheres and above, which explains why the melting point is higher than the boiling point.
Arsenic compounds resemble in many respects those of phosphorus as both arsenic and phosphorus occur in the same group (column) of the periodic table. The most important compounds of arsenic are arsenic(III) oxide, As2O3, ("white arsenic"), the yellow sulfide orpiment (As2S3) and red realgar (As4S4), Paris Green, calcium arsenate, and lead hydrogen arsenate. The latter three have been used as agricultural insecticides and poisons. Whilst arsenic trioxide forms during oxidation of arsenic, arsenic pentoxide is formed by the dehydration of arsenic acid. Both oxides dissolve in strong alkaline solution, with the formation of arsenite AsO and arsenate AsO respectively. The protonation steps between the arsenate and arsenic acid are similar to those between phosphate and phosphoric acid. However, arsenite and arsenous acid contain arsenic bonded to three oxygen and not hydrogen atoms, in contrast to phosphite and phosphorous acid (more accurately termed 'phosphonic acid'), which contain non-acidic P-H bonds. Arsenous acid is genuinely tribasic, whereas phosphonic acid is not. A broad variety of sulfur compounds of arsenic are known, As4S3, As4S4, As2S3 and As4S10. All arsenic(III) halogen compounds (except with astatine) are known and stable. For the arsenic(V) compounds the situation is different: only the arsenic pentafluoride is stable at room temperature. Arsenic pentachloride is only stable at temperatures below −50 °C and the pentabromide and pentaiodide are unknown. Arsenic also has a formal oxidation state of +2 in As4S4, realgar. This is achieved by pairing As atoms to produce dimeric cations [As-As]2+, so the total covalency of As is still in fact three.
Arsenopyrite, also unofficially called mispickel, (FeAsS) is the most common arsenic-bearing mineral. In the lithosphere, the minerals of the formula M(II)AsS, with M(II) being mostly Fe, Ni and Co, are the dominant arsenic minerals. Orpiment and realgar were formerly used as painting pigments, though they have fallen out of use owing to their toxicity and reactivity. Although arsenic is sometimes found native in nature, its main economic source is the mineral arsenopyrite mentioned above; it is also found in arsenides of metals such as silver, cobalt (cobaltite: CoAsS and skutterudite: CoAs3) and nickel, as sulfides, and when oxidised as arsenate minerals such as mimetite, Pb5(AsO4)3Cl and erythrite, Co3(AsO4)2·8H2O, and more rarely arsenites ('arsenite' = arsenate(III), AsO33− as opposed to arsenate (V), AsO43−). In addition to the inorganic forms mentioned above, arsenic also occurs in various organic forms in the environment. Other naturally occurring pathways of exposure include volcanic ash, weathering of the arsenic-containing mineral and ores as well as groundwater. It is also found in food, water, soil and air.
In 2005, China was the top producer of white arsenic with almost 50% world share, followed by Chile, Peru and Morocco, reports the British Geological Survey and the United States Geological Survey. The arsenic was recovered mostly during mining operations, for example the production from Peru comes mostly from copper mining and the production in China is owing to gold mining. Arsenic is part of the smelter dust from copper, gold, and lead smelters. On roasting in air of arsenopyrite, arsenic sublimes as arsenic (III) oxide leaving iron oxides, while roasting without air results in the production of metallic arsenic. Further purification from sulfur and other chalcogens is achieved by sublimation in vacuum or in a hydrogen atmosphere or by distillation from molten lead-arsenic mixture.
The toxicity of arsenic to insects, bacteria, and fungi led to its use as a wood preservative. In the 1950s a process of treating wood with chromated copper arsenate (also known as CCA or Tanalith) was invented, and for decades this treatment was the most extensive industrial use of arsenic. Due to improved understanding of arsenic's high level of toxicity, most countries banned the use of CCA in consumer products. The European Union and United States led this ban, beginning in 2004. As of 2002, US-based industries consumed 19,600 metric tons of arsenic. 90% of this was used for treatment of wood with CCA. In 2007, 50% of the 5,280 metric tons of consumption was still used for this purpose. In the United States, the use of arsenic in consumer products was discontinued for residential and general consumer construction on December 31, 2003 and alternative chemicals are now used, such as ACQ, borates, copper azole, cyproconazole, and propiconazole. Although discontinued, this application is also one of the most concern to the general public. The vast majority of older pressure-treated wood was treated with CCA. CCA lumber is still in widespread use in many countries, and was heavily used during the latter half of the 20th century as a structural and outdoor building material. Although the use of CCA lumber was banned in many areas after studies showed that arsenic could leach out of the wood into the surrounding soil (from playground equipment, for instance), a risk is also presented by the burning of older CCA timber. The direct or indirect ingestion of wood ash from burnt CCA lumber has caused fatalities in animals and serious poisonings in humans; the lethal human dose is approximately 20 grams of ash. Scrap CCA lumber from construction and demolition sites may be inadvertently used in commercial and domestic fires. Protocols for safe disposal of CCA lumber do not exist evenly throughout the world; there is also concern in some quarters about the widespread landfill disposal of such timber.
During the 18th, 19th, and 20th centuries, a number of arsenic compounds have been used as medicines, including arsphenamine (by Paul Ehrlich) and arsenic trioxide (by Thomas Fowler). Arsphenamine as well as Neosalvarsan was indicated for syphilis and trypanosomiasis, but has been superseded by modern antibiotics. Arsenic trioxide has been used in a variety of ways over the past 500 years, but most commonly in the treatment of cancer. The US Food and Drug Administration in 2000 approved this compound for the treatment of patients with acute promyelocytic leukemia that is resistant to ATRA. It was also used as Fowler's solution in psoriasis. Recently new research has been done in locating tumours using arsenic-74 (a positron emitter). The advantages of using this isotope instead of the previously used iodine-124 is that the signal in the PET scan is clearer as the iodine tends to transport iodine to the thyroid gland producing a lot of noise.
After World War I the United States built up a stockpile of 20,000 tons of lewisite; a chemical weapon, acting as a vesicant (blister agent) and lung irritant. The stockpile was neutralized with bleach and dumped into the Gulf of Mexico after the 1950s. During the Vietnam War the United States used Agent Blue (a mixture of sodium cacodylate) and dimethyl arsenic acid (cacodylic acid) as one of the rainbow herbicides to deprive the Vietnamese of valuable crops.
Inorganic arsenic and its compounds, upon entering the food chain, are progressively metabolised to less toxic forms of arsenic through a process of methylation. For example, the mold Scopulariopsis brevicaulis produce significant amounts of trimethylarsine if inorganic arsenic is present. The organic compound arsenobetaine is found in some marine foods such as fish and algae, and also in mushrooms in larger concentrations. The average person's intake is about 10–50 µg/day. Values about 1000 µg are not unusual following consumption of fish or mushrooms. But there is little danger in eating fish because this arsenic compound is nearly non-toxic. Some species of bacteria obtain their energy by oxidizing various fuels while reducing arsenate to arsenite. The enzymes involved are known as arsenate reductases (Arr). In 2008, bacteria were discovered that employ a version of photosynthesis in the absence of oxygen with arsenites as electron donors, producing arsenates (just like ordinary photosynthesis uses water as electron donor, producing molecular oxygen). Researchers conjecture that historically these photosynthesizing organisms produced the arsenates that allowed the arsenate-reducing bacteria to thrive. One strain PHS-1 has been isolated and is related to the γ-Proteobacterium "Ectothiorhodospira shaposhnikovii". The mechanism is unknown, but an encoded Arr enzyme may function in reverse to its known homologues. Arsenic has been linked to epigenetic changes which are heritable changes in gene expression that occur without changes in DNA sequence and include DNA methylation, histone modification and RNA interference. Toxic levels of arsenic cause significant DNA hypermethylation of tumour suppressor genes p16 and p53 thus increasing risk of carcinogenesis. These epigenetic events have been observed in "in vitro" studies with human kidney cells and "in vivo" tests with rat liver cells and peripheral blood leukocytes in humans. Inductive coupled plasma mass spectrometry (ICP-MS) is used to detect precise levels of intracellular of arsenic and its other bases involved in epigenetic modification of DNA. Studies investigating arsenic as an epigenetic factor will help in developing precise biomarkers of exposure and susceptibility.
Arsenic and many of its compounds are especially potent poisons. Arsenic disrupts ATP production through several mechanisms. At the level of the citric acid cycle, arsenic inhibits lipoic acid which is a cofactor for pyruvate dehydrogenase; and by competing with phosphate it uncouples oxidative phosphorylation, thus inhibiting energy-linked reduction of NAD+, mitochondrial respiration, and ATP synthesis. Hydrogen peroxide production is also increased, which might form reactive oxygen species and oxidative stress. These metabolic interferences lead to death from multi-system organ failure, probably from necrotic cell death, not apoptosis. A post mortem reveals brick red coloured mucosa, owing to severe haemorrhage. Although arsenic causes toxicity, it can also play a protective role. Elemental arsenic and arsenic compounds are classified as "toxic" and "dangerous for the environment" in the European Union under EEC. The International Agency for Research on Cancer (IARC) recognizes arsenic and arsenic compounds as group 1 carcinogens, and the EU lists arsenic trioxide, arsenic pentoxide and arsenate salts as category 1 carcinogens. Arsenic is known to cause arsenicosis owing to its manifestation in drinking water, “the most common species being arsenate [HAsO42-; As(V)] and arsenite [H3AsO3; As(III)]”. The ability of arsenic to undergo redox conversion between As(III) and As(V) makes its availability in the environment more abundant. According to Croal, Gralnick, Malasarn, and Newman, “[the] understanding [of] what stimulates As(III) oxidation and/or limits As(V) reduction is relevant for bioremediation of contaminated sites (Croal). The study of chemolithoautotrophic As(III) oxidizers and the heterotrophic As(V) reducers can help the understanding of the oxidation and/or reduction of arsenic. Treatment of chronic arsenic poisoning is easily accomplished. British anti-lewisite (dimercaprol) is prescribed in dosages of 5 mg/kg up to 300 mg each 4 hours for the first day. Then administer the same dosage each 6 hours for the second day. Then prescribe this dosage each 8 hours for eight additional days.
Arsenic contamination of groundwater has led to a massive epidemic of arsenic poisoning in Bangladesh and neighbouring countries. Presently 42 major incidents around the world have been reported on groundwater arsenic contamination. It is estimated that approximately 57 million people are drinking groundwater with arsenic concentrations elevated above the World Health Organization's standard of 10 parts per billion. However, a study of cancer rates in Taiwan suggested that significant increases in cancer mortality appear only at levels above 150 parts per billion. The arsenic in the groundwater is of natural origin, and is released from the sediment into the groundwater owing to the anoxic conditions of the subsurface. This groundwater began to be used after local and western NGOs and the Bangladeshi government undertook a massive shallow tube well drinking-water program in the late twentieth century. This program was designed to prevent drinking of bacterially contaminated surface waters, but failed to test for arsenic in the groundwater. Many other countries and districts in South East Asia, such as Vietnam, Cambodia, and China have geological environments conducive to generation of high-arsenic groundwaters. Arsenicosis was reported in Nakhon Si Thammarat, Thailand in 1987, and the dissolved arsenic in the Chao Phraya River is suspected of containing high levels of naturally occurring arsenic, but has not been a public health problem owing to the use of bottled water. In the United States, arsenic is most commonly found in the ground waters of the southwest. Parts of New England, Michigan, Wisconsin, Minnesota and the Dakotas are also known to have significant concentrations of arsenic in ground water. Increased levels of skin cancer have been associated with arsenic exposure in Wisconsin, even at levels below the 10 part per billion drinking water standard. According to a recent film funded by the US Superfund, millions of private wells have unknown arsenic levels, and in some areas of the US, over 20% of wells may contain levels that exceed established limits. Low-level exposure to arsenic at concentrations found commonly in US drinking water compromises the initial immune response to H1N1 or swine flu infection according to NIEHS-supported scientists. The study, conducted in laboratory mice, suggests that people exposed to arsenic in their drinking water may be at increased risk for more serious illness or death in response to infection from the virus. Epidemiological evidence from Chile shows a dose dependent connection between chronic arsenic exposure and various forms of cancer, particularly when other risk factors, such as cigarette smoking, are present. These effects have been demonstrated to persist below 50 parts per billion. Analyzing multiple epidemiological studies on inorganic arsenic exposure suggests a small but measurable risk increase for bladder cancer at 10 parts per billion. According to Peter Ravenscroft of the Department of Geography at the University of Cambridge, roughly 80 million people worldwide consume between 10 and 50 parts per billion arsenic in their drinking water. If they all consumed exactly 10 parts per billion arsenic in their drinking water, the previously cited multiple epidemiological study analysis would predict an additional 2,000 cases of bladder cancer alone. This represents a clear underestimate of the overall impact, since it does not include lung or skin cancer, and explicitly underestimates the exposure. Those exposed to levels of arsenic above the current WHO standard should weigh the costs and benefits of arsenic remediation. Early (1973) evaluations of the removal of dissolved arsenic by drinking water treatment processes demonstrated that arsenic is very effectively removed by co-precipitation with either iron or aluminum oxides. The use of iron as a coagulant, in particular, was found to remove arsenic with efficiencies exceeding 90%. Several adsorptive media systems have been approved for point-of-service use in a study funded by the United States Environmental Protection Agency (U.S.EPA) and the National Science Foundation (NSF). A team of European and Indian scientists and engineers have set up six arsenic treatment plants in West Bengal based on in-situ remediation method (SAR Technology). This technology does not use any chemicals and arsenic is left as an insoluble form (+5 state) in the subterranean zone by recharging aerated water into the aquifer and thus developing an oxidation zone to support arsenic oxidizing micro-organisms. This process does not produce any waste stream or sludge and is relatively cheap. Magnetic separations of arsenic at very low magnetic field gradients have been demonstrated in point-of-use water purification with high-surface-area and monodisperse magnetite (Fe3O4) nanocrystals. Using the high specific surface area of Fe3O4 nanocrystals the mass of waste associated with arsenic removal from water has been dramatically reduced. Epidemiological studies have suggested a correlation between chronic consumption of drinking water contaminated with arsenic and the incidence of type 2 diabetes. However, the literature provides insufficient scientific evidence to show cause and effect between arsenic and the onset of diabetes mellitus type 2.
Antimony () is a chemical element with the symbol Sb (, meaning "mark") and atomic number 51. A metalloid, antimony has four allotropic forms. The stable form of antimony is a blue-white metalloid. Yellow and black antimony are unstable non-metals. Antimony is used in electronics and flame-proofing, in paints, rubber, ceramics, enamels, drugs to treat "Leishmania" infection and a wide variety of alloys.
Antimony in its elemental form is a silvery white, brittle, fusible, crystalline solid that exhibits poor electrical and heat conductivity properties and vaporizes at low temperatures. A metalloid, antimony resembles a metal in its appearance and in many of its physical properties, but does not chemically react as a metal. It is reactive with oxidizing acids and halogens. Antimony and some of its alloys are unusual in that they expand on cooling. Antimony is geochemically categorized as a chalcophile, occurring with sulfur and the heavy metals lead, copper, and silver. The abundance of antimony in the Earth's crust is estimated at 0.2 to 0.5 parts per million.
Antimony is increasingly being used in the semiconductor industry in the production of diodes, infrared detectors, and Hall-effect devices. As an alloy, this metalloid greatly increases lead's hardness and mechanical strength. The most important use of antimony is as a hardener in lead for storage batteries. Uses include Antimony compounds in the form of oxides, sulfides, sodium antimonate, and antimony trichloride are used in the making of flame-proofing compounds, ceramic enamels, glass, paints, and pottery. Antimony trioxide is the most important of the antimony compounds and is primarily used in flame-retardant formulations. These flame-retardant applications include such markets as children's clothing, toys, aircraft and automobile seat covers. It is also used in the fiberglass composites industry as an additive to polyester resins for such items as light aircraft engine covers. The resin will burn while a flame is held to it but will extinguish itself as soon as the flame is removed. Antimony sulfide is also one of the ingredients of safety matches. In the 1950s, tiny beads of a lead-antimony alloy were used to dope the emitters and collectors of NPN alloy junction transistors with antimony. The natural sulfide of antimony, stibnite, was known and used in Biblical times, as a medication and in Islamic/Pre-Islamic times as a cosmetic. The Sunan Abi Dawood reports, “Muhammad said: 'Among the best types of collyrium is antimony (ithmid) for it clears the vision and makes the hair sprout.'” Stibnite is still used in some developing countries as a medication. Antimony has been used for the treatment of schistosomiasis. Antimony attaches itself to sulfur atoms in certain enzymes which are used by both the parasite and human host. Small doses can kill the parasite without causing damage to the patient. Antimony and its compounds are used in several veterinary preparations like Anthiomaline or Lithium antimony thiomalate, which is used as a skin conditioner in ruminants. Antimony has a nourishing or conditioning effect on keratinized tissues, at least in animals. Tartar emetic is another antimony preparation which is used as an anti-schistosomal drug. Treatments chiefly involving antimony have been called antimonials. Antimony-based drugs such as meglumine antimoniate, is also considered the drugs of choice for the treatment of leishmaniasis in domestic animals. Unfortunately, as well as having low therapeutic indices, the drugs are poor at penetrating the bone marrow, where some of the Leishmania amastigotes reside, and so cure of the disease - especially the visceral form - is very difficult. A coin made of antimony was issued in the Keichow Province of China in 1931. The coins were not popular, being too soft and they wore quickly when in circulation. After the first issue no others were produced.
The ancient words for antimony mostly have, as their chief meaning, kohl, the sulfide of antimony. Pliny the Elder, however, distinguishes between male and female forms of antimony; his male form is probably the sulfide, the female form, which is superior, heavier, and less friable, is probably native metallic antimony. The Egyptians called antimony "mśdmt"; in hieroglyphics, the vowels are uncertain, but there is an Arabic tradition that the word is "mesdemet". The Greek word, "stimmi", is probably a loan word from Arabic or Egyptian, and is used by the Attic tragic poets of the 5th century BC; later Greeks also used "stibi", as did Celsus and Pliny, writing in Latin, in the first century AD. Pliny also gives the names "stimi" ["sic"], "larbaris", alabaster, and the "very common" "platyophthalmos", "wide-eye" (from the effect of the cosmetic). Later Latin authors adapted the word to Latin as "stibium". The Arabic word for the substance, as opposed to the cosmetic, can appear as "ithmid, athmoud, othmod", or "uthmod". Littré suggests the first form, which is the earliest, derives from "stimmida", (one) accusative for "stimmi". The use of Sb as the standard chemical symbol for antimony is due to the 18th century chemical pioneer, Jöns Jakob Berzelius, who used this abbreviation of the name "stibium". The medieval Latin form, from which the modern languages and late Byzantine Greek, take their names, is "antimonium". The origin of this is uncertain; all suggestions have some difficulty either of form or interpretation. The popular etymology, from "anti-monachos" or French "antimoine", still has adherents; this would mean "monk-killer", and is explained by many early alchemists being monks, and antimony being poisonous. So does the hypothetical Greek word "antimonos", "against one", explained as "not found as metal", or "not found unalloyed". Lippmann conjectured a Greek word, "anthemonion", which would mean "floret", and he cites several examples of related Greek words (but not that one) which describe chemical or biological efflorescence. The early uses of "antimonium" include the translations, in 1050-1100, by Constantine the African of Arabic medical treatises. Several authorities believe that "antimonium" is a scribal corruption of some Arabic form; Meyerhof derives it from "ithmid"; other possibilities include "Athimar", the Arabic name of the "metal", and a hypothetical "*as-stimmi", derived from or parallel to the Greek.
Antimony's sulfide compound, antimony(III) trisulfide, Sb2S3 was recognized in antiquity, at least as early as 3000 BC. Pastes of Sb2S3 powder in fat or in other materials have been used since that date as eye cosmetics in the Middle East and farther afield; in this use, Sb2S3 is called kohl. It was used to darken the brows and lashes, or to draw a line around the perimeter of the eye. An artifact made of antimony dating to about 3000 BC was found at Tello, Chaldea (part of present-day Iraq), and a copper object plated with antimony dating between 2500 BC and 2200 BC has been found in Egypt. There is some uncertainty as to the description of the artifact from Tello. Although it is sometimes reported to be a vase, a recent detailed discussion reports it to be rather a fragment of indeterminate purpose. The first European description of a procedure for isolating antimony is in the book "De la pirotechnia" of 1540 by Vannoccio Biringuccio, written in Italian. This book precedes the more famous 1556 book in Latin by Agricola, "De re metallica", even though Agricola has been often incorrectly credited with the discovery of metallic antimony. A text describing the preparation of metallic antimony that was published in Germany in 1604 purported to date from the early fifteenth century, and if authentic it would predate Biringuccio. The book, written in Latin, was called "Currus Triumphalis Antimonii" (The Triumphal Chariot of Antimony), and its putative author was a certain Benedictine monk, writing under the name Basilius Valentinus. Already in 1710 Wilhelm Gottlob Freiherr von Leibniz, after careful inquiry, concluded that the work was spurious, that there was no monk named Basilius Valentinus, and the book's author was its ostensible editor, Johann Thölde (ca. 1565-ca. 1624). There is now agreement among professional historians that the "Currus Triumphalis..." was written after the middle of the sixteenth century and that Thölde was likely its author. An English translation of the "Currus Triumphalis" appeared in English in 1660, under the title The Triumphant Chariot of Antimony. The work remains of great interest, chiefly because it documents how followers of the renegade German physician, Philippus Theophrastus Paracelsus von Hohenheim (of whom Thölde was one), came to associate the practice of alchemy with the preparation of chemical medicines. According to the traditional history of Middle Eastern alchemy, pure antimony was well known to Geber, sometimes called "the Father of Chemistry", in the 8th century. Here there is still an open controversy: Marcellin Berthelot, who translated a number of Geber's books, stated that antimony is never mentioned in them, but other authors claim that Berthelot translated only some of the less important books, while the more interesting ones (some of which might describe antimony) are not yet translated, and their content is completely unknown. The first natural occurrence of pure antimony ('native antimony') in the Earth's crust was described by the Swedish scientist and local mine district engineer Anton von Swab in 1783. The type-sample was collected from the Sala Silvermine in the Bergslagen mining district of Sala, Västmanland, Sweden.
Even though this element is not abundant, it is found in over 100 mineral species. Antimony is sometimes found native, but more frequently it is found in the sulfide stibnite (Sb2S3) which is the predominant ore mineral. Commercial forms of antimony are generally ingots, broken pieces, granules, and cast cake. Other forms are powder, shot, and single crystals. In 2005, China was the top producer of antimony with about 84% world share followed at a distance by South Africa, Bolivia and Tajikistan, reports the British Geological Survey. The mine with the largest deposites in China is Xikuangshan mine in Hunan Province with a estimated deposit of 2.1 million metric tons. "Chiffres de 2003, métal contenue dans les minerais et concentrés, source: L'état du monde 2005"
Actinium (,) is a radioactive chemical element with the symbol Ac and atomic number 89, which was discovered in 1899. It was the first non-primordial radioactive element to be isolated. Polonium, radium and radon were observed before actinium, but they were not isolated until 1902. Actinium gave the name to the actinoid series, a group of 15 similar elements between actinium and lawrencium in the periodic table.
André-Louis Debierne, a French chemist, announced the discovery of a new element in 1899. He separated it from pitchblende and described the substance (in 1899) as similar to titanium and (in 1900) as similar to thorium. Friedrich Oskar Giesel independently discovered actinium in 1902 as a substance being similar to lanthanum and called it "emanium" in 1904. After a comparison of substances in 1904, Debierne's name was retained because it had seniority. The stated history of the discovery of actinium remained questionable for decades. In publications starting in 1971 and continuing especially in 2000, it was shown that Debierne's published results in 1904 conflict with those in his articles published in 1899 and 1900. The word actinium comes from the Greek "aktis, aktinos" (ακτίς, ακτίνος), meaning beam or ray.
Actinium shows similar chemical behavior to lanthanum. Due to this similarity the separation of actinium from lanthanum and the other rare earth elements, which are also present in uranium ores was difficult. Solvent extraction and ion exchange chromatography was used for the separation. Only a limited number of actinium compounds are known, for example AcF3, AcCl3, AcBr3, AcOF, AcOCl, AcOBr, Ac2S3, Ac2O3 and AcPO4. All the mentioned compounds are similar to the corresponding lanthanum compounds and show that actinium compounds are generally in the oxidation state of +3.
Naturally occurring actinium is composed of 1 radioactive isotope; 227Ac. 36 radioisotopes have been characterized with the most stable being 227Ac with a half-life of 21.772 y, 225Ac with a half-life of 10.0 days, and 226Ac with a half-life of 29.37 h. All of the remaining radioactive isotopes have half-lives that are less than 10 hours and the majority of these have half-lives that are less than 1 minute. The shortest-lived isotope of actinium is 217Ac which decays through alpha decay and electron capture. It has a half-life of 69 ns. Actinium also has 2 meta states. Purified 227Ac comes into equilibrium with its decay products at the end of 185 days, and then decays according to its 21.773-year half-life; the successive decay products are part of the actinium series. The isotopes of actinium range in atomic weight from 206 u (206Ac) to 236 u (236Ac).
Actinium is found in trace amounts in uranium ore, but more commonly is made in milligram amounts by the neutron irradiation of 226 in a nuclear reactor. Actinium metal has been prepared by the reduction of actinium fluoride with lithium vapor at about 1100 to 1300°C. Actinium is found only in traces in uranium ores as 227Ac, an α and β emitter with a half-life of 21.773 years. One ton of uranium ore contains about a tenth of a gram of actinium. The actinium isotope 227Ac is a transient member of the actinium series decay chain, which begins with the parent isotope 235U (or 239Pu) and ends with the stable lead isotope 207Pb. Another actinium isotope (225Ac) is transiently present in the neptunium series decay chain, beginning with 237Np (or 233U) and ending with near-stable bismuth (209Bi).
It is about 150 times as radioactive as radium, making it valuable as a neutron source for energy. Otherwise it has no significant industrial applications. 225Ac is used in medicine to produce 213 in a reusable generator or can be used alone as an agent for radio-immunotherapy for Targeted Alpha Therapy (TAT). 225Ac was first produced artificially by the Institute for Transuranium Elements (ITU) in Germany using a cyclotron and by Dr Graeme Melville at St George Hospital in Sydney using a linac in 2000.
Americium (,) is a synthetic element that has the symbol Am and atomic number 95. A radioactive metallic element, americium is an actinide that was obtained in 1944 by Glenn T. Seaborg who was bombarding plutonium with neutrons and was the fourth transuranic element to be discovered. It was named for the Americas, by analogy with europium. Americium is widely used in commercial ionization chamber smoke detectors, as well as in neutron sources and industrial gauges.
Pure americium has a silvery and white luster. At room temperature it slowly tarnishes in dry air. It is more silvery than plutonium or neptunium and apparently more malleable than neptunium or uranium. Alpha emission from 241Am is approximately 3.5 times that of 226radium. 241Am emits low energy gamma rays, creating a serious exposure problem for anyone handling gram quantities of the element. Americium is also fissile; the critical mass for an unreflected sphere of 241Am is approximately 60 kilograms. It is unlikely that americium would be used as a weapons material, as its minimum critical mass is considerably larger than that of more readily obtained plutonium or uranium isotopes.
Americium oxidizes to AmO in air. Similarly, reaction with hydrogen results in AmH2 where Am is divalent. However, the most common oxidation state of Am is +3, especially in solutions which are colored red. It is much harder to oxidize Am(III) to Am(IV) than it is to oxidize Pu(III) to Pu(IV). Americium, unlike uranium, does not readily form a dioxide americyl core (AmO2). This is because americium is very hard to oxidise above the +3 oxidation state when it is in an aqueous solution. In the environment, this americyl core could complex with carbonate as well as other oxygen moieties (OH, NO, NO, and SO) to form charged complexes which tend to be readily mobile with low affinities to soil: AmO2(OH)+, AmO2(OH), AmO2CO, AmO2(CO3) and AmO2(CO3). Examples of americium +4 compounds are Am(OH)4 and AmF4. All pentavalent and hexavalent americium compounds are complex salts such as KAmO2F2, Li3AmO4 and Li6AmO6, Ba3AmO6, AmO2F2. Hexavalent americium is a strong oxidizing agent and is reduced to AmO2+ in oxidation-reduction reactions.
A large amount of work has been done on the solvent extraction of americium, as americium and other transuranic elements are responsible for much of the long-lived radiotoxicity of spent nuclear fuel. It is thought that by removal of the americium and curium that the used fuel will only need to be isolated from people and the environment for a shorter time than that required for the isolation of untreated used fuel. One recent EU funded project on this topic was known by the codename "EUROPART". Within this project triazines and other compounds were studied as potential extraction agents.
Eighteen radioisotopes of americium have been characterized, with the most stable being 243Am with a half-life of 7370 years, and 241Am with a half-life of 432.2 years. All of the remaining radioactive isotopes have half-lives that are less than 51 hours, and the majority of these have half-lives that are less than 100 minutes. This element also has 8 meta states, with the most stable being 242mAm (t½ 141 years). The isotopes of americium range in atomic weight from 231.046 u (231Am) to 249.078 u (249Am).
Americium was first isolated by Glenn T. Seaborg, Leon O. Morgan, Ralph A. James, and Albert Ghiorso in late 1944 at the wartime Metallurgical Laboratory at the University of Chicago (now known as Argonne National Laboratory). The team created the isotope 241Am by subjecting 239Pu to successive neutron capture reactions in a nuclear reactor. This created 240Pu and then 241Pu which in turn decayed into 241Am via beta decay. Seaborg was granted a patent for "Element 95 and Method of Producing Said Element", whose unusually terse claim number 1 reads simply, "Element 95." The discovery of americium and curium was first announced informally on a children's quiz show in 1945.
Americium can be produced in kilogram amounts and has some uses, mostly involving 241Am since it is easiest to produce relatively pure samples of this isotope. Americium is the only synthetic element to have found its way into the household, where one common type of smoke detector uses 241Am in the form of americium dioxide as its source of ionizing radiation. The amount of americium in a typical smoke detector when new is 1 microcurie or 0.28 microgram. This amount declines slowly as the americium decays into neptunium-237, a different transuranic element with a much longer half-life (about 2.14 million years). With its half-life of 432.2 years, the americium in a smoke detector includes about 3% neptunium after 19 years, and about 5% after 32 years. 241Am has been used as a portable source of both gamma rays and alpha particles for a number of medical and industrial uses. Gamma ray emissions from 241Am can be used for indirect analysis of materials radiography and for quality control in manufacturing fixed gauges. For example, the element has been employed to gauge glass thickness to help create flat glass. 241Am gamma rays were also used to provide passive diagnosis of thyroid function. This medical application is obsolete. 241Am can be combined with lighter elements (e.g., beryllium or lithium) to become a neutron emitter. This application has found uses in neutron radiography as well as a neutron emitting radioactive source. The most widespread use of 241AmBe neutron sources is found in moisture/density gauges used for quality control in highway construction. 241Am neutron sources are also critical for well logging applications. 242mAm has been cited for use as an advanced nuclear rocket propulsion fuel. This isotope is, however, extremely expensive to produce in usable quantities. 241Am has recently been suggested for use as a denaturing agent in plutonium reactor fuel rods to render the fuel unusable for conversion to nuclear weapons.
Astatine (or) is a radioactive chemical element with the symbol At and atomic number 85. It is the heaviest of the discovered halogens. Although astatine is produced by radioactive decay in nature, due to its short half life it is found only in minute amounts. Astatine was first produced by Dale R. Corson, Kenneth Ross MacKenzie, and Emilio Segrè in 1940. Three years passed before traces of astatine were also found in natural minerals. Until recently most of the physical and chemical characteristics of astatine were inferred from comparison with other elements. Some astatine isotopes are used as alpha-particle emitters in science applications, and medical applications for astatine 211 have been tested. Astatine is currently the rarest naturally-occurring element, with less than 30g estimated to be contained in the entire Earth's crust.
This highly radioactive element has been confirmed by mass spectrometers to behave chemically much like other halogens, especially iodine (it would probably accumulate in the thyroid gland like iodine), though astatine is thought to be more metallic than iodine. Researchers at the Brookhaven National Laboratory have performed experiments that have identified and measured elementary reactions that involve astatine; however, chemical research into astatine is limited by its extreme rarity, which is a consequence of its extremely short half-life. Its most stable isotope has a half-life of around 8.3 hours. The final products of the decay of astatine are isotopes of lead. The halogens get darker in color with increasing molecular weight and atomic number. Thus, following the trend, astatine would be expected to be a nearly black solid, which, when heated, sublimes into a dark, purplish vapor (darker than iodine). Astatine is expected to form ionic bonds with metals such as sodium, like the other halogens, but it can be displaced from the salts by lighter, more reactive halogens. Astatine can also react with hydrogen to form hydrogen astatide, which when dissolved in water, forms the exceptionally strong hydroastatic acid. Astatine is the least reactive of the halogens, being less reactive than iodine.
The existence of "eka-iodine" had been predicted by Dmitri Mendeleev. Astatine (after Greek αστατος "astatos", meaning "unstable") was first synthesized in 1940 by Dale R. Corson, Kenneth Ross MacKenzie, and Emilio Segrè at the University of California, Berkeley by bombarding bismuth with alpha particles. As the periodic table of elements was long known, several scientists tried to find the element following iodine in the halogen group. The unknown substance was called Eka-iodine before its discovery because the name of the element was to be suggested by the discoverer. The claimed discovery in 1931 at the Alabama Polytechnic Institute (now Auburn University) by Fred Allison and associates, led to the spurious name for the element as "alabamine" (Ab) for a few years. This discovery was later shown to be an erroneous one. The name "dakin" was proposed for this element in 1937 by the chemist Rajendralal De working in Dhaka, Bangladesh. The name helvetium was chosen by the Swiss chemist Walter Minder, when he announced the discovery of element 85 in 1940, but changed his suggested name to anglohelvetium in 1942. It took three years before astatine was found as product of the natural decay processes. The short-lived element was found by the two scientists Berta Karlik and Traude Bernert.
Astatine occurs naturally in three natural radioactive decay series, but because of its short half-life is found only in minute amounts. Astatine-218 (218At) is found in the uranium series, 216At is in the thorium series, and 215At as well as 219At are in the actinium series. The most long-lived of these naturally-occurring astatine isotopes is 219At with a half-life of 56 seconds. Astatine is the rarest naturally-occurring element, with the total amount in Earth's crust estimated to be less than 1 oz (28 g) at any given time. This amounts to less than one teaspoon of the element. "Guinness World Records" has dubbed the element the rarest on Earth, stating: "Only around 0.9 oz (25 g) of the element astatine (At) occurring naturally". Isaac Asimov, in a 1957 essay on large numbers, scientific notation, and the size of the atom, wrote that in "all of North and South America to a depth of ten miles", the number of astatine-215 atoms at any time is "only a trillion".
Multiple compounds of astatine have been synthesized in microscopic amounts and studied as intensively as possible before their inevitable radioactive disintegration. The reactions are normally tested with dilute solutions of astatine mixed with larger amounts of iodine. The iodine acts as a carrier, ensuring that there is sufficient material for laboratory techniques such as filtration and precipitation to work. While these compounds are primarily of theoretical interest, they are being studied for potential use in nuclear medicine. Astatine is expected to form ionic bonds with metals such as sodium, like the other halogens, but it can be displaced from the salts by lighter, more reactive halogens. Astatine can also react with hydrogen to form hydrogen astatide (HAt), which when dissolved in water, forms hydroastatic acid. Some examples of astatic compounds are
The least stable isotopes of astatine have no practical applications other than scientific study due to their extremely short life, but heavier isotopes have medical uses. Astatine-211 is an alpha emitter with a physical half-life of 7.2 h. These features have led to its use in radiation therapy. An investigation of the efficacy of astatine-211–tellurium colloid for the treatment of experimental malignant ascites in mice reveals that this alpha-emitting radiocolloid can be curative without causing undue toxicity to normal tissue. By comparison, beta-emitting phosphorus-32 as colloidal chromic phosphate had no antineoplastic activity. The most compelling explanation for this striking difference is the dense ionization and short range of action associated with alpha-emission. These results have important implications for the development and use of alpha-emitters as radiocolloid therapy for the treatment of human tumors.
Since astatine is extremely radioactive, it should be handled with extreme care. Because of its extreme rarity, it is not likely that the general public will be exposed. Astatine is a halogen, and standard precautions apply. It is reactive, sharing similar chemical characteristics with iodine. There are toxicologic studies of astatine-211 on mice indicating that radioactive poisoning is the major effect on living organisms.
The atom is a basic unit of matter consisting of a dense, central nucleus surrounded by a cloud of negatively charged electrons. The atomic nucleus contains a mix of positively charged protons and electrically neutral neutrons (except in the case of hydrogen-1, which is the only stable nuclide with no neutron). The electrons of an atom are bound to the nucleus by the electromagnetic force. Likewise, a group of atoms can remain bound to each other, forming a molecule. An atom containing an equal number of protons and electrons is electrically neutral, otherwise it has a positive or negative charge and is an ion. An atom is classified according to the number of protons and neutrons in its nucleus: the number of protons determines the chemical element, and the number of neutrons determine the isotope of the element. The name atom comes from the Greek ἄτομος/átomos, α-τεμνω, which means uncuttable, or indivisible, something that cannot be divided further. The concept of an atom as an indivisible component of matter was first proposed by early Indian and Greek philosophers. In the 17th and 18th centuries, chemists provided a physical basis for this idea by showing that certain substances could not be further broken down by chemical methods. During the late 19th and early 20th centuries, physicists discovered subatomic components and structure inside the atom, thereby demonstrating that the 'atom' was divisible. The principles of quantum mechanics were used to successfully model the atom. Relative to everyday experience, atoms are minuscule objects with proportionately tiny masses. Atoms can only be observed individually using special instruments such as the scanning tunneling microscope. Over 99.9% of an atom's mass is concentrated in the nucleus, with protons and neutrons having roughly equal mass. Each element has at least one isotope with unstable nuclei that can undergo radioactive decay. This can result in a transmutation that changes the number of protons or neutrons in a nucleus. Electrons that are bound to atoms possess a set of stable energy levels, or orbitals, and can undergo transitions between them by absorbing or emitting photons that match the energy differences between the levels. The electrons determine the chemical properties of an element, and strongly influence an atom's magnetic properties.
The concept that matter is composed of discrete units and cannot be divided into arbitrarily tiny quantities has been around for millennia, but these ideas were founded in abstract, philosophical reasoning rather than experimentation and empirical observation. The nature of atoms in philosophy varied considerably over time and between cultures and schools, and often had spiritual elements. Nevertheless, the basic idea of the atom was adopted by scientists thousands of years later because it elegantly explained new discoveries in the field of chemistry. The earliest references to the concept of atoms date back to ancient India in the 6th century BCE, appearing first in Jainism. The Nyaya and Vaisheshika schools developed elaborate theories of how atoms combined into more complex objects. In the West, the references to atoms emerged a century later from Leucippus, whose student, Democritus, systematized his views. In approximately 450 BCE, Democritus coined the term "átomos" (), which means "uncuttable" or "the smallest indivisible particle of matter". Although the Indian and Greek concepts of the atom were based purely on philosophy, modern science has retained the name coined by Democritus. in the 13th-century by the alchemist Pseudo-Geber (Geber), that all physical bodies possess an inner and outer layer of minute particles or corpuscles. Corpuscularianism is similar to the theory atomism, except that where atoms were supposed to be indivisible, corpuscles could in principle be divided. In this manner, for example, it was theorized that mercury could penetrate into metals and modify their inner structure. Corpuscularianism stayed a dominant theory over the next several hundred years and was blended with alchemy by Robert Boyle and Isaac Newton in the 17th century. It was used by Newton, for instance, in his development of the corpuscular theory of light.
Further progress in the understanding of atoms did not occur until the science of chemistry began to develop. In 1661, natural philosopher Robert Boyle published "The Sceptical Chymist" in which he argued that matter was composed of various combinations of different "corpuscules" or atoms, rather than the classical elements of air, earth, fire and water. In 1789 the term "element" was defined by the French nobleman and scientific researcher Antoine Lavoisier to mean basic substances that could not be further broken down by the methods of chemistry. In 1803, English instructor and natural philosopher John Dalton used the concept of atoms to explain why elements always react in a ratio of small whole numbers—the law of multiple proportions—and why certain gases dissolve better in water than others. He proposed that each element consists of atoms of a single, unique type, and that these atoms can join together to form chemical compounds. Dalton is considered the originator of modern atomic theory. Additional validation of particle theory (and by extension atomic theory) occurred in 1827 when botanist Robert Brown used a microscope to look at dust grains floating in water and discovered that they moved about erratically—a phenomenon that became known as "Brownian motion". J. Desaulx suggested in 1877 that the phenomenon was caused by the thermal motion of water molecules, and in 1905 Albert Einstein produced the first mathematical analysis of the motion. French physicist Jean Perrin used Einstein's work to experimentally determine the mass and dimensions of atoms, thereby conclusively verifying Dalton's atomic theory. In 1869, building upon earlier discoveries by such scientists as Lavoisier, Dmitri Mendeleev published the first functional periodic table. The table itself is a visual representation of the periodic law which states certain chemical properties of elements repeat "periodically" when arranged by atomic number.
The physicist J. J. Thomson, through his work on cathode rays in 1897, discovered the electron. These subatomic particles had the same properties, regardless of the type of atom whence they came. This universal component of all atoms destroyed the concept of atoms as being indivisible units. Thomson postulated that the low mass, negatively-charged electrons were distributed throughout the atom, possibly rotating in rings, with their charge balanced by the presence of a uniform sea of positive charge. This later became known as the plum pudding model. In 1909, Hans Geiger and Ernest Marsden, under the direction of physicist Ernest Rutherford, bombarded a sheet of gold foil with alpha rays—by then known to be positively charged helium atoms—and discovered that a small percentage of these particles were deflected through much larger angles than was predicted using Thomson's proposal. Rutherford interpreted the gold foil experiment as suggesting that the positive charge of a heavy gold atom and most of its mass was concentrated in a nucleus at the center of the atom—the Rutherford model. While experimenting with the products of radioactive decay, in 1913 radiochemist Frederick Soddy discovered that there appeared to be more than one type of atom at each position on the periodic table. The term isotope was coined by Margaret Todd as a suitable name for different atoms that belong to the same element. J.J. Thomson created a technique for separating atom types through his work on ionized gases, which subsequently led to the discovery of stable isotopes. Meanwhile, in 1913, physicist Niels Bohr suggested that the electrons were confined into clearly defined, quantized orbits, and could jump between these, but could not freely spiral inward or outward in intermediate states. An electron must absorb or emit specific amounts of energy to transition between these fixed orbits. When the light from a heated material was passed through a prism, it produced a multi-colored spectrum. The appearance of fixed lines in this spectrum was successfully explained by these orbital transitions. Chemical bonds between atoms were now explained, by Gilbert Newton Lewis in 1916, as the interactions between their constituent electrons. As the chemical properties of the elements were known to largely repeat themselves according to the periodic law, in 1919 the American chemist Irving Langmuir suggested that this could be explained if the electrons in an atom were connected or clustered in some manner. Groups of electrons were thought to occupy a set of electron shells about the nucleus. The Stern–Gerlach experiment of 1922 provided further evidence of the quantum nature of the atom. When a beam of silver atoms was passed through a specially shaped magnetic field, the beam was split based on the direction of an atom's angular momentum, or spin. As this direction is random, the beam could be expected to spread into a line. Instead, the beam was split into two parts, depending on whether the atomic spin was oriented up or down. In 1926, Erwin Schrödinger, using Louis de Broglie's 1924 proposal that particles behave to an extent like waves, developed a mathematical model of the atom that described the electrons as three-dimensional waveforms, rather than point particles. A consequence of using waveforms to describe particles is that it is mathematically impossible to obtain precise values for both the position and momentum of a particle at the same time; this became known as the uncertainty principle, formulated by Werner Heisenberg in 1926. In this concept, for a given accuracy in measuring a position one could only obtain a range of probable values for momentum, and vice versa. This model was able to explain observations of atomic behavior that previous models could not, such as certain structural and spectral patterns of atoms larger than hydrogen. Thus, the planetary model of the atom was discarded in favor of one that described atomic orbital zones around the nucleus where a given electron is most likely to be observed. The development of the mass spectrometer allowed the exact mass of atoms to be measured. The device uses a magnet to bend the trajectory of a beam of ions, and the amount of deflection is determined by the ratio of an atom's mass to its charge. The chemist Francis William Aston used this instrument to show that isotopes had different masses. The atomic mass of these isotopes varied by integer amounts, called the whole number rule. The explanation for these different isotopes awaited the discovery of the neutron, a neutral-charged particle with a mass similar to the proton, by the physicist James Chadwick in 1932. Isotopes were then explained as elements with the same number of protons, but different numbers of neutrons within the nucleus. Fission, high energy physics and condensed matter. In 1938, the German chemist Otto Hahn, a student of Rutherford, directed neutrons onto uranium atoms expecting to get transuranium elements. Instead, his chemical experiments showed barium as a product. A year later, Lise Meitner and her nephew Otto Frisch verified that Hahn's result were the first experimental "nuclear fission". In 1944, Hahn received the Nobel prize in chemistry in which, despite the efforts of Hahn, the contributions of Meitner and Frisch were not recognized. In the 1950s, the development of improved particle accelerators and particle detectors allowed scientists to study the impacts of atoms moving at high energies. Neutrons and protons were found to be hadrons, or composites of smaller particles called quarks. Standard models of nuclear physics were developed that successfully explained the properties of the nucleus in terms of these sub-atomic particles and the forces that govern their interactions. Around 1985, Steven Chu and co-workers at Bell Labs developed a technique for lowering the temperatures of atoms using lasers. In the same year, a team led by William D. Phillips managed to contain atoms of sodium in a magnetic trap. The combination of these two techniques and a method based on the Doppler effect, developed by Claude Cohen-Tannoudji and his group, allows small numbers of atoms to be cooled to several microkelvin. This allows the atoms to be studied with great precision, and later led to the Nobel prize-winning discovery of Bose-Einstein condensation. Historically, single atoms have been prohibitively small for scientific applications. Recently, devices have been constructed that use a single metal atom connected through organic ligands to construct a single electron transistor. Experiments have been carried out by trapping and slowing single atoms using laser cooling in a cavity to gain a better physical understanding of matter.
Though the word "atom" originally denoted a particle that cannot be cut into smaller particles, in modern scientific usage the atom is composed of various subatomic particles. The constituent particles of an atom are the electron, the proton and the neutron. However, the hydrogen-1 atom has no neutrons and a positive hydrogen ion has no electrons. The electron is by far the least massive of these particles at 9.11 kg, with a negative electrical charge and a size that is too small to be measured using available techniques. Protons have a positive charge and a mass 1,836 times that of the electron, at 1.6726 kg, although this can be reduced by changes to the energy binding the proton into an atom. Neutrons have no electrical charge and have a free mass of 1,839 times the mass of electrons, or 1.6929 kg. Neutrons and protons have comparable dimensions—on the order of 2.5 m—although the 'surface' of these particles is not sharply defined. In the Standard Model of physics, both protons and neutrons are composed of elementary particles called quarks. The quark belongs to the fermion group of particles, and is one of the two basic constituents of matter—the other being the lepton, of which the electron is an example. There are six types of quarks, each having a fractional electric charge of either +2/3 or −1/3. Protons are composed of two up quarks and one down quark, while a neutron consists of one up quark and two down quarks. This distinction accounts for the difference in mass and charge between the two particles. The quarks are held together by the strong nuclear force, which is mediated by gluons. The gluon is a member of the family of gauge bosons, which are elementary particles that mediate physical forces.
where "A" is the total number of nucleons. This is much smaller than the radius of the atom, which is on the order of 105 fm. The nucleons are bound together by a short-ranged attractive potential called the residual strong force. At distances smaller than 2.5 fm this force is much more powerful than the electrostatic force that causes positively charged protons to repel each other. Atoms of the same element have the same number of protons, called the atomic number. Within a single element, the number of neutrons may vary, determining the isotope of that element. The total number of protons and neutrons determine the nuclide. The number of neutrons relative to the protons determines the stability of the nucleus, with certain isotopes undergoing radioactive decay. The neutron and the proton are different types of fermions. The Pauli exclusion principle is a quantum mechanical effect that prohibits "identical" fermions, such as multiple protons, from occupying the same quantum physical state at the same time. Thus every proton in the nucleus must occupy a different state, with its own energy level, and the same rule applies to all of the neutrons. This prohibition does not apply to a proton and neutron occupying the same quantum state. For atoms with low atomic numbers, a nucleus that has a different number of protons than neutrons can potentially drop to a lower energy state through a radioactive decay that causes the number of protons and neutrons to more closely match. As a result, atoms with roughly matching numbers of protons and neutrons are more stable against decay. However, with increasing atomic number, the mutual repulsion of the protons requires an increasing proportion of neutrons to maintain the stability of the nucleus, which modifies this trend. Thus, there are no stable nuclei with equal proton and neutron numbers above atomic number Z = 20 (calcium); and as Z increases toward the heaviest nuclei, the ratio of neutrons per proton required for stability increases to about 1.5. The number of protons and neutrons in the atomic nucleus can be modified, although this can require very high energies because of the strong force. Nuclear fusion occurs when multiple atomic particles join to form a heavier nucleus, such as through the energetic collision of two nuclei. For example, at the core of the Sun protons require energies of 3–10 keV to overcome their mutual repulsion—the coulomb barrier—and fuse together into a single nucleus. Nuclear fission is the opposite process, causing a nucleus to split into two smaller nuclei—usually through radioactive decay. The nucleus can also be modified through bombardment by high energy subatomic particles or photons. If this modifies the number of protons in a nucleus, the atom changes to a different chemical element. If the mass of the nucleus following a fusion reaction is less than the sum of the masses of the separate particles, then the difference between these two values may be emitted as a type of usable energy (such as a gamma ray, or the kinetic energy of a beta particle), as described by Albert Einstein's mass–energy equivalence formula, "E" = "mc"2, where "m" is the mass loss and "c" is the speed of light. This deficit is part of the binding energy of the new nucleus, and it is the non-recoverable loss of the energy which causes the fused particles to remain together in a state which require this energy to separate. The fusion of two nuclei that create larger nuclei with lower atomic numbers than iron and nickel—a total nucleon number of about 60—is usually an exothermic process that releases more energy than is required to bring them together. It is this energy-releasing process that makes nuclear fusion in stars a self-sustaining reaction. For heavier nuclei, the binding energy per nucleon in the nucleus begins to decrease. That means fusion processes producing nuclei that have atomic numbers higher than about 26, and atomic masses higher than about 60, is an endothermic process. These more massive nuclei can not undergo an energy-producing fusion reaction that can sustain the hydrostatic equilibrium of a star.
The electrons in an atom are attracted to the protons in the nucleus by the electromagnetic force. This force binds the electrons inside an electrostatic potential well surrounding the smaller nucleus, which means that an external source of energy is needed in order for the electron to escape. The closer an electron is to the nucleus, the greater the attractive force. Hence electrons bound near the center of the potential well require more energy to escape than those at greater separations. Electrons, like other particles, have properties of both a particle and a wave. The electron cloud is a region inside the potential well where each electron forms a type of three-dimensional standing wave—a wave form that does not move relative to the nucleus. This behavior is defined by an atomic orbital, a mathematical function that characterises the probability that an electron will appear to be at a particular location when its position is measured. Only a discrete (or quantized) set of these orbitals exist around the nucleus, as other possible wave patterns will rapidly decay into a more stable form. Orbitals can have one or more ring or node structures, and they differ from each other in size, shape and orientation. Each atomic orbital corresponds to a particular energy level of the electron. The electron can change its state to a higher energy level by absorbing a photon with sufficient energy to boost it into the new quantum state. Likewise, through spontaneous emission, an electron in a higher energy state can drop to a lower energy state while radiating the excess energy as a photon. These characteristic energy values, defined by the differences in the energies of the quantum states, are responsible for atomic spectral lines. The amount of energy needed to remove or add an electron—the electron binding energy—is far less than the binding energy of nucleons. For example, it requires only 13.6 eV to strip a ground-state electron from a hydrogen atom, compared to 2.23 "million" eV for splitting a deuterium nucleus. Atoms are electrically neutral if they have an equal number of protons and electrons. Atoms that have either a deficit or a surplus of electrons are called ions. Electrons that are farthest from the nucleus may be transferred to other nearby atoms or shared between atoms. By this mechanism, atoms are able to bond into molecules and other types of chemical compounds like ionic and covalent network crystals. The structure of the cloud varies with the number of electrons present in the cloud. There exist a number of different methods of electron counting, such as the octet rule and eighteen electron rule. These tend to be rules of thumb and are not valid across all atoms. Beginning chemistry students are often told the shell structure is simply 2, 8, 8, 8, 8, 8, 8, [...] to make the teaching process easier. The actual numbers of electrons per shell in the larger atoms can be considerably different, such as 2, 8, 18, 32, 50, 72, but this complexity is reserved for the more advanced student.
By definition, any two atoms with an identical number of "protons" in their nuclei belong to the same chemical element. Atoms with equal numbers of protons but a different number of "neutrons" are different isotopes of the same element. For example, all hydrogen atoms admit exactly one proton, but isotopes exist with no neutrons hydrogen-1, one neutron (deuterium), two neutrons (tritium) and more than two neutrons. The hydrogen-1 is by far the most common form, and is sometimes called protium. The known elements form a set of atomic numbers from hydrogen with a single proton up to the 118-proton element ununoctium. All known isotopes of elements with atomic numbers greater than 82 are radioactive. About 339 nuclides occur naturally on Earth, of which 256 (about 76%) have not been observed to decay, and are referred to as "stable isotopes". For 80 of the chemical elements, there is at least one stable isotope. Elements 43, 61, and all elements numbered 83 or higher have no stable isotopes. As a rule, there is, for each element, only a handful of stable isotopes, the average being 3.1 stable isotopes per element which has any stable isotopes. Twenty-seven elements have only a single stable isotope, while the largest number of stable isotopes observed for any element is ten, for the element tin. Stability of isotopes is affected by the ratio of protons to neutrons, and also by presence of certain "magic numbers" of neutrons or protons which represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within the shell model of the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. Of the 256 known stable nuclides, only four have both an odd number of protons "and" odd number of neutrons: hydrogen-2 (deuterium), lithium-6, boron-10 and nitrogen-14. Also, only four naturally occurring, radioactive odd-odd nuclides have a half-life over a billion years: potassium-40, vanadium-50, lanthanum-138 and tantalum-180m. Most odd-odd nuclei are highly unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects.
Because the large majority of an atom's mass comes from the protons and neutrons, the total number of these particles in an atom is called the mass number. The mass of an atom at rest is often expressed using the unified atomic mass unit (u), which is also called a Dalton (Da). This unit is defined as a twelfth of the mass of a free neutral atom of carbon-12, which is approximately 1.66 kg. Hydrogen-1, the lightest isotope of hydrogen and the atom with the lowest mass, has an atomic weight of 1.007825 u. An atom has a mass approximately equal to the mass number times the atomic mass unit. The heaviest stable atom is lead-208, with a mass of 207.9766521 u. As even the most massive atoms are far too light to work with directly, chemists instead use the unit of moles. The mole is defined such that one mole of any element will always have the same number of atoms (about 6.022). This number was chosen so that if an element has an atomic mass of 1 u, a mole of atoms of that element will have a mass very close to 0.001 kg, or 1 gram. Because of the definition of the unified atomic mass unit, carbon has an atomic mass of "exactly" 12 u, and so a mole of carbon atoms weighs exactly 0.012 kg. Other nuclides have atomic masses and molar masses very close to whole numbers in their usual units, such as hydrogen-1. However, except for carbon-12, they cannot be exactly integer numbers, because the masses of different nuclides are not exact integer ratios of each other, although they do not differ from whole number ratios by more than 1%, and often much less.
Atoms lack a well-defined outer boundary, so the dimensions are usually described in terms of the distances between two nuclei when the two atoms are joined in a chemical bond. The radius varies with the location of an atom on the atomic chart, the type of chemical bond, the number of neighboring atoms (coordination number) and a quantum mechanical property known as spin. On the periodic table of the elements, atom size tends to increase when moving down columns, but decrease when moving across rows (left to right). Consequently, the smallest atom is helium with a radius of 32 pm, while one of the largest is caesium at 225 pm. These dimensions are thousands of times smaller than the wavelengths of light (400–700 nm) so they can not be viewed using an optical microscope. However, individual atoms can be observed using a scanning tunneling microscope. Some examples will demonstrate the minuteness of the atom. A typical human hair is about 1 million carbon atoms in width. A single drop of water contains about 2 sextillion (2) atoms of oxygen, and twice the number of hydrogen atoms. A single carat diamond with a mass of 2 kg contains about 10 sextillion (1022) atoms of carbon. If an apple were magnified to the size of the Earth, then the atoms in the apple would be approximately the size of the original apple.
Every element has one or more isotopes that have unstable nuclei that are subject to radioactive decay, causing the nucleus to emit particles or electromagnetic radiation. Radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force, which only acts over distances on the order of 1 fm. Other more rare types of radioactive decay include ejection of neutrons or protons or clusters of nucleons from a nucleus, or more than one beta particle, or result (through internal conversion) in production of high-speed electrons which are not beta rays, and high-energy photons which are not gamma rays. Each radioactive isotope has a characteristic decay time period—the half-life—that is determined by the amount of time needed for half of a sample to decay. This is an exponential decay process that steadily decreases the proportion of the remaining isotope by 50% every half life. Hence after two half-lives have passed only 25% of the isotope will be present, and so forth.
Elementary particles possess an intrinsic quantum mechanical property known as spin. This is analogous to the angular momentum of an object that is spinning around its center of mass, although strictly speaking these particles are believed to be point-like and cannot be said to be rotating. Spin is measured in units of the reduced Planck constant (ħ), with electrons, protons and neutrons all having spin ½ ħ, or "spin-½". In an atom, electrons in motion around the nucleus possess orbital angular momentum in addition to their spin, while the nucleus itself possesses angular momentum due to its nuclear spin. The magnetic field produced by an atom—its magnetic moment—is determined by these various forms of angular momentum, just as a rotating charged object classically produces a magnetic field. However, the most dominant contribution comes from spin. Due to the nature of electrons to obey the Pauli exclusion principle, in which no two electrons may be found in the same quantum state, bound electrons pair up with each other, with one member of each pair in a spin up state and the other in the opposite, spin down state. Thus these spins cancel each other out, reducing the total magnetic dipole moment to zero in some atoms with even number of electrons. In ferromagnetic elements such as iron, an odd number of electrons leads to an unpaired electron and a net overall magnetic moment. The orbitals of neighboring atoms overlap and a lower energy state is achieved when the spins of unpaired electrons are aligned with each other, a process known as an exchange interaction. When the magnetic moments of ferromagnetic atoms are lined up, the material can produce a measurable macroscopic field. Paramagnetic materials have atoms with magnetic moments that line up in random directions when no magnetic field is present, but the magnetic moments of the individual atoms line up in the presence of a field. The nucleus of an atom can also have a net spin. Normally these nuclei are aligned in random directions because of thermal equilibrium. However, for certain elements (such as xenon-129) it is possible to polarize a significant proportion of the nuclear spin states so that they are aligned in the same direction—a condition called hyperpolarization. This has important applications in magnetic resonance imaging.
When an electron is bound to an atom, it has a potential energy that is inversely proportional to its distance from the nucleus. This is measured by the amount of energy needed to unbind the electron from the atom, and is usually given in units of electronvolts (eV). In the quantum mechanical model, a bound electron can only occupy a set of states centered on the nucleus, and each state corresponds to a specific energy level. The lowest energy state of a bound electron is called the ground state, while an electron at a higher energy level is in an excited state. In order for an electron to transition between two different states, it must absorb or emit a photon at an energy matching the difference in the potential energy of those levels. The energy of an emitted photon is proportional to its frequency, so these specific energy levels appear as distinct bands in the electromagnetic spectrum. Each element has a characteristic spectrum that can depend on the nuclear charge, subshells filled by electrons, the electromagnetic interactions between the electrons and other factors. When a continuous spectrum of energy is passed through a gas or plasma, some of the photons are absorbed by atoms, causing electrons to change their energy level. Those excited electrons that remain bound to their atom will spontaneously emit this energy as a photon, traveling in a random direction, and so drop back to lower energy levels. Thus the atoms behave like a filter that forms a series of dark absorption bands in the energy output. (An observer viewing the atoms from a different direction, which does not include the continuous spectrum in the background, will instead see a series of emission lines from the photons emitted by the atoms.) Spectroscopic measurements of the strength and width of spectral lines allow the composition and physical properties of a substance to be determined. Close examination of the spectral lines reveals that some display a fine structure splitting. This occurs because of spin-orbit coupling, which is an interaction between the spin and motion of the outermost electron. When an atom is in an external magnetic field, spectral lines become split into three or more components; a phenomenon called the Zeeman effect. This is caused by the interaction of the magnetic field with the magnetic moment of the atom and its electrons. Some atoms can have multiple electron configurations with the same energy level, which thus appear as a single spectral line. The interaction of the magnetic field with the atom shifts these electron configurations to slightly different energy levels, resulting in multiple spectral lines. The presence of an external electric field can cause a comparable splitting and shifting of spectral lines by modifying the electron energy levels, a phenomenon called the Stark effect. If a bound electron is in an excited state, an interacting photon with the proper energy can cause stimulated emission of a photon with a matching energy level. For this to occur, the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon. The emitted photon and the interacting photon will then move off in parallel and with matching phases. That is, the wave patterns of the two photons will be synchronized. This physical property is used to make lasers, which can emit a coherent beam of light energy in a narrow frequency band.
The outermost electron shell of an atom in its uncombined state is known as the valence shell, and the electrons in that shell are called valence electrons. The number of valence electrons determines the bonding behavior with other atoms. Atoms tend to chemically react with each other in a manner that will fill (or empty) their outer valence shells. For example, a transfer of a single electron between atoms is a useful approximation for bonds which form between atoms which have one-electron more than a filled shell, and others which are one-electron short of a full shell, such as occurs in the compound sodium chloride and other chemical ionic salts. However, many elements display multiple valences, or tendencies to share differing numbers of electrons in different compounds. Thus, chemical bonding between these elements takes many forms of electron-sharing that are more than simple electron transfers. Examples include the element carbon and the organic compounds. The chemical elements are often displayed in a periodic table that is laid out to display recurring chemical properties, and elements with the same number of valence electrons form a group that is aligned in the same column of the table. (The horizontal rows correspond to the filling of a quantum shell of electrons.) The elements at the far right of the table have their outer shell completely filled with electrons, which results in chemically inert elements known as the noble gases.
Quantities of atoms are found in different states of matter that depend on the physical conditions, such as temperature and pressure. By varying the conditions, materials can transition between solids, liquids, gases and plasmas. Within a state, a material can also exist in different phases. An example of this is solid carbon, which can exist as graphite or diamond. At temperatures close to absolute zero, atoms can form a Bose–Einstein condensate, at which point quantum mechanical effects, which are normally only observed at the atomic scale, become apparent on a macroscopic scale. This super-cooled collection of atoms then behaves as a single super atom, which may allow fundamental checks of quantum mechanical behavior.
The scanning tunneling microscope is a device for viewing surfaces at the atomic level. It uses the quantum tunneling phenomenon, which allows particles to pass through a barrier that would normally be insurmountable. Electrons tunnel through the vacuum between two planar metal electrodes, on each of which is an adsorbed atom, providing a tunneling-current density that can be measured. Scanning one atom (taken as the tip) as it moves past the other (the sample) permits plotting of tip displacement versus lateral separation for a constant current. The calculation shows the extent to which scanning-tunneling-microscope images of an individual atom are visible. It confirms that for low bias, the microscope images the space-averaged dimensions of the electron orbitals across closely packed energy levels—the Fermi level local density of states. An atom can be ionized by removing one of its electrons. The electric charge causes the trajectory of an atom to bend when it passes through a magnetic field. The radius by which the trajectory of a moving ion is turned by the magnetic field is determined by the mass of the atom. The mass spectrometer uses this principle to measure the mass-to-charge ratio of ions. If a sample contains multiple isotopes, the mass spectrometer can determine the proportion of each isotope in the sample by measuring the intensity of the different beams of ions. Techniques to vaporize atoms include inductively coupled plasma atomic emission spectroscopy and inductively coupled plasma mass spectrometry, both of which use a plasma to vaporize samples for analysis. A more area-selective method is electron energy loss spectroscopy, which measures the energy loss of an electron beam within a transmission electron microscope when it interacts with a portion of a sample. The atom-probe tomograph has sub-nanometer resolution in 3-D and can chemically identify individual atoms using time-of-flight mass spectrometry. Spectra of excited states can be used to analyze the atomic composition of distant stars. Specific light wavelengths contained in the observed light from stars can be separated out and related to the quantized transitions in free gas atoms. These colors can be replicated using a gas-discharge lamp containing the same element. Helium was discovered in this way in the spectrum of the Sun 23 years before it was found on Earth.
Atoms form about 4% of the total energy density of the observable universe, with an average density of about 0.25 atoms/m3. Within a galaxy such as the Milky Way, atoms have a much higher concentration, with the density of matter in the interstellar medium (ISM) ranging from 105 to 109 atoms/m3. The Sun is believed to be inside the Local Bubble, a region of highly ionized gas, so the density in the solar neighborhood is only about 103 atoms/m3. Stars form from dense clouds in the ISM, and the evolutionary processes of stars result in the steady enrichment of the ISM with elements more massive than hydrogen and helium. Up to 95% of the Milky Way's atoms are concentrated inside stars and the total mass of atoms forms about 10% of the mass of the galaxy. (The remainder of the mass is an unknown dark matter.)
Stable protons and electrons appeared one second after the Big Bang. During the following three minutes, Big Bang nucleosynthesis produced most of the helium, lithium, and deuterium in the universe, and perhaps some of the beryllium and boron. The first atoms (complete with bound electrons) were theoretically created 380,000 years after the Big Bang—an epoch called recombination, when the expanding universe cooled enough to allow electrons to become attached to nuclei. Since then, atomic nuclei have been combined in stars through the process of nuclear fusion to produce elements up to iron. Isotopes such as lithium-6 are generated in space through cosmic ray spallation. This occurs when a high-energy proton strikes an atomic nucleus, causing large numbers of nucleons to be ejected. Elements heavier than iron were produced in supernovae through the r-process and in AGB stars through the s-process, both of which involve the capture of neutrons by atomic nuclei. Elements such as lead formed largely through the radioactive decay of heavier elements.
Most of the atoms that make up the Earth and its inhabitants were present in their current form in the nebula that collapsed out of a molecular cloud to form the Solar System. The rest are the result of radioactive decay, and their relative proportion can be used to determine the age of the Earth through radiometric dating. Most of the helium in the crust of the Earth (about 99% of the helium from gas wells, as shown by its lower abundance of helium-3) is a product of alpha decay. There are a few trace atoms on Earth that were not present at the beginning (i.e., not "primordial"), nor are results of radioactive decay. Carbon-14 is continuously generated by cosmic rays in the atmosphere. Some atoms on Earth have been artificially generated either deliberately or as by-products of nuclear reactors or explosions. Of the transuranic elements—those with atomic numbers greater than 92—only plutonium and neptunium occur naturally on Earth. Transuranic elements have radioactive lifetimes shorter than the current age of the Earth and thus identifiable quantities of these elements have long since decayed, with the exception of traces of plutonium-244 possibly deposited by cosmic dust. Natural deposits of plutonium and neptunium are produced by neutron capture in uranium ore. The Earth contains approximately 1.33 atoms. In the planet's atmosphere, small numbers of independent atoms of noble gases exist, such as argon and neon. The remaining 99% of the atmosphere is bound in the form of molecules, including carbon dioxide and diatomic oxygen and nitrogen. At the surface of the Earth, atoms combine to form various compounds, including water, salt, silicates and oxides. Atoms can also combine to create materials that do not consist of discrete molecules, including crystals and liquid or solid metals. This atomic matter forms networked arrangements that lack the particular type of small-scale interrupted order associated with molecular matter.
While isotopes with atomic numbers higher than lead (82) are known to be radioactive, an "island of stability" has been proposed for some elements with atomic numbers above 103. These superheavy elements may have a nucleus that is relatively stable against radioactive decay. The most likely candidate for a stable superheavy atom, unbihexium, has 126 protons and 184 neutrons. Each particle of matter has a corresponding antimatter particle with the opposite electrical charge. Thus, the positron is a positively charged antielectron and the antiproton is a negatively charged equivalent of a proton. When a matter and corresponding antimatter particle meet, they annihilate each other. Because of this, along with an imbalance between the number of matter and antimatter particles, the latter are rare in the universe. (The first causes of this imbalance is not yet fully understood, although the baryogenesis theories may offer an explanation.) As a result, no antimatter atoms have been discovered in nature. However, in 1996, antihydrogen, the antimatter counterpart of hydrogen, was synthesized at the CERN laboratory in Geneva. Other exotic atoms have been created by replacing one of the protons, neutrons or electrons with other particles that have the same charge. For example, an electron can be replaced by a more massive muon, forming a muonic atom. These types of atoms can be used to test the fundamental predictions of physics.
In geography, arable land (from Latin "arare", to plough) is an agricultural term, meaning land that can be used for growing crops. It is distinct from cultivated land and includes all land where soil and climate is suitable for agriculture, including forests and natural grasslands, and areas falling under human settlement. According to FAO report, the global land area without major soil fertility constraints is about 31.8 million square kilometers, and total potential arable land is about 41.4 million square kilometers. Although constrained by land mass and topology, the amount of arable land, both regionally and globally fluctuates due to human and climactic factors such as irrigation, deforestation, desertification, terracing, landfill, and urban sprawl. Researchers study the impact of these changes on food production. The most productive portion of arable land is that from sediments left by rivers and the sea in geological times. In modern times, the rivers do not generally flood as much agricultural land, due to the demands of flood control to support intensive agriculture required of a heavily-populated Earth.
The Nile continues to flood regularly, overspilling its banks. When the flood is over, the waters recede, leaving behind rich silt. This silt provides excellent fertilizer for crops. Even if the land is over-farmed and all the nutrients are depleted from the soil, the land renews its fertility when new deposits of silt arrive following the next flood. Flood-control projects in the region, such as levees, may increase human comfort but cause substantial adverse impact to the quantity and quality of arable land.
Land which is unsuitable for arable farming usually has at least one of the following deficiencies: no source of fresh water; too hot (desert); too cold (Arctic); too rocky; too mountainous; too salty; too rainy; too snowy; too polluted; or too nutrient poor. Clouds may block the sunlight plants need for photosynthesis, reducing productivity. Plants can starve without light. Starvation and nomadism often exists on marginally arable land. Non-arable land is sometimes called "wasteland", "badlands", "worthless" or "no man's land". However, non-arable land can sometimes be converted into arable land. New arable land makes more food, and can reduce starvation. This outcome also makes a country more self-sufficient and politically independent, because food importation is reduced. Making non-arable land arable often involves digging new irrigation canals and new wells, aqueducts, desalination plants, planting trees for shade in the desert, hydroponics, fertilizer, nitrogen fertilizer, pesticides, reverse osmosis water processors, PET film insulation or other insulation against heat and cold, digging ditches and hills for protection against the wind, and greenhouses with internal light and heat for protection against the cold outside and to provide light in cloudy areas. This process is often extremely expensive.
Aluminium (, or,) or aluminum (, "spelling" below) is a silvery white and ductile member of the boron group of chemical elements. It has the symbol Al; its atomic number is 13. It is not soluble in water under normal circumstances. Aluminium is the most abundant metal in the Earth's crust, and the third most abundant element therein, after oxygen and silicon. It makes up about 8% by weight of the Earth's solid surface. Aluminium is too reactive chemically to occur in nature as a free metal. Instead, it is found combined in over 270 different minerals. The chief source of aluminium is bauxite ore. Aluminium is remarkable for the metal's low density and for its ability to resist corrosion due to the phenomenon of passivation. Structural components made from aluminium and its alloys are vital to the aerospace industry and are very important in other areas of transportation and building. Its reactive nature makes it useful as a catalyst or additive in chemical mixtures, including ammonium nitrate explosives, to enhance blast power.
Aluminium is a soft, durable, lightweight, malleable metal with appearance ranging from silvery to dull grey, depending on the surface roughness. Aluminium is nonmagnetic and nonsparking. It is also insoluble in alcohol, though it can be soluble in water in certain forms. The yield strength of pure aluminium is 7–11 MPa, while aluminium alloys have yield strengths ranging from 200 MPa to 600 MPa. Aluminium has about one-third the density and stiffness of steel. It is ductile, and easily machined, cast, drawn and extruded. Corrosion resistance can be excellent due to a thin surface layer of aluminium oxide that forms when the metal is exposed to air, effectively preventing further oxidation. The strongest aluminium alloys are less corrosion resistant due to galvanic reactions with alloyed copper. This corrosion resistance is also often greatly reduced when many aqueous salts are present, particularly in the presence of dissimilar metals. Aluminium atoms are arranged in a face-centred cubic (fcc) structure. Aluminium has a stacking-fault energy of approximately 200 mJ/m2. Aluminium is one of the few metals that retain full silvery reflectance in finely powdered form, making it an important component of silver paints. Aluminium mirror finish has the highest reflectance of any metal in the 200–400 nm (UV) and the 3,000–10,000 nm (far IR) regions; in the 400–700 nm visible range it is slightly outperformed by tin and silver and in the 700–3000 (near IR) by silver, gold, and copper. Aluminium is a good thermal and electrical conductor, having 62% the conductivity of copper. Aluminium is capable of being a superconductor, with a superconducting critical temperature of 1.2 kelvins and a critical magnetic field of about 100 gauss (10 milliteslas).
Aluminium has nine isotopes, whose mass numbers range from 23 to 30. Only 27Al (stable isotope) and 26Al (radioactive isotope, 2 = 7.2×105 y) occur naturally; however, 27Al has a natural abundance above 99.9%. 26Al is produced from argon in the atmosphere by spallation caused by cosmic-ray protons. Aluminium isotopes have found practical application in dating marine sediments, manganese nodules, glacial ice, quartz in rock exposures, and meteorites. The ratio of 26Al to 10Be has been used to study the role of transport, deposition, sediment storage, burial times, and erosion on 105 to 106 year time scales. Cosmogenic 26Al was first applied in studies of the Moon and meteorites. Meteoroid fragments, after departure from their parent bodies, are exposed to intense cosmic-ray bombardment during their travel through space, causing substantial 26Al production. After falling to Earth, atmospheric shielding protects the meteorite fragments from further 26Al production, and its decay can then be used to determine the meteorite's terrestrial age. Meteorite research has also shown that 26Al was relatively abundant at the time of formation of our planetary system. Most meteorite scientists believe that the energy released by the decay of 26Al was responsible for the melting and differentiation of some asteroids after their formation 4.55 billion years ago.
In the Earth's crust, aluminium is the most abundant (8.3% by weight) metallic element and the third most abundant of all elements (after oxygen and silicon). Because of its strong affinity to oxygen, however, it is almost never found in the elemental state; instead it is found in oxides or silicates. Feldspars, the most common group of minerals in the Earth's crust, are aluminosilicates. Native aluminium metal can be found as a minor phase in low oxygen fugacity environments, such as the interiors of certain volcanoes. It also occurs in the minerals beryl, cryolite, garnet, spinel and turquoise. Impurities in Al2O3, such as chromium or cobalt yield the gemstones ruby and sapphire, respectively. Pure Al2O3, known as corundum, is one of the hardest materials known. Although aluminium is an extremely common and widespread element, the common aluminium minerals are not economic sources of the metal. Almost all metallic aluminium is produced from the ore bauxite (AlO"x"(OH)3-2"x"). Bauxite occurs as a weathering product of low iron and silica bedrock in tropical climatic conditions. Large deposits of bauxite occur in Australia, Brazil, Guinea and Jamaica but the primary mining areas for the ore are in Ghana, Indonesia, Jamaica, Russia and Surinam. Smelting of the ore mainly occurs in Australia, Brazil, Canada, Norway, Russia and the United States. Because smelting is an energy-intensive process, regions with excess natural gas supplies (such as the United Arab Emirates) are becoming aluminium refiners.
Although aluminium is the most abundant metallic element in the Earth's crust, it is rare in its free form, occurring in oxygen-deficient environments such as volcanic mud, and it was once considered a precious metal more valuable than gold. Napoleon III, Emperor of France, is reputed to have given a banquet where the most honoured guests were given aluminium utensils, while the others had to make do with gold. The Washington Monument was completed, with the 100 ounce (2.8 kg) aluminium capstone being put in place on December 6, 1884, in an elaborate dedication ceremony. It was the largest single piece of aluminium cast at the time. At that time, aluminium was as expensive as silver. Aluminium has been produced in commercial quantities for just over 100 years. Aluminium is a strongly reactive metal that forms a high-energy chemical bond with oxygen. Compared to most other metals, it is difficult to extract from ore, such as bauxite, due to the energy required to reduce aluminium oxide (Al2O3). For example, direct reduction with carbon, as is used to produce iron, is not chemically possible, since aluminium is a stronger reducing agent than carbon. However there is an indirect carbothermic reduction possible by using carbon and Al2O3 which forms an intermediate Al4C3 and this can further yield aluminium metal at a temperature of 1900–2000°C. This process is still under development. This process costs less energy and yields less CO2 than the Hall-Héroult process. Aluminium oxide has a melting point of about. Therefore, it must be extracted by electrolysis. In this process, the aluminium oxide is dissolved in molten cryolite and then reduced to the pure metal. The operational temperature of the reduction cells is around. Cryolite is found as a mineral in Greenland, but in industrial use it has been replaced by a synthetic substance. Cryolite is a chemical compound of aluminium, sodium, and calcium fluorides: (Na3AlF6). The aluminium oxide (a white powder) is obtained by refining bauxite in the Bayer process of Karl Bayer. (Previously, the Deville process was the predominant refining technology.) Here the aluminium ion is being reduced. The aluminium metal then sinks to the bottom and is tapped off, usually cast into large blocks called aluminium billets for further processing. The anodes in a reduction cell must therefore be replaced regularly, since they are consumed in the process. Unlike the anodes, the cathodes are not oxidized because there is no oxygen present, as the carbon cathodes are protected by the liquid aluminium inside the cells. Nevertheless, cathodes do erode, mainly due to electrochemical processes and metal movement. After five to ten years, depending on the current used in the electrolysis, a cell has to be rebuilt because of cathode wear. Aluminium electrolysis with the Hall-Héroult process consumes a lot of energy, but alternative processes were always found to be less viable economically and/or ecologically. The worldwide average specific energy consumption is approximately 15±0.5 kilowatt-hours per kilogram of aluminium produced (52 to 56 MJ/kg). The most modern smelters achieve approximately 12.8 kW·h/kg (46.1 MJ/kg). (Compare this to the heat of reaction, 31 MJ/kg, and the Gibbs free energy of reaction, 29 MJ/kg.) Reduction line currents for older technologies are typically 100 to 200 kiloamperes; state-of-the-art smelters operate at about 350 kA. Trials have been reported with 500 kA cells. Electric power represents about 20% to 40% of the cost of producing aluminium, depending on the location of the smelter. Smelters tend to be situated where electric power is both plentiful and inexpensive, such as South Africa, Ghana, the South Island of New Zealand, Australia, the People's Republic of China, the Middle East, Russia, Quebec and British Columbia in Canada, and Iceland. In 2005, the People's Republic of China was the top producer of aluminium with almost a one-fifth world share, followed by Russia, Canada, and the USA, reports the British Geological Survey. Over the last 50 years, Australia has become a major producer of bauxite ore and a major producer and exporter of alumina. Australia produced 62 million tonnes of bauxite in 2005. The Australian deposits have some refining problems, some being high in silica but have the advantage of being shallow and relatively easy to mine.
Aluminium is 100% recyclable without any loss of its natural qualities. Recovery of the metal via recycling has become an important facet of the aluminium industry. Recycling involves melting the scrap, a process that requires only five percent of the energy used to produce aluminium from ore. However, a significant part (up to 15% of the input material) is lost as dross (ash-like oxide). The dross can undergo a further process to extract aluminium. Recycling was a low-profile activity until the late 1960s, when the growing use of aluminium beverage cans brought it to the public awareness. In Europe aluminium experiences high rates of recycling, ranging from 42% of beverage cans, 85% of construction materials and 95% of transport vehicles. Recycled aluminium is known as secondary aluminium, but maintains the same physical properties as primary aluminium. Secondary aluminium is produced in a wide range of formats and is employed in 80% of the alloy injections. Another important use is for extrusion. White dross from primary aluminium production and from secondary recycling operations still contains useful quantities of aluminium which can be extracted industrially. The process produces aluminium billets, together with a highly complex waste material. This waste is difficult to manage. It reacts with water, releasing a mixture of gases (including, among others, hydrogen, acetylene, and ammonia) which spontaneously ignites on contact with air; contact with damp air results in the release of copious quantities of ammonia gas. Despite these difficulties, however, the waste has found use as a filler in asphalt and concrete.
AlH is produced when aluminium is heated in an atmosphere of hydrogen. Al2O is made by heating the normal oxide, Al2O3, with silicon at in a vacuum. Al2S can be made by heating Al2S3 with aluminium shavings at in a vacuum. It quickly disproportionates to the starting materials. The selenide is made in a parallel manner. AlF, AlCl and AlBr exist in the gaseous phase when the tri-halide is heated with aluminium. Aluminium halides usually exist in the form AlX3, where X is F, Cl, Br, or I.
Fajans' rules show that the simple trivalent cation Al3+ is not expected to be found in anhydrous salts or binary compounds such as Al2O3. The hydroxide is a weak base and aluminium salts of weak acids, such as carbonate, cannot be prepared. The salts of strong acids, such as nitrate, are stable and soluble in water, forming hydrates with at least six molecules of water of crystallization. Aluminium hydride, (AlH3)"n", can be produced from trimethylaluminium and an excess of hydrogen. It burns explosively in air. It can also be prepared by the action of aluminium chloride on lithium hydride in ether solution, but cannot be isolated free from the solvent. Alumino-hydrides of the most electropositive elements are known, the most useful being lithium aluminium hydride, Li[AlH4]. It decomposes into lithium hydride, aluminium and hydrogen when heated, and is hydrolysed by water. It has many uses in organic chemistry, particularly as a reducing agent. The aluminohalides have a similar structure. Aluminium hydroxide may be prepared as a gelatinous precipitate by adding ammonia to an aqueous solution of an aluminium salt. It is amphoteric, being both a very weak acid, and forming aluminates with alkalis. It exists in various crystalline forms. Aluminium carbide, Al4C3 is made by heating a mixture of the elements above. The pale yellow crystals have a complex lattice structure, and react with water or dilute acids to give methane. The acetylide, Al2(C2)3, is made by passing acetylene over heated aluminium. Aluminium nitride, AlN, can be made from the elements at. It is hydrolysed by water to form ammonia and aluminium hydroxide. Aluminium phosphide, AlP, is made similarly, and hydrolyses to give phosphine. Aluminium oxide, Al2O3, occurs naturally as corundum, and can be made by burning aluminium in oxygen or by heating the hydroxide, nitrate or sulfate. As a gemstone, its hardness is only exceeded by diamond, boron nitride, and carborundum. It is almost insoluble in water. Aluminium sulfide, Al2S3, may be prepared by passing hydrogen sulfide over aluminium powder. It is polymorphic. Aluminium iodide, AlI3, is a dimer with applications in organic synthesis. Aluminium fluoride, AlF3, is made by treating the hydroxide with HF, or can be made from the elements. It consists of a giant molecule which sublimes without melting at. It is very inert. The other trihalides are dimeric, having a bridge-like structure. When aluminium and fluoride are together in aqueous solution, they readily form complex ions such as, and. Of these, is the most stable. This is explained by the fact that aluminium and fluoride, which are both very compact ions, fit together just right to form the octahedral aluminium hexafluoride complex. When aluminium and fluoride are together in water in a 1:6 molar ratio, is the most common form, even in rather low concentrations. Organometallic compounds of empirical formula AlR3 exist and, if not also polymers, are at least dimers or trimers. They have some uses in organic synthesis, for instance trimethylaluminium.
Aluminium is the most widely used non-ferrous metal. Global production of aluminium in 2005 was 31.9 million tonnes. It exceeded that of any other metal except iron (837.5 million tonnes). Forecast for 2012 is 42–45 million tons, driven by rising Chinese output. Relatively pure aluminium is encountered only when corrosion resistance and/or workability is more important than strength or hardness. A thin layer of aluminium can be deposited onto a flat surface by physical vapour deposition or (very infrequently) chemical vapour deposition or other chemical means to form optical coatings and mirrors. When so deposited, a fresh, pure aluminium film serves as a good reflector (approximately 92%) of visible light and an excellent reflector (as much as 98%) of medium and far infrared radiation. Pure aluminium has a low tensile strength, but when combined with thermo-mechanical processing, aluminium alloys display a marked improvement in mechanical properties, especially when tempered. Aluminium alloys form vital components of aircraft and rockets as a result of their high strength-to-weight ratio. Aluminium readily forms alloys with many elements such as copper, zinc, magnesium, manganese and silicon (e.g., duralumin). Today, almost all bulk metal materials that are referred to loosely as "aluminium", are actually alloys. For example, the common aluminium foils are alloys of 92% to 99% aluminium. Aluminium alloys in structural applications. Aluminium alloys with a wide range of properties are used in engineering structures. Alloy systems are classified by a number system (ANSI) or by names indicating their main alloying constituents (DIN and ISO). The strength and durability of aluminium alloys vary widely, not only as a result of the components of the specific alloy, but also as a result of heat treatments and manufacturing processes. A lack of knowledge of these aspects has from time to time led to improperly designed structures and gained aluminium a bad reputation. One important structural limitation of aluminium alloys is their fatigue strength. Unlike steels, aluminium alloys have no well-defined fatigue limit, meaning that fatigue failure will eventually occur under even very small cyclic loadings. This implies that engineers must assess these loads and design for a fixed life rather than an infinite life. Another important property of aluminium alloys is their sensitivity to heat. Workshop procedures involving heating are complicated by the fact that aluminium, unlike steel, will melt without first glowing red. Forming operations where a blow torch is used therefore requires some expertise, since no visual signs reveal how close the material is to melting. Aluminium alloys, like all structural alloys, also are subject to internal stresses following heating operations such as welding and casting. The problem with aluminium alloys in this regard is their low melting point, which make them more susceptible to distortions from thermally induced stress relief. Controlled stress relief can be done during manufacturing by heat-treating the parts in an oven, followed by gradual cooling—in effect annealing the stresses. The low melting point of aluminium alloys has not precluded their use in rocketry; even for use in constructing combustion chambers where gases can reach 3500 K. The Agena upper stage engine used a regeneratively cooled aluminium design for some parts of the nozzle, including the thermally critical throat region.
Compared to copper, aluminium has about 65% of the electrical conductivity by volume, although 200% by weight. Traditionally copper is used as household wiring material. In the 1960s aluminium was considerably cheaper than copper, and so was introduced for household electrical wiring in the United States, even though many fixtures had not been designed to accept aluminium wire. In some cases the greater coefficient of thermal expansion of aluminium causes the wire to expand and contract relative to the dissimilar metal screw connection, eventually loosening the connection. Also, pure aluminium has a tendency to "creep" under steady sustained pressure (to a greater degree as the temperature rises), again loosening the connection. Finally, Galvanic corrosion from the dissimilar metals increased the electrical resistance of the connection. All of this resulted in overheated and loose connections, and this in turn resulted in fires. Builders then became wary of using the wire, and many jurisdictions outlawed its use in very small sizes in new construction. Eventually, newer fixtures were introduced with connections designed to avoid loosening and overheating. The first generation fixtures were marked "Al/Cu" and were ultimately found suitable only for copper-clad aluminium wire, but the second generation fixtures, which bear a "CO/ALR" coding, are rated for unclad aluminium wire. To adapt older assemblies, workers forestall the heating problem using a properly-done crimp of the aluminium wire to a short "pigtail" of copper wire. Today, new alloys, designs, and methods are used for aluminium wiring in combination with aluminium termination.
Ancient Greeks and Romans used aluminium salts as dyeing mordants and as astringents for dressing wounds; alum is still used as a styptic. In 1761 Guyton de Morveau suggested calling the base alum "alumine." In 1808, Humphry Davy identified the existence of a metal base of alum, which he at first termed "alumium" and later "aluminum" (see Etymology section, below). The metal was first produced in 1825 (in an impure form) by Danish physicist and chemist Hans Christian Ørsted. He reacted anhydrous aluminium chloride with potassium amalgam and yielded a lump of metal looking similar to tin. Friedrich Wöhler was aware of these experiments and cited them, but after redoing the experiments of Ørsted he concluded that this metal was pure potassium. He conducted a similar experiment in 1827 by mixing anhydrous aluminium chloride with potassium and yielded aluminium. Wöhler is generally credited with isolating aluminium (Latin "alumen", alum), but also Ørsted can be listed as its discoverer. Further, Pierre Berthier discovered aluminium in bauxite ore and successfully extracted it. Frenchman Henri Etienne Sainte-Claire Deville improved Wöhler's method in 1846, and described his improvements in a book in 1859, chief among these being the substitution of sodium for the considerably more expensive potassium. Before the Hall-Héroult process was developed, aluminium was exceedingly difficult to extract from its various ores. This made pure aluminium more valuable than gold. Bars of aluminium were exhibited at the Exposition Universelle of 1855, and Napoleon III was said to have reserved a set of aluminium dinner plates for his most honoured guests. Aluminium was selected as the material to be used for the apex of the Washington Monument in 1884, a time when one ounce (30 grams) cost the daily wage of a common worker on the project; aluminium was about the same value as silver. The Cowles companies supplied aluminium alloy in quantity in the United States and England using smelters like the furnace of Carl Wilhelm Siemens by 1886. Charles Martin Hall of Ohio in the U.S. and Paul Héroult of France independently developed the Hall-Héroult electrolytic process that made extracting aluminium from minerals cheaper and is now the principal method used worldwide. The Hall-Heroult process cannot produce Super Purity Aluminium directly. Hall's process, in 1888 with the financial backing of Alfred E. Hunt, started the Pittsburgh Reduction Company today known as Alcoa. Héroult's process was in production by 1889 in Switzerland at Aluminium Industrie, now Alcan, and at British Aluminium, now Luxfer Group and Alcoa, by 1896 in Scotland. By 1895 the metal was being used as a building material as far away as Sydney, Australia in the dome of the Chief Secretary's Building. Many navies use an aluminium superstructure for their vessels, however, the 1975 fire aboard USS "Belknap" that gutted her aluminium superstructure, as well as observation of battle damage to British ships during the Falklands War, led to many navies switching to all steel superstructures. The "Arleigh Burke" class was the first such U.S. ship, being constructed entirely of steel. In 2008 the price of aluminium peaked at $1.45/lb in July but dropped to $0.70/lb by December.
The earliest citation given in the Oxford English Dictionary for any word used as a name for this element is "alumium", which British chemist and inventor Humphry Davy employed in 1808 for the metal he was trying to isolate electrolytically from the mineral "alumina". The citation is from the journal "Philosophical Transactions of the Royal Society of London": "Had I been so fortunate as to have obtained more certain evidences on this subject, and to have procured the metallic substances I was in search of, I should have proposed for them the names of silicium, alumium, zirconium, and glucium." Davy had settled on "aluminum" by the time he published his 1812 book "Chemical Philosophy": "This substance appears to contain a peculiar metal, but as yet Aluminum has not been obtained in a perfectly free state, though alloys of it with other metalline substances have been procured sufficiently distinct to indicate the probable nature of alumina." But the same year, an anonymous contributor to the "Quarterly Review," a British political-literary journal, in a review of Davy's book, objected to "aluminum" and proposed the name "aluminium", "for so we shall take the liberty of writing the word, in preference to aluminum, which has a less classical sound." The "-ium" suffix conformed to the precedent set in other newly discovered elements of the time: potassium, sodium, magnesium, calcium, and strontium (all of which Davy had isolated himself). Nevertheless, "-um" spellings for elements were not unknown at the time, as for example platinum, known to Europeans since the sixteenth century, molybdenum, discovered in 1778, and tantalum, discovered in 1802. The "-um" suffix is consistent with the universal spelling alumina for the oxide, as lanthana is the oxide of lanthanum, and magnesia, ceria, and thoria are the oxides of magnesium, cerium, and thorium respectively. The spelling used throughout the 19th century by most U.S. chemists ended in "-ium", but common usage is less clear. The "-um" spelling is used in the Webster's Dictionary of 1828. In his advertising handbill for his new electrolytic method of producing the metal 1892, Charles Martin Hall used the "-um" spelling, despite his constant use of the "-ium" spelling in all the patents he filed between 1886 and 1903. It has consequently been suggested that the spelling reflects an easier to pronounce word with one fewer syllable, or that the spelling on the flier was a mistake. Hall's domination of production of the metal ensured that the spelling "aluminum" became the standard in North America; the "Webster Unabridged Dictionary" of 1913, though, continued to use the "-ium" version. In 1926, the American Chemical Society officially decided to use "aluminum" in its publications; American dictionaries typically label the spelling "aluminium" as a British variant. The name "aluminum" is derived from its status as a base of alum; "alum" in turn is a Latin word which literally means "bitter salt".
Most countries use the spelling "aluminium" (with an "i" before -"um"). In the United States, this spelling is largely unknown, and the spelling "aluminum" predominates. The Canadian Oxford Dictionary prefers "aluminum", whereas the Australian Macquarie Dictionary prefers "aluminium". The International Union of Pure and Applied Chemistry (IUPAC) adopted "aluminium" as the standard international name for the element in 1990, but three years later recognized "aluminum" as an acceptable variant. Hence their periodic table includes both. IUPAC officially prefers the use of "aluminium" in its internal publications, although several IUPAC publications use the spelling "aluminum".
Despite its natural abundance, aluminium has no known function in living cells and presents some toxic effects in elevated concentrations. Its toxicity can be traced to deposition in bone and the central nervous system, which is particularly increased in patients with reduced renal function. Because aluminium competes with calcium for absorption, increased amounts of dietary aluminium may contribute to the reduced skeletal mineralization (osteopenia) observed in preterm infants and infants with growth retardation. In very high doses, aluminium can cause neurotoxicity, and is associated with altered function of the blood-brain barrier. A small percentage of people are allergic to aluminium and experience contact dermatitis, digestive disorders, vomiting or other symptoms upon contact or ingestion of products containing aluminium, such as deodorants or antacids. In those without allergies, aluminium is not as toxic as heavy metals, but there is evidence of some toxicity if it is consumed in excessive amounts. Although the use of aluminium cookware has not been shown to lead to aluminium toxicity in general, excessive consumption of antacids containing aluminium compounds and excessive use of aluminium-containing antiperspirants provide more significant exposure levels. Studies have shown that consumption of acidic foods or liquids with aluminium significantly increases aluminium absorption, and maltol has been shown to increase the accumulation of aluminium in nervous and osseus tissue. Furthermore, aluminium increases estrogen-related gene expression in human breast cancer cells cultured in the laboratory. These salts' estrogen-like effects have led to their classification as a metalloestrogen. Because of its potentially toxic effects, aluminium's use in some antiperspirants, dyes (such as aluminium lake), and food additives is controversial. Although there is little evidence that normal exposure to aluminium presents a risk to healthy adults, several studies point to risks associated with increased exposure to the metal. Aluminium in food may be absorbed more than aluminium from water. Some researchers have expressed concerns that the aluminium in antiperspirants may increase the risk of breast cancer, and aluminium has controversially been implicated as a factor in Alzheimer's disease. The Camelford water pollution incident involved a number of people consuming aluminium sulphate. Investigations of the long-term health effects are still ongoing, but elevated brain aluminium concentrations have been found in post-mortem examinations of victims who have later died, and further research to determine if there is a link with cerebral amyloid angiopathy has been commissioned. According to The Alzheimer's Society, the overwhelming medical and scientific opinion is that studies have not convincingly demonstrated a causal relationship between aluminium and Alzheimer's disease. Nevertheless, some studies, such as those on the PAQUID cohort, cite aluminium exposure as a risk factor for Alzheimer's disease. Some brain plaques have been found to contain increased levels of the metal. Research in this area has been inconclusive; aluminium accumulation may be a consequence of the disease rather than a causal agent. In any event, if there is any toxicity of aluminium, it must be via a very specific mechanism, since total human exposure to the element in the form of naturally occurring clay in soil and dust is enormously large over a lifetime. Scientific consensus does not yet exist about whether aluminium exposure could directly increase the risk of Alzheimer's disease.
Aluminium is primary among the factors that reduce plant growth on acid soils. Although it is generally harmless to plant growth in pH-neutral soils, the concentration in acid soils of toxic Al3+ cations increases and disturbs root growth and function. Most acid soils are saturated with aluminium rather than hydrogen ions. The acidity of the soil is therefore a result of hydrolysis of aluminium compounds. This concept of "corrected lime potential" to define the degree of base saturation in soils became the basis for procedures now used in soil testing laboratories to determine the "lime requirement" of soils. Wheat's adaptation to allow aluminium tolerance is such that the aluminium induces a release of organic compounds that bind to the harmful aluminium cations. Sorghum is believed to have the same tolerance mechanism. The first gene for aluminium tolerance has been identified in wheat. It was shown that sorghum's aluminium tolerance is controlled by a single gene, as for wheat. This is not the case in all plants.
Advanced Chemistry is a German hip hop group from Heidelberg, a scenic city in Baden-Württemberg, South Germany. Advanced Chemistry was founded in 1987 by Toni L, Linguist, Gee-One, DJ Mike MD (Mike Dippon) and MC Torch. Each member of the group holds German citizenshhip, and Toni L, Linguist, and Torch are of Italian, Ghanaian, and Haitian backgrounds, respectively.. Influenced by North American socially conscious rap and the Native tongues movement, Advanced Chemistry is regarded as one of the main pioneers in German hip hop. They were one of the first groups to rap in German (although their name is in English). Furthermore, their songs tackled controversial social and political issues, distinguishing them from early German hip hop group "Die Fantastischen Vier" (The Fantastic Four), which had a more light-hearted, playful, party image.. The rivalry between Advanced Chemistry and Die Fantastischen Vier has served to highlight a dichotomy in the routes that hip hop has taken in becoming a part of the German soundscape. While Die Fantastischen Vier may be said to view hip hop primarily as an esthetic art form, Advanced Chemistry understand hip hop as being inextricably linked to the social and political circumstances under which it is created. For Advanced Chemistry, hip hop is a “vehicle of general human emancipation,”. In their undertaking of social and political issues, the band introduced the term "Afro-German" in to the context of German hip hop, and the theme of race is highlighted in much of their music. With the release of the single “Fremd im eigenen Land”, Advanced Chemistry separated itself from the rest of the rap being produced in Germany. This single was the first of its kind to go beyond simply imitating US rap and addressed the current issues of the time. Fremd im eigenen Land which translates to “foreign in my own country” dealt with the widespread racism that non-white German citizens faced. This change from simple imitation to political commentary was the start of German identification with rap.. The sound of “Fremd im eigenen Land” was influenced by the 'wall of noise' created by Public Enemy's producers, The Bomb Squad. After the reunification of Germany, an abundance of anti-immigrant sentiment emerged, as well as attacks on the homes of refugees in the early 90's. Advanced Chemistry came to prominence in the wake of these actions because of their pro-multicultural society stance in their music. Advanced Chemistry's attitudes revolve around their attempts to create a distinct "Germanness" in hip hop, as opposed to imitating American hip hop as other groups had done. Torch has said, "What the Americans do is exotic for us because we don't live like they do. What they do seems to be more interesting and newer. But not for me. For me it's more exciting to experience my fellow Germans in new contexts...For me, it's interesting to see what the kids try to do that's different from what I know." Advanced Chemistry were the first to use the term "Afro-German" in a hip hop context. This was part of the pro-immigrant political message they sent via their music. While Advanced Chemistry's use of the German language in their rap allows them to make claims to authenticity and true German heritage, bolstering pro-immigration sentiment, their style can also be problematic for immigrant notions of any real ethnic roots. Indeed, part of the Turkish ethnic minority of Frankfurt views Advanced Chemistry's appeal to the German image as a "symbolic betrayal of the right of ethnic minorities to 'roots' or to any expression of cultural heritage." In this sense, their rap represents a complex social discourse internal to the German soundscape in which they attempt to negotiate immigrant assimilation into a xenophobic German culture with the maintenance of their own separate cultural traditions. It is quite possibly the feelings of alienation from the pure-blooded German demographic that drive Advanced Chemistry to attack nationalistic ideologies by asserting their "Germanness" as a group composed primarily of ethnic others. The response to this pseudo-German authenticity can be seen in what Andy Bennett refers to as "alternative forms of local hip hop culture which actively seek to rediscover and, in many cases, reconstruct notions of identity tied to cultural roots." These alternative local hip hop cultures include Oriental hip hop, the members of which cling to their Turkish heritage and are confused by Advanced Chemistry's elicitation of a German identity politics to which they technically do not belong. This cultural binary illustrates that rap has taken different routes in Germany and that, even among an already isolated immigrant population, there is still disunity and, especially, disagreement on the relative importance of assimilation versus cultural defiance. According to German hip hop enthusiast 9@home, Advanced Chemistry is part of a "hip-hop movement [which] took a clear stance for the minorities and against the [marginalization] of immigrants who...might be German on paper, but not in real life," which speaks to the group's hope of actually being recognized as German citizens and not foreigners, despite their various other ethnic and cultural ties.
One of the first issues that confronts us when we move outside the English-speaking market for recorded music is to establish whether or not the discrete musical genres we know from that market are fully congruent with similar divisions in other pop worlds. This is important in two ways. First, although no single country comes close to matching the amounts spent on recorded music in the United States, these markets are nonetheless economically significant. Germany, for instance, is the largest single market in western Europe, with estimated annual sales of U.S. $3.74 billion in 1996. This represents around 30 percent of reported U.S. sales and makes Germany the third biggest music market in the world. Advanced Chemistry frequently rapped about their lives and experiences as children of immigrants, exposing the marginalization experienced by most ethnic minorities in Germany, and the feelings of frustration and resentment that being denied a German identity can cause.. The song "Fremd im eigenem Land" (Foreign in your own nation) was released by Advanced Chemistry in November 1992. The single became a staple in the German hip hop scene. It made a strong statement about the status of immigrants throughout Germany, as the group was composed of multi-national and multi-racial members. The video shows several members brandishing their German passports as a demonstration of their German citizenship to skeptical and unaccepting 'ethnic' Germans. This idea of national identity is important, as many rap artists in Germany have been of foreign origin. These so-called "Gastarbeiter" (guest workers) children saw breakdance, graffiti, rap music, and hip hop culture as a means of expressing themselves. Since the release of "Fremd im eigenen Land", many other German-language rappers have also tried to confront anti-immigrant ideas and develop themes of citizenship. Thus, rap and hip hop are central to local strategies of resistance to racism and racial exclusion in Germany. However, though many ethnic minority youth in Germany find the these German identity themes appealing, others view the desire of immigrants to be seen as German negatively, and they have actively sought to revive and recreate concepts of identity in connection to traditional ethnic origins. Advanced Chemistry helped to found the German chapter of the Zulu nation.. Several years after the founding, Gee One and DJ Mike MD left the group, seeking to realize new dreams. Members include Toni L, Linguist, Torch; affiliated: Boulevard Bou.
The Anglican Communion is an international association of national and regional Anglican churches. There is no single "Anglican Church" with universal juridical authority as each national or regional church has full autonomy. As the name suggests, the Anglican "Communion" is an association of these churches in full communion with the Church of England (which may be regarded as the mother church of the worldwide communion) and specifically with its principal primate, the Archbishop of Canterbury. The status of full communion means, ideally, that there is mutual agreement on essential doctrines, and that full participation in the sacramental life of each national church is available to all communicant Anglicans. With approximately 77 million members, the Anglican Communion is the third largest Christian communion in the world, after the Roman Catholic Church and the Eastern Orthodox Churches. Some of these churches are known as Anglican, explicitly recognising the historical link to England ("Ecclesia Anglicana" means "Church of England"); others, such as the American and Scottish Episcopal churches, or the Church of Ireland, prefer a separate name. Each church has its own doctrine and liturgy, based in most cases on that of the Church of England; and each church has its own legislative process and overall episcopal polity, under the leadership of a local primate. The Archbishop of Canterbury, religious head of the Church of England, has no formal authority outside that jurisdiction, but is recognised as symbolic head of the worldwide communion. Among the other primates he is "primus inter pares", which translates "first among equals". The Anglican Communion considers itself to be part of the One, Holy, Catholic, and Apostolic Church and to be both Catholic and Reformed. For some adherents it represents a non-papal Catholicism, for others a form of Protestantism though without a dominant guiding figure such as Luther, Knox, Calvin, Zwingli or Wesley. For others, their self-identity represents some combination of the two. The communion encompasses a wide spectrum of belief and practice including evangelical, liberal, and catholic.
The Anglican Communion has no official legal existence nor any governing structure which might exercise authority over the member churches. There is an Anglican Communion Office in London, under the aegis of the Archbishop of Canterbury, but it only serves a supporting and organisational role. The Communion is held together by a shared history, expressed in its ecclesiology, polity and ethos and also by participation in international consultative bodies. Three elements have been important in holding the Communion together: First, the shared ecclesial structure of the component churches, manifested in an episcopal polity maintained through the apostolic succession of bishops and synodical government; second, the principle of belief expressed in worship, investing importance in approved prayer books and their rubrics; and third, the historical documents and standard divines that have influenced the ethos of the Communion. Originally, the Church of England was self-contained and relied for its unity and identity on its own history, its traditional legal and episcopal structure and its status as an established church of the state. As such Anglicanism was, from the outset, a movement with an explicitly episcopal polity, a characteristic which has been vital in maintaining the unity of the Communion by conveying the episcopate's role in manifesting visible catholicity and ecumenism. Early in its development, Anglicanism developed a vernacular prayer book, called the Book of Common Prayer. Unlike other traditions, Anglicanism has never been governed by a magisterium nor by appeal to one founding theologian, nor by an extra-credal summary of doctrine (such as the Westminster Confession of the Presbyterian Church). Instead, Anglicans have typically appealed to the Book of Common Prayer and its offshoots as a guide to Anglican theology and practice. This had the effect of inculcating the principle of "lex orandi, lex credendi" ("the law of prayer is the law of belief") as the foundation of Anglican identity and confession. Protracted conflict through the seventeenth century with more radical Protestants on the one hand and Roman Catholics who still recognised the primacy of the Pope on the other, resulted in an association of churches that were both deliberately vague about doctrinal principles, yet bold in developing parameters of acceptable deviation. These parameters were most clearly articulated in the various rubrics of the successive prayer books, as well as the Thirty-Nine Articles of Religion. These Articles, while never binding, have had an influence on the ethos of the Communion, an ethos reinforced by their interpretation and expansion by such influential early theologians as Richard Hooker, Lancelot Andrewes, John Cosin, and others. With the expansion of the British Empire, and hence the growth of Anglicanism outside Great Britain and Ireland, the Communion sought to establish new vehicles of unity. The first major expression of this were the Lambeth Conferences of the communion's bishops, first convened by Archbishop of Canterbury Charles Longley in 1867. From the outset, these were not intended to displace the autonomy of the emerging provinces of the Communion, but to "discuss matters of practical interest, and pronounce what we deem expedient in resolutions which may serve as safe guides to future action."
Since there is no binding authority in the Communion, these international bodies are a vehicle for consultation and persuasion. In recent years, persuasion has tipped over into debates over conformity in certain areas of doctrine, discipline, worship, and ethics. The most notable example has been the objection of many provinces of the Communion (particularly in Africa and Asia) to the changing role of homosexuals in the North American churches (e.g., by blessing same-sex unions and ordaining and consecrating gays and lesbians in same-sex relationships), and to the process by which changes were undertaken. Those who objected condemned these actions as unscriptural, unilateral, and without the agreement of the Communion prior to these steps being taken. In response, the American Episcopal Church and the Anglican Church of Canada answered that the actions had been undertaken after lengthy scriptural and theological reflection, legally in accordance with their own canons and constitutions and after extensive consultation with the provinces of the Communion. The Primates' Meeting voted to request the two churches to withdraw their delegates from the 2005 meeting of the Anglican Consultative Council, and Canada and the United States decided to attend the meeting but without exercising their right to vote. They have not been expelled or suspended, since there is no mechanism in this voluntary association to suspend or expel an independent province of the Communion. Since membership is based on a province's communion with Canterbury, expulsion would require the Archbishop of Canterbury's refusal to be in communion with the affected jurisdiction(s). In line with the suggestion of the Windsor Report, Dr Williams has recently established a working group to examine the feasibility of an Anglican covenant which would articulate the conditions for communion in some fashion. Provinces of the Anglican Communion. In addition, there are six extraprovincial churches, five of which are under the metropolitical authority of the Archbishop of Canterbury.
The Anglican Communion is a relatively recent concept. The Church of England (which until the 20th century included the Church in Wales) initially separated from the Roman Catholic Church in 1538 in the reign of King Henry VIII, reunited in 1555 under Queen Mary I and then separated again in 1570 under Queen Elizabeth I (the Roman Catholic Church excommunicated Elizabeth I in 1570 in response to the Act of Supremacy 1559). The Church of England has always thought of itself not as a new foundation but rather as a reformed continuation of the ancient "English Church" (Ecclesia Anglicana) and a reassertion of that church's rights. As such it was a distinctly national phenomenon. The Church of Scotland separated from the Roman Catholic Church with the Scottish Reformation in 1560, and the split from it of the Scottish Episcopal Church began in 1582, in the reign of James VI of Scotland, over disagreements about the role of bishops. The oldest-surviving Anglican church outside of the British Isles (Britain and Ireland) is St. Peter's Church, in St. George's, Bermuda, established in 1612 (though the actual building had to be rebuilt several times over the following century). This is also the oldest surviving Protestant church in the New World. It remained part of the Church of England until 1978, when the Anglican Church of Bermuda separated. The Church of England was the state religion not only in England, but in her trans-Oceanic colonies. Thus the only member churches of the present Anglican Communion existing by the mid-18th century were the Church of England, its closely-linked sister church, the Church of Ireland (which also separated from Roman Catholicism under Henry VIII), and the Scottish Episcopal Church which for parts of the 17th and 18th centuries was partially underground (it was suspected of Jacobite sympathies). However, the enormous expansion in the 18th and 19th centuries of the British Empire brought the church along with it. At first all these colonial churches were under the jurisdiction of the Bishop of London. After the American Revolution, the parishes in the newly independent country found it necessary to break formally from a church whose Supreme Governor was (and remains) the British monarch. Thus they formed their own dioceses and national church, the Episcopal Church in the United States of America, in a mostly amicable separation. At about the same time, in the colonies which remained linked to the crown, the Church of England began to appoint colonial bishops. In 1787 a bishop of Nova Scotia was appointed with a jurisdiction over all of British North America; in time several more colleagues were appointed to other cities in present-day Canada. In 1814 a bishop of Calcutta was made; in 1824 the first bishop was sent to the West Indies and in 1836 to Australia. By 1840 there were still only ten colonial bishops for the Church of England; but even this small beginning greatly facilitated the growth of Anglicanism around the world. In 1841 a "Colonial Bishoprics Council" was set up and soon many more dioceses were created. In time, it became natural to group these into provinces, and a metropolitan appointed for each province. Although it had at first been somewhat established in many colonies, in 1861 it was ruled that, except where specifically established, the Church of England had just the same legal position as any other church. Thus a colonial bishop and colonial diocese was by nature quite a different thing from their counterparts back home. In time bishops came to be appointed locally rather than from England, and eventually national synods began to pass ecclesiastical legislation independent of England. A crucial step in the development of the modern communion was the idea of the Lambeth Conferences, as discussed above. These conferences demonstrated that the bishops of disparate churches could manifest the unity of the church in their episcopal collegiality, despite the absence of universal legal ties. Some bishops were initially reluctant to attend, fearing that the meeting would declare itself a council with power to legislate for the church; but it agreed to pass only advisory resolutions. These Lambeth Conferences have been held roughly decennially since 1878 (the second such conference) and remain the most visible coming-together of the whole Communion.
"Anglican clergy who join the Orthodox Church are reordained; but [some Orthodox Churches hold that] if Anglicanism and Orthodoxy were to reach full unity in the faith, perhaps such reordination might not be found necessary. It should be added, however, that a number of individual Orthodox theologians hold that under no circumstances would it be possible to recognise the validity of Anglican Orders."
One effect of the Communion's dispersed authority has been that conflict and controversy regularly arise over the effect divergent practices and doctrines in one part of the Communion have on others. Disputes that had been confined to the Church of England could be dealt with legislatively in that realm, but as the Communion spread out into new nations and disparate cultures, such controversies multiplied and intensified. These controversies have generally been of two types: liturgical and social. The first such controversy of note concerned that of the growing influence of the Catholic Revival manifested in the tractarian and so-called ritualism controversies of the late nineteenth and early twentieth centuries. Later, rapid social change and the dissipation of British cultural hegemony over its former colonies contributed to disputes over the role of women, the parameters of marriage and divorce, and the practice of contraception and abortion. More recently, disagreements over homosexuality have strained the unity of the Communion as well as its relationships with other Christian denominations. Simultaneous with debates about social theology and ethics, the Communion has debated prayer book revision and the acceptable grounds for achieving full communion with non-Anglican churches.
Arne Kaijser (born 1950) is a professor of History of Technology at the Royal Institute of Technology in Stockholm, and the head of the university's department of History of science and technology. Kaijser has published two books in Swedish: "Stadens ljus. Etableringen av de första svenska gasverken" and "I fädrens spår. Den svenska infrastrukturens historiska utveckling och framtida utmaningar", and has co-edited several anthologies. Kaijser is a member of the Royal Swedish Academy of Engineering Sciences since 2007 and also a member of the editorial board of two scientific journals: "Journal of Urban Technology" and "Centaurus". Lately, he has been occupied with the history of Large Technical Systems.
An archipelago () is a chain or cluster of islands that are formed tectonically. The word "archipelago", is directly derived from the Greek "ἄρχι- - arkhi-" ("chief") and "πέλαγος - pelagos" ("sea"). In Italian, possibly following a tradition of antiquity, the Archipelago (from medieval Greek *ἀρχιπέλαγος) was the proper name for the Aegean Sea and, later, usage shifted to refer to the Aegean Islands (since the sea is remarkable for its large number of islands). It is now used to generally refer to any island group or, sometimes, to a sea containing a large number of scattered islands like the Aegean Sea.
Archipelagos are usually found in the open sea; less commonly, a large land mass may neighbour them. For example, Scotland has more than 700 islands surrounding its mainland. Archipelagos are often volcanic, forming along island arcs generated by subduction zones or hotspots, but there are many other processes involved in their construction, including erosion, deposition and land elevation. The five largest modern countries that are mainly archipelagos are Japan, the Philippines, New Zealand, the United Kingdom and Indonesia. The largest archipelago in the world, by size, is Indonesia. The archipelago with the most islands is the Archipelago Sea in Finland, but these islands are generally small.
In copyright law, there is necessarily little flexibility as to what constitutes authorship. The United States Copyright Office defines copyright as "a form of protection provided by the laws of the United States (title 17, U.S. Code) to authors of "original works of authorship". Holding the title of "author" over any "literary, dramatic, musical, artistic, [or] certain other intellectual works" giverights this person, the owner of the copyright, exclusive right to do or authorize any production or distribution of their work. Any person or entity wishing to use intellectual property held under copyright must receive permission from the copyright holder to use this work, and often will be asked to pay for the use of copyrighted material. After a fixed amount of time, the copyright expires on intellectual work and it enters the public domain, where it can be used without limit. Copyright law has been amended time and time again since the inception of the law to extend the length of this fixed period where the work is exclusively controlled by the copyright holder. However, copyright is merely the legal reassurance that one owns his/her work. Technically, someone owns their work from the time it's created. An interesting aspect of authorship emerges with copyright in that it can be passed down to another upon one's death. The person who inherits the copyright is not the author, but enjoys the same legal benefits. Questions arise as to the application of copyright law. How does it, for example, apply to the complex issue of fan fiction? If the media agency responsible for the authorised production allows material from fans, what is the limit before legal constraints from actors, music, and other considerations, come into play? As well, how does copyright apply to fan-generated stories for books? What powers do the original authors, as well as the publishers, have in regulating or even stopping the fan fiction?
In literary theory, critics find complications in the term "author" beyond what constitutes authorship in a legal setting. In the wake of postmodern literature, critics such as Roland Barthes and Michel Foucault have examined the role and relevance of authorship to the meaning or interpretation of a text. Barthes challenges the idea that a text can be attributed to any single author. He quotes, in his essay "Death of the Author" (1968), that "it is language which speaks, not the author". The words and language of a text itself determine and expose meaning for Barthes, and not someone possessing legal responsibility for the process of its production. Every line of written text is a mere reflection of references from any of a multitude of traditions, or, as Barthes puts it, "the text is a tissue of quotations drawn from the innumerable centres of culture"; it is never original. With this, the perspective of the author is removed from the text, and the limits formerly imposed by the idea of one authorial voice, one ultimate and universal meaning, are destroyed. The explanation and meaning of a work does not have to be sought in the one who produced it, "as if it were always in the end, through the more or less transparent allegory of the fiction, the voice of a single person, the author 'confiding' in us". The psyche, culture, fanaticism of an author can be disregarded when interpreting a text, because the words are rich enough themselves with all of the traditions of language. To expose meanings in a written work without appealing to the celebrity of an author, their tastes, passions, vices, is, to Barthes, to allow language to speak, rather than author. Michel Foucault argues in his essay "What is an author?" (1969), that all authors are writers, but not all writers are authors. He states that "a private letter may have a signatory—it does not have an author". For a reader to assign the title of author upon any written work is to attribute certain standards upon the text which, for Foucault, are working in conjunction with the idea of "the author function". Foucault's author function is the idea that an author exists only as a function of a written work, a part of its structure, but not necessarily part of the interpretive process. The author's name "indicates the status of the discourse within a society and culture", and at one time was used as an anchor for interpreting a text, a practice which Barthes would argue is not a particularly relevant or valid endeavor. Expanding upon Foucault's position, Alexander Nehamas writes that Foucault suggests "an author [...] is whoever can be understood to have produced a particular text as we interpret it", not necessarily who penned the text. It is this distinction between producing a written work and producing the interpretation or meaning in a written work that both Barthes and Foucault are interested in. Foucault warns of the risks of keeping the author's name in mind during interpretation, because it could affect the value and meaning with which one handles an interpretation. Literary critics Barthes and Foucault suggest that readers should not rely on or look for the notion of one overarching voice when interpreting a written work, because of the complications inherent with a writer's title of "author." They warn of the dangers interpretations could suffer from when associating the subject of inherently meaningful words and language with the personality of one authorial voice. Instead, readers should allow a text to be interpreted in terms of the language as "author." Relationship between author and publisher. The publisher of a work might receive a percentage calculated on a wholesale or a specific price and or a fixed amount on each book that is sold. Publishers, at times, reduced the risk of this type of arrangement, by agreeing only to pay this after a certain amount of copies had sold. In Canada this practice occurred during the 1890s, but was not commonplace until the 1920s. Relationship between author and editor. The relationship between the author and the editor, often the author’s only liaison to the publishing company, is often characterized as the site of tension. For the author to reach his or her audience, the work usually must attract the attention of the editor. The idea of the author as the sole meaning-maker of necessity changes to include the influences of the editor and the publisher in order to engage the audience in writing as a social act. Pierre Bourdieu’s essay “The Field of Cultural Production” depicts the publishing industry as a “space of literary or artistic position-takings,” also called the “field of struggles,” which is defined by the tension and movement inherent among the various positions in the field. Bourdieu claims that the “field of position-takings [...] is not the product of coherence-seeking intention or objective consensus,” meaning that an industry characterized by position-takings is not one of harmony and neutrality. In particular for the writer, their authorship in their work makes their work part of their identity, and there is much at stake personally over the negotiation of authority over that identity. However, it is the editor who has “the power to impose the dominant definition of the writer and therefore to delimit the population of those entitled to take part in the struggle to define the writer”. As “cultural investors,” publishers rely on the editor position to identify a good investment in “cultural capital” which may grow to yield economic capital across all positions. According to the studies of James Curran, the system of shared values among editors in Britain has generated a pressure among authors to write to fit the editors’ expectations, removing the focus from the reader-audience and putting a strain on the relationship between authors and editors and on writing as a social act. Even the book review by the editors has more significance than the readership’s reception. Good relationships between authors and editors are largely found to be the product of an awareness of writing as a social act, and an effort to create a balance wherein the authority over the text is negotiated among all of the positions in the industry, so that the meaning is effectively carried from the meaning-maker to the readership.
Andrey (Andrei) Andreyevich Markov () (June 14, 1856 N.S. – July 20, 1922) was a Russian mathematician. He is best known for his work on theory of stochastic processes. His research later became known as Markov chains. He and his younger brother Vladimir Andreevich Markov (1871–1897) proved Markov brothers' inequality. His son, another Andrey Andreevich Markov (1903–1979), was also a notable mathematician, making contributions on constructive mathematics and recursive function theory.
Andrey Andreevich Markov was born in Ryazan as the son of the secretary of the public forest management of Ryazan, Andrey Grigorevich Markov, and his first wife Nadezhda Petrovna Markova. In the beginning of the 1860s Andrey Grigorevich moved to St Petersburg to become an asset manager of the princess Ekaterina Aleksandrovna Valvatyeva. In 1866 Andrey Andreevich's school life began with his entrance into Saint Petersburg's fifth grammar school. Already during his school time Andrey was intensely engaged in higher mathematics. As a 17-year-old grammar school student he informed Bunyakovsky, Korkin and Yegor Zolotarev about an apparently new method to solve linear ordinary differential equations and was invited to the so-called Korkin Saturdays, where Korkin's students regularly met. In 1874 he finished the school and began his studies at the physico-mathematical faculty of St Petersburg University. Among his teachers were Yulian Sokhotski (differential calculus, higher algebra), Konstantin Posse (analytic geometry), Yegor Zolotarev (integral calculus), Pafnuty Chebyshev (number theory, probability theory), Aleksandr Korkin (ordinary and partial differential equations), Okatov (mechanism theory), Osip Somov (mechanics) and Budaev (descriptive and higher geometry). In 1877 he was awarded the gold medal for his outstanding solution of the problem "About Integration of Differential Equations by Continuous Fractions with an Application to the Equation formula_1" In the following year he passed the candidate examinations and remained at the university to prepare for the lecturer's position. In April 1880 Markov defended his master thesis "About Binary Quadratic Forms with Positive Determinant", which was encouraged by Aleksandr Korkin and Yegor Zolotarev. Five years later, in January 1885, there followed his doctoral thesis "About Some Applications of Algebraic Continuous Fractions". His pedagogical work began after the defense of his master thesis in autumn 1880. As a privatdozent he lectured on differential and integral calculus. Later he lectured alternately on "introduction to analysis", probability theory (succeeding Chebyshev who had left the university in 1882) and calculus of differences. From 1895/96 until 1905 he additionally lectured on differential calculus. One year after the defense of the doctoral thesis, he was appointed extraordinary professor (1886) and in the same year he was elected adjunct to the Academy of Sciences. In 1890, after the death of Viktor Bunyakovsky, Markov became extraordinary member of the academy. His promotion to an ordinary professor of St Petersburg University followed in autumn 1894. In 1896, he was elected ordinary member of the academy as the successor of Chebyshev. In 1905 he was appointed merited professor and got the right to retire which he immediately used. Till 1910, however, he continued to lecture calculus of differences. In connection with student riots in 1908, professors and lecturers of Saint Petersburg University were ordered to observe their students. Markov initially refused to accept this decree and wrote an explanation in which he declined to be an "agent of the governance". Markov was rejected from a further teaching activity at the Saint Petersburg University, and he eventually decided to retire from the university. In 1913 the council of Saint Petersburg elected nine scientists honorary members of the university. Markov was among them, but his election was not affirmed by the minister of education. The affirmation was done only four years later, after the February revolution in 1917. Markov then resumed his teaching activities and lectured probability theory and calculus of differences until his death in 1922. Excommunication from the Russian Orthodox Church. In 1912 Markov in protest to Leo Tolstoy's excommunication from the Russian Orthodox Church requested that he too be excommunicated. As such Markov was formally excommunicated from the Church.
"Angst" is a German, Danish, Norwegian and Dutch word for fear or anxiety. ("Anguish" is its Latinate equivalent.) It is used in English to describe an intense feeling of strife. The term "Angst" distinguishes itself from the word "Furcht" (German for "fear") in that "Furcht" usually refers to a material threat (arranged fear), while "Angst" is usually a nondirectional emotion. In other languages having the meaning of the Latin word "pavor", the derived words differ in meaning, e.g. as in the French "anxiété" and "peur". The word "Angst" has existed since the 8th century, from the Proto-Indo-European root "*anghu-", "restraint" from which Old High German "angust" develops. It is pre-cognate with the Latin "angustia", "tensity, tightness" and "angor", "choking, clogging"; compare to the Greek "άγχος" (ankhos): stress.
Existentialist philosophers use the term "angst" with a different connotation. The use of the term was first attributed to Danish philosopher Søren Kierkegaard (1813–1855). In "The Concept of Anxiety" (also known as "The Concept of Dread", depending on the translation), Kierkegaard used the word "Angest" (in common Danish, "angst", meaning "dread" or "anxiety") to describe a profound and deep-seated spiritual condition of insecurity and fear in the free human being. Where the animal is a slave to its instincts but always conscious in its own actions, Kierkegaard believed that the freedom given to people leaves the human in a constant fear of failing his/her responsibilities to God. Kierkegaard's concept of angst is considered to be an important stepping stone for 20th-century existentialism. While Kierkegaard's feeling of angst is fear of actual responsibility to God, in modern use, angst was broadened by the later existentialists to include general frustration associated with the conflict between actual responsibilities to self, one's principles, and others (possibly including God). Martin Heidegger used the term in a slightly different way. Angst is very often misused in today's society as a synonym for anger.
Angst in serious musical composition has been a reflection of the times. Musical composition embodying angst as a primary theme have primarily come from European Jewish composers such as Gustav Mahler and Alban Berg, written during a period a great persecution of the Jewish people shortly before and during European Nazi rule. A notable exception is the Russian composer Dmitri Shostakovich whose symphonies use the theme of angst in post-World War II compositions depicting Russian strife during the war. However, it is the Jewish artists, Gustav Mahler and Franz Kafka in music and literature that have embraced the theme of angst so highly in their work that they have become synonymous with the term to the point of popular joking and cartoons today. Angst appears to be absent from important French music. Erik Satie’s Gymnopédie and Maurice Ravel’s Pavane pour une infante défunte, composed before World War II, reflect melancholy sentiment without angst in soft, quiet compositions. The effect of angst is achieved by Shostakovich, Mahler and Berg in compositions of wide dynamic range, at times seemingly spinning out of control (Mahler), and atonal music using the twelve-tone row method of composition (Berg and others) to create an angst ridden atmosphere of grotesque sound. The theme of angst is portrayed in Mahler's Symphony No. 6 ("The Tragic") and in Alban Berg's poignant Violin Concerto, dedicated to "To the memory of an angel", for the death of friend Gustav Mahler’s daughter.
Anxiety is a psychological and physiological state characterized by cognitive, somatic, emotional, and behavioral components. These components combine to create an unpleasant feeling that is typically associated with uneasiness, apprehension, fear, or worry. Anxiety is a generalized mood condition that can often occur without an identifiable triggering stimulus. As such, it is distinguished from fear, which occurs in the presence of an observed threat. Additionally, fear is related to the specific behaviors of escape and avoidance, whereas anxiety is the result of threats that are perceived to be uncontrollable or unavoidable. Another view is that anxiety is "a future-oriented mood state in which one is ready or prepared to attempt to cope with upcoming negative events" suggesting that it is a distinction between future vs. present dangers that divides anxiety and fear. Anxiety is considered to be a normal reaction to stress. It may help a person to deal with a difficult situation, for example at work or at school, by prompting one to cope with it. When anxiety becomes excessive, it may fall under the classification of an anxiety disorder.
Physical effects of anxiety may include heart palpitations, muscle weakness and tension, fatigue, nausea, chest pain, shortness of breath, stomach aches, or headaches. The body prepares to deal with a threat: blood pressure and heart rate are increased, sweating is increased, bloodflow to the major muscle groups is increased, and immune and digestive system functions are inhibited (the "fight or flight" response). External signs of anxiety may include pale skin, sweating, trembling, and pupillary dilation. Someone suffering from anxiety might also experience it as a sense of dread or panic. Although panic attacks are not experienced by every anxiety sufferer, they are a common symptom. Panic attacks usually come without warning, and although the fear is generally irrational, the perception of danger is very real. A person experiencing a panic attack will often feel as if he or she is about to die or pass out. Panic attacks may be confused with heart attacks therefore only a doctor can differentiate between a panic attack or a heart attack. Anxiety does not only consist of physical effects; there are many emotional ones as well. They include "feelings of apprehension or dread, trouble concentrating, feeling tense or jumpy, anticipating the worst, irritability, restlessness, watching (and waiting) for signs (and occurrences) or danger, and, feeling like your mind's gone blank" as well as "nightmares/bad dreams, obsessions about sensations, deja vu, a trapped in your mind feeling, and feeling like everything is scary." Cognitive effects of anxiety may include thoughts about suspected dangers, such as fear of dying. "You may...fear that the chest pains [a physical symptom of anxiety] are a deadly heart attack or that the shooting pains in your head [another physical symptom of anxiety] are the result of a tumor or aneurysm. You feel an intense fear when you think of dying, or you may think of it more often than normal, or can’t get it out of your mind."
Neural circuitry involving the amygdala and hippocampus is thought to underlie anxiety. When confronted with unpleasant and potentially harmful stimuli such as foul odors or tastes, PET-scans show increased bloodflow in the amygdala. In these studies, the participants also reported moderate anxiety. This might indicate that anxiety is a protective mechanism designed to prevent the organism from engaging in potentially harmful behaviors. Research upon adolescents that were as infants highly apprehensive, vigilant, and fearful finds that their nucleus accumbens is more sensitive than that in other people when they selected to make an action that determined whether they received a reward. This suggests a link between circuits responsible for fear and also reward in anxious people. As researchers note "a sense of ‘responsibility,’ or self agency, in a context of uncertainty (probabilistic outcomes) drives the neural system underlying appetitive motivation (i.e., nucleus accumbens) more strongly in temperamentally inhibited than noninhibited adolescents." Although single genes have little effect on complex traits and interact heavily both between themselves and with the external factors, research is underway to unravel possible molecular mechanisms underlying anxiety and comorbid conditions. One candidate gene with polymorphisms that influence anxiety is PLXNA2. Pre-existing health issues including chronic obstructive pulmonary disease (COPD), heart failure, and arrythmia can be the cause of anxiety or anxiety symptoms
The HAM-A (Hamilton Anxiety Scale) is a widely used interview scale that measures the severity of a patient's anxiety, based on 14 parameters, including anxious mood, tension, fears, insomnia, somatic complaints and behavior at the interview. Developed by M. Hamilton in 1959, the scale predates the current definition of generalized anxiety disorder (GAD). However, it covers many of the features of GAD and can be helpful in assessing its severity.
Philosopher Søren Kierkegaard, in "The Concept of Anxiety," described anxiety or dread associated with the "dizziness of freedom" and suggested the possibility for positive resolution of anxiety through the self-conscious exercise of responsibility and choosing. In "Art and Artist" (1932), psychologist Otto Rank wrote that the psychological trauma of birth was the pre-eminent human symbol of existential anxiety and encompasses the creative person's simultaneous fear of--and desire for--separation, individuation and differentiation. Theologian Paul Tillich characterized existential anxiety as "the state in which a being is aware of its possible nonbeing" and he listed three categories for the nonbeing and resulting anxiety: ontic (fate and death), moral (guilt and condemnation), and spiritual (emptiness and meaninglessness). According to Tillich, the last of these three types of existential anxiety, i.e. spiritual anxiety, is predominant in modern times while the others were predominant in earlier periods. Tillich argues that this anxiety can be accepted as part of the human condition or it can be resisted but with negative consequences. In its pathological form, spiritual anxiety may tend to "drive the person toward the creation of certitude in systems of meaning which are supported by tradition and authority" even though such "undoubted certitude is not built on the rock of reality". According to Viktor Frankl, author of "Man's Search for Meaning", when faced with extreme mortal dangers the most basic of all human wishes is to find a meaning of life to combat the "trauma of nonbeing" as death is near.
According to Yerkes-Dodson law, an optimal level of arousal is necessary to best complete a task such as an exam, performance, or competitive event. However, when the anxiety or level of arousal exceeds that optimum, it results in a decline in performance. Test anxiety is the uneasiness, apprehension, or nervousness felt by students who had a fear of failing an exam. Students suffering from test anxiety may experience any of the following: the association of grades with personal worth, fear of embarrassment by a teacher, fear of alienation from parents or friends, time pressures, or feeling a loss of control. Sweating, dizziness, headaches, racing heartbeats, nausea, fidgeting, and drumming on a desk are all common. Because test anxiety hinges on fear of negative evaluation, debate exists as to whether test anxiety is itself a unique anxiety disorder or whether it is a specific type of social phobia. While the term "test anxiety" refers specifically to students, many adults share the same experience with regard to their career or profession. The fear of failing a task and being negatively evaluated for it can have a similarly negative effect on the adult.
Anxiety when meeting or interacting with unknown people is a common stage of development in young people. For others, it may persist into adulthood and become social anxiety or social phobia. "Stranger anxiety" in small children is "not" a phobia. Rather it is a developmentally appropriate fear by toddlers and preschool children of those who are not parents or family members. In adults, an excessive fear of other people is not a developmentally common stage; it is called social anxiety.
Paradoxical anxiety is anxiety arising from use of methods or techniques which are normally used to reduce anxiety. This includes relaxation or meditation techniques as well as use of certain medications. In some buddhist meditation literature, this effect, although it is not referred to as anxiety there due to the religious context of the writing, is described as something which arises naturally and should be turned toward and mindfully explored in order to gain insight into the nature emotion, and more profoundly, the nature of self.
A. A. Milne was born in Kilburn, London, England to parents John Vine Milne and Sarah Maria (née Heginbotham) and grew up at Henley House School, 6/7 Mortimer Road (now Crescent), Kilburn, London, a small independent school run by his father. One of his teachers was H. G. Wells who taught there in 1889–90. Milne attended Westminster School and Trinity College, Cambridge, where he studied on a mathematics scholarship. While there, he edited and wrote for "Granta", a student magazine. He collaborated with his brother Kenneth and their articles appeared over the initials AKM. Milne's work came to the attention of the leading British humour magazine "Punch", where Milne was to become a contributor and later an assistant editor. Milne joined the British Army in World War I and served as an officer in the Royal Warwickshire Regiment and later, after a debilitating illness, the Royal Corps of Signals. After the war, he wrote a denunciation of war titled "Peace with Honour" (1934), which he retracted somewhat with 1940's "War with Honour". During World War II, Milne was one of the most prominent critics of English humour writer P. G. Wodehouse, who was captured at his country home in France by the Nazis and imprisoned for a year. Wodehouse made radio broadcasts about his internment, which were broadcast from Berlin. Although the lighthearted broadcasts made fun of the Germans, Milne accused Wodehouse of committing an act of near treason by cooperating with his country's enemy. Wodehouse got some revenge on his former friend by creating fatuous parodies of the Christopher Robin poems in some of his later stories, and claiming that Milne "was probably jealous of all other writers... But I loved his stuff." He married Dorothy "Daphne" de Sélincourt in 1913, and their only son, Christopher Robin Milne, was born in 1920. In 1925, A. A. Milne bought a country home, Cotchford Farm, in Hartfield, East Sussex. During World War II, A. A. Milne was Captain of the Home Guard in Hartfield & Forest Row, insisting on being plain 'Mr. Milne' to the members of his platoon. He retired to the farm after a stroke and brain surgery in 1952 left him an invalid and by August 1953 "he seemed very old and disenchanted".
After graduating from Cambridge in 1903, A. A. Milne contributed humorous verse and whimsical essays to the British humour magazine "Punch", joining the staff in 1906 and becoming an assistant editor. During this period he published 18 plays and 3 novels, including the murder mystery "The Red House Mystery" (1922). His son was born in August 1920 and in 1924 Milne produced a collection of children's poems "When We Were Very Young", which were illustrated by "Punch" staff cartoonist E. H. Shepard. A collection of short stories for children "Gallery of Children", and other stories that became part of the Winnie-the-Pooh books, were first published in 1925. Looking back on this period (in 1926) Milne observed that when he told his agent that he was going to write a detective story, he was told that what the country wanted from a "Punch" humorist" was a humorous story; when two years later he said he was writing nursery rhymes, his agent and publisher were convinced he should write another detective story; and after another two years he was being told that writing a detective story would be in the worst of taste given the demand for children's books. He concluded that "the only excuse which I have yet discovered for writing anything is that I want to write it; and I should be as proud to be delivered of a Telephone Directory "con amore" as I should be ashamed to create a Blank Verse Tragedy at the bidding of others."
Milne is most famous for his two "Pooh" books about a boy named Christopher Robin after his son, and various characters inspired by his son's stuffed animals, most notably the bear named Winnie-the-Pooh. Christopher Robin Milne's stuffed bear, originally named "Edward", was renamed "Winnie-the-Pooh" after a Canadian black bear named Winnie (after Winnipeg), which was used as a military mascot in World War I, and left to London Zoo during the war. "The pooh" comes from a swan called "Pooh". E. H. Shepard illustrated the original Pooh books, using his own son's teddy, Growler ("a magnificent bear"), as the model. Christopher Robin Milne's own toys are now under glass in New York. "Winnie-the-Pooh" was published in 1926, followed by "The House at Pooh Corner" in 1928. A second collection of nursery rhymes, "Now We Are Six", was published in 1927. All three books were illustrated by E. H. Shepard. Milne also published four plays in this period. He also "gallantly stepped forward" to contribute a quarter of the costs of dramatising P. G. Wodehouse's "A Damsel in Distress".
The success of his children's books was to become a source of considerable annoyance to Milne, whose self-avowed aim was to write whatever he pleased and who had, until then, found a ready audience for each change of direction: he had freed pre-war "Punch" from its ponderous facetiousness; he had made a considerable reputation as a playwright (like his idol J. M. Barrie) on both sides of the Atlantic; he had produced a witty piece of detective writing in The Red House Mystery (although this was severely criticised by Raymond Chandler for the implausibility of its plot). But once Milne had, in his own words, "said goodbye to all that in 70,000 words" (the approximate length of his four principal children's books), he had no intention of producing any reworkings lacking in originality, given that one of the sources of inspiration, his son, was growing older. His reception remained warmer in America than Britain, and he continued to publish novels and short stories, but by the late 1930s the audience for Milne's grown-up writing had largely vanished: he observed bitterly in his autobiography that a critic had said that the hero of his latest play ("God help it") was simply "Christopher Robin grown up...what an obsession with me children are become!". Even his old literary home, "Punch", where the "When We Were Very Young" verses had first appeared, was ultimately to reject him, as Christopher Milne details in his autobiography "The Enchanted Places", although Methuen continued to publish whatever Milne wrote, including the long poem 'The Norman Church' and an assembly of articles entitled "Year In, Year Out" (which Milne likened to a benefit night for the author). He also adapted Kenneth Grahame's novel "The Wind in the Willows" for the stage as "Toad of Toad Hall". The title was an implicit admission that such chapters as Chapter 7, "The Piper at the Gates of Dawn", could not survive translation to the theatre. A special introduction written by Milne is included in some editions of Grahame's novel. Several of Milne's children's poems were set to music by the composer Harold Fraser-Simson. His poems have been parodied many times, including with the books "When We Were Rather Older" and "Now We Are Sixty". After Milne's death, his widow sold the rights to the Pooh characters to the Walt Disney Company, which has made many Pooh cartoon movies, a Disney Channel television show, as well as thousands of Pooh-related merchandise and millions of adoring fans. Royalties from the Pooh characters paid by Disney to the Royal Literary Fund, part-owner of the Pooh copyright, provide the income used to run the Fund's Fellowship Scheme, placing professional writers in U.K. universities.
The club were the most successful teams in the amateur era of Argentine football, winning 10 of the 14 league championships they contested. Alumni played a local derby with Belgrano Athletic Club, the local rivals dominated Argentine football for more than a decade. The two clubs won all 13 league championships contested between 1899 and 1911. Alumni participated in the inaugural Association Football League (AAFL) league in 1893, and played again in 1895 and 1900, under the name English High School. In 1901 they changed their name to Alumni, they continued to play in the league until the club were disbanded in 1911. The club were disbanded for two main reasons, the first was the shortage of players caused by the fact that they rarely allowed players from outside the institution of the English High School. The second main reason was that the club was losing a lot of money and it seemed unlikely that they could fulfil their fixtures for 1912.
The meaning of the word addiction in the English lexicon varies according to context. A positive addiction is a beneficial habit--where the benefits outweigh the costs. A negative addiction is a detrimental habit—where the benefits are not worth the negative financial, physical, spiritual and mental costs. A neutral addiction is a habit in which it is not clear if the organism (or species) benefits from the activity. Examples of "negative addictions" are: sexual addiction and compulsion, drug addiction (e.g. alcoholism, nicotine addiction), problem gambling, ergomania, compulsive overeating, shopping addiction, computer addiction, pornography addiction, television addiction, etc. Examples of "positive addictions" are: exercise, eating healthy and volunteering. An example of a "neutral addiction" is mowing the lawn.
In medicine, an addiction is a chronic neurobiological disorder that has genetic, psychosocial, and environmental dimensions and is characterized by one of the following: the continued use of a substance despite its detrimental effects, impaired control over the use of a drug (compulsive behavior), and preoccupation with a drug's use for non-therapeutic purposes (i.e. craving the drug). Addiction is often accompanied by the presence of deviant behaviors (for instance stealing money and forging prescriptions) that are used to obtain a drug. Tolerance to a drug and physical dependence are not defining characteristics of addiction, although they typically accompany addiction to certain drugs. Tolerance is a pharmacologic phenomenon where the dose of a medication needs to be continually increased in order to maintain its desired effects. For instance, individuals with severe chronic pain taking opiate medications (like morphine) will need to continually increase the dose in order to maintain the drug's analgesic (pain-relieving) effects. Physical dependence is also a pharmacologic property and means that if a certain drug is abruptly discontinued, an individual will experience certain characteristic withdrawal signs and symptoms. Many drugs used for therapeutic purposes produce withdrawal symptoms when abruptly stopped, for instance oral steroids, certain antidepressants, benzodiazepines, and opiates. However, common usage of the term "addiction" has spread to include psychological dependence. In this context, the term is used in drug addiction and substance abuse problems, but also refers to behaviors that are not generally recognized by the medical community as problems of addiction, such as compulsive overeating. The term "addiction" is also sometimes applied to compulsions that are not substance-related, such as problem gambling and computer addiction. In these kinds of common usages, the term "addiction" is used to describe a recurring compulsion by an individual to engage in some specific activity, despite harmful consequences, as deemed by the user himself to his individual health, mental state, or social life. Not all doctors agree on the exact nature of addiction or dependency however the biopsychosocial model is generally accepted in scientific fields as the most comprehensive theorem for addiction. Historically, addiction has been defined with regard solely to psychoactive substances (for example alcohol, tobacco and other drugs) which cross the blood-brain barrier once ingested, temporarily altering the chemical milieu of the brain. However, "studies on phenomenology, family history, and response to treatment suggest that intermittent explosive disorder, kleptomania, problem gambling, pyromania, and trichotillomania may be related to mood disorders, alcohol and psychoactive substance abuse, and anxiety disorders (especially obsessive–compulsive disorder)." However, such disorders are classified by the American Psychological Association as impulse control disorders and therefore not as addictions. Many people, both psychology professionals and laypersons, now feel that there should be accommodation made to include psychological dependency on such things as gambling, food, sex, pornography, computers, work, exercise, spiritual obsession (as opposed to religious devotion), pain, cutting and shopping so these behaviors count as 'addictions' as well and cause guilt, shame, fear, hopelessness, failure, rejection, anxiety, or humiliation symptoms associated with, among other medical conditions, depression and epilepsy. Although, the above mentioned are things or tasks which, when used or performed, do not fit into the traditional view of addiction and may be better defined as an obsessive–compulsive disorder, withdrawal symptoms may occur with abatement of such behaviors. It is said by those who adhere to a traditionalist view that these withdrawal-like symptoms are not strictly reflective of an addiction, but rather of a behavioral disorder. However, understanding of neural science, the brain, the nervous system, human behavior, and affective disorders has revealed "the impact of molecular biology in the mechanisms underlying developmental processes and in the pathogenesis of disease". The use of thyroid hormones as an effective adjunct treatment for affective disorders has been studied over the past three decades and has been confirmed repeatedly. Modern research into addiction is generally focused on Dopaminergic pathways. There is great and sometimes heated debate around the definition of addiction with parties falling into two main camps the Disease model of addiction and the behaviorists, explanations of various models can be found in the article on Drug rehabilitation.
"Substance dependence When an individual persists in use of alcohol or other drugs despite problems related to use of the substance, substance dependence may be diagnosed. Compulsive and repetitive use may result in tolerance to the effect of the drug and withdrawal symptoms when use is reduced or stopped. This, along with Substance Abuse are considered Substance Use Disorders..." Terminology has become quite complicated in the field. Pharmacologists continue to speak of addiction from a physiologic standpoint (some call this a physical dependence); psychiatrists refer to the disease state as psychological dependence; most other physicians refer to the disease as addiction. The field of psychiatry is now considering, as they move from DSM-IV to DSM-V, transitioning from "substance dependence" to "addiction" as terminology for the disease state. The medical community now makes a careful theoretical distinction between "physical dependence" (characterized by symptoms of withdrawal) and "psychological dependence" (or simply "addiction"). Addiction is now narrowly defined as "uncontrolled, compulsive use"; if there is no harm being suffered by, or damage done to, the patient or another party, then clinically it may be considered compulsive, but to the definition of some it is not categorized as 'addiction'. In practice, the two kinds of addiction are not always easy to distinguish. Addictions often have both physical and psychological components. There is also a lesser known situation called pseudo-addiction. A patient will exhibit drug-seeking behavior reminiscent of psychological addiction, but they tend to have genuine pain or other symptoms that have been under-treated. Unlike true psychological addiction, these behaviors tend to stop when the pain is adequately treated. The obsolete term physical addiction is deprecated, because of its connotations. In modern pain management with opioids physical dependence is nearly universal. While opiates are essential in the treatment of acute pain, the benefit of this class of medication in chronic pain is not well proven. Clearly, there are those who would not function well without opiate treatment; on the other hand, many states are noting significant increases in non-intentional deaths related to opiate use. High-quality, long-term studies are needed to better delineate the risks and benefits of chronic opiate use.
Physical dependence on a substance is defined by the appearance of characteristic withdrawal symptoms when the substance is suddenly discontinued. Opiates, benzodiazepines, barbiturates, alcohol and nicotine induce physical dependence. On the other hand, some categories of substances share this property and are still not considered addictive: cortisone, beta blockers and most antidepressants are examples. So, while physical dependency can be a major factor in the psychology of addiction and most often becomes a primary motivator in the continuation of an addiction, the initial primary attribution of an addictive substance is usually its ability to induce pleasure, although with continued use the goal is not so much to induce pleasure as it is to relieve the anxiety caused by the absence of a given addictive substance, causing it to become used compulsively. Some substances induce physical dependence or physiological tolerance - but not addiction — for example many laxatives, which are not psychoactive; nasal decongestants, which can cause rebound congestion if used for more than a few days in a row; and some antidepressants, most notably venlafaxine, paroxetine and sertraline, as they have quite short half-lives, so stopping them abruptly causes a more rapid change in the neurotransmitter balance in the brain than many other antidepressants. Many non-addictive prescription drugs should not be suddenly stopped, so a doctor should be consulted before abruptly discontinuing them. The speed with which a given individual becomes addicted to various substances varies with the substance, the frequency of use, the means of ingestion, the intensity of pleasure or euphoria, and the individual's genetic and psychological susceptibility. Some people may exhibit alcoholic tendencies from the moment of first intoxication, while most people can drink socially without ever becoming addicted. Opioid dependent individuals have different responses to even low doses of opioids than the majority of people, although this may be due to a variety of other factors, as opioid use heavily stimulates pleasure-inducing neurotransmitters in the brain. Nonetheless, because of these variations, in addition to the adoption and twin studies that have been well replicated, much of the medical community is satisfied that addiction is in part genetically moderated. That is, one's genetic makeup may regulate how susceptible one is to a substance and how easily one may become psychologically attached to a pleasurable routine. Eating disorders are complicated pathological mental illnesses and thus are not the same as addictions described in this article. Eating disorders, which some argue are not addictions at all, are driven by a multitude of factors, most of which are highly different than the factors behind addictions described in this article. It has been reported, however, that patients with eating disorders can successfully be treated with the same non-pharmacological protocols used in patients with chemical addiction disorders. Gambling is another potentially addictive behavior with some biological overlap. Conversely gambling urges have emerged with the administration of Mirapex (pramipexole), a dopamine agonist. The DSM definition of addiction can be boiled down to compulsive use of a substance (or engagement in an activity) despite ongoing negative consequences—this is also a summary of what used to be called "psychological dependency." Physical dependence, on the other hand, is simply needing a substance to function. Humans are all physically dependent on oxygen, food and water. A drug can cause physical dependence and not addiction (for example, some blood pressure medications, which can produce fatal withdrawal symptoms if not tapered) and can cause addiction without physical dependence (the withdrawal symptoms associated with cocaine are all psychological, there is no associated vomiting or diarrhea as there is with opioid withdrawal). In the now outdated conceptualization of the problem, psychological dependency leads to psychological withdrawal symptoms (such as cravings, irritability, insomnia, depression, anorexia, etc). Addiction can in theory be derived from any rewarding behaviour, and is believed to be strongly associated with the dopaminergic system of the brain's reward system (as in the case of cocaine and amphetamines). Some claim that it is a habitual means to avoid undesired activity, but typically it is only so to a clinical level in individuals who have emotional, social, or psychological dysfunctions (psychological addiction is defined as such), replacing normal positive stimuli not otherwise attained. A person who is physically dependent, but not psychologically dependent can have their dose slowly dropped until they are no longer physically dependent. However, if that person is psychologically dependent, they are still at serious risk for relapse into abuse and subsequent physical dependence. Psychological dependence does not have to be limited only to substances; even activities and behavioural patterns can be considered addictions, if they become uncontrollable, e.g. problem gambling, Internet addiction, computer addiction, sexual addiction / pornography addiction, overeating, self-injury, compulsive buying, or work addiction.
Some medical systems, including those of at least 15 states of the United States, refer to an Addiction Severity Index to assess the severity of problems related to substance use. The index assesses problems in six areas: medical, employment/support, alcohol and other drug use, legal, family/social, and psychiatric. While addiction or dependency is related to seemingly uncontrollable urges, and arguably could have roots in genetic predispositions, treatment of dependency is conducted by a wide range of medical and allied professionals, including Addiction Medicine specialists, psychiatrists, psychologists, and appropriately trained nurses, social workers, and counselors. Early treatment of acute withdrawal often includes medical detoxification, which can include doses of anxiolytics or narcotics to reduce symptoms of withdrawal. An experimental drug, ibogaine, is also proposed to treat withdrawal and craving. Alternatives to medical detoxification include acupuncture detoxification. A wide variety of controlled clinical trials, outcome summaries and anecdotal reports about the use of acupuncture in addiction treatment have been appearing since the 1970s in journals specializing in addictions, mental health, public health, criminal justice and acupuncture. These reports differed vastly in terms of methodology, populations studied, statistical sophistication and clinical relevance as well as in their findings about the value of acupuncture. A sub-category of this published work has focused specifically on acupuncture detoxification. Neurofeedback therapy has shown statistically significant improvements in numerous researches conducted on alcoholic as well as mixed substance abuse population. In chronic opiate addiction, a surrogate drug such as methadone is sometimes offered as a form of opiate replacement therapy. But treatment approaches universal focus on the individual's ultimate choice to pursue an alternate course of action. Therapists often classify patients with chemical dependencies as either interested or not interested in changing. Treatments usually involve planning for specific ways to avoid the addictive stimulus, and therapeutic interventions intended to help a client learn healthier ways to find satisfaction. Clinical leaders in recent years have attempted to tailor intervention approaches to specific influences that affect addictive behavior, using therapeutic interviews in an effort to discover factors that led a person to embrace unhealthy, addictive sources of pleasure or relief from pain. From the applied behavior analysis literature and the behavioral psychology literature several evidenced based intervention programs have emerged (1) behavioral maritial therapy (2) community reinforcement approach (3) cue exposure therapy and (4) contingency management strategies. In addition, the same author suggest that Social skills training adjunctive to inpatient treatment of alcohol dependence is probably efficacious.
The development of addiction is thought to involve a simultaneous process of 1) increased focus on and engagement in a particular behavior and 2) the attenuation or "shutting down" of other behaviors. For example, under certain experimental circumstances such as social deprivation and boredom, animals allowed the unlimited ability to self-administer certain psychoactive drugs will show such a strong preference that they will forgo food, sleep, and sex for continued access. The neuro-anatomical correlate of this is that the brain regions involved in driving goal-directed behavior grow increasingly selective for particular motivating stimuli and rewards, to the point that the brain regions involved in the inhibition of behavior can no longer effectively send "stop" signals. A good analogy is to imagine flooring the gas pedal in a car with very bad brakes. In this case, the limbic system is thought to be the major "driving force" and the orbitofrontal cortex is the substrate of the top-down inhibition. A specific portion of the limbic circuit known as the mesolimbic dopaminergic system is hypothesized to play an important role in translation of motivation to motor behavior- and reward-related learning in particular. It is typically defined as the ventral tegmental area (VTA), the nucleus accumbens, and the bundle of dopamine-containing fibers that are connecting them. This system is commonly implicated in the seeking out and consumption of rewarding stimuli or events, such as sweet-tasting foods or sexual interaction. However, its importance to addiction research goes beyond its role in "natural" motivation: while the specific site or mechanism of action may differ, all known drugs of abuse have the common effect in that they elevate the level of dopamine in the nucleus accumbens. This may happen directly, such as through blockade of the dopamine re-uptake mechanism (see cocaine). It may also happen indirectly, such as through stimulation of the dopamine-containing neurons of the VTA that synapse onto neurons in the accumbens (see opiates). The euphoric effects of drugs of abuse are thought to be a direct result of the acute increase in accumbal dopamine. The human body has a natural tendency to maintain homeostasis, and the central nervous system is no exception. Chronic elevation of dopamine will result in a decrease in the number of dopamine receptors available in a process known as downregulation. The decreased number of receptors changes the permeability of the cell membrane located post-synaptically, such that the post-synaptic neuron is less excitable- i.e.: less able to respond to chemical signaling with an electrical impulse, or action potential. It is hypothesized that this dulling of the responsiveness of the brain's reward pathways contributes to the inability to feel pleasure, known as anhedonia, often observed in addicts. The increased requirement for dopamine to maintain the same electrical activity is the basis of both physiological tolerance and withdrawal associated with addiction. Downregulation can be classically conditioned. If a behavior consistently occurs in the same environment or contingently with a particular cue, the brain will adjust to the presence of the conditioned cues by decreasing the number of available receptors in the absence of the behavior. It is thought that many drug overdoses are not the result of a user taking a higher dose than is typical, but rather that the user is administering the same dose in a new environment. In cases of physical dependency on depressants of the central nervous system such as opioids, barbiturates, or alcohol, the absence of the substance can lead to symptoms of severe physical discomfort. Withdrawal from alcohol or sedatives such as barbiturates or benzodiazepines (valium-family) can result in seizures and even death. By contrast, withdrawal from opioids, which can be extremely uncomfortable, is rarely if ever life-threatening. In cases of dependence and withdrawal, the body has become so dependent on high concentrations of the particular chemical that it has stopped producing its own natural versions (endogenous ligands) and instead produces opposing chemicals. When the addictive substance is withdrawn, the effects of the opposing chemicals can become overwhelming. For example, chronic use of sedatives (alcohol, barbiturates, or benzodiazepines) results in higher chronic levels of stimulating neurotransmitters such as glutamate. Very high levels of glutamate kill nerve cells, a phenomenon called excitatory neurotoxicity.
The word "addiction" is also sometimes used colloquially to refer to something for which a person has a passion, such as books, chocolate, work, the web, running, postage stamp collecting, or eating — although eating disorders are a genuine health risk, unlike most so-called casual addictions. Addiction and drug control legislation. Most countries have legislation which brings various drugs and drug-like substances under the control of licensing systems. Typically this legislation covers any or all of the opiates, amphetamines, cannabinoids, cocaine, barbiturates, hallucinogens (tryptamines, LSD, phencyclidine, and psilocybin) and a variety of more modern synthetic drugs, and unlicensed production, supply or possession may be a criminal offense. Usually, however, drug classification under such legislation is not related simply to addictiveness. The substances covered often have very different addictive properties. Some are highly prone to cause physical dependency, whilst others rarely cause any form of compulsive need whatsoever. Also, although the legislation may be justifiable on moral grounds to some, it can make addiction or dependency a much more serious issue for the individual. Reliable supplies of a drug become difficult to secure as illegally produced substances may have contaminants. Withdrawal from the substances or associated contaminants can cause additional health issues and the individual becomes vulnerable to both criminal abuse and legal punishment. Criminal elements that can be involved in the profitable trade of such substances can also cause physical harm to users.
Thomas Szasz denies that addiction is a psychiatric problem. In many of his works, he argues that addiction is a choice, and that a drug addict is one who simply prefers a socially taboo substance rather than, say, a low risk lifestyle. In "Our Right to Drugs", Szasz cites the biography of Malcolm X to corroborate his economic views towards addiction: Malcolm claimed that quitting cigarettes was harder than shaking his heroin addiction. Szasz postulates that humans always have a choice, and it is foolish to call someone an 'addict' just because they prefer a drug induced euphoria to a more popular and socially welcome lifestyle. Professor John Booth Davies at the University of Strathclyde has argued in his book "The Myth of Addiction" that 'people take drugs because they want to and because it makes sense for them to do so given the choices available' as opposed to the view that 'they are compelled to by the pharmacology of the drugs they take'. He uses an adaptation of attribution theory (what he calls the theory of functional attributions) to argue that the statement 'I am addicted to drugs' is functional, rather than veridical. Stanton Peele has put forward similar views. Experimentally, Bruce K. Alexander used the classic experiment of Rat Park to show that 'addicted' behaviour in rats only occurred when the rats had no other options. When other options and behavioural opportunities were put in place, the rats soon showed far more complex behaviours.
In traditional logic, an axiom or postulate is a proposition that is not proved or demonstrated but considered to be either self-evident, or subject to necessary decision. Therefore, its truth is taken for granted, and serves as a starting point for deducing and inferring other (theory dependent) truths. In mathematics, the term "axiom" is used in two related but distinguishable senses: "logical axioms" and "non-logical axioms". In both senses, an axiom is any mathematical statement that serves as a starting point from which other statements are logically derived. Unlike theorems, axioms (unless redundant) cannot be derived by principles of deduction, nor are they demonstrable by mathematical proofs, simply because they are starting points; there is nothing else from which they logically follow (otherwise they would be classified as theorems). Logical axioms are usually statements that are taken to be universally true (e.g., "A" and "B" implies "A"), while non-logical axioms (e.g.,) are actually defining properties for the domain of a specific mathematical theory (such as arithmetic). When used in the latter sense, "axiom," "postulate", and "assumption" may be used interchangeably. In general, a non-logical axiom is not a self-evident truth, but rather a formal logical expression used in deduction to build a mathematical theory. To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms). There are typically multiple ways to axiomatize a given mathematical domain. Outside logic and mathematics, the term "axiom" is used loosely for any established principle of some field.
The word "axiom" comes from the Greek word ("axioma"), a verbal noun from the verb ("axioein"), meaning "to deem worthy", but also "to require", which in turn comes from ("axios"), meaning "being in balance", and hence "having (the same) value (as)", "worthy", "proper". Among the ancient Greek philosophers an axiom was a claim which could be seen to be true without any need for proof. The root meaning of the word 'postulate' is to 'demand'; for instance, Euclid demands of us that we agree that the some things can be done, e.g. any two points can be joined by a straight line, etc. Ancient geometers maintained some distinction between axioms and postulates. While commenting Euclid's books Proclus remarks that "Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property". Boethius translated 'postulate' as "petitio" and called the axioms "notiones communes" but in later manuscripts this usage was not always strictly kept.
The logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference), was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, if we are talking about mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms "axiom" and "postulate" hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid. The ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view. An “axiom”, in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion that "When an equal amount is taken from equals, an equal amount results." At the foundation of the various sciences lay certain additional hypotheses which were accepted without proof. Such a hypothesis was termed a "postulate". While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Indeed, Aristotle warns that the content of a science cannot be successfully communicated, if the learner is in doubt about the truth of the postulates. The classical approach is well illustrated by Euclid's Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of "common notions" (very basic, self-evident assertions).
A lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. This abstraction, one might even say formalization, makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Structuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without "any" particular application in mind. The distinction between an “axiom” and a “postulate” disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid's fifth postulate we get theories that have meaning in wider contexts, hyperbolic geometry for example. We must simply be prepared to use labels like “line” and “parallel” with greater flexibility. The development of hyperbolic geometry taught mathematicians that postulates should be regarded as purely formal statements, and not as facts based on experience. When mathematicians employ the axioms of a field, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all. It is not correct to say that the axioms of field theory are “propositions that are regarded as true without proof.” Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system. Modern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and logic itself can be regarded as a branch of mathematics. Frege, Russell, Poincaré, Hilbert, and Gödel are some of the key figures in this development. In the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axiom. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom. It was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms. In a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here the emergence of Russell's paradox, and similar antinomies of naive set theory raised the possibility that any such system could turn out to be inconsistent. The formalist project suffered a decisive setback, when in 1931 Gödel showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, Gödel proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory. It is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo–Fraenkel axioms for set theory. The axiom of choice, a key hypothesis of this theory, remains a very controversial assumption. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo–Franekel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.
These are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms "at least" some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.
Each of these patterns is an "axiom schema", a rule for generating an infinite number of axioms. For example, if formula_9, formula_10, and formula_11 are propositional variables, then formula_12 and formula_13 are both instances of axiom schema 1, and hence are axioms. It can be shown that with only these three axiom schemata and "modus ponens", one can prove all tautologies of the propositional calculus. It can also be shown that no pair of these schemata is sufficient for proving all tautologies with "modus ponens". Other axiom schemas involving the same or different sets of primitive connectives can be alternatively constructed. These axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.
This means that, for any variable symbol formula_15, the formula formula_16 can be regarded as an axiom. Also, in this example, for this not to fall into vagueness and a never-ending series of "primitive notions", either a precise notion of what we mean by formula_16 (or, for that matter, "to be equal") has to be well established first, or a purely formal and syntactical usage of the symbol formula_20 has to be enforced, only regarding it as a string and only a string of symbols, and mathematical logic does indeed do that. Axiom scheme for Universal Instantiation. Given a formula formula_21 in a first-order language formula_14, a variable formula_15 and a term formula_24 that is substitutable for formula_15 in formula_21, the formula
Non-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not "tautologies". Another name for a non-logical axiom is "postulate". Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought that in principle every theory could be axiomatized in this way and formalized down to the bare language of logical formulas. This turned out to be impossible and proved to be quite a story ("see below"); however recently this approach has been resurrected in the form of neo-logicism. Non-logical axioms are often simply referred to as "axioms" in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups. Thus, an "axiom" is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.
This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms. Basic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory, most often Von Neumann–Bernays–Gödel set theory, abbreviated NBG. This is a conservative extension of ZFC, with identical theorems about sets, and hence very closely related. Sometimes slightly stronger theories such as Morse-Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe are used, but in fact most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic. The study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of "abstract algebra" brought with itself group theory, rings and fields, Galois theory. This list could be expanded to include most fields of mathematics, including axiomatic set theory, measure theory, ergodic theory, probability, representation theory, and differential geometry. Combinatorics is an example of a field of mathematics which does not, in general, follow the axiomatic method.
The Peano axioms are the most widely used "axiomatization" of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem. The standard structure is formula_52 where formula_53 is the set of natural numbers, formula_46 is the successor function and formula_45 is naturally interpreted as the number 0.
Probably the oldest, and most famous, list of axioms are the 4 + 1 Euclid's postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. Indeed, one can assume that no parallels through a point outside a line exist, that exactly one exists, or that infinitely many exist. These choices give us alternative forms of geometry in which the interior angles of a triangle add up to less than, exactly, or more than 180 degrees respectively and are known as elliptic, Euclidean, and hyperbolic geometries.
The object of study is the real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a "Dedekind complete ordered field", meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires use of second-order logic. The Löwenheim-Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.
that is, for any statement that is a "logical consequence" of formula_57 there actually exists a "deduction" of the statement from formula_57. This is sometimes expressed as "everything that is true is provable", but it must be understood that "true" here means "made true by the set of axioms", and not, for example, "true in the intended interpretation". Gödel's completeness theorem establishes the completeness of a certain commonly-used type of deductive system. Note that "completeness" has a different meaning here than it does in the context of Gödel's first incompleteness theorem, which states that no "recursive", "consistent" set of non-logical axioms formula_57 of the Theory of Arithmetic is "complete", in the sense that there will always exist an arithmetic statement formula_21 such that neither formula_21 nor formula_67 can be proved from the given set of axioms. There is thus, on the one hand, the notion of "completeness of a deductive system" and on the other hand that of "completeness of a set of non-logical axioms". The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.
Early mathematicians regarded axiomatic geometry as a model of physical space, and obviously there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details and modern algebra was born. In the modern view we may take as axioms any set of formulas we like, as long as they are not known to be inconsistent.
Alpha (uppercase Α, lowercase α;) is the first letter of the Greek alphabet. In the system of Greek numerals it has a value of 1. It was derived from the Phoenician letter Aleph. Letters that arose from Alpha include the Latin A and the Cyrillic letter А. In both Classical Greek and Modern Greek, alpha represents the Open front unrounded vowel. The lower case today represents an open back unrounded vowel in phonetics. Plutarch in "Moralia", presents a discussion on why the letter alpha stands first in the alphabet. Ammonius asks Plutarch what he, being a Boeotian, thinks of Cadmus, the Phoenician who reputedly settled in Thebes and introduced the alphabet to Greece, placing "alpha" first because it is the Phoenician name for ox -- which, unlike Hesiod, the Phoenicians considered not the second or third, but the first of all necessities. "Nothing at all" Plutarch replied. He then added that he would rather be assisted by Lamprias, his own grandfather, than by Dionysus' grandfather, i.e. Cadmus. For Lamprias had said that the first articulate sound made is "alpha", because it is very plain and simple — the air coming off the mouth does not require any motion of the tongue — and therefore this is the first sound that children make. The Homeric word "alphesiboios" (') is associated with both the root "alph-" and "ox". It is derived from "alphanō" () meaning "to yield, earn" and "bous" () meaning "ox", hence "alphesiboios" means "bringing in" or "acquiring oxen". According to Plutarch's natural order of attribution of the vowels to the planets, alpha was connected with the Moon. Oxen were also associated with the Moon in both early Sumerian and Egyptian religious symbolism, possibly due to the crescent shape of their horns. Alpha, both as a symbol and term, is used to refer to or describe a variety of things, including the first or most significant occurrence of something. The New Testament has God declaring himself to be the "Alpha and Omega, the beginning and the end, the first and the last." (Revelation 22:13, KJV, and see also 1:8). The uppercase letter alpha is not generally used as a symbol because it tends to be rendered identically to the uppercase Latin A.
Alvin Toffler (born October 3, 1928) is an American writer and futurist, known for his works discussing the digital revolution, communication revolution, corporate revolution and technological singularity. A former associate editor of "Fortune" magazine, his early work focused on technology and its impact (through effects like information overload). Then he moved to examining the reaction of and changes in society. His later focus has been on the increasing power of 21st century military hardware, weapons and technology proliferation, and capitalism. He is married to Heidi Toffler, also a writer and futurist. They live in Los Angeles. Accenture, the management consultancy firm, has dubbed him the third most influential voice among business leaders, after Bill Gates and Peter Drucker. He has also been described in the "Financial Times" as the "world's most famous futurologist". People's Daily classes him among the 50 foreigners that shaped modern China. He lives in the Bel Air section of Los Angeles, California, just north of Sunset Boulevard.
Toffler explains, "Society needs people who take care of the elderly and who know how to be compassionate and honest. Society needs people who work in hospitals. Society needs all kinds of skills that are not just cognitive; they're emotional, they're affectional. You can't run the society on data and computers alone." Toffler also states, in "Rethinking the Future", that "The illiterate of the 21st century will not be those who cannot read and write, but those who cannot learn, unlearn, and relearn." In his book "The Third Wave" Toffler describes three types of societies, based on the concept of 'waves' - each wave pushes the older societies and cultures aside. In this post-industrial society, there is a lot of diversity in lifestyles ("subcultures"). Adhocracies (fluid organizations) adapt quickly to changes. Information can substitute most of the material resources (see ersatz) and becomes the main material for workers (cognitarians instead of proletarians), who are loosely affiliated. Mass customization offers the possibility of cheap, personalized, production catering to small niches (see just-in-time production). The gap between producer and consumer is bridged by technology using a so called configuration system. "Prosumers" can fill their own needs (see open source, assembly kit, freelance work). This was the notion that new technologies are enabling the radical fusion of the producer and consumer into the "prosumer". In some cases "prosuming" entails a "third job" where the corporation "outsources" its labor not to other countries, but to the unpaid consumer, such as when we do our own banking through an ATM instead of a teller that the bank must employ, or trace our own postal packages on the internet instead of relying on a paid clerk. Aging societies will be using new (medical) technologies from self-diagnosis to instant toilet urinalysis to self-administered therapies delivered by nanotechnology to do for themselves what doctors used to do. This will change the way the whole health industry works. Since the 1960s, people have been trying to make sense out of the impact of new technologies and social change. Toffler's writings have been influential beyond the confines of scientific, economic and public policy discussions. Techno music pioneer Juan Atkins cites Toffler's phrase "techno rebels" in "Future Shock" as inspiring him to use the word "techno" to describe the musical style he helped to create. Toffler's works and ideas have been subject to various criticisms, usually with the same argumentation used against futurology: that foreseeing the future is nigh impossible. In the 1990s, his ideas were publicly lauded by Newt Gingrich. In 1996 Alvin and Heidi Toffler founded Toffler Associates, an executive advisory firm committed to helping commercial firms and government agencies adjust to the changes described in the Tofflers' works. The development Toffler believes may go down as this era's greatest turning point is the creation of wealth in outer space. Wealth today, he argues, is created everywhere (globalisation), nowhere (cyberspace), and out there (outer space). Global positioning satellites are key to synchronising precision time and data streams for everything from cellphone calls to ATM withdrawals. They allow just-in-time (JIT) productivity because of precise tracking. GPS is also becoming central to air-traffic control. And satellites increase agricultural productivity through tracking weather, enabling more accurate forecasts. Two major predictions of Toffler's - the paperless office and human cloning - have yet to be realized, not due to technological barriers but to sociological and politico-religious conditions. Also influenced Timothy Leary (see Info-Psychology; New Falcon Press, 2004) Alvin Toffler is mentioned in the Dance Exponents' 1983 hit "Victoria".
Spider-Man first appeared in the comic book "Amazing Fantasy" #15 (Aug. 1962). The series was canceled with that issue, but response to the character was so positive that a solo title, "The Amazing Spider-Man", was launched with a March 1963 cover-date. The character was created by writer-editor Stan Lee and artist and co-plotter Steve Ditko, and the pair produced 38 issues from 1963 to 1966. Ditko left after the 38th issue, while Lee remained as writer until issue 100. Since then, many writers and artists have taken over the monthly comic through the years, chronicling the adventures of Marvel's most identifiable hero. "The Amazing Spider-Man" is the character's flagship series, and the only monthly series to star Spider-Man until "Peter Parker, The Spectacular Spider-Man" in 1976. Most of the major characters and villains of the Spider-Man saga have been introduced here, and it is where key events occur. The title was published continuously until #441 (Nov. 1998) when Marvel Comics relaunched it as vol. 2, #1 (Jan. 1999), but on Spider-Man's 40th anniversary, this new title reverted to using the numbering of the original series, beginning again with issue #500 (Dec. 2003).
The initial years of the book, under Lee and Ditko, chronicled Spider-Man's nascent career with his civilian life as hard-luck yet perpetually good-humored teenager Peter Parker. Parker balanced his career as Spider-Man with his job as a freelance photographer for "The Daily Bugle" (under the bombastic editor-publisher J. Jonah Jameson) to help support himself and his frail Aunt May. At the same time, Parker dealt with public hostility towards Spider-Man and the antagonism of his classmates Flash Thompson and Liz Allan at Midtown High School, while also embarking on a tentative, ill-fated romance with Jameson's secretary, Betty Brant. By focusing on Parker's everyday problems, Lee and Ditko created a groundbreakingly flawed, self-doubting superhero, and the first major teenaged superhero to be a protagonist and not a sidekick. Ditko's quirky art provided a stark contrast to the more cleanly dynamic stylings of Marvel's most prominent artist, Jack Kirby, and Ditko's Spider-Man, slightly sinister yet affectionately cartoony, combined with the humor and pathos of Lee's writing to lay the foundation for what became an enduring mythos. Most of Spider-Man's key villains and supporting characters were introduced during this time. Issue #1 (March 1963) featured the first appearances of J. Jonah Jameson and his astronaut son John Jameson, and the supervillain the Chameleon. It also included the hero's first encounter with the superhero team The Fantastic Four. Issue #2 (May 1963) featured the first appearance of The Vulture and the beginning of Parker's freelance photography career at the newspaper "The Daily Bugle". The Lee-Ditko era continued to usher in a significant number of villains and supporting characters, including Doctor Octopus in #3 (July 1963); the Sandman and Betty Brant in #4 (Sept. 1963); the Lizard in #6 (Nov. 1963); Electro in #9 (March 1964); Mysterio in #13 (June 1964); the Green Goblin in #14 (July 1964); Kraven The Hunter in #15 (Aug. 1964); reporter Ned Leeds in #18 (Nov. 1964); the Scorpion in #20 (Jan. 1965); and the Molten Man in #28 (Sept. 1965), an issue that also featured Parker's graduation from high school. Parker began attending Empire State University in #31 (Dec. 1965), the issue which also featured the first appearances of friends and classmates Gwen Stacy and Harry Osborn. Harry's father, Norman Osborn first appeared in #23 (April 1965) as a member of Jameson's country club but is not named nor revealed as Harry's father until #37 (June 1966). Probably the most celebrated issue of the Lee-Ditko run is #33 (Feb. 1966), the third part of the story arc "If This Be My Destiny", and featuring the dramatic scene of Spider-Man, through force of will and thoughts of family, escaping from being pinned by heavy machinery. Although credited only as artist for most of his run, Ditko would eventually plot the stories as well as draw them, leaving Lee to script the dialogue. However, a rift between Ditko and Lee developed, and the two men were not on speaking terms long before Ditko completed his last issue, "The Amazing Spider-Man" 38 (July 1966). The exact reasons for the Ditko-Lee split have been a source of controversy ever since. In successor penciler John Romita Sr.'s first issue, #39 (Aug. 1966), archnemesis the Green Goblin discovers Spider-Man's secret identity and reveals his own to the captive hero. Romita's Spider-Man – more muscular and heroic-looking than Ditko's – became the model for two decades. The Lee-Romita era saw the introduction of such characters as "Daily Bugle" managing editor Joseph "Robbie" Robertson in #52 (Sept. 1967) and NYPD Captain George Stacy, father of Parker's girlfriend Gwen Stacy, in #56 (Jan. 1968). The most important supporting character to be introduced during the Romita era was Mary Jane Watson, who made her first full appearance in #42, (Nov. 1966), although she first appeared in #25 (June 1965) with her face obscured and had been mentioned since #15 (Aug. 1964). Lee and Romita toned down the prevalent sense of antagonism in Parker's world by improving Parker's relationship with the supporting characters and having stories focused as much on the social and college lives of the characters as they did on Spider-Man's adventures. The stories also became more topical, addressing issues such as civil rights, racism, prisoners' rights, the Vietnam War, and political elections. Issue #50 (June 1967) introduced the highly enduring criminal mastermind the Kingpin, who would become a major force as well in the superhero series "Daredevil". Other notable first appearances in the Lee-Romita era include the Rhino in #41 (Oct. 1966), the Shocker in #46 (March 1967), the Prowler in #78 (Nov. 1969), and the Kingpin's son, Richard Fisk, in #83 (April 1970).
Two spin-off series debuted in the 1970s: "Marvel Team-Up" in 1972, and "The Spectacular Spider-Man" in 1976. The flagship title's second decade took a grim turn with a story in #89-90 (Oct.-Nov. 1970) featuring the death of Captain George Stacy. This was also the first Spider-Man story to be penciled by Gil Kane, who would alternate drawing duties with Romita for the next year-and-a-half and would draw several landmark issues. One such story took place in the controversial issues #96-98 (May-July 1971). Writer-editor Lee defied the Comics Code Authority with this story, in which Parker's friend Harry Osborn, was hospitalized after tripping on LSD. Lee wrote this story upon a request from the US Department of Health, Education and Welfare for a story about the dangers of drugs. Citing its dictum against depicting drug use, even in an anti-drug context, the CCA refused to put its seal on these issues. With the approval of Marvel publisher Martin Goodman, Lee had the comics published without the seal. The comics sold well and Marvel won praise for its socially conscious efforts. The CCA subsequently loosened the Code to permit negative depictions of drugs, among other new freedoms. The "Six-Arm Saga" of #100-102 (September-November 1971) introduced Morbius, the Living Vampire. The second installment was the first "Amazing Spider-Man" story not written by co-creator Lee, with Roy Thomas taking over writing the book for several months before Stan Lee returned to write #105-110 (Feb.-July 1972). Lee, who was going on to become Marvel Comics' publisher, with Thomas becoming editor-in-chief, then turned writing duties over to 19-year-old wunderkind Gerry Conway, who scripted the series through 1975. Romita penciled Conway's first half-dozen issues, which introduced the gangster Hammerhead in #113 (Oct.1972). Kane then succeeded Romita as penciler, although Romita would continue inking Kane for a time. The most memorable work of the Conway/Kane/Romita team was #121-122 (June-July 1973), which featured the death of Gwen Stacy at the hands of The Green Goblin in the story which shocked readers, "The Night Gwen Stacy Died" (#121). Her demise and the Goblin's apparent death one issue later formed a story arc widely considered as the most defining in the history of Spider-Man. The aftermath of the story also deepened both the characterization of Mary Jane Watson and her relationship with Parker. By late 1973, Gil Kane was succeeded by Ross Andru, whose run lasted nearly 60 issues, from 1973 to 1978. Issue #129 (February 1974) introduced the Punisher, who would become one of Marvel Comics' principal and most widely recognized characters. The Conway-Andru era also featured the first appearances of the Man-Wolf in #124-125 (Sept.-Oct. 1973); the near-marriage of Doctor Octopus and Aunt May in #131 (April 1974); Harry Osborn stepping into his father's role as the Green Goblin in #135-137 (Aug.-Oct.1974); and the original "Clone Saga", containing the introduction of Spider-Man's clone, in #147-149 (Aug.-Oct. 1975). Archie Goodwin and Gil Kane produced the title's 150th issue (November 1975) before Len Wein became writer for two-and-a-half years.During Wein's tenure, Harry Osborn and Liz Allen dated and became engaged, J. Jonah Jameson was introduced to his eventual second wife, Marla Madison, and Aunt May suffered a heart attack. Wein's last story on "Amazing" was a five-issue arc in #176-180 (Jan.-May 1978) featuring a third Green Goblin (Harry Osborn’s psychiatrist, Bart Hamilton). Marv Wolfman, Marvel's editor-in-chief from 1975 to 1976, succeeded Wein as writer, and in his first issue, #182 (July 1978), had Parker propose marriage to Watson (who refused, in the following issue). Keith Pollard succeeded Ross Andru as artist shortly afterward, and with Wolfman introduced the likable rogue the Black Cat (Felicia Hardy) in #194 (July 1979). As a love interest for Spider-Man, the Black Cat would go on to be an important supporting character for the better part of the next decade.
"The Amazing Spider-Man" #200 (Jan. 1980) featured the return and death of the burglar who killed Spider-Man's Uncle Ben. Writer Marv Wolfman and penciler Keith Pollard both left the title by mid-year, succeeded by Dennis O'Neil, a writer known for groundbreaking 1970s work at rival DC Comics, and penciler John Romita, Jr.. Roger Stern, who had written nearly 20 issues of sister title "The Spectacular Spider-Man", took over "Amazing" in late 1981. During his two years on the title, Stern augmented the backgrounds of long-established Spider-Man villains, and with Romita Jr. created the mysterious supervillain the Hobgoblin in #238-239 (March-April 1983). Fans engaged with the mystery of the Hobgoblin's secret identity, which continued throughout #244-245 and 249-251 (Sept.-Oct. 1983 & Feb.-April 1984). One lasting change was the reintroduction of Mary Jane Watson as a much more serious, mature woman who becomes Peter's confidant after she reveals that she knows his secret identity. By mid-1984, Tom DeFalco and Ron Frenz took over scripting and penciling. DeFalco helped establish the maturation in Parker and Watson's relationship, laying the foundation for the character's eventual wedding. Notably, in #257 (Oct. 1984), Watson tells Parker that she knows he is Spider-Man, and in #259 (Dec. 1984), she reveals to Parker the extent of her troubled childhood. Other notable issues of the DeFalco-Frenz era include #252 (May 1984), with the first appearance of Spider-Man's black costume, which the hero would wear almost exclusively for the next four years' worth of comics; the debut of criminal mastermind the Rose, in #253 (June 1984); the revelation in #258 (Nov. 1984) that the black costume is a living being, a symbiote; and the introduction of the female mercenary Silver Sable in #265 (June 1985). Tom DeFalco and Ron Frenz were both removed from "Amazing Spider-Man" in 1986 by editor Owsley under acrimonious circumstances. A succession of artists including Alan Kupperberg, John Romita, Jr., and Alex Saviuk penciled the book from 1987 to 1988; Owsley wrote the book for the first half of 1987, scripting the five-part "Gang War" story (#284-288) that DeFalco plotted. Former "Spectacular Spider-Man" writer Peter David scripted #289 (June 1987), which revealed Ned Leeds as being the Hobgoblin (although this was retconned in 1996 by Roger Stern into Leeds not being the original Hobgoblin after all). David Michelinie took over as writer in the next issue, for a story arc in #290-292 (July-Sept. 1987) that led to the marriage of Peter Parker and Mary Jane Watson in "Amazing Spider-Man Annual" #21. Issue #298 (March 1988) was the first Spider-Man comic to be drawn by future industry star Todd McFarlane, the first regular artist on "Amazing Spider-Man" since Frenz's departure. McFarlane revolutionized Spider-Man's look. His depiction – large-eyed, with wiry, contorted limbs, and messy, knotted, convoluted webbing – influenced the way virtually all subsequent artists would draw the character. McFarlane's other significant contribution to the Spider-Man canon was the design for what would become one of Spider-Man's most wildly popular antagonists, the supervillain Venom. Issue #299 (April 1988) featured Venom's first appearance (a last-page cameo) before his first full appearance in #300 (May 1988). The latter issue also featured Spider-Man reverting to his original red-and-blue costume. Other notable issues of the Michelinie-McFarlane era include #312 (February 1989), featuring the Green Goblin vs. the Hobgoblin; and #315-317 (May-July 1989), with the return of Venom. After the editorial and creative turmoil that beset "Amazing Spider-Man" in 1987, the Michelinie/McFarlane team at the tail-end of the 1980s restored a sense of creative consistency and quality to the book, and set the tone for Spider-Man for the next decade.
With a civilian life as a married man, the Spider-Man of the 1990s was different from the superhero of the previous three decades. Following his 1988-1989 run on "Amazing Spider-Man", Todd McFarlane left the title in 1990 to write and draw a new series titled simply "Spider-Man". McFarlane's successor, Erik Larsen, penciled the book from early 1990 to mid-1991. After issue #350, Larsen was succeeded by Mark Bagley, who had won the 1986 Marvel Tryout Contest and was assigned a number of low-profile penciling jobs followed by a run on "New Warriors" in 1990. Bagley penciled the flagship Spider-Man title from 1991 to 1996, with his art forming the basis for most Spider-Man licensed merchandise of the decade and onward. Issues #361-363 (April-June 1992) introduced Carnage, a second symbiote nemesis for Spider-Man. The series' 30th-anniversary issue, #365 (Aug. 1992), was a double-sized, hologram-cover issue with the cliffhanger ending of Peter Parker's parents, long thought dead, reappearing alive. It would be close to two years before they were revealed to be impostors, who are killed in #388 (April 1994), scripter Michelinie's last issue. His 1987-1994 stint gave him the second-longest run as writer on the title, behind Stan Lee. With #389, writer J.M. DeMatteis, whose Spider-Man credits included the 1987 "Kraven's Last Hunt" story arc and a 1991-1993 run on "The Spectacular Spider-Man", took over the title. From October 1994 to June 1996, "Amazing" stopped running stories exclusive to it, and ran installments of multi-part stories that crossed over into all the Spider-Man books. One of the few self-contained stories during this period was in #400 (April 1995), which featured the death of Aunt May — later revealed to have been faked. The "Clone Saga" culminated with the revelation that the Spider-Man who had appeared in the previous 20 years of comics was a clone of the real Spider-Man. This plot twist was massively unpopular with many readers, and was later reversed in the "Revelations" story arc that crossed over the Spider-Man books in late 1996. The Clone Saga tied into a publishing gap after #406 (October 1995), when the title was temporarily replaced by "The Amazing Scarlet Spider" #1-2 (Nov.-Dec. 1995), featuring Ben Reilly. The series picked up again with #407 (Jan. 1996), with Tom DeFalco returning as writer. Bagley completed his 5½-year run by September 1996. A succession of artists, including Ron Garney, Steve Skroce, Joe Bennett, and Rafael Kayanan, penciled the book until the final issue, #441 (November 1998), after which Marvel rebooted the title with vol. 2, #1 (Jan. 1999).
Marvel began "The Amazing Spider-Man" anew with vol. 2, #1 (Jan. 1999). Howard Mackie wrote the first 29 issues. The relaunch proved controversial with the Sandman being regressed to his criminal ways and the "death" of Mary-Jane, which was ultimately reversed due to fan backlash. Other elements included the introduction of a new Spider-Woman (who was spun off into her own short-lived series) and major references to John Byrne's controversial ", which launched at the same time as the reboot. Mackie's run would end with "The Amazing Spider-Man Annual 2000", which saw the return of Mary Jane, who left her husband upon reuniting with him. With #30 (June 2001), J. Michael Straczynski took over as writer. While his arrival saw the title regain lost readers, Straczynski oversaw additional controversial storylines — most notably his lengthy "Spider-Totem" arc, which raised the issue of whether Spider-Man's powers were magic-based, rather than as the result of a radioactive spider's bite. Additionally, Straczynski resurrected the plot point of Aunt May discovering her nephew was Spider-Man, and returned Mary Jane, with the couple reuniting in "The Amazing Spider-Man" #50. Straczynski also gave Spider-Man a new profession, having Parker teach at his former high school. Issue #30 began a dual numbering system, with the original series numbering (#471) returned and placed alongside the volume-two number on the cover. Other longtime, rebooted Marvel Comics titles, including "Fantastic Four", were given the dual numbering around this time. In October 2000, John Romita, Jr. succeeded John Byrne as artist. After vol. 2, #58 (Nov. 2003), the title reverted to its original numbering for #500 (December 2003), with vol. 2, #1-58 considered #442-499 of the original run. Mike Deodato, Jr. penciled the book from mid-2004 until 2006.
As part of the 2006 "Civil War" crossover story arc, which included "The Amazing Spider-Man," Spider-Man initially works with Iron Man and the government to fulfill the Superhuman Registration Act, capturing superheroes who refuse to reveal their identities and register with the government. On live television, he reveals his own true identity as Peter Parker, but eventually changes side and, alongside Captain America and others, becomes an opponent of registration and exposes several of Iron Man's dirty secrets in the process (most notably, Iron Man's concentration camp-style prison for non-registered super-heroes). Due to Iron Man's revoking of security protecting Mary Jane and Aunt May in the wake of Peter's denouncement of Iron Man, the two join Spider-Man on the run. However, the villainous Kingpin of Crime Wilson Fisk orders the Parker family killed, culminating in Aunt May being mortally wounded as a result. This, plus the death of Captain America, causes Parker to retrieve his long-unused black costume to wear. After discovering that Kingpin gave the order that led to May being shot, he breaks into prison and delivers Kingpin a humiliating beating as a warning to leave his family alone.
"One More Day" is a four-part, crossover story arc, written partially by J. Michael Straczynski and illustrated by Joe Quesada, running through "The Amazing Spider-Man" #544-545 (Nov.-Dec. 2007), and "Friendly Neighborhood Spider-Man" #24 (Nov. 2007) and "Sensational Spider-Man" #41 (Dec. 2007), the final issues of those two titles. The demon Mephisto makes a Faustian bargain with Peter Parker and Mary Jane Watson-Parker, offering to save Parker's dying Aunt May if the couple will allow their marriage to have never existed, rewriting that portion of their pasts. This story arc crossover also marked the end of Straczynski's six and a half years run as writer (from 2001–2007) on "The Amazing Spider-Man" after 75 issues (including issues of "Friendly Neighborhood Spider-Man" and "The Sensational Spider-Man" as part of crossovers).
Following this, Marvel made "The Amazing Spider-Man" the company's sole Spider-Man title, upped its frequency of publication to three issues monthly, and inaugurated the series with a sequence of "back to basics" story arcs under the banner of "Brand New Day", wherein Peter now exists in a changed world where he and Mary Jane had never married — neither having any memory of being married together — and with domino-effect differences in their immediate world. The editorial decision that Peter "shouldn't be married, be a loser with no money, and act like an adult with arrested development has been met with disdain by some longtime Spider-man readers of various ages, while Marvel argued they wanted to "lighten" up the character again and kids don't like a married superhero (though fans of the Fantastic Four, Superman, The Flash in various incarnations and such would argue back otherwise). The alternating regular writers are Dan Slott, Bob Gale, Marc Guggenheim, Fred Van Lente, and Zeb Wells, joined by a rotation of artists that includes Chris Bachalo, Phil Jimenez, Mike McKone, John Romita, Jr. and Marcos Martin. Joe Kelly, Mark Waid and Roger Stern later joined the writing team and Barry Kitson the artists roster.
Antigua and Barbuda (Spanish for "ancient" and "bearded") is a twin-island nation lying between the Caribbean Sea and the Atlantic Ocean. It consists of two major inhabited islands, Antigua () and Barbuda (), and a number of smaller islands (including Great Bird, Green, Guinea, Long, Maiden and York Islands). Separated by a few sea miles, the group is in the middle of the Leeward Islands part of the Lesser Antilles, roughly at 17 degrees north of the Equator. It could well be that Codrington Island is actually Barbuda Island; certainly, there is a town of Codrington on Barbuda.
Its prehistoric peoples had no written language; analyses of archaeological excavations were used to develop current knowledge of their existence. Antigua was first settled by Archaic Age hunter-gatherer Amerindians, erroneously referred to as Siboney or Cibony. Carbon-dating has established that the earliest settlements started around 3100 BCE. They were succeeded by the Ceramic Age pre-Columbian Arawak-speaking Saladoid people who migrated from the lower Orinoco River. The Arawaks introduced agriculture, raising, among other crops, the famous Antigua Black Pineapple (Moris cultivar of "Ananas comosus"), corn, sweet potatoes (white with firmer flesh than the bright orange "sweet potato" used in the United States), chiles, guava, tobacco and cotton. The indigenous West Indians made excellent sea-going vessels which they used to sail the Atlantic and the Caribbean. As a result, Caribs and Arawaks were able to colonize much of South America and the Caribbean Islands. Their descendants still live there, notably in Brazil, Venezuela and Colombia. Most Arawaks left Antigua around 1100 CE; those who remained were later raided by the Caribs. According to the "Catholic Encyclopedia", the Caribs' superior weapons and seafaring prowess allowed them to defeat most of the West Indian Arawak nations, enslaving some and possibly cannibalizing others. The "Catholic Encyclopedia" does make it clear that the European invaders had some difficulty differentiating between the native peoples they encountered. As a result, the number and types of ethnic/tribal groups in existence at that time may have been much more varied and numerous than just the two mentioned in this article. According to "A Brief History of the Caribbean" (Jan Rogozinski, Penguin Putnam, Inc., September 2000), European and African diseases, malnutrition and slavery eventually killed most of the Caribbean's native population, although no researcher has conclusively proven any of these causes as the real reason for these deaths. In fact, some historians believe that the psychological stress of slavery may also have played a part in the massive number of deaths amongst enslaved natives. Others believe that the reportedly abundant, but starchy, low-protein diet may have contributed to severe malnutrition of the Amerindians, who were used to a diet fortified with protein from the sea. The island of Antigua, originally called "Wa'ladli" by Arawaks, is today called "Land of Wadadli" by locals. It is possible that Caribs called it "Wa'omoni". Christopher Columbus, while sailing by in 1493, may have named it Santa Maria la Antigua after an icon in the Spanish Seville Cathedral. The Spaniards did not colonize Antigua because it lacked fresh water but not aggressive Caribs. The English settled on Antigua in 1632; Sir Christopher Codrington settled on Barbuda in 1684. Slavery, established to run sugar plantations around 1684, was abolished in 1834. The British ruled from 1632 to 1981, with a brief French interlude in 1666. The islands became an independent state within the Commonwealth of Nations on November 1, 1981, with Elizabeth II as the first Queen of Antigua and Barbuda. The Right Honourable Vere Cornwall Bird became the first Prime Minister.
The politics of Antigua and Barbuda take place within a framework of a federal, parliamentary, representative democratic monarchy, in which the Head of State is the Monarch who appoints the Governor General as vice-regal representative. Elizabeth II is the present Queen of Antigua and Barbuda, having served in that position since the islands' independence from the United Kingdom in 1981. The Queen is now (2007-2010) represented by Governor General Dane Louise Agnetha Lake-Tack (1944-) who became the first woman to hold this position. A Council of Ministers is appointed by the Governor General on the advice of the Prime Minister, currently Winston Baldwin Spencer (1948-). The Prime Minister is the Head of Government. Executive power is exercised by the government while legislative power is vested in both the government and the two Chambers of Parliament. The bicameral Parliament consists of the Senate (seventeen members appointed by members of the government and the opposition party, and approved by the Governor-General), and the House of Representatives (seventeen members elected by first past the post (check this entry) to serve five-year terms). The Speaker of the House is author and former St. John's University professor (New York) D. Gisele Isaac (check), while the President of the Senate is educator Hazlyn Francis-Mason. The last elections held were on March 12, 2009, during which the Antigua Labour Party won seven seats, the United Progressive Party nine and the Barbuda People's Movement one. Since 1949, the party system had been dominated by the personalist Antigua Labour Party. However, the Antigua and Barbuda legislative election of 2004 saw the defeat of the longest-serving elected government in the Caribbean. Prime Minister Lester Bryant Bird (succeeded his father Vere Cornwall Bird) and Deputy Robin Yearwood had been in office since 1994. The elder Bird was Prime Minister from 1981 to 1994 and Chief Minister of Antigua from 1960 to 1981, except for the 1971-1976 period when the Progressive Labour Movement (PLM) defeated his party. Vere Cornwall Bird, the nation's first Prime Minister, is credited with having brought Antigua and Barbuda and the Caribbean into a new era of independence. The Judicial Branch is the Eastern Caribbean Supreme Court (based in Saint Lucia; one judge of the Supreme Court is a resident of the islands and presides over the Court of Summary Jurisdiction). In addition, Antigua is a member of the Caribbean Court of Justice. The Supreme Court of Appeal was the British Judicial Committee of the Privy Council up until 2001, when the nations of the Caribbean Community voted to abolish the right of appeal to the Privy Council in favor of a Caribbean Court of Justice. Some debate between member countries repeatedly delayed the court's date of inauguration. As of March 2005 (check the status of this action), only Barbados was set to replace appeals to the Privy Council with appeals the Caribbean Court of Justice, which by then had come into operation.
Tourism dominates the economy, accounting for more than half of the Gross Domestic Product (GDP). Antigua is famous for its many exclusive luxury resorts. Weak tourist activity since early 2000 has slowed the economy, however, and squeezed the government into a tight fiscal corner. Investment banking and financial services also make up an important part of the economy. Major world banks with offices in Antigua include the Bank of America (Bank of Antigua), Barclays, the Royal Bank of Canada (RBC) and Scotia Bank. Financial-services corporations with offices in Antigua include PriceWaterhouseCoopers. The US Securities and Exchange Commission has accused the Antigua-based Stanford International Bank owned by Texas billionaire Allen Stanford of orchestrating a huge fraud which may have bilked investors of some $8 billion. (check status 20100312) The twin-island nation's agricultural production is focussed on its domestic market and constrained by a limited water supply and a labor shortage stemming from the lure of higher wages in tourism and construction work. Manufacturing is made up of enclave-type assembly for export, the major products being bedding, handicrafts and electronic components. Prospects for economic growth in the medium term will continue to depend on income growth in the industrialized world, especially in the United States, from which about one-third of all tourists come.
Antigua has a population of 85,632, mostly made up of people of West African, British, and Portuguese descent. (moved from the Introoduction on 20100313) The ethnic/racial distribution consists of 91% Black or mulatto, 4.4% other mixed races, 1.7% White and 2.9% other [check for the difference between "other mixed races" and "other"]. Most Whites are of Irish or British descent. Christian Levantine Arabs (mostly from Syria, Lebanon and Palestine), Portuguese, and a small number of Asians and Sephardic Jews make up the remainder of the population. Behind the late 20th-century revival and redefinition of the role of Afro-Antiguans and Barbudans in the society's cultural life is a history of racial/ethnic tensions which systematically excluded non-Whites. Within the colonial framework established by the British soon after their initial settlement of Antigua in 1623, five distinct and carefully ranked racial/ethnic groups emerged. At the top of this social structure were the British, who justified their hegemony with arguments of White Supremacy and civilizing missions. Amongst them were divisions between British Antiguans and non-creolized Britons, with the latter coming out on top. In short, this was a racial/ethnic hierarchy which gave maximum recognition to people and cultural practices of Anglican origin. Immediately below the British were the mulattos, a mixed-race group of Afro-European origin. Mulattos, lighter in shade than most Africans, developed a complex system based on skin shade to distinguish themselves from the latter and to legitimate their claims to higher status. In many ways, they paralleled the British White Supremacy ideology. In the middle of this social stratification were the Portuguese, 2,500 of whom migrated as workers from Madeira (a Portuguese island off the Moroccan coast) between 1847 and 1852 because of a severe famine there. Many established small businesses and joined the ranks of the mulatto class. The British never really considered the Portuguese as Whites and did not allow them into their ranks. Amongst Antiguans and Barbudans of Portuguese descent, status differences were based on the varying degrees of assimilation into the dominant group's Anglicized practices. Next to the bottom were Middle Easterners who began migrating to Antigua and Barbuda around the turn of the 20th century. Starting as itinerant traders, they soon worked their way into the social mix. Although Middle Easterners came from a variety of areas, as a group they are usually referred to as Syrians. Afro-Antiguans and Afro-Barbudans were at the bottom. Forced into slavery, Africans started arriving in Antigua and Barbuda in large numbers during the 1670's. Very quickly, they grew into the largest racial/ethnic group. Their entry into the local social structure was marked by a profound racialization: They ceased being Yoruba, Igbo, or Akan and became Negroes or Blacks. In the 20th century, the colonial social structure gradually started to be phased out with the introduction of universal education and better economic opportunities. This process allowed Blacks to rise to the highest echelons of society and government. In the last decade (check specific decade), Spanish-speaking immigrants from the Dominican Republic and Afro-Caribbean immigrants from Guyana and Dominique were added to this ethnic mosaic. They have entered at the social structure's bottom; it is still too early to predict their patterns of assimilation and social mobility. Today, an increasingly large percentage of the population lives abroad, most notably in the United Kingdom (Antiguan Britons), United States and Canada. A minority of Antiguan residents are immigrants from other countries, particularly from Dominica, Guyana and Jamaica, and, increasing, from the Dominican Republic, St. Vincent and the Grenadines and Nigeria. An estimated 4,500 American citizens also make their home in Antigua and Barbuda, making their numbers one of the largest American populations in the English-speaking Eastern Caribbean.
English is the official language, but many of the locals speak Antiguan Creole. The Barbudan accent is slightly different from the Antiguan. In the years before Antigua and Barbuda's independence, Standard English was widely spoken in preference to Antiguan Creole, but afterwards Antiguans began treating Antiguan Creole as a respectable aspect of their culture. Generally, the upper and middle classes shun Antiguan Creole. The educational system dissuades the use of Antiguan Creole and instruction is done in Standard (British) English. Many of the words used in the Antiguan dialect are derived from British as well as African languages. This can be easily seen in phrases such as: "Me nah go" meaning "I am not going". Another example is: "Ent it?" meaning "Ain't it?" which is itself dialectical and means "Isn't it?". Common island proverbs often can be traced to Africa.
The culture is predominantly British: For example, cricket is the national sport and Antigua has produced several famous cricket players including Sir Vivian Richards, Anderson "Andy" Roberts, and Richard "Richie" Richardson. Other popular sports include football. boat racing and surfing (the Antigua Sailing Week attracts locals and visitors from all over the world). American popular culture and fashion also have a heavy influence. Most of the country's media is made up of major United States networks. Antiguans pay close attention to American fashion trends, and major designer items are available at boutiques in St. John's and elsewhere, although many Antiguans prefer to make a special shopping trip to St. Martin, North America, or San Juan in Puerto Rico. Family and religion play an important roles in the lives of Antiguans. Most attend religious services on Sunday, although there is a growing number of Seventh-day Adventists who observe the Sabbath on Saturday. The national Carnival held each August commemorates the abolition of slavery in the British West Indies, although on some islands, Carnival may celebrate the coming of Lent. Its festive pageants, shows, contests and other activities are a major tourist attraction. Calypso and soca music are important in Antigua and Barbuda. Corn and sweet potatoes play an important role in Antiguan cuisine. For example, a popular Antiguan dish, Dukuna (DOO-koo-NAH) is a sweet, steamed dumpling made from grated sweet potatoes, flour and spices. One of the Antiguan staple foods, fungi (FOON-ji), is a cooked paste made of cornmeal and water.
Like many Commonwealth countries, "cricket" is the most popular sport. The 2007 Cricket World Cup was hosted in the West Indies from March 11 to April 28, 2007. Antigua hosted eight matches at the Sir Vivian Richards Stadium, which was completed on February 11, 2007 and can hold up to 20,000 people. Antigua is a Host of Stanford Twenty20 – Twenty20 Cricket, a version started by Allen Stanford in 2006 as a regional cricket game with almost all Caribbean islands taking part. Antiguan Viv Richards scored the fastest Test Century and Brian Lara twice scored the World Test Record at the Antigua Recreation Ground. Association football is also a very popular sport. Antigua does have a national "football" team but it is inexperienced. "Athletics" are popular. Talented athletes are trained from a young age, and Antigua and Barbuda has produced a few fairly adept athletes. Janill Williams, a young athlete with much promise comes from Gray's Farm, Antigua. Sonia Williams and Heather Samuel represented Antigua and Barbuda at the Olympic Games. Other prominent rising stars include Brendan Christian (100 m, 200 m), Daniel Bailey (100 m, 200 m) and James Grayman (high jump). Antigua can boast of some excellent "tennis" players, most notably Brian Philip #1 and Roberto Esposito #2 on the island for under-18 tournaments, who both are also involved in under-18 ITF tournaments. Their coach's (Eli Armstrong) daughter Keishora Armstrong, who will be turning 13 later this year, is the under-18's champion on the girls' circuit.
The people of Antigua & Barbuda enjoy a more-than-90% literacy rate. In 1998, Antigua and Barbuda adopted a national mandate to become the pre-eminent provider of medical services in the Caribbean. As part of this mission, Antigua and Barbuda is building the most technologically advanced hospital in the Caribbean, the Mt. St. John Medical Centre. The island of Antigua currently has two medical schools, the American University of Antigua (AUA), founded in 2004, and The University of Health Sciences Antigua (UHSA), founded in 1982. Other institutions of higher education include the government-owned state college in Antigua and the Antigua and Barbuda Institute of Information Technology (ABIIT); The University of the West Indies has a branch in Antigua for locals who wish to continue university studies.
Antigua and Barbuda is a member of the United Nations, the Bolivarian Alliance for the Americas, the Commonwealth of Nations, the Caribbean Community, the Organization of Eastern Caribbean States, the Organization of American States, the World Trade Organization and the Eastern Caribbean's Regional Security System. Antigua and Barbuda is also a member of the International Criminal Court (with a Bilateral Immunity Agreement of Protection for the US military as covered under Article 98).
The original battlefield museum in the village featured model knights made out of Action Man figures. However, this has now been replaced by a more professional exhibition space incorporating laser, video, slide shows, audio commentaries, and some interactive elements. The museum building is shaped like one of the many longbows famously deployed at the battle by King Henry's archers. The village holds a festival in every other year, commemorating the battle and featuring various stalls and displays. The latest was held on July 21 through July 22, 2007.
Albert Speer (born Berthold Konrad Hermann Albert Speer, (; March 19, 1905 – September 1, 1981) was a German architect who was, for part of World War II, Minister of Armaments and War Production for the Third Reich. Speer was Adolf Hitler's chief architect before assuming ministerial office. As "the Nazi who said sorry", he accepted responsibility at the Nuremberg trials and in his memoirs for crimes of the Nazi regime. His level of involvement in the persecution of the Jews and his level of knowledge of the Holocaust remain matters of dispute. Speer joined the Nazi Party in 1931. His architectural skills made him increasingly prominent within the Party and he became a member of Hitler's inner circle. The dictator commissioned him to design and construct a number of structures, including the Reich Chancellery and the "Zeppelinfeld" stadium in Nuremberg where Party rallies were held. Speer also made plans to reconstruct Berlin on a grand scale, with huge buildings, wide boulevards, and a reorganised transportation system. As Hitler's Minister of Armaments and War Production, Speer was so successful that Germany's war production continued to increase despite massive and devastating Allied bombing. After the war, he was tried at Nuremberg and sentenced to 20 years in prison for his role in the Nazi regime, principally for the use of forced labor. He served his full sentence, most of it at Spandau Prison in West Berlin. Following his release from Spandau in 1966, Speer published two bestselling autobiographical works, "Inside the Third Reich" and ', detailing his often close personal relationship with Hitler, and providing readers and historians with a unique perspective within the workings of the Nazi regime. He later wrote a third book, "Infiltration", about the SS. Speer died of natural causes in 1981 while on a visit to London.
Speer was born in Mannheim, Germany, into a wealthy middle class family. He was the second of three sons of Albert and Luise Speer. In 1918, the family moved permanently to their summer home, Schloss-Wolfsbrunnenweg, in Heidelberg. According to Henry T. King, deputy prosecutor at Nuremberg who later wrote a book about Speer, "Love and warmth were lacking in the household of Speer's youth." Speer wanted to become a mathematician, but his father said if Speer chose this occupation he would "lead a life without money, without a position, and without a future". Instead, Speer followed in the footsteps of his father and grandfather and studied architecture. Speer began his architectural studies at the University of Karlsruhe instead of a more highly acclaimed institution because the hyperinflation crisis of 1923 limited his parents' income. In 1924 when the crisis had abated, he transferred to the "much more reputable" Technical University of Munich. In 1925 he transferred again, this time to the Technical University of Berlin where he studied under Heinrich Tessenow, whom Speer greatly admired. After passing his exams in 1927, Speer became Tessenow's assistant, a high honor for a man of 22. As Tessenow's assistant, Speer taught some of his classes while continuing his own postgraduate studies. In Munich, and continuing in Berlin, Speer began a close friendship, ultimately spanning over 50 years, with Rudolf Wolters, who also studied under Tessenow. In the summer of 1922, Speer began to date Margarete (Margret) Weber (1905–1987). The relationship was frowned upon by Speer's class-conscious mother, who felt the Webers were socially inferior ("Herr" Weber was a successful craftsman who employed 50 workers). Despite this opposition, the two married in Berlin on August 28, 1928, but seven years elapsed before Margarete Speer was first invited to stay at her in-laws' home.
Speer stated he was apolitical when he was a young man, and that he attended a Berlin Nazi rally in December 1930 at the urging of some of his students. He was surprised to find Hitler dressed in a neat blue suit, rather than the brown uniform seen on Nazi Party posters, and was greatly impressed, not only with Hitler's proposals, but also with the man himself. Several weeks later he attended another rally, though this one was presided over by Joseph Goebbels. Speer was disturbed by the way Goebbels whipped the crowd into a frenzy. Despite this unease, Speer could not shake the impression Hitler had made on him. On March 1, 1931, he applied to join the Nazi Party and became member number 474,481. Speer's first Nazi Party position was as head of the Party's motorist association for the Berlin suburb of Wannsee; he was in fact the only Nazi in the town with a car. Speer reported to the Party's leader for the West End of Berlin, Karl Hanke, who hired Speer — without fee — to redecorate a villa he had just rented. Hanke was enthusiastic about the resulting work. In 1931, Speer surrendered his position as Tessenow's assistant due to pay cuts and moved to Mannheim, hoping to use his father's connections to get commissions. He had little success, and his father gave him a job as manager of the elder Speer's properties. In July 1932, the Speers visited Berlin to help out the Party prior to the "Reichstag" elections. While they were there, Hanke recommended the young architect to Goebbels to help renovate the Party's Berlin headquarters. Speer, who had been about to leave with his wife for a vacation in East Prussia, agreed to do the work. When the commission was completed, Speer returned to Mannheim and remained there as Hitler took office in January 1933. After the Nazis took control, Hanke recalled Speer to Berlin. Goebbels, the new Propaganda Minister, commissioned Speer to renovate his Ministry's building on Wilhelmplatz. Speer also designed the 1933 May Day commemoration in Berlin. In "Inside the Third Reich", he wrote that, on seeing the original design for the Berlin rally on Hanke's desk, he remarked that the site would resemble a "Schützenfest" — a rifle club meet. Hanke, now Goebbels' State Secretary, challenged him to create a better design. As Speer learned later, Hitler was enthusiastic about Speer's design (which used giant flags), though Goebbels took credit for it. Tessenow was dismissive: "Do you think you have created something? It's showy, that's all." The organizers of the 1933 Nuremberg Nazi Party rally asked Speer to submit designs for the rally, bringing him into contact with Hitler for the first time. Neither the organizers nor Rudolf Hess were willing to decide whether to approve the plans, and Hess sent Speer to Hitler's Munich apartment to seek his approval. When Speer entered, the new Chancellor was busy cleaning a pistol, which he briefly laid aside to cast a short, interested glance at the plans, approving them without even looking at the young architect. This work won Speer his first national post, as Nazi Party "Commissioner for the Artistic and Technical Presentation of Party Rallies and Demonstrations". Speer's next major assignment was as liaison to the Berlin building trades for Paul Troost's renovation of the Chancellery. As Chancellor, Hitler had a residence in the building and came by every day to be briefed by Speer and the building supervisor on the progress of the renovations. After one of these briefings, Hitler invited Speer to lunch, to the architect's great excitement. Hitler evinced considerable interest in Speer during the luncheon, and later told Speer that he had been looking for a young architect capable of carrying out his architectural dreams for the new Germany. Speer quickly became part of Hitler's inner circle; he was expected to call on Hitler in the morning for a walk or chat, to provide consultation on architectural matters, and to discuss Hitler's ideas. Most days he was invited to dinner. The two men found much in common: Hitler spoke of Speer as a "kindred spirit" for whom he had always maintained "the warmest human feelings". The young, ambitious architect was dazzled by his rapid rise and close proximity to Hitler, which guaranteed him a flood of commissions from the government and from the very highest ranks of the Party. Speer testified at Nuremberg, "I belonged to a circle which consisted of other artists and his personal staff. If Hitler had had any friends at all, I certainly would have been one of his close friends." First Architect of the Third Reich (1934–1939). When Troost died on January 21, 1934, Speer effectively replaced him as the Party's chief architect. Hitler appointed Speer as head of the Chief Office for Construction, which placed him nominally on Hess's staff. One of Speer's first commissions after Troost's death was the "Zeppelinfeld" stadium—the Nuremberg parade grounds seen in Leni Riefenstahl's propaganda masterpiece "Triumph of the Will". This huge work was capable of holding 340,000 people. The tribune was influenced by the Pergamon Altar in Anatolia, but was magnified to an enormous scale. Speer insisted that as many events as possible be held at night, both to give greater prominence to his lighting effects and to hide the individual Nazis, many of whom were overweight. Speer surrounded the site with 130 anti-aircraft searchlights. This created the effect of a "cathedral of light" or, as it was called by British Ambassador Sir Neville Henderson, a "cathedral of ice". Speer described this as his most beautiful work, and as the only one that has stood the test of time. Nuremberg was to be the site of many more official Nazi buildings, most of which were never built; for example, the German Stadium would have accommodated 400,000 spectators, while an even larger rally ground would have held half a million Nazis. While planning these structures, Speer invented the concept of "ruin value": that major buildings should be constructed in such a way that they would leave aesthetically pleasing ruins for thousands of years into the future. Such ruins would be a testament to the greatness of the Third Reich, just as ancient Greek or Roman ruins were symbols of the greatness of those civilizations. Hitler enthusiastically embraced this concept, and ordered that all the Reich's important buildings be constructed in accord with it. Speer could not avoid seeing the brutal excesses of the Nazi regime, although they had little effect on him. Shortly after the Night of the Long Knives, Hitler ordered Speer to take workmen and go to the building housing the offices of Vice-Chancellor Franz von Papen to begin its conversion into a security headquarters, even though it was still occupied by von Papen's officials. Speer and his group entered the building, to be confronted with a pool of blood, apparently from the body of Herbert von Bose, von Papen's secretary, who had been killed there. Speer related that the sight had no effect on him, other than to cause him to avoid that room. When Hitler deprecated Werner March's design for the Olympic Stadium for the 1936 Summer Olympics as too modernistic, Speer modified the plans by adding a stone exterior. Speer designed the German Pavilion for the 1937 international exposition in Paris. The German and Soviet pavilion sites were opposite each other. On learning (through a clandestine look at the Soviet plans) that the Soviet design included two colossal figures seemingly about to overrun the German site, Speer modified his design to include a cubic mass which would check their advance, with a huge eagle on top looking down on the Soviet figures. Both pavilions were awarded gold medals for their designs. Speer would also receive, from Hitler Youth Leader and later fellow Spandau prisoner Baldur von Schirach, the Golden Hitler Youth Honor Badge with oak leaves. In 1937, Hitler appointed Speer as General Building Inspector for the Reich Capital with the rank of undersecretary of state in the Reich government. The position carried with it extraordinary powers over the Berlin city government and made Speer answerable to Hitler alone. It also made Speer a member of the "Reichstag", though the body by then had little effective power. Hitler ordered Speer to make plans to rebuild Berlin. The plans centered around a three-mile long grand boulevard running from north to south, which Speer called the "Prachtstrasse", or Street of Magnificence; he also referred to it as the "North-South Axis". At the north end of the boulevard, Speer planned to build the "Volkshalle", a huge assembly hall with a dome which would have been over high, with floor space for 180,000 people. At the southern end of the avenue would be a huge triumphal arch; it would be almost high, and able to fit the Arc de Triomphe inside its opening. The outbreak of World War II in 1939 led to the postponement, and eventual abandonment, of these plans. Part of the land for the boulevard was to be obtained by consolidating Berlin's railway system. Speer hired Wolters as part of his design team, with special responsibility for the "Prachtstrasse". When Speer's father saw the model for the new Berlin, he said to his son, "You've all gone completely insane." In January 1938, Hitler asked Speer to build a new Reich Chancellery on the same site as the existing structure, and said he needed it for urgent foreign policy reasons no later than his next New Year's reception for diplomats on January 10, 1939. This was a huge undertaking, especially since the existing Chancellery was in full operation. After consultation with his assistants, Speer agreed. Although the site could not be cleared until April, Speer was successful in building the large, impressive structure in nine months. The structure included the "Marble Gallery": at 146 metres long, almost twice as long as the Hall of Mirrors in the Palace of Versailles. Speer employed thousands of workers in two shifts. Hitler, who had remained away from the project, was overwhelmed when Speer turned it over, fully furnished, two days early. In appreciation for the architect's work on the Chancellery, Hitler awarded Speer the Nazi Golden Party Badge. Tessenow was less impressed, suggesting to Speer that he should have taken nine years over the project. The second Chancellery was damaged by the Battle of Berlin in 1945 and was eventually dismantled by the Soviets, its stone used for a war memorial. During the Chancellery project, the pogrom of Kristallnacht took place. Speer would make no mention of it in the first draft of "Inside the Third Reich", and it was only on the urgent advice of his publisher that he added a mention of seeing the ruins of the Central Synagogue in Berlin from his car. Soon after Hitler had given me the first large architectural commissions, I began to suffer from anxiety in long tunnels, in airplanes, or in small rooms. My heart would begin to race, I would become breathless, the diaphragm would seem to grow heavy, and I would get the impression that my blood pressure was rising tremendously... Anxiety amidst all my freedom and power!
Speer supported the German invasion of Poland and subsequent war, though he recognized that it would lead to the postponement, at the least, of his architectural dreams. In his later years, Speer, talking with his biographer-to-be Gitta Sereny, explained how he felt in 1939: "Of course I was perfectly aware that [Hitler] sought world domination... [A]t that time I asked for nothing better. That was the whole point of my buildings. They would have looked grotesque if Hitler had sat still in Germany. All I "wanted" was for this great man to dominate the globe." Speer placed his department at the disposal of the "Wehrmacht". When Hitler remonstrated, and said it was not for Speer to decide how his workers should be used, Speer simply ignored him. Among Speer's innovations were quick-reaction squads to construct roads or clear away debris; before long, these units would be used to clear bomb sites. As the war progressed, initially to great German success, Speer continued preliminary work on the Berlin and Nuremberg plans, at Hitler's insistence, but failed to convince him of the need to suspend peacetime construction projects. Speer also oversaw the construction of buildings for the "Wehrmacht" and "Luftwaffe", and developed a considerable organization to deal with this work. In 1940, Joseph Stalin proposed that Speer pay a visit to Moscow. Stalin had been particularly impressed by Speer's work in Paris, and wished to meet the "Architect of the Reich". Hitler, alternating between amusement and anger, did not allow Speer to go, fearing that Stalin would put Speer in a "rat hole" until a new Moscow arose. When Germany invaded the Soviet Union in 1941, Speer came to doubt, despite Hitler's reassurances, that his projects for Berlin would ever be completed.
On February 8, 1942, Minister of Armaments Fritz Todt died in a plane crash shortly after taking off from Hitler's eastern headquarters at Rastenburg. Speer, who had arrived in Rastenburg the previous evening, had accepted Todt's offer to fly with him to Berlin, but had canceled some hours before takeoff (Speer stated in his memoirs that the cancellation was due to exhaustion from travel and a late-night meeting with Hitler). Later that day, Hitler appointed Speer as Todt's successor to all of his posts. In "Inside the Third Reich", Speer recounts his meeting with Hitler and his reluctance to take ministerial office, only doing so because Hitler commanded it. Speer also states that Hermann Göring raced to Hitler's headquarters on hearing of Todt's death, hoping to claim Todt's powers. Hitler instead presented Göring with the "fait accompli" of Speer's appointment, causing Göring to leave without even attending Todt's funeral. At the time of Speer's accession to the office, the German economy, unlike the British one, was not fully geared for war production. Consumer goods were still being produced at nearly as high a level as during peacetime. No fewer than five "Supreme Authorities" had jurisdiction over armament production — one of which, the Ministry of Economic Affairs, had declared in November 1941 that conditions did not permit an increase in armament production. Few women were employed in the factories, which were running only one shift. One evening soon after his appointment, Speer went to visit a Berlin armament factory; he found no one on the premises. Speer overcame these difficulties by centralizing power over the war economy in himself. Factories were given autonomy, or as Speer put it, "self-responsibility", and each factory concentrated on a single product. Backed by Hitler's strong support (the dictator stated, "Speer, I'll sign anything that comes from you"), he divided the armament field according to weapon system, with experts rather than civil servants overseeing each department. No department head could be older than 55 — anyone older being susceptible to "routine and arrogance" — and no deputy older than 40. Over these departments was a central planning committee headed by Speer, which took increasing responsibility for war production, and as time went by, for the German economy itself. According to the minutes of a conference at "Wehrmacht" High Command in March 1942, "It is only Speer's word that counts nowadays. He can interfere in all departments. Already he overrides all departments... On the whole, Speer's attitude is to the point." Goebbels would note in his diary in June 1943, "Speer is still tops with the "Führer". He is truly a genius with organization." Speer was so successful in his position that by late 1943, he was widely regarded among the Nazi elite as a possible successor to Hitler. While Speer had tremendous power, he was of course subordinate to Hitler. Nazi officials sometimes went around Speer by seeking direct orders from the dictator. When Speer ordered peacetime building work suspended, the "Gauleiters" (Nazi Party district leaders) obtained an exemption for their pet projects. When Speer sought the appointment of Hanke as a labor czar to optimize the use of German labor, Hitler, under the influence of Martin Bormann, instead appointed Fritz Sauckel. Rather than increasing female labor and taking other steps to better organize German labor, as Speer favored, Sauckel advocated importing labor from the occupied nations — and did so, obtaining workers for (among other things) Speer's armament factories, using the most brutal methods. On December 10, 1943, Speer visited the underground Mittelwerk V-2 rocket factory that used concentration camp labor. Shocked by the conditions there (5.7 percent of the work force died that month), and to ensure the workers were in good enough shape to perform the labor, Speer ordered improved conditions for the workers and the construction of the above-ground Dora camp. In spite of these changes, half of the workers at Mittelwerk eventually died. Speer later commented, "[t]he conditions for these prisoners were in fact barbarous, and a sense of profound involvement and personal guilt seizes me whenever I think of them." By 1943, the Allies had gained air superiority over Germany, and bombings of German cities and industry had become commonplace. However, the Allies in their strategic bombing campaign did not concentrate on industry, and Speer, with his improvisational skill, was able to overcome bombing losses. In spite of these losses, German production of tanks more than doubled in 1943, production of planes increased by 80 percent, and production time for Kriegsmarine's submarines was reduced from one year to two months. Production would continue to increase until the second half of 1944, by which time enough equipment to supply 270 army divisions was being produced—although the "Wehrmacht" had only 150 divisions in the field. In January 1944, Speer fell ill with complications from an inflamed knee, and was away from the office for three months. During his absence, his political rivals mainly Göring, and Martin Bormann, attempted to have some of his powers permanently transferred to them, while SS chief Heinrich Himmler tried to have him physically isolated by placing him under SS and Gestapo surveillance through his personal physician Karl Gebhardt whose "care" had made it worse. His wife and friends had him ultimately placed under the care of his friend Karl Brandt. In April, they succeeded in having him deprived of responsibility for construction, and Speer promptly sent Hitler a bitter letter, concluding with an offer of his resignation. Judging Speer indispensable to the war effort, Field Marshal Erhard Milch persuaded Hitler to try to get his minister to reconsider. Hitler sent Milch to Speer with a message not addressing the dispute but instead stating that he still regarded Speer as highly as ever. According to Milch, upon hearing the message, Speer burst out, "The "Führer" can kiss my ass!" After a lengthy argument, Milch persuaded Speer to withdraw his offer of resignation, on the condition his powers were restored. On April 23, 1944, Speer went to see Hitler who agreed that "everything [will] stay as it was, [Speer will] remain the head of all German construction". According to Speer, while he was successful in this debate, Hitler had also won, "because he wanted and needed me back in his corner, and he got me".
Speer's name was included on the list of members of a post-Hitler government drawn up by the conspirators behind the July 1944 assassination plot to kill Hitler. However, the list had a question mark and the annotation "to be won over" by his name, which likely saved him from the extensive purges that followed the scheme's failure. By February 1945, Speer, who had long concluded that the war was lost, was working to supply areas about to be occupied with food and materials to get them through the hard times ahead. On March 19, 1945, Hitler issued his Nero Decree, ordering a scorched earth policy in both Germany and the occupied territories. Hitler's order, by its terms, deprived Speer of any power to interfere with the decree, and Speer went to confront Hitler, telling him the war was lost. Hitler gave Speer 24 hours to reconsider his position, and when the two met the following day, Speer answered, "I stand unconditionally behind you." However, he demanded the exclusive power to implement the Nero Decree, and Hitler signed an order to that effect. Using this order, Speer worked to persuade generals and "Gauleiters" to evade the Nero Decree and avoid needless sacrifice of personnel and destruction of industry that would be needed after the war. Speer managed to reach a relatively safe area near Hamburg as the Nazi regime finally collapsed, but decided on a final, risky visit to Berlin to see Hitler one more time. Speer stated at Nuremberg, "I felt that it was my duty not to run away like a coward, but to stand up to him again." Speer visited the "Führerbunker" on April 22. Hitler seemed calm and somewhat distracted, and the two had a long, disjointed conversation in which the dictator defended his actions and informed Speer of his intent to commit suicide and have his body burned. In the published edition of "Inside the Third Reich", Speer relates that he confessed to Hitler that he had defied the Nero Decree, but then assured Hitler of his personal loyalty, bringing tears to the dictator's eyes. However, Speer biographer Gitta Sereny notes, "Psychologically, it is possible that this is the way he remembered the occasion, because it was how he would have liked to behave, and the way he would have liked Hitler to react. But the fact is that none of it happened; our witness to this is Speer himself." Sereny goes on to note that Speer's original draft of his memoirs lacks the confession and Hitler's tearful reaction, and contains an explicit denial that any confession or emotional exchange took place, as had been alleged in a French magazine article. The following morning, Speer left the "Führerbunker", with Hitler curtly bidding him farewell. Speer toured the damaged Chancellery one last time before leaving Berlin to return to Hamburg. On April 29, the day before his suicide, Hitler prepared his final political testament. That document excluded Speer from the Cabinet and specified that Speer was to be replaced by his subordinate, Karl-Otto Saur.
After Hitler's death, Speer offered his services to the so-called Flensburg Government, headed by Hitler's successor, Karl Dönitz, and took a significant role in that short-lived regime. On May 15, the Americans arrived and asked Speer if he would be willing to provide information on the effects of the air war. Speer agreed, and over the next several days, provided information on a broad range of subjects. It was not until May 23, weeks after the surrender of German troops, that the Allies arrested the members of the Flensburg Government and brought Nazi Germany to a formal end. Speer was taken to several internment centers for Nazi officials and interrogated. In September 1945, Speer was told that he would be tried for war crimes, and several days later, he was taken to Nuremberg and incarcerated there. Speer was indicted on all four possible counts: first, participating in a common plan or conspiracy for the accomplishment of crime against peace, second, planning, initiating and waging wars of aggression and other crimes against peace, third, war crimes, and lastly, crimes against humanity. In political life, there is a responsibility for a man's own sector. For that he is of course fully responsible. But beyond that there is a collective responsibility when he has been one of the leaders. Who else is to be held responsible for the course of events, if not the closest associates around the Chief of State? An observer at the trial, journalist and author William L. Shirer, wrote that, compared to his codefendants, Speer “made the most straightforward impression of all and... during the long trial spoke honestly and with no attempt to shirk his responsibility and his guilt”. Speer also testified that he had planned to kill Hitler in early 1945 by dropping a canister of poison gas into the bunker's air intake. He said his efforts were frustrated by a high wall that had been built around the air intake. Speer stated his motive was despair at realizing that Hitler intended to take the German people down with him. Speer's supposed assassination plan subsequently met with some skepticism, with Speer's architectural rival Hermann Giesler sneering, "the second most powerful man in the state did not have a ladder." Speer was found guilty of war crimes and crimes against humanity, though he was acquitted on the other two counts. On October 1, 1946, he was sentenced to 20 years' imprisonment. While three of the eight judges (two Soviet and one American) initially advocated the death penalty for Speer, the other judges did not, and a compromise sentence was reached "after two days' discussion and some rather bitter horse-trading". ... in the closing stages of the war [Speer] was one of the few men who had the courage to tell Hitler that the war was lost and to take steps to prevent the senseless destruction of production facilities, both in occupied territories and in Germany. He carried out his opposition to Hitler's scorched earth program... by deliberately sabotaging it at considerable personal risk. Twelve of the defendants were sentenced to death (including Bormann, in absentia) and three acquitted; only seven of the defendants were sentenced to imprisonment. They remained in the cells at Nuremberg as the Allies debated where, and under what conditions, they should be incarcerated.
On July 18, 1947, Speer and his six fellow prisoners, all former high officials of the Nazi regime, were flown from Nuremberg to Berlin under heavy guard. The prisoners were taken to Spandau Prison in the British Sector of what would become West Berlin, where they would be designated by number, with Speer given Number Five. Initially, the prisoners were kept in solitary confinement for all but half an hour a day, and were not permitted to address each other or their guards. As time passed, the strict regimen was relaxed, especially during the three months in four that the three Western powers were in control; the four occupying powers took overall control on a monthly rotation. Speer considered himself an outcast among his fellow prisoners for his acceptance of responsibility at Nuremberg. Speer made a deliberate effort to make as productive a use of his time as possible. He wrote, "I am obsessed with the idea of using this time of confinement for writing a book of major importance... That could mean transforming prison cell into scholar's den." The prisoners were forbidden to write memoirs, and mail was severely limited and censored. However, due to an offer from a sympathetic orderly, Speer was able to have his writings, which eventually amounted to 20,000 sheets, sent to Wolters. By 1954, Speer had completed his memoirs, which became the basis of "Inside the Third Reich", and which Wolters arranged to have transcribed onto 1,100 typewritten pages. He was also able to send letters and financial instructions, and to obtain writing paper and letters from the outside. His many letters to his children, all secretly transmitted, eventually formed the basis for "Spandau: The Secret Diaries". With the draft memoir complete and clandestinely transmitted, Speer sought a new project. He found one while taking his daily exercise, walking in circles around the prison yard. Measuring the path's distance carefully, Speer set out to walk the distance from Berlin to Heidelberg. He then expanded his idea into a worldwide journey, visualizing the places he was "traveling" through while walking the path around the prison yard. Speer ordered guidebooks and other materials about the nations through which he imagined he was passing, so as to envision as accurate a picture as possible. Meticulously calculating every meter traveled, and mapping distances to the real-world geography, he began in northern Germany, passed through Asia by a southern route before entering Siberia, then crossed the Bering Strait and continued southwards, finally ending his sentence 35 kilometers south of Guadalajara, Mexico. Speer devoted much of his time and energy to reading. Though the prisoners brought some books with them in their personal property, Spandau Prison had no library so books were sent from Spandau's municipal library. From 1952 the prisoners were also able to order books from the Berlin central library in Wilmersdorf. Speer was a voracious reader and he completed well over 500 books in the first three years at Spandau alone. He read classic novels, travelogues, books on ancient Egypt, and biographies of such figures as Lucas Cranach, Friedrich Preller, and Genghis Khan. Speer took to the prison garden for enjoyment and work, at first to do something constructive while afflicted with writer's block. He was allowed to build an ambitious garden, transforming what he initially described as a "wilderness" into what the American commander at Spandau described as "Speer's Garden of Eden". Speer's supporters maintained a continual call for his release. Among those who pledged support for Speer's sentence to be commuted were Charles de Gaulle, U.S. diplomat George Ball, former U.S. High Commissioner John J. McCloy, and former Nuremberg prosecutor Hartley Shawcross. Willy Brandt was a strong advocate of Speer's, supporting his release, sending flowers to his daughter on the day of his release, and putting an end to the de-Nazification proceedings against Speer, which could have caused his property to be confiscated. A reduced sentence required the consent of all four of the occupying powers, and the Soviets adamantly opposed any such proposal. Speer served his full sentence, and was released on the stroke of midnight as October 1, 1966 began.
Speer's release from prison was a worldwide media event, as reporters and photographers crowded both the street outside Spandau and the lobby of the Berlin hotel where Speer spent his first hours of freedom in over 20 years. However, Speer said little, reserving most comments for a major interview published in "Der Spiegel" in November 1966, in which he again took personal responsibility for crimes of the Nazi regime. Abandoning plans to return to architecture (two proposed partners died shortly before his release), he revised his Spandau writings into two autobiographical books, and later researched and published a third work, about Himmler and the SS. His books, most notably "Inside the Third Reich" (in German, "Erinnerungen", or "Reminiscences") and "Spandau: The Secret Diaries", provide a unique and personal look into the personalities of the Nazi era, and have become much valued by historians. Speer was aided in shaping the works by Joachim Fest and Wolf Jobst Siedler from the publishing house Ullstein. Following the publication of his bestselling books, he donated a considerable amount of money to Jewish charities. According to Siedler, these donations were as high as 80% of his royalties. Speer kept the donations anonymous, both for fear of rejection, and for fear of being called a hypocrite. As early as 1953, when Wolters strongly objected to Speer referring to Hitler in the memoirs draft as a criminal, Speer had predicted that were the writings to be published, he would lose a "good many friends". This came to pass, as following the publication of "Inside the Third Reich", close friends, such as Wolters and sculptor Arno Breker, distanced themselves from him. Hans Baur, Hitler's personal pilot, suggested, "Speer must have taken leave of his senses." Wolters wondered that Speer did not now "walk through life in a hair shirt, distributing his fortune among the victims of National Socialism, forswear all the vanities and pleasures of life and live on locusts and wild honey". Speer made himself widely available to historians and other enquirers. He did an extensive, in-depth interview for the June 1971 issue of "Playboy" magazine, in which he stated, "If I didn't see it, then it was because I didn't want to see it." In October 1973, Speer made his first trip to Britain, flying to London under an assumed name to be interviewed on the BBC "Midweek" programme by Ludovic Kennedy. Upon arrival, he was detained for almost 8 hours at Heathrow Airport when British immigration authorities discovered his true identity. The Home Secretary, Robert Carr, allowed Speer into the country for 48 hours. While in London eight years later to participate in the BBC "Newsnight" programme, Speer suffered a stroke and died on September 1, 1981. Even to the end of his life, Speer continued to question his actions under Hitler. In his final book, "Infiltration", he asks, "What would have happened if Hitler had asked me to make decisions that required the utmost hardness?... How far would I have gone?... If I had occupied a different position, to what extent would I have ordered atrocities if Hitler had told me to do so?" Speer leaves the questions unanswered.
Little remains of Speer's personal architectural works, other than the plans and photographs. No buildings designed by Speer in the Nazi era remain in Berlin; a double row of lampposts along the Strasse des 17. Juni designed by Speer still stands. The tribune of the "Zeppelinfeld" stadium in Nuremberg, though partly demolished, may also be seen. Speer's work may also be seen in London, where he redesigned the interior of the German Embassy to the United Kingdom, then located at 7–9 Carlton House Terrace. Since 1967, it has served as the offices of the Royal Society. His work there, stripped of its Nazi fixtures and partially covered beneath carpets, survives in part. A perhaps more important legacy was the "Arbeitsstab Wiederaufbau zerstörter Städte" (Working group on Reconstruction of destroyed cities), authorized by Speer in 1943 to rebuild bombed German cities to make them more livable in the age of the automobile. Headed by Wolters, the working group actually took a possible military defeat into their calculations. The "Arbeitsstabs recommendations served as the basis of the postwar redevelopment plans in many cities, and "Arbeitsstab" members became prominent in the rebuilding.
As General Building Inspector, Speer was responsible for the Central Department for Resettlement. From 1939 onwards, the Department used the Nuremberg Laws to evict Jewish tenants of non-Jewish landlords in Berlin, to make way for non-Jewish tenants displaced by redevelopment or bombing. Eventually, 75,000 Jews were displaced by these measures. Speer was aware of these activities, and inquired as to their progress. At least one original memo from Speer so inquiring still exists, as does the "Chronicle" of the Department's activities, kept by Wolters. Following his release from Spandau, Speer presented to the German Federal Archives an edited version of the "Chronicle", stripped by Wolters of any mention of the Jews. When David Irving discovered discrepancies between the edited "Chronicle" and other documents, Wolters explained the situation to Speer, who responded by suggesting to Wolters that the relevant pages of the original "Chronicle" should "cease to exist". Wolters did not destroy the "Chronicle", and, as his friendship with Speer deteriorated, allowed access to the original "Chronicle" to doctoral student Matthias Schmidt (who, after obtaining his doctorate, developed his thesis into a book, "Albert Speer: The End of a Myth"). Speer considered Wolters' actions to be a "betrayal" and a "stab in the back". The original "Chronicle" reached the Archives in 1983, after both Speer and Wolters had died.
These seconds [when Hanke told Speer this, and Speer did not inquire] were uppermost in my mind when I stated to the international court at the Nuremberg Trial that, as an important member of the leadership of the Reich, I had to share the total responsibility for all that had happened. For from that moment on I was inescapably contaminated morally; from fear of discovering something which might have made me turn from my course, I had closed my eyes... Because I failed at that time, I still feel, to this day, responsible for Auschwitz in a wholly personal sense. Much of the controversy over Speer's knowledge of the Holocaust has centered on his presence at the Posen Conference on October 6, 1943, at which Himmler gave a speech detailing the ongoing Holocaust to Nazi leaders. Himmler said, "The grave decision had to be taken to cause this people to vanish from the earth... In the lands we occupy, the Jewish question will be dealt with by the end of the year." Speer is mentioned several times in the speech, and Himmler seems to address him directly. In "Inside the Third Reich", Speer mentions his own address to the officials (which took place earlier in the day) but does not mention Himmler's speech. In 1971, American historian Erich Goldhagen published an article arguing that Speer was present for Himmler's speech. According to Fest in his biography of Speer, "Goldhagen's accusation certainly would have been more convincing" had he not placed supposed incriminating statements linking Speer with the Holocaust in quotation marks, attributed to Himmler, which were in fact invented by Goldhagen. In response, after considerable research in the German Federal Archives in Koblenz, Speer said he had left Posen around noon (long before Himmler's speech) in order to journey to Hitler's headquarters at Rastenburg. In "Inside the Third Reich", published before the Goldhagen article, Speer recalled that on the evening after the conference, many Nazi officials were so drunk that they needed help boarding the special train which was to take them to a meeting with Hitler. One of his biographers, Dan van der Vat, suggests this necessarily implies he must have still been present at Posen then, and must have heard Himmler's speech. In response to Goldhagen's article, Speer had alleged that in writing "Inside the Third Reich", he erred in reporting an incident that happened at another conference at Posen a year later, as happening in 1943. In 2005, British newspaper "The Daily Telegraph" reported that documents had surfaced indicating that Speer had approved the allocation of materials for the expansion of Auschwitz after two of his assistants toured the facility on a day when almost a thousand Jews were murdered. The documents supposedly bore annotations in Speer's own handwriting. Speer biographer Gitta Sereny stated that, due to his massive workload, Speer would not have been personally aware of such activities. The debate over Speer's knowledge of, or complicity in, the Holocaust made him a symbol for people who were involved with the Nazi regime yet did not have (or claimed not to have had) an active part in the regime's atrocities. As film director Heinrich Breloer remarked, "[Speer created] a market for people who said, 'Believe me, I didn't know anything about [the Holocaust]. Just look at the "Führer's" friend, he didn't know about it either.'"
Alliaceae is a family of herbaceous perennial flowering plants. They are monocots, part of order Asparagales. The family has been widely but not universally recognised; in the past, the plants involved were often treated as belonging to the family Liliaceae, and still are by some botanists. Note that quite a few of the plants that were once included in family Alliaceae are assigned to the family Themidaceae by both APG and APG II. The most important genus is "Allium", which includes several important food plants, including onions ("Allium cepa"), chives ("A. schoenoprasum"), garlic ("A. sativum" and "A. scordoprasum"), and leeks ("A. porrum").
The Asteraceae or Compositae, the aster, daisy, or sunflower family, is the second largest family of flowering plants, in terms of number of species. The name "Asteraceae" is derived from the type genus "Aster", while Compositae, an older but still valid name, means "composite" and refers to the characteristic inflorescence, a special type of pseudanthium found in only a few other angiosperm families. The study of this family is known as synantherology. According to the Royal Botanic Gardens of Kew, the family comprises more than 1,600 genera and 23,000 species. The largest genera are "Senecio" (1,500 species), "Vernonia" (1,000 species), "Cousinia" (600 species) and "Centaurea" (600 species). The circumscription of the genera is often problematic and some of these have been frequently divided into minor subgroups. Asteraceae are cosmopolitan, but are most common in temperate regions and tropical mountains.
The family has been universally recognised and placed in the order "Asterales". Traditionally two subfamilies were recognised: "Asteroideae" (or "Tubuliflorae") and "Cichorioideae" (or "Liguliflorae"). The latter is paraphyletic and has been divided into many minor groups in most newer systems. The phylogenetic tree presented below is based on Panero & Funk (2002) and also shown in the APG system. A tentative cladogram is shown below. The diamond denotes a very poorly supported branching (<50%), the dot a poorly supported branching (<80%). It is noteworthy that the four subfamilies Asteroideae, Cichorioideae, Carduoideae and Mutisioideae comprise 99% of the specific diversity of the whole family (appr. 70%, 14%, 11% and 3% respectively). Other subfamilies have been recognised by some authors, e.g. Helianthoideae.
The most evident characteristic of Asteraceae is perhaps their inflorescence: a specialised "capitulum", technically called a "calathid" or "calathidium", but generally referred to as "flower head" or, alternatively, simply "capitulum". The "capitulum" is a contracted raceme composed of numerous individual sessile flowers, called the "florets", all sharing the same receptacle. The capitulum of the Asteraceae has evolved many characteristics that make it look superficially like a big single flower. This kind of flower-like inflorescences are quite widespread amongst plants and have been given the name of "pseudanthia". Many bracts form an involucre under the basis of the capitulum; these are called "phyllaries", or "involucral bracts". They may simulate the sepals of the pseudanthium. These are mostly herbaceous but can also be brightly coloured (e.g. "Helichrysum") or have a scarious texture. The bracts can be free or fused, and arranged in one to many rows, overlapping like the tiles of a roof ("imbricate") or not (this variation is important in identification of tribes and genera). Each floret may itself be subtended by a bract, called a "palea" or "receptacular bract". These bracts as a group are often called "chaff". The presence or absence of these bracts, their distribution on the receptacle, and their size and shape are all important diagnostic characteristics for genera and tribes. The florets have five petals fused at the base to form a corolla tube and they may be either actinomorphic or zygomorphic. "Disc florets" are usually actinomorphic, with five petal lips on the rim of the corolla tube. The petal lips may be either very short, or long, in which case they form deeply lobed petals. The latter is the only kind of floret in the Carduoideae, while the first kind is more widespread. "Ray florets" are always highly zygomorphic and are characterised by the presence of a "ligule", a strap-shaped structure on the edge of the corolla tube consisting of fused petals. In the Asteroideae and other minor subfamilies these are usually borne only on florets at the circumference of the capitulum and have a 3+2 scheme – above the fused corolla tube, three very long fused petals form the ligule, with the other two petals being inconspicuously small. The Cichorioidea has only ray florets, with a 5+0 scheme – all five petals form the ligule. A 4+1 scheme is found in the Barnadesioideae. The tip of the ligule is often divided into teeth, each one representing a petal. Some marginal florets may have no petals at all (filiform floret). The calyx of the florets may be absent, but when present it is always modified into a pappus of two or more teeth, scales or bristles and this is often involved in the dispersion of the seeds. As with the bracts, the nature of the pappus is an important diagnostic feature. There are usually five stamens. The filaments are fused to the corolla, while the anthers are generally connate ("syngenesious" anthers), thus forming a sort of tube around the style ("theca"). They commonly have basal and/or apical appendages. Pollen is released inside the tube and is collected around the growing style, expelled with a sort of pump mechanism ("nüdelspritze") or a brush. The pistil is made of two connate carpels. The style has two lobes; stigmatic tissue may be located in the interior surface or form two lateral lines. The ovary is inferior and has only one ovule, with basal placentation.
The fruit of the Asteraceae is achene-like, and is called a cypsela (plural "cypselae"). Although there are two fused carpels, there is only one locule, and only one seed per fruit is formed. It may sometimes be winged or spiny because the pappus, which is derived from calyx tissue often remains on the fruit (for example in dandelion). In some species, however, the pappus falls off (for example in "Helianthus"). Cypsela morphology is often used to help determine plant relationships at the genus and species level. The mature seeds usually have little endosperm or none.
Asteraceae are especially common in open and dry environments. Many members of the Asteraceae are pollinated by insects, which explains their value in attracting beneficial insects, but anemophyly is also present (e.g. "Ambrosia", "Artemisia"). There are many apomictic species in the family. Seeds are ordinarily dispersed intact with the fruiting body, the cypsela. Wind dispersal is common ("anemochory") assisted by a hairy pappus. Another common variation is "epizoochory", in which the dispersal unit, a single cypsela (e.g. "Bidens") or entire capitulum (e.g. "Arctium") provided with hooks, spines or some equivalent structure, sticks to the fur or plumage of an animal (or even to clothes, like in the photo) just to fall off later far from its mother plant.
Commercially important plants in the Asteraceae include the food crops "Lactuca sativa" (lettuce), "Cichorium" (chicory), "Cynara scolymus" (globe artichoke), "Helianthus annuus" (sunflower), "Smallanthus sonchifolius" (yacón), "Carthamus tinctorius" (safflower) and "Helianthus tuberosus" (Jerusalem artichoke). Other commercially important species include Compositae used as herbs and in herbal teas and other beverages. Chamomile, which comes from two different species, the annual "Matricaria recutita" or German chamomile, and the perennial "Chamaemelum nobile", also called Roman chamomile. "Calendula", also called the pot marigold is grown commercially for herbal teas and the potpourri industry. Echinacea ("Echinacea purpurea"), used as a medicinal tea. Winter tarragon, also called Mexican mint marigold, "Tagetes lucida" is commonly grown and used as a tarragon substitute in climates where tarragon will not survive. Finally, the wormwood genus "Artemisia" includes absinthe ("A. absinthium") and tarragon ("A. dracunculus"). Industrial use of Compositae is also known. Common in all commercial poultry feed, marigold ("Tagetes patula") is grown primarily in Mexico. Marigold oil, extracted from "Tagetes minuta" is used by the metric ton in the cola and cigarette industry. Plants in Asteraceae are medically important in areas that don't have access to Western medicine. They are also commonly featured in medical and phytochemical journals because the sesquiterpene lactone compounds contained within them are an important cause of allergic contact dermatitis. Allergy to these compounds is the leading cause of allergic contact dermatitis in florists in the US. Pollen from ragweed "Ambrosia" is among the main causes of so called hay fever in the United States. Many members of the family are grown as ornamental plants for their flowers and some are important ornamental crops for the cut flower industry. Some examples are "Chrysanthemum", "Gerbera", "Calendula", "Dendranthema", "Argyranthemum", "Dahlia", "Tagetes", "Zinnia" and many others. Many members of Asteraceae are copious nectar producers and are useful for evaluating pollinator populations during their bloom. "Centaurea" (knapweed), "Helianthus annuus" (domestic sunflower), and some species of "Solidago" (goldenrod) are major "honey plants" for beekeepers. "Solidago" produces relatively high protein pollen, which helps honey bees overwinter. Some members of the Asteraceae are economically important as weeds. Notably in the United States are the ragwort, "Senecio jacobaea", groundsel "Senecio vulgaris" and "Taraxacum" (dandelion). The genera "Tanacetum", "Chrysanthemum" and "Pulicaria" contain species with insecticidal properties. "Parthenium argentatum" (guayule) is a source of hypoallergenic latex.
The Apiaceae or Umbelliferae (both names are allowed by the ICBN) is a family of usually aromatic plants with hollow stems, commonly known as umbellifers. It includes cumin, parsley, anise, carrot, coriander/cilantro, dill, caraway, fennel, parsnip, celery, Queen Anne's Lace and other relatives. It is a large family with about 300 genera and more than 3,000 species. The earlier name Umbelliferae derives from the inflorescence being generally in the form of a compound "umbel", and has the same root as the word "umbrella". The botanical subspeciality that studies Apiaceae is sometimes called "sciadophytography".
The small flowers are radially symmetrical with 5 small sepals, 5 petals and 5 stamens. The family includes some highly toxic plants, such as hemlock. Many plants in this family, such as wild carrot, have estrogenic properties and have been used as folk medicine for birth control. Most notable for this use is the extinct giant fennel, silphium. The cultivated plants in this category are almost all considered good companion plants, as the umbrella of tiny flowers attracts omnivorous beneficial insects, especially ladybugs, parasitic wasps and predatory flies, which then will hunt insect pests on nearby crops. The family is closely related to Araliaceae and the boundaries between these families remain unclear. Some recent systems include Araliaceae in an expanded Apiaceae but this has not been widely followed. "Hydrocotyle" and "Trachymene", traditionally included in Apiaceae, are now generally included in Araliaceae. Notable members include "Anethum graveolens" - Dill, "Anthriscus cerefolium" - Chervil, "Angelica" spp. - Angelica, "Apium graveolens" - Celery, "Arracacia xanthorrhiza" - Arracacha, "Carum carvi" - Caraway, "Centella asiatica" - Gotu Kola (pennywort), "Conium maculatum" - Poison hemlock, "Coriandrum sativum" - Coriander, "Cuminum cyminum" - Cumin, "Daucus carota" - Carrot, "Eryngium" spp. - Sea holly, "Foeniculum vulgare" - Fennel, "Myrrhis odorata" - Cicely, "Ferula gummosa" - galbanum, "Pastinaca sativa" - Parsnip, "Petroselinum crispum" - Parsley, "Pimpinella anisum" - Anise, "Levisticum officinale" - Lovage
Many members of this plant group are cultivated, for various purposes. The plant structure includes a tap root, which on more than one occasion has been bred to grow large enough to be useful in food, as with parsnips, carrots, and hamburg root parsley. Plants of this category also are adapted to conditions that encourage heavy concentrations of essential oils, so that some are used as flavorful or aromatic herbs, such as parsley, cilantro, and dill. The plentiful seeds of the umbers, likewise, are sometimes used in cuisine, as with coriander, fennel, cumin, and caraway.
Almost every widely cultivated plant of this group is a companion plant. In large part, this is because the tiny flowers forming the umbels, for which the group is named, are perfectly suited for ladybugs, parasitic wasps, and predatory flies, which actually drink nectar when not reproducing. They then will prey upon insect pests on nearby plants. Some of the plants, too, are herbs that produce enough scent to possibly dilute the odors of nearby plants, or the pheromones emitted by insects that find those plants, which would otherwise attract more pests.
An axon or nerve fiber is a long, slender projection of a nerve cell, or neuron, that conducts electrical impulses away from the neuron's cell body or soma. An axon is one of two types of protoplasmic protrusions that extrude from the cell body of a neuron, the other type being dendrites. Axons are distinguished from dendrites by several features, including shape (dendrites often taper while axons usually maintain a constant radius), length (dendrites are restricted to a small region around the cell body while axons can be much longer), and function (dendrites usually receive signals while axons usually transmit them). All of these rules have exceptions, however. Some types of neurons have no axon—these are called amacrine cells, and transmit signals from their dendrites. No neuron ever has more than one axon; however in invertebrates such as insects the axon sometimes consists of several regions that function more or less independently of each other. Most axons branch, in some cases very profusely. Axons make contact with other cells—usually other neurons but sometimes muscle or gland cells—at junctions called synapses. At a synapse, the membrane of the axon closely adjoins the membrane of the target cell, and special molecular structures serve to transmit electrical or electrochemical signals across the gap. Some synaptic junctions appear partway along an axon as it extends—these are called "en passant" ("in passing") synapses. Other synapses appear as terminals at the ends of axonal branches. A single axon, with all its branches taken together, can innervate multiple parts of the brain and generate thousands of synaptic terminals.
Axons are in effect the primary transmission lines of the nervous system, and as bundles they help make up nerves. Individual axons are microscopic in diameter (typically about 1μm across), but may be up to several feet in length. The longest axons in the human body, for example, are those of the sciatic nerve, which run from the base of the spine to the big toe of each foot. These single-cell fibers of the sciatic nerve may extend a meter or even longer. In vertebrates, the axons of many neurons are sheathed in myelin, which is formed by either of two types of glial cells: Schwann cells ensheathing peripheral neurons and oligodendrocytes insulating those of the central nervous system. Along myelinated nerve fibers, gaps in the sheath known as nodes of Ranvier occur at evenly-spaced intervals. The myelination enables an especially rapid mode of electrical impulse propagation called saltation. The demyelination of axons is what causes the multitude of neurological symptoms found in the disease Multiple Sclerosis. The axons of some neurons branch to form axon collaterals, that can be divided into a number of smaller branches called telodendria. Along these the bifurcated impulse travels simultaneously to signal more than one other cell.
The physiology can be described by the Hodgkin-Huxley Model, extended to vertebrates in Frankenhaeuser-Huxley equations. Peripheral nerve fibers can be classified based on axonal conduction velocity, mylenation, fiber size etc. For example, there are slow-conducting unmyelinated C fibers and faster-conducting myelinated Aδ fibers. More complex mathematical modeling continues to be done today. There are several types of sensory- as well as motorfibers. Other fibers not mentioned in table are e.g. fibers of the autonomic nervous system
Growing axons move through their environment via the growth cone, which is at the tip of the axon. The growth cone has a broad sheet like extension called lamellipodia which contain protrusions called filopodia. The filopodia are the mechanism by which the entire process adheres to surfaces and explores the surrounding environment. Actin plays a major role in the mobility of this system. Environments with high levels of cell adhesion molecules or CAM's create an ideal environment for axonal growth. This seems to provide a "sticky" surface for axons to grow along. Examples of CAM's specific to neural systems include N-CAM, neuroglial CAM or NgCAM, TAG-1, MAG, and DCC, all of which are part of the immunoglobulin superfamily. Another set of molecules called extracellular matrix adhesion molecules also provide a sticky substrate for axons to grow along. Examples of these molecules include laminin, fibronectin, tenascin, and perlecan. Some of these are surface bound to cells and thus act as short range attractants or repellents. Others are difusible ligands and thus can have long range effects. Cells called guidepost cells assist in the guidance of neuronal axon growth. These cells are typically other, sometimes immature, neurons.
Some of the first intracellular recordings in a nervous system were made in the late 1930s by K. Cole and H. Curtis. Alan Hodgkin and Andrew Huxley also employed the squid giant axon (1939) and by 1952 they had obtained a full quantitative description of the ionic basis of the action potential, leading the formulation of the Hodgkin-Huxley Model. Hodgkin and Huxley were awarded jointly the Nobel Prize for this work in 1963. The formulas detailing axonal conductance were extended to vertebrates in the Frankenhaeuser-Huxley equations. Erlanger and Gasser earlier developed the classification system for peripheral nerve fibers, based on axonal conduction velocity, myelination, fiber size etc. Even recently our understanding of the biochemical basis for action potential propagation has advanced, and now includes many details about individual ion channels.
The Aramaic alphabet is adapted from the Phoenician alphabet, and became distinctive from it by the 8th century BCE. The letters all represent consonants, some of which are "matres lectionis", which also indicate long vowels. The Aramaic alphabet is historically significant since virtually all modern Middle Eastern writing systems use a script that can be traced back to it, as well as numerous Altaic languages of Central and East Asia. This is primarily due to the widespread usage of the Aramaic language as both a lingua franca and the official language of the Neo-Assyrian, and its successor, the Achaemenid Empire. Among the scripts in modern use, the Hebrew alphabet bears the closest relation to the Imperial Aramaic script of the 5th century BCE, with an identical letter inventory and for the most part nearly identical letter shapes. Writing systems that, like the Aramaic one, indicate consonants but do not indicate most vowels, or indicate them with added diacritical signs, have been called abjads by Peter T. Daniels, to distinguish them from later alphabets like Greek that represent vowels more systematically. This is to avoid the notion that a writing system that represents sounds must be either a syllabary or an alphabet, which implies that a system like Aramaic must be either a syllabary (as argued by Gelb) or an incomplete or deficient alphabet (as most other writers have said); rather, it is a different type.
Around 500 BCE, following the Achaemenid conquest of Mesopotamia under Darius I, Old Aramaic was adopted by the conquerors as the "vehicle for written communication between the different regions of the vast empire with its different peoples and languages. The use of a single official language, which modern scholarship has dubbed Official Aramaic or Imperial Aramaic, can be assumed to have greatly contributed to the astonishing success of the Achaemenids in holding their far-flung empire together for as long as they did". Imperial Aramaic was highly standardised; its orthography was based more on historical roots than any spoken dialect, and was inevitably influenced by Old Persian. For centuries after the fall of the Achaemenid Empire in 331 BCE, Imperial Aramaic or near enough for it to be recognisable would remain an influence on the various native Iranian languages. The Aramaic script would survive as the essential characteristics of the Pahlavi writing system. A group of thirty Aramaic documents from Bactria have been recently discovered. An analysis was published in November 2006. The texts, which were rendered on leather, reflect the use of Aramaic in the fourth century BCE Achaemenid administration of Bactria and Sogdiana. Its widespread usage led to the gradual adoption of the Aramaic alphabet for writing the Hebrew language. Formerly, Hebrew had been written using an alphabet closer in form to that of Phoenician (the Paleo-Hebrew alphabet).
Since the evolution of the Aramaic alphabet out of the Phoenician one was a gradual process, the division of the world's alphabets into those derived from the Phoenician one directly and those derived from Phoenician via Aramaic is somewhat artificial. In general, the alphabets of the Mediterranean region (Anatolia, Greece, Italy) are classified as Phoenician-derived, adapted from around the 8th century BCE, while those of the east (the Levant, Persia, Central Asia and India) are considered Aramaic-derived, adapted from around the 6th century BCE from the "Imperial Aramaic" script of the Achaemenid Empire. After the fall of the Achaemenid Empire, the unity of the Imperial Aramaic script was lost, diversifying into a number of descendant cursives. The Hebrew and Nabataean alphabets as they stood by the Roman era were little changed in style from the Imperial Aramaic alphabet. A Cursive Hebrew variant developed from the early centuries AD, but it remained restricted to the status of a variant used alongside the non-cursive. By contrast, the cursive developed out of the Nabataean alphabet in the same period soon became the standard for writing Arabic, evolving into the Arabic alphabet as it stood by the time of the early spread of Islam. The development of cursive versions of Aramaic also led to the creation of the Syriac, Palmyrenean and Mandaic alphabets. These scripts formed the basis of the historical scripts of Central Asia, such as the Sogdian and Mongolian alphabets. The Indian Kharosthi and Brahmi scripts, and by extension the Brahmic family of scripts, are also considered derivations from the Aramaic script. The Old Turkic script evident in epigraphy from the 8th century likely also has its origins in the Aramaic script, possibly via Karosthi.
Today, Biblical Aramaic, Jewish Neo-Aramaic dialects and the Aramaic language of the Talmud are written in the Hebrew alphabet. Syriac and Christian Neo-Aramaic dialects are written in the Syriac alphabet. Mandaic is written in the Mandaic alphabet. Due to the near-identity of the Aramaic and the classical Hebrew alphabets, Aramaic text is mostly typeset in standard Hebrew script in scholarly literature. Consequently, Unicode as of version 5.1 (2008) does not consider Aramaic an alphabet separate from the Hebrew one.
The letters Waw and Yudh, put following the consonants that were followed by the vowels "u" and "i" (and often also "o" and "e"), used to indicate the long vowels "û" and "î" respectively (often also "ô" and "ê" respectively). These letters, which stand for both consonant and vowel sounds, are known as "matres lectionis". The letter Alaph, likewise, had some of the characteristics of a mater lectionis: in initial positions, it indicated a specific consonant called "glottal stop" (followed by a vowel), and in the middle of the word and word finally it often also stood for the long vowels "â" or "ê". Among Jews, influence of Hebrew spelling often led to the use of He instead of Alaph in word final positions. The practice of using certain letters to hold vowel values spread to child writing systems of Aramaic, such as Hebrew and Arabic, where they are still used today.
"American shot'" is a translation of a phrase from French film criticism, "plan américain" and refers to a medium-long ("knee") film shot of a group of characters, who are arranged so that all are visible to the camera. The usual arrangement is for the actors to stand in an irregular line from one side of the screen to the other, with the actors at the end coming forward a little and standing more in profile than the others. The purpose of the composition is to allow complex dialogue scenes to be played out without changes in camera position. In some literature, this is simply referred to as a 3/4 shot. The French critics thought it was characteristic of American films of the 1930s or 1940s; however, it was mostly characteristic of "cheaper" American movies, such as Charlie Chan mysteries where people collected in front of a fireplace or at the foot of the stairs in order to explain what happened a few minutes ago. Howard Hawks legitimized this style in his films, allowing characters to act, even when not talking, when most of the audience would not be paying attention. It became his trademark style.
Acute disseminated encephalomyelitis (ADEM) is an immune mediated disease of the brain. It usually occurs following a viral infection but may appear following vaccination, bacterial or parasitic infection, or even appear spontaneously. As it involves autoimmune demyelination, it is similar to multiple sclerosis, and is considered part of the Multiple sclerosis borderline diseases. The incidence rate is about 8 per 1,000,000 people per year.. Although it occurs in all ages, most reported cases are in children and adolescents, with the average age around 5 to 8 years old. The mortality rate may be as high as 5%, full recovery is seen in 50 to 75% of cases, while up to 70 to 90% recover with some minor residual disability. The average time to recover is one to six months. ADEM produces multiple inflammatory lesions in the brain and spinal cord, particularly in the white matter. Usually these are found in the subcortical and central white matter and cortical gray-white junction of both cerebral hemispheres, cerebellum, brainstem, and spinal cord, but periventricular white matter and gray matter of the cortex, thalami and basal ganglia may also be involved. When the patient suffers more than one demyelinating episode, it is called Recurrent disseminated encephalomyelitis or Multiphasic disseminated encephalomyelitis(MDEM).
Viral infections thought to induce ADEM include influenza virus, enterovirus, measles, mumps, rubella, varicella zoster, Epstein Barr virus, cytomegalovirus, herpes simplex virus, hepatitis A, and coxsackievirus; while the bacterial infections include Mycoplasma pneumoniae, Borrelia burgdorferi, Leptospira, and beta-hemolytic Streptococci. The only vaccine proven to induce ADEM is the Semple form of the rabies vaccine, but hepatitis B, pertussis, diphtheria, measles, mumps, rubella, pneumococcus, varicella, influenza, Japanese encephalitis, and polio vaccines have all been implicated. In rare cases, ADEM seems to follow from organ transplantation. The risk of ADEM from measles vaccination is about 1 to 2 per million, which is far lower than the risk of developing ADEM from an actual measles infection, which is about 1 per 1000 for measles (and 1 per 5000 for rubella). Measles infection also appears to lead to worse ADEM outcomes than cases associated with measles immunization. Some vaccines, later shown to have been contaminated with host animal CNS tissue, have ADEM incident rates as high as 1 in 600.
No controlled clinical trials have been conducted on ADEM treatment, but aggressive treatment aimed at rapidly reducing inflammation of the CNS is standard. The widely accepted first-line treatment is high doses of intravenous corticosteroids, such as methylprednisolone or dexamethasone, followed by 3–6 weeks of gradually lower oral doses of prednisolone. Patients treated with methylprednisolone have shown better outcomes than those treated with dexamethasone. Oral tapers of less than three weeks duration show a higher chance of relapsing, and tend to show poorer outcomes. Other antiinflamatory and immunosuppressive therapies have been reported to show beneficial effect, such as plasmapheresis, high doses of intravenous immunoglobulin (IVIg), mitoxantrone and cyclophosphamide. These are considered alternative therapies, used when corticosteroids cannot be used, or fail to show an effect. There is some evidence to suggest that patients may respond to a combination of methylprednisolone and immunoglobulins if they fail to respond to either separately In a study of 16 children with ADEM, 10 recovered completely after high-dose methylprednisolone, one severe case that failed to respond to steroids recovered completely after IVIg; the five most severe cases -with ADAM and severe peripheral neuropathy- were treated with combined high-dose methylprednisolone and immunoglobulin, two remained paraplegic, one had motor and cognitive handicaps, and two recovered. A recent review of IVIg treatment of ADEM (of which the previous study formed the bulk of the cases) found that 70% of children showed complete recovery after treatment with IVIg, or IVIg plus corticosteroids. A study of IVIg treatment in adults with ADEM showed that IVIg seems more effective in treating sensory and motor disturbances, while steroids seem more effective in treating impairments of cognition, consciousness and rigor. This same study found one subject, a 71 year old man who had not responded to steroids, that responded to a IVIg treatment 58 days after disease onset.
Full recovery is seen in 50 to 75% of cases, ranging to 70 to 90% recovery with some minor residual disability (typically assessed using measures such as mRS or EDSS), average time to recover is one to six months. The mortality rate may be as high as 5%. Poorer outcomes are associated with unresponsiveness to steroid therapy, unusually severe neurological symptoms, or sudden onset. Children tend to have more favorable outcomes than adults, and cases presenting without fevers tend to have poorer outcomes. The latter effect may be due to either protective effects of fever, or that diagnosis and treatment is sought more rapidly when fever is present.
Patients with demyelinating illnesses, such as MS, have shown cognitive deficits even when there is minimal physical disability. Research suggests that similar effects are seen after ADEM, but that the deficits are less severe than those seen in MS. A study of six children with ADEM (mean age at presentation 7.7 years) were tested for a range of neurocognitive tests after an average of 3.5 years of recovery. All six children performed in the normal range on most tests, including verbal IQ and performance IQ, but performed at least one standard deviation below age norms in at least one cognitive domain, such as complex attention (one child), short-term memory (one child) and internalizing behaviour/affect (two children). Group means for each cognitive domain were all within one standard deviation of age norms, demonstrating that, as a group, they were normal. These deficits were less severe than those seen in similar aged children with a diagnosis of MS. Another study compared nineteen children with a history of ADEM, of which 10 were five years of age or younger at the time (average age 3.8 years old, tested an average of 3.9 years later) and nine were older (mean age 7.7y at time of ADEM, tested an average of 2.2 years later) to nineteen matched controls. Scores on IQ tests and educational achievement were lower for the young onset ADEM group (average IQ 90) compared to the late onset (average IQ 100) and control groups (average IQ 106), while the late onset ADEM children scored lower on verbal processing speed. Again, all groups means were within one standard deviation of the controls, meaning that while effects were statistically reliable, the children were as a whole, still within the normal range. There were also more behavioural problems in the early onset group, although there is some suggestion that this may be due, at least in part, to the stress of hospitalization at a young age.
Acute hemorrhagic leukoencephalitis (AHL, or AHLE), also known as acute necrotizing encephalopathy (ANE), acute hemorrhagic encephalomyelitis (AHEM), acute necrotizing hemorrhagic leukoencephalitis (ANHLE), Weston-Hurst syndrome, or Hurst's disease, is a hyperacute and frequently fatal form of ADEM. AHL is relatively rare (less than 100 cases have been reported in the medical literature as of 2006), it is seen in about 2% of ADEM cases, and is characterized by necrotizing vasculitis of venules and hemorrhage, and edema. Death is common in the first week and overall mortality is about 70%, but increasing evidence points to favorable outcomes after aggressive treatment with corticosteroids, immunoglobulins, cyclophosphamide, and plasma exchange. About 70% of survivors show residual neurological deficits, but some survivors have shown surprisingly little deficit considering the magnitude of the white matter affected. This disease has been occasionally associated with ulcerative colitis and Crohn's disease, septicemia associated with immune complex deposition, methanol poisoining and other underlying conditions.
Ataxia (from Greek "α-" [used as a negative prefix] + "-τάξις" [order], meaning "lack of order") is a neurological sign and symptom consisting of gross lack of coordination of muscle movements. Ataxia is a non-specific clinical manifestation implying dysfunction of parts of the nervous system that coordinate movement, such as the cerebellum. Several possible causes exist for these patterns of neurological dysfunction. The term "dystaxia" is rarely used as a synonym. The International Ataxia Awareness Day is observed on September 25 each year.
The term sensory ataxia is employed to indicate ataxia due to loss of proprioception (sensitivity to joint and body part position), which generally depends on dysfunction of the dorsal columns of the spinal cord, since they carry proprioceptive information up to the brain; in some cases, the cause may instead be dysfunction of the various brain parts that receive that information, including the cerebellum, thalamus, and parietal lobes. Sensory ataxia presents with an unsteady "stomping" gait with heavy heel strikes, as well as postural instability that is characteristically worsened when the lack of proprioceptive input cannot be compensated by visual input, such as in poorly lit environments. Doctors can evidence this during physical examination by having the patient stand with his / her feet together and eyes shut, which will cause the patient's instability to markedly worsen, producing wide oscillations and possibly a fall (this is called a positive Romberg's test). Worsening of the finger-pointing test with the eyes closed is another feature of sensory ataxia. Also, when the patient is standing with arms and hands extended toward the examiner, if the eyes are closed, the patient's finger will tend to "fall down" and be restored to the horizontal extended position by sudden extensor contractions ("ataxic hand").
Exogenous substances that cause ataxia mainly do so because they have a depressant effect on central nervous system function. The most common example is ethanol, which is capable of causing reversible cerebellar and vestibular ataxia. Other examples include various prescription drugs (e.g. most antiepileptic drugs have cerebellar ataxia as a possible adverse effect), marijuana ingestion and various other recreational drugs (e.g. ketamine, PCP or dextromethorphan, all of which are NMDA receptor antagonists that produce a dissociative state at high doses).
Vitamin B12 deficiency may cause, among several neurological abnormalities, overlapping cerebellar and sensory ataxia. Causes of isolated sensory ataxia. Peripheral neuropathies may cause generalised or localised sensory ataxia (e.g. a limb only) depending on the extent of the neuropathic involvement. Spinal disorders of various types may cause sensory ataxia from the lesioned level below, when they involve the dorsal columns.
Ataxia may depend on hereditary disorders consisting of degeneration of the cerebellum and/or of the spine; most cases feature both to some extent, and therefore present with overlapping cerebellar and sensory ataxia, even though one is often more evident than the other. Hereditary disorders causing ataxia include autosomal dominant ones such as spinocerebellar ataxia, episodic ataxia, and dentatorubropallidoluysian atrophy, as well as autosomal recessive disorders such as Friedreich's ataxia (sensory and cerebellar, with the former predominating) and Niemann Pick disease, ataxia-telangiectasia (sensory and cerebellar, with the latter predominating), and abetalipoproteinaemia. An example of X-linked ataxic condition is the rare ataxia syndrome.
There is no specific treatment for ataxia, although the consequent disability may also be reduced by physical therapy, including exercises, along with leg braces or shoe splints, if foot alignment has been affected; a cane or walker is often used in the effort to prevent falls. Severe cases may require the use of a wheelchair. Other uses of the term. The term "ataxia" is sometimes used in a broader sense to indicate lack of coordination in some physiological process. Examples include optic ataxia (lack of coordination between visual inputs and hand movements, resulting in inability to reach and grab objects, usually part of Balint's syndrome, but can be seen in isolation with injuries to the superior parietal lobule, as it represents a disconnection between visual-association cortex and the frontal premotor and motor cortex), and ataxic respiration (lack of coordination in respiratory movements, usually due to dysfunction of the respiratory centres in the medulla oblongata).
Abdul Alhazred is a fictional character created by American horror writer H. P. Lovecraft. He is the so-called "Mad Arab" credited with authoring the imaginary book "Kitab al-Azif" (the "Necronomicon"), and as such an integral part of Cthulhu Mythos lore. "There are those that believe that one or more of the published hoaxes are in fact actually the work of Lovecraft's creation, Abdul Alhazred. Some controversy has developed even among those who realize that these books are modern fabrications, as there are claims that the rituals and spells described in these books produce actual results."
The name "Abdul Alhazred" is a pseudonym that Lovecraft created in his youth, which he took on after reading "1001 Arabian Nights" at the age of about five years. The name was invented either by Lovecraft, or by Albert Baker, the Phillips' family lawyer. "Abdul" is a common Arabic name component (but never a name by itself; additionally the ending -ul and the beginning Al- are redundant), but "Alhazred" may allude to "Hazard", a name from Lovecraft's family tree. It might also have been a pun on "all-has-read", since Lovecraft was an avid reader in youth. "Abdul Alhazred" is not a real Arabic name, and seems to contain the Arabic definite article morpheme "al-" twice in a row (anomalous in terms of Arabic grammar). The more proper Arabic form might be "Abd-al-Hazred" or "Abdul Hazred". In Arabic translations, his name has appeared as "Abdullah Alared" (عبدالله الحظرد): Arabic 'حظر = "he fenced in", "he prohibited". Hazred could come from the Persian or Arabic word "Hazrat" meaning Great Lord with a twist that makes it sound like "red" and "hazard" both indicative of danger. However Abdul is a common Arabic prefix meaning "Servant of the" and "Al" is Arabic for "the", and if "hazra" means "he prohibited", "he fenced in" or "Great Lord", then the name would mean "Servant of the Prohibited", "Servant of the Fenced in", or "Servant of the Great Lord" which would make sense considering his role, even if it is not a proper Arabic name. Similarly, an article (written from an in-universe perspective) in the "Call of Cthulhu" RPG speculates that it may be a corruption of "Abd Al-Azrad", which it claims translates to "The Worshipper of the Great Devourer". The phrase "mad Arab", sometimes with both words capitalized in Lovecraft's stories, is used so commonly before Alhazred's name that it almost constitutes a title. A reference to the "Mad Arab" in Cthulhu Mythos fiction is invariably a synonym for Abdul Alhazred.
August Derleth later made alterations to the biography of Alhazred, such as redating his death to 731. Derleth also changed Alhazred's final fate, as described in his short story "The Keeper of the Key", first published in May 1951. In the story, Professor Laban Shrewsbury (a recurring Derleth character) and his assistant at the time, Nayland Colum, discover Alhazred's burial site. While the two are heading a caravan from Salalah, Oman, they cross the border into Yemen and find the unexplored desert area that the "Necronomicon" calls "Roba el Ehaliyeh" or "Roba el Khaliyeh" — presumably a reference to the Empty Quarter or "Rub al Khali". At the center of the area they discover the Nameless City (the setting of the Lovecraft story of the same name) and in Derleth's text the domain of the Great Old One Hastur. Shrewsbury, an old agent of Hastur and the devoted enemy of Hastur's half-brother, Cthulhu, crosses its gates in search of Alhazred's burial site. He indeed finds Alhazred's burial chamber and learns of his fate. Alhazred had been kidnapped in Damascus and brought to the Nameless City, where he had earlier studied and learned some of the "Necronomicon"s lore. As punishment for betraying their secrets, Alhazred was tortured. Then they blinded him, severed his tongue and executed him. Although the entrance to the chamber warns against disturbing him, Shrewsbury opens Alhazred's sarcophagus anyway, finding that only rags, bones, and dust remain of Alhazred. However, the sarcophagus also contains Alhazred's personal, incomplete copy of the "Necronomicon", written in the Arabic alphabet. Shrewsbury then uses necromancy to recall Alhazred's spirit and orders it to draw a map of the world as he knew it. After obtaining the map, which reveals the location of R'lyeh and other secret places, Shrewsbury finally lets Alhazred return to his eternal rest.
Augusta Ada King, Countess of Lovelace (10 December 1815 – 27 November 1852), born Augusta Ada Byron, was an English writer chiefly known for her work on Charles Babbage's early mechanical general-purpose computer, the analytical engine. Her notes on the engine include what is recognized as the first algorithm intended to be processed by a machine; as such she is often regarded as the world's first computer programmer. She was the only legitimate child of the poet Lord Byron and Anne Isabella Milbanke, but had no relationship with her father, who died when she was nine. As a young adult she took an interest in mathematics, and in particular Babbage's work on the analytical engine. Between 1842 and 1843 she translated an article by Italian mathematician Luigi Menabrea on the engine, which she supplemented with a set of notes of her own. These notes contain what is considered the first computer program, that is, an algorithm encoded for processing by a machine. Though Babbage's engine was never built, Lovelace's notes are important in the early history of computers. She also foresaw the capability of computers to go beyond mere calculating or number-crunching while others, including Babbage himself, focused only on these capabilities.
Ada Lovelace, born 10 December 1815, was the only child of the poet Lord Byron and his wife, Anne Isabella "Annabella" Milbanke. Byron, and many of those who knew Byron, expected that the baby would be "the glorious boy", and there was some disappointment at the contrary news. She was named after Byron's half-sister, Augusta Leigh, and was called "Ada" by Byron himself. On 16 January 1816, Annabella, at Byron's behest, left for her parents' home at Kirkby Mallory taking one-month-old Lovelace with her. Although English law gave fathers full custody of their children in cases of separation, Byron made no attempt to claim his parental rights. On 21 April, Byron signed the Deed of Separation, although very reluctantly, and left England for good a few days later. Byron did not have a relationship with his daughter and he died in 1824 when she was nine; her mother was the only significant parental figure in her life. Lovelace was often ill, dating from her early childhood. At the age of eight she experienced headaches that obscured her vision. In June 1829, she was paralyzed after a bout of the measles. She was subjected to continuous bed rest for nearly a year, which may have extended her period of disability. By 1831 she was able to walk with crutches. Throughout her illnesses, Lovelace continued her education. Her mother's obsession with rooting out any of the insanity of which she accused Lord Byron was one of the reasons that Lovelace was taught mathematics from an early age. Lovelace was privately home-schooled in mathematics and science by William Frend, William King and Mary Somerville. One of her later tutors was the noted mathematician Augustus De Morgan. From 1832, when she was seventeen, her remarkable mathematical abilities began to emerge, and her interest in mathematics dominated her life even after her marriage. In a letter to Lovelace's mother, De Morgan suggested that Lovelace's skill in mathematics could lead her to become "an original mathematical investigator, perhaps of first-rate eminence". Lovelace never met her younger half-sister, Allegra Byron, daughter of Lord Byron and Claire Clairmont,who died in 1822 at the age of five. She did however have some contact with Elizabeth Medora Leigh, the daughter of Byron's half-sister Augusta Leigh. Augusta Leigh purposely avoided Lovelace as much as possible when she was introduced at Court.
Lovelace knew Mary Somerville, noted researcher and scientific author of the 19th century, who introduced her to Charles Babbage on 5 June 1833. Other acquaintances were Sir David Brewster, Charles Wheatstone, Charles Dickens and Michael Faraday. By 1834, Lovelace was a regular at Court and started attending various events. She danced often and was able to charm many people and was described by most people as being dainty. However, John Hobhouse, Lord Byron's friend, was the exception and he described her as "a large, coarse-skinned young woman but with something of my friend's features, particularly the mouth". This description followed their meeting on 24 February 1834 in which Lovelace made it clear to Hobhouse that she did not like him, probably due to the influence of her mother, which led her to dislike all of her father's friends. This first impression was not to last, and they later became friends. On 8 July 1835 she married William King, 8th Baron King, later 1st Earl of Lovelace in 1838. Her full title for most of her married life was "The Right Honourable the Countess of Lovelace". Their residence was a large estate at Ockham Park, in Ockham, Surrey, along with another estate and a home in London. They had three children; Byron born 12 May 1836, Anne Isabella (called Annabella, later Lady Anne Blunt) born 22 September 1837 and Ralph Gordon born 2 July 1839. Immediately after the birth of Annabella, Lovelace experienced "a tedious and suffering illness which took months to cure". In 1841, Lovelace and Medora Leigh (daughter of Byron's half-sister Augusta Leigh) were told by Lovelace's mother that Byron was Medora's father. On 27 February 1841, Lovelace wrote to her mother: "I am not in the least "astonished". In fact you merely "confirm" what I have for "years and years" felt scarcely a doubt about, but should have considered it most improper in me to hint to you that I in any way suspected". Lovelace did not blame the incestuous relationship on Byron, but instead blamed Augusta Leigh: "I fear "she" is "more inherently" wicked than "he" ever was". This did not prevent Lovelace's mother from attempting to destroy her daughter's image of her father, but instead drove her to attack Byron's image with greater intensity.
During a nine-month period in 1842-43, Lovelace translated Italian mathematician Luigi Menabrea's memoir on Babbage's newest proposed machine, the Analytical Engine. With the article, she appended a set of notes. The notes are longer than the memoir itself and include (Section G), in complete detail, a method for calculating a sequence of Bernoulli numbers with the Engine, which would have run correctly had the Analytical Engine ever been built. Based on this work, Lovelace is now widely credited with being the first computer programmer and her method is recognised as the world's first computer program. The level of impact of Lovelace on Babbage's engines is difficult to resolve due to Babbage's tendency not to acknowledge (either orally or in writing) the influence of other people in his work. However, Lovelace was certainly one of the few people who fully understood Babbage's ideas and created a program for the Analytical Engine, indeed there are numerous clues that she might also have suggested the usage of punched cards for Babbage's second machine since her notes in Menabrea's memoir suggest she deeply understood the Jacquard's Loom as well as the Analytical Engine. Her prose also acknowledged some possibilities of the machine which Babbage never published, such as speculation that "the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent".
In 1842 Charles Babbage was invited to give a seminar at the University of Turin about his analytical engine. Luigi Menabrea, a young Italian engineer, and future prime minister of Italy, wrote up Babbage's lecture in French, and this transcript was subsequently published in the Bibliothèque Universelle de Genève in October 1842. Babbage asked Ada Lovelace to translate Menabrea's paper into English, subsequently requesting that she augment the notes she had added to the translation. Ada spent most of a year doing this. These notes, which are more extensive than Menabrea's paper, were then published in "The Ladies Diary" and "Taylor's Scientific Memoirs" under the initialism "A.A.L.". In 1953, over one hundred years after her death, Lovelace's notes on Babbage's Analytical Engine were republished. The engine has now been recognized as an early model for a computer and Lovelace's notes as a description of a computer and software. Her notes were labeled alphabetically from A to G. Note G is the longest of the seven. In note G, Ada describes an algorithm for the analytical engine to compute Bernoulli numbers. It is generally considered the first algorithm ever specifically tailored for implementation on a computer, and for this reason she is considered by many to be the first computer programmer. The computer language Ada, created on behalf of the United States Department of Defense, was named after Lovelace. The reference manual for the language was approved on 10 December 1980, and the Department of Defense Military Standard for the language, "MIL-STD-1815", was given the number of the year of her birth. In addition Lovelace's image can be seen on the Microsoft product authenticity hologram stickers. Since 1998, the British Computer Society has awarded a medal in her name and in 2008 initiated an annual competition for women students of computer science. In popular media, Lovelace has been portrayed in the movie "Conceiving Ada" and the book "The Difference Engine" by William Gibson and Bruce Sterling.
August William Derleth (February 24, 1909 – July 4, 1971) was an American writer and anthologist. Though best remembered as the first publisher of the writings of H. P. Lovecraft, and for his own contributions to the Cthulhu Mythos genre of horror, Derleth was a leading American regional writer of his day, as well as prolific in several other genres, including historical fiction, poetry, detective fiction, science fiction and biography. A 1938 Guggenheim Fellow, Derleth considered his most serious work to be the ambitious "Sac Prairie Saga", a series of fiction, historical fiction, poetry, and non-fiction naturalist works designed to memorialize life in the Wisconsin he knew. Derleth can also be considered a pioneering naturalist and conservationist in his writing.
The son of William Julius Derleth and Rose Louise Volk, Derleth grew up in Sauk City, Wisconsin. He was educated in local parochial and public high school. Derleth wrote his first fiction at age 13. Forty rejected stories and three years later, according to anthologist Jim Stephens, he sold his first story, "Bat's Belfrey", to "Weird Tales" magazine. Derleth wrote throughout his four years at the University of Wisconsin, where he received a B.A. in 1930. During this time he also served briefly as associate editor of Minneapolis-based Fawcett Publications "Mystic Magazine". Returning to Sauk City in the summer of 1931, Derleth worked in a local canning factory and collaborated with childhood friend Mark Schorer (later Chairman of the University of California, Berkeley English Department) writing Gothic and other horror stories. As a result of his early work on the "Sac Prairie Saga", Derleth was awarded the prestigious Guggenheim Fellowship; his sponsors were Helen C. White, Nobel Prize-winning novelist Sinclair Lewis and poet Edgar Lee Masters of "Spoon River Anthology" fame. In the mid-1930s, Derleth organized a Ranger's Club for young people, served as clerk and president of the local school board, served as a parole officer, organized a local men's club and a parent-teacher association. He also lectured in American regional literature at the University of Wisconsin and was a contributing editor of "Outdoors Magazine". With longtime friend Donald Wandrei, Derleth in 1939 founded Arkham House. Its initial objective was to publish the works of H.P Lovecraft, with whom Derleth had corresponded since his teenage years. At the same time, he began teaching a course in American Regional Literature at the University of Wisconsin. In 1941, he became literary editor of "The Capital Times" newspaper in Madison, a post he held until his resignation in 1960. His hobbies included fencing, swimming, chess, stamp-collecting and comic-strips (Derleth reportedly deployed the funding from his Guggenheim Fellowship to bind his comic book collection, most recently valued in the millions of dollars, rather than to travel abroad as the award intended.). Derleth's true avocation, however, was hiking the terrain of his native Wisconsin lands, and observing and recording nature with an expert eye. Derleth once wrote of his writing methods, "I write very swiftly, from 750,000 to a million words yearly, very little of it pulp material." He was married April 6, 1953, to Sandra Evelyn Winters. They divorced six years later. His bisexuality has been asserted in a biography by Dorothy Litersky. Derleth retained custody of the couple's two children, April Rose and Walden William. April Rose is the current manager of Arkham House. In 1960, Derleth began editing and publishing a magazine called "Hawk and Whippoorwill", dedicated to poems of man and nature. Derleth died of a massive and sudden heart attack on July 4, 1971, and is buried in St. Aloysius Cemetery in Sauk City.
Derleth wrote an expansive series of novels, short stories, journals, poems, and other works about Sac Prairie (whose prototype is Sauk City). Derleth intended this series to comprise up to 50 novels telling the projected life-story of the region from the 1800s onwards, with analogies to Balzac's "Human Comedy" and Proust's "Remembrance of Things Past". This, and other early work by Derleth, made him a well known figure among the regional literary figures of his time: early Pulitzer Prize winners Hamlin Garland and Zona Gale, as well as Sinclair Lewis, the latter both an admirer and critic of Derleth. As Edward Wagenknecht writes in "Cavalcade of the American Novel": "What Mr. Derleth has that is lacking...in modern novelists generally, is a country. He belongs. He writes of a land and a people that are bone of his bone and flesh of his flesh. In his fictional world, there is a unity much deeper and more fundamental than anything that can be conferred by an ideology. It is clear, too, that he did not get the best, and most fictionally useful, part of his background material from research in the library; like Scott, in his Border novels, he gives, rather, the impression of having drunk it in with his mother's milk." Jim Stephens, editor of "An August Derleth Reader", (1992), argues: "what Derleth accomplished...was to gather a Wisconsin mythos which gave respect to the ancient fundament of our contemporary life." The author inaugurated the "Sac Pairie Saga" with four novellas comprising "Place of Hawks", published by Loring & Mussey in 1935. At publication, "The Detroit News" wrote: "Certainly with this book Mr. Derleth may be added to the American writers of distinction." Derleth's first novel, "Still is the Summer Night", was published two years later by the famous Charles Scribners' editor Maxwell Perkins, and was the second in his Sac Pairie Saga. "Village Year", the first in a series of journals–meditations on nature, Midwestern village American life, and more–was published in 1941 to praise from "The New York Times Book Review": "A book of instant sensitive responsiveness...recreates its scene with acuteness and beauty, and makes an unusual contribution to the Americana of the present day." The "New York Herald Tribune" observed that "Derleth...deepens the value of his village setting by presenting in full the enduring natural background; with the people projected against this, the writing comes to have the quality of an old Flemish picture, humanity lively and amusing and loveable in the foreground and nature magnificent beyond." James Grey, writing in the "St. Louis Dispatch" concluded, "Derleth has achieved a kind of prose equivalent of the "Spoon River Anthology"." In the same year, "Evening in Spring" was published by Charles Scribners & Sons. This work Derleth considered among his finest. What "The Milwaukee Journal" called "this beautiful little love story," is an autobiographical novel of first love beset by small town religious bigotry. The work received critical praise: "The New Yorker" considered it a story told "with tenderness and charm," while the "Chicago Tribune" concluded: "It's as though he turned back the pages of an old diary and told, with rekindled emotion, of the pangs of pain and the sharp, clear sweetness of a boy's first love." Helen Constance White, wrote in "The Capital Times" that it was "...the best articulated, the most fully disciplined of his stories." These were followed in 1943 with "Shadow of Night," a Scribners' novel of which "The Chicago Sun" wrote: "Structurally it has the perfection of a carved jewel...A psychological novel of the first order, and an adventure tale that is unique and inspiriting." In November 1945, however, Derleth's work was attacked by his one-time admirer and mentor, Sinclair Lewis. Writing in "Esquire", Lewis said, "It is a proof of Mr. Derleth's merit that he makes one want to make the journey and see his particular Avalon: The Wisconsin River shining among its islands, and the castles of Baron Pierneau and Hercules Dousman. He is a champion and a justification of regionalism. Yet he is also a burly, bounding, bustling, self-confident, opinionated, and highly sweatered young man with faults so grievous that a melancholoy perusal of them may be of more value to apprentices than a study of his serious virtues. If he could ever be persuaded that he isn't half as good as he thinks he is, if he would learn the art of sitting still and using a blue pencil, he might become twice as good as he thinks he is–which would about rank him with Homer." Derleth good humoredly reprinted the criticism along with a photograph of himself sans sweater, on the back cover of his 1948 country journal: "Village Daybook". A lighter side to the "Sac Prairie Saga" is a series of quasi-autobiographical short stories known as the "Gus Elker Stories," amusing tales of country life that Peter Ruber, Derleth's last editor, said were "...models of construction and...fused with some of the most memorable characters in American literature." Most were written between 1934 and the late 1940s, though the last, "Tail of the Dog", was published in 1959 and won the "Scholastic Magazine" short story award for the year. The series was collected and republished in "Country Matters" in 1996. "Walden West," published in 1961, is considered by many Derleth's finest work. This prose meditation is built out of the same fundamental material as the series of Sac Prairie journals, but is organized around three themes: "the persistence of memory...the sounds and odors of the country...and Thoreau's observation that the 'mass of men lead lives of quiet desperation.'" A blend of nature writing, philosophic musings, and careful observation of the people and place of "Sac Prairie." Of this work, George Vukelich, author of "North Country Notebook," writes: "Derleth's "Walden West" is...the equal of Sherwood Anderson's "Winesburg,Ohio", Thornton Wilder's "Our Town", and Edgar Lee Masters' "Spoon River Anthology"." This was followed eight years later by "Return to Walden West," a work of similar quality, but with a more noticeable environmentalist edge to the writing, notes critic Norbert Blei. A close literary relative of the "Sac Prairie Saga" was Derleth's "Wisconsin Saga", which comprises several historical novels.
Detective fiction represented another substantial body of Derleth's work. Most notable among this work was a series of 70 stories in affectionate pastiche of Sherlock Holmes, whose creator, Sir Arthur Conan Doyle, he admired greatly. These included one published novel as well ("Mr. Fairlie's Journey"). The series features a (Sherlock Holmes-styled) British detective named Solar Pons, of Praed Street in London. The series was greatly admired by such notable writers and critics of mystery and detective fiction as Ellery Queen (Frederic Dannay), Anthony Boucher, Vincent Starrett and Howard Haycraft. In his 1944 volume "The Misadventures of Sherlock Holmes", Ellery Queen wrote of Derleth's "The Norcross Riddle", an early Pons story: "How many budding authors, not even old enough to vote, could have captured the spirit and atmosphere with as much fidelity?" Queen adds, "...and his choice of the euphonic Solar Pons is an appealing addition to the fascinating lore of Sherlockian nomenclature." Vincent Starrett, in his foreword to the 1964 edition of "The Casebook of Solar Pons", wrote that the series is "...as sparkling a galaxy of Sherlockian pastiches as we have had since the canonical entertainments came to an end." Despite close similarities to Doyle's creation, Pons lived in the post-World War I era, in the decade of the 1920s. Though Derleth never wrote a Pons novel to equal "The Hound of the Baskervilles", editor Peter Ruber wrote: "...Derleth produced more than a few Solar Pons stories almost as good as Sir Arthur's, and many that had better plot construction." Although these stories were a form of diversion for Derleth, Ruber, who edited the "The Original Text Solar Pons Omnibus Edition" (2000), argued: "Because the stories were generally of such high quality, they ought to be assessed on their own merits as a unique contribution in the annals of mystery ficton, rather than suffering comparison as one of the endless imitators of Sherlock Holmes." Some of the stories were self-published, through a new imprint called "Mycroft & Moran", an appellation of humorous significance to Holmesian scholars. For approximately a decade, an active supporting group was the Praed Street Irregulars, patterned after the Baker Street Irregulars. In 1946, Conan Doyle's two sons made some attempts to force Derleth to cease publishing the Solar Pons series, but the efforts were unsuccessful and eventually withdrawn. Derleth's mystery and detective fiction also included a series of works set in Sac Prairie and featuring Judge Peck as the central character.
Derleth wrote many and varied children's works, including biographies meant to introduce younger readers to explorer Fr. Marquette, as well as Ralph Waldo Emerson and Henry David Thoreau. Arguably most important among his works for younger readers, however, is the "Steve and Sim Mystery Series". The ten-volume series is set in Sac Prairie of the 1920s and can thus be considered in its own right a part of the "Sac Prairie Saga", as well as an extension of Derleth's body of mystery fiction. Robert Hood, writing in the "New York Times" said: "Steve and Sim, the major characters, are twentieth-century cousins of Huck Finn and Tom Sawyer; Derleth's minor characters, little gems of comic drawing." The first novel in the series, "The Moon Tenders", does, in fact, involve a rafting adventure down the Wisconsin River, which led regional writer Jesse Stuart to suggest the novel was one that "older people might read to recapture the spirit and dream of youth." The connection to the "Sac Prairie Saga" was noted by the "Chicago Tribune": "Once again a small midwest community in the 1920's is depicted with perception, skill, and dry humor." Arkham House and the Cthulhu Mythos. Derleth was a correspondent and friend of H. P. Lovecraft – when Lovecraft wrote about "le Comte d'Erlette" in his fiction, it was in homage to Derleth. Derleth invented the term Cthulhu Mythos to describe the fictional universe described in the series of stories shared by Lovecraft and other writers in his circle. Derleth's own writing emphasized the struggle between good and evil, in line with his own Christian world view and in contrast with Lovecraft's depiction of an amoral universe. Derleth also treated Lovecraft's Old Ones as representatives of elemental forces, creating new entities to flesh out this framework. When Lovecraft died in 1937, Derleth and Donald Wandrei assembled a collection of that author's stories and tried to get them published. With existing publishers showing little interest, they founded Arkham House in 1939 to do it themselves. The name of the company came from Lovecraft's fictional town of Arkham, Massachusetts, which is featured in many of his stories. In 1939 Arkham House published "The Outsider and Others", a huge collection that contained most of Lovecraft's known short stories. Derleth and Wandrei soon expanded Arkham House and began a regular publishing schedule after its second book, "Someone in the Dark", a collection of some of Derleth's own horror stories, was published in 1941. Following Lovecraft's death, Derleth wrote a number of stories based on fragments and notes left by Lovecraft. These were published in "Weird Tales" and later in book form, under the byline "H. P. Lovecraft and August Derleth", with Derleth calling himself a "posthumous collaborator." This practice has raised objections in some quarters that Derleth simply used Lovecraft's name to market what was essentially his own fiction; S. T. Joshi refers to the "posthumous collaborations" as marking the beginning of "perhaps the most disreputable phase of Derleth's activities". A significant number of H. P. Lovecraft fans and critics, such as Dirk W. Mosig and S. T. Joshi, were dissatisfied with Derleth's invention of the term "Cthulhu Mythos" and his belief that Lovecraft's fiction had an overall pattern reflecting Derleth's own Christian world view. Still there is little but praise for Derleth for his founding of Arkham House and for his successful effort to rescue Lovecraft from literary obscurity. Ramsey Campbell has also acknowledged Derleth's encouragement and guidance during the early part of his own writing career, and Kirby McCauley has cited Derleth and Arkham House as an inspiration for his own anthology, "Dark Forces". Arkham House and Derleth published "Dark Carnival", the first book by Ray Bradbury, as well. Brian Lumley cites the importance of Derleth to his own Lovecraftian work, and contends in a 2009 introduction to Derleth's work that he was "...one of the first, finest, and most discerning editors and publishers of macabre fiction." Important as was Derleth's work to rescue H.P. Lovecraft from literary obscurity at the time of Lovecraft's death, Derleth also built a body of horror and spectral fiction of his own; still frequently anthologized. The best of this work, recently reprinted in four volumes of short stories–most of which were originally published in "Weird Tales", illustrates Derleth's original abilities in the genre. While Derleth considered his work in this genre less than his most serious literary efforts, the compilers of these four anthologies, including Ramsey Campbell, note that the stories still resonate after more than fifty years. In 2009, The Library of America selected Derleth’s story "The Panelled Room" for inclusion in its two-century retrospective of American Fantastic Tales.
Although Derleth was not a trained historian, he wrote many historical novels, as part of both the "Sac Prairie Saga" and the "Wisconsin Saga". He also wrote history; arguably most notable among these was "The Wisconsin: River of a Thousand Isles", published in 1942. The work was one in a series entitled "The Rivers of America", conceived by writer Constance Lindsay Skinner in the Great Depression as a series that would connect Americans to their heritage through the history of the great rivers of the nation. Skinner wanted the series to be written by artists, not academicians. Derleth, while not a professional historian, was, according to former Wisconsin state historian William F. Thompson, "...a very competent regional historian who based his historical writing upon research in the primary documents and who regularly sought the help of professionals...." In the foreword to the 1985 reissue of the work by The University of Wisconsin Press, Thompson concluded: "No other writer, of whatever background or training, knew and understood his particular 'corner of the earth' better than August Derleth." Derleth wrote several volumes of poems, as well as biographies of Zona Gale, Ralph Waldo Emerson and Henry David Thoreau. He also wrote introductions to several collections of classic early 20th century comics, such as "Buster Brown", "Little Nemo in Slumberland", and "Katzenjammer Kids", as well as a book of children's poetry entitled "A Boy's Way". Derleth also wrote under the nom de plumes Stephen Grendon, Kenyon Holmes and Tally Mason. Derleth's papers and comic book collection (valued at a considerable sum upon his death) were donated to the Wisconsin Historical Society in Madison.
The Alps (;;;;;;) are one of the great mountain range systems of Europe, stretching from Austria and Slovenia in the east; through Italy, Switzerland, Liechtenstein and Germany; to France in the west. The highest mountain in the Alps is Mont Blanc, at, on the Italian–French border. All the main peaks of the Alps can be found in the list of mountains of the Alps and list of Alpine peaks by prominence. The English name "Alps" was taken via French from Latin "Alpes", which may be ultimately cognate with Latin "albus" ("white"). The German "Albe", "Alpe" or "Alp" (f., Old High German "alpâ", plural "alpûn"), and the French "Alpage" or "Alpe" in the singular mean "alpine pasture", and only in the plural may also refer to the mountain range as a whole.
The Alps are generally divided into the Western Alps and the Eastern Alps. The division is along the line between Lake Constance and Lake Como, following the rivers Rhine, Liro and Mera. The Western Alps are higher, but their central chain is shorter and curved; they are located in Italy, France and Switzerland. The Eastern Alps (main ridge system elongated and broad) belong to Italy, Austria, Switzerland, Germany, Liechtenstein and Slovenia. The highest peak of the Western Alps is Mont Blanc, at. The highest peak of the Eastern Alps is Piz Bernina, at. The Dufourspitze, and Ortler, are the second-highest, respectively. The border between the Central Alps and the Southern Limestone Alps is the Periadriatic Seam. The Northern Limestone Alps are separated from the Central Eastern Alps by the Greywacke zone. Series of lower mountain ranges run parallel to the main chain of the Alps, including the French Prealps. (See Alpine geography.) The geologic subdivision is different and makes no difference between the Western and Eastern Alps: the Helveticum in the north, the Penninicum and Austroalpine system in the center and, south of the Periadriatic Seam, the Southern Alpine system and parts of the Dinarides (see Alpine geology). Geographically, the Jura Mountains do not belong to the Alps; geologically, however, they do.
The main chain of the Alps follows the watershed from the Mediterranean Sea to the Wienerwald, passing over many of the highest and most famous peaks in the Alps. From the Colle di Cadibona to Col de Tende it runs westwards, before turning to the northwest and then, near the Colle della Maddalena, to the north. Upon reaching the Swiss border, the line of the main chain heads approximately east-northeast, a heading it follows until its end near Vienna.
The Alps form a part of a Tertiary orogenic belt of mountain chains, called the Alpide belt, that stretches through southern Europe and Asia from the Atlantic all the way to the Himalayas. This belt of mountain chains was formed during the Alpine orogeny. A gap in these mountain chains in central Europe separates the Alps from the Carpathians off to the east. Orogeny took place continuously and tectonic subsidence is to blame for the gaps in between. The Alps arose as a result of the collision of the African and European tectonic plates, in which the western part of the Tethys Ocean, which was formerly in between these continents, disappeared. Enormous stress was exerted on sediments of the Tethys Ocean basin and its Mesozoic and early Cenozoic strata were pushed against the stable Eurasian landmass by the northward-moving African landmass. Most of this occurred during the Oligocene and Miocene epochs. The pressure formed great recumbent folds, or "nappes", that rose out of what had become the Tethys Sea and pushed northward, often breaking and sliding one over the other to form gigantic thrust faults. Crystalline basement rocks, which are exposed in the higher central regions, are the rocks forming Mont Blanc, the Matterhorn, and high peaks in the Pennine Alps and Hohe Tauern. The formation of the Mediterranean Sea is a more recent development, and does not mark the northern shore of the African landmass.
The Alps are split into five climate zones, each with a different kind of environment. The climate, plant life and animal life vary on different sections or zones of the mountain. 1.The section of the Alps that is above 3000 metres is called the névé zone. This area, which has the coldest climate, is permanently coated with compressed snow. That is why plants are scarce in the névé zone. 2. The alpine zone lies between the height of 2000 and 3000 metres. This zone is less cold than in the névé zone. Wildflowers and grasses grow here. 3. Just below the alpine zone is the subalpine zone, 1500 to 2000 metres high. Forests of fir trees and spruce trees grow in the subalpine zone as the temperature slowly goes up. 4. At about 1000 to 1500 metres high is the arable zone. Millions of oak trees sprout in this area. This is also where farming takes place. 5. Below 1000 metres are the lowlands. Here, a larger variety of plants produce. Aside from plants, villages are also in the lowlands because the temperature is more bearable for both humans and animals. The Alps are a classic example of what happens when a temperate area at lower altitude gives way to higher-elevation terrain. Elevations around the world which have cold climates similar to those found in polar areas have been called Alpine. A rise from sea level into the upper regions of the atmosphere causes the temperature to decrease (see adiabatic lapse rate). The effect of mountain chains on prevailing winds is to carry warm air belonging to the lower region into an upper zone, where it expands in volume at the cost of a proportionate loss of heat, often accompanied by the precipitation in the form of snow or rain.
Little is known of the early dwellers of the Alps, save from scanty accounts preserved by Roman and Greek historians and geographers. A few details have come down to us of the conquest of many of the Alpine tribes by Augustus. Also, recent research into Mitochondrial DNA indicates that MtDNA Haplogroup K very likely originated in or near the southeastern Alps approximately 12–15,000 years ago. During the Second Punic War in 218 BC, the Carthaginian general Hannibal successfully crossed the Alps along with an army numbering 38,000 infantry, 8,000 cavalry, and 37 war elephants. This was one of the most celebrated achievements of any military force in ancient warfare. Much of the Alpine region was gradually settled by Germanic tribes (Langobards, Alemanni, Bavarii) from the 6th to the 13th centuries, the latest expansion corresponding to the Walser migrations. Not until after the final breakup of the Carolingian Empire in the 10th and 11th century can the local history of the Alps be traced out.
The higher regions of the Alps were long left to the exclusive attention of the people of the adjoining valleys even when Alpine travellers (as distinguished from Alpine climbers) began to visit these valleys. The two men who first explored the regions of ice and snow were H.B. de Saussure (1740–1799) in the Pennine Alps and the Benedictine monk of Disentis Placidus a Spescha (1752–1833), most of whose ascents were made before 1806 in the valleys at the sources of the Rhine.
The Alps are popular both in summer and in winter as a destination for sightseeing and sports. Winter sports (Alpine and Nordic skiing, snowboarding, tobogganing, snowshoeing, ski tours) can be practised in most regions from December to April. In summer, the Alps are popular with hikers, mountain bikers, paragliders, mountaineers, while many alpine lakes attract swimmers, sailors and surfers. The lower regions and larger towns of the Alps are well served by motorways and main roads, but higher passes and by-roads can be treacherous even in summer. Many passes are closed in winter. A multitude of airports around the Alps (and some within), as well as long-distance rail links from all neighbouring countries, afford large numbers of travellers easy access from abroad. The Alps typically see more than 100 million visitors a year.
A natural vegetation limit with altitude is given by the presence of the chief deciduous trees—oak, beech, ash and sycamore maple. These do not reach exactly to the same elevation, nor are they often found growing together; but their upper limit corresponds accurately enough to the change from a temperate to a colder climate that is further proved by a change in the presence of wild herbaceous vegetation. This limit usually lies about above the sea on the north side of the Alps, but on the southern slopes it often rises to, sometimes even to. This region is not always marked by the presence of the characteristic trees. Human interference has nearly exterminated them in many areas, and, except for the beech forests of the Austrian Alps, forests of deciduous trees are rarely found. In many districts where such woods once existed, they have been replaced by the Scots pine and Norway spruce, which are less sensitive to the ravages of goats who are the worst enemies of such trees. Above the forestry, there is often a band of short pine trees ("Pinus mugo"), which is in turn superseded by dwarf shrubs, typically "Rhododendron ferrugineum" (on acid soils) or "Rhododendron hirsutum" (on alkaline soils). Above this is the alpine meadow, and even higher, the vegetation becomes more and more sparse. At these higher altitudes, the plants tend to form isolated cushions. In the Alps, several species of flowering plants have been recorded above, including "Ranunculus glacialis", "Androsace alpina" and "Saxifraga biflora".
Albert Camus (; 7 November 1913 – 4 January 1960) was a French Algerian author, philosopher, and journalist who was awarded the Nobel Prize for Literature in 1957. He was a key philosopher of the 20th-century and his most famous work is the novel "L'Étranger" (The Stranger)". In 1949, Camus founded the Group for International Liaisons within the Revolutionary Union Movement, which was a group opposed to some tendencies of the surrealistic movement of André Breton. Camus was the second-youngest recipient of the Nobel Prize for Literature - after Rudyard Kipling - when he became the first African-born writer to receive the award. He is the shortest-lived of any literature laureate to date, having died in an automobile accident just over two years after receiving the award. He is often cited as a proponent of existentialism, the philosophy that he was associated with during his own lifetime, but Camus himself rejected this particular label. In an interview in 1945, Camus rejected any ideological associations: "No, I am not an existentialist. Sartre and I are always surprised to see our names linked..." Specifically, his views contributed to the rise of the more current philosophy known as absurdism. He wrote in his essay "The Rebel" that his whole life was devoted to opposing the philosophy of nihilism while still delving deeply into individual freedom.
Albert Camus was born on 7 November 1913 in Dréan (then known as Mondovi) in French Algeria to a Pied-Noir settler family. His mother was of Spanish extraction and was half-deaf. Pied-Noir was a term used to refer to colonists of French Algeria until Algerian independence in 1962. His father Lucien, a poor agricultural worker, died in the Battle of the Marne in 1914 during World War I, while serving as a member of the Zouave infantry regiment. Camus lived in poor conditions during his childhood in the Belcourt section of Algiers. In 1923, he was accepted into the lycée and eventually to the University of Algiers. However, he contracted tuberculosis in 1930, which put an end to his football activities (he had been a goalkeeper for the university team) and forced him to make his studies a part-time pursuit. He took odd jobs including private tutor, car parts clerk and work for the Meteorological Institute. He completed his "licence de philosophie" (BA) in 1935; in May 1936, he successfully presented his thesis on Plotinus, "Néo-Platonisme et Pensée Chrétienne", for his "diplôme d'études supérieures" (roughly equivalent to an M.A. thesis). Camus joined the French Communist Party in the Spring of 1935 seeing it as a way to "fight inequalities between Europeans and 'natives' in Algeria." He did not suggest he was a Marxist or that he had read "Das Kapital", but did write that "[w]e might see communism as a springboard and asceticism that prepares the ground for more spiritual activities". In 1936, the independence-minded Algerian Communist Party (PCA) was founded. Camus joined the activities of the Algerian People's Party ("Le Parti du Peuple Algérien"), which got him into trouble with his Communist party comrades. As a result, he was denounced as a Trotskyite and expelled from the party in 1937. Camus went on to be associated with the French anarchist movement. The anarchist Andre Prudhommeaux first introduced him at a meeting in 1948 of the Cercle des Etudiants Anarchistes (Anarchist Student Circle) as a sympathiser who was familiar with anarchist thought. Camus went on to write for anarchist publications such as Le Libertaire, "La révolution Proletarienne" and "Solidaridad Obrera" (the organ of the anarcho-syndicalist CNT). Camus also stood with the anarchists when they expressed support for the uprising of 1953 in East Germany. He again stood with the anarchists in 1956, first with the workers’ uprising in Poznan, Poland, and then later in the year with the Hungarian Revolution. In 1934, he married Simone Hie, a morphine addict, but the marriage ended as a consequence of infidelities on both sides. In 1935, he founded "Théâtre du Travail" — "Worker's Theatre" — (renamed "Théâtre de l'Equipe" ("Team's Theatre") in 1937), which survived until 1939. From 1937 to 1939 he wrote for a socialist paper, "Alger-Républicain", and his work included an account of the peasants who lived in Kabylie in poor conditions, which apparently cost him his job. From 1939 to 1940, he briefly wrote for a similar paper, "Soir-Republicain". He was rejected by the French army because of his tuberculosis. In 1940, Camus married Francine Faure, a pianist and mathematician. Although he loved Francine, he had argued passionately against the institution of marriage, dismissing it as unnatural. Even after Francine gave birth to twins, Catherine and Jean, on 5 September 1945, he continued to joke wearily to friends that he was not cut out for marriage. Camus conducted numerous affairs, particularly an irregular and eventually public affair with the Spanish-born actress Maria Casares. In the same year, Camus began to work for "Paris-Soir" magazine. In the first stage of World War II, the so-called Phony War stage, Camus was a pacifist. However, he was in Paris to witness how the Wehrmacht took over. On 15 December 1941, Camus witnessed the execution of Gabriel Péri, an event that Camus later said crystallized his revolt against the Germans. Afterwards he moved to Bordeaux alongside the rest of the staff of "Paris-Soir". In the same year he finished his first books, "The Stranger" and "The Myth of Sisyphus". He returned briefly to Oran, Algeria in 1942.
During the war Camus joined the French Resistance cell "Combat", which published an underground newspaper of the same name. This group worked against the Nazis, and in it Camus assumed the nom de guerre "Beauchard". Camus became the paper's editor in 1943, and when the Allies liberated Paris Camus reported on the last of the fighting. He was one of the few French editors to publicly express opposition to the use of the atomic bomb in Hiroshima soon after the event on 8 August 1945. He eventually resigned from "Combat" in 1947, when it became a commercial paper. It was then that he became acquainted with Jean-Paul Sartre. After the war, Camus began frequenting the Café de Flore on the Boulevard Saint-Germain in Paris with Sartre and others. He also toured the United States to lecture about French thinking. Although he leaned left, politically, his strong criticisms of Communist doctrine did not win him any friends in the Communist parties and eventually also alienated Sartre. In 1949 his tuberculosis returned and he lived in seclusion for two years. In 1951 he published "The Rebel", a philosophical analysis of rebellion and revolution which made clear his rejection of communism. The book upset many of his colleagues and contemporaries in France and led to the final split with Sartre. The dour reception depressed him and he began instead to translate plays. Camus's first significant contribution to philosophy was his idea of the absurd, the result of our desire for clarity and meaning within a world and condition that offers neither, which he explained in "The Myth of Sisyphus" and incorporated into many of his other works, such as "The Stranger" and "The Plague". Despite the split from his "study partner", Sartre, some still argue that Camus falls into the existentialist camp. However, he rejected that label himself in his essay "Enigma" and elsewhere (see: "The Lyrical and Critical Essays of Albert Camus"). The current confusion may still arise, as many recent applications of existentialism have much in common with many of Camus's "practical" ideas (see: "Resistance, Rebellion, and Death"). However, the personal understanding he had of the world (e.g. "a benign indifference", in "The Stranger"), and every vision he had for its progress (e.g. vanquishing the "adolescent furies" of history and society, in "The Rebel") undoubtedly set him apart. In the 1950s Camus devoted his efforts to human rights. In 1952 he resigned from his work for UNESCO when the UN accepted Spain as a member under the leadership of General Franco. In 1953 he criticized Soviet methods to crush a workers' strike in East Berlin. In 1956 he protested against similar methods in Poland (protests in Poznań) and the Soviet repression of the Hungarian revolution in October. He maintained his pacifism and resistance to capital punishment anywhere in the world. One of his most significant contributions to the movement against capital punishment was an essay collaboration with Arthur Koestler, the writer, intellectual and founder of the League Against Capital Punishment. When the Algerian War began in 1954 it presented a moral dilemma for Camus. He identified with pied-noirs, and defended the French government on the grounds that the revolt in Algeria was really an integral part of the 'new Arab imperialism' led by Egypt and an 'anti-Western' offensive orchestrated by Russia to 'encircle Europe' and 'isolate the United States'. Although favouring greater Algerian autonomy or even federation, though not full-scale independence, he believed that the pied-noirs and Arabs could co-exist. During the war he advocated civil truce that would spare the civilians, which was rejected by both sides who regarded it as foolish. Behind the scenes, he began to work for imprisoned Algerians who faced the death penalty. From 1955 to 1956 Camus wrote for "L'Express". In 1957 he was awarded the Nobel Prize in literature "for his important literary production, which with clear-sighted earnestness illuminates the problems of the human conscience in our times", not for his novel "The Fall", published the previous year, but for his writings against capital punishment in the essay "Réflexions sur la Guillotine". When he spoke to students at the University of Stockholm, he defended his apparent inactivity in the Algerian question and stated that he was worried about what might happen to his mother, who still lived in Algeria. This led to further ostracism by French left-wing intellectuals. Revolutionary Union Movement and the European Union. As he wrote in "L'Homme révolté" (in the chapter about "The Thought on Midday") he was a follower of the ancient Greek 'Solar Tradition' (la "pensée solaire"). So, not only was he the leader of the French resistance movement "Combat," but he set up in 1947-8 the Revolutionary Union Movement (Groupes de liaison internationale - GLI) which was formed in 1949 and can be described as a trade union movement in the context of revolutionary syndicalism (Syndicalisme révolutionnaire) - according to Olivier Todd, in the biography, 'Albert Camus, une vie', it was a group opposed to some tendencies of the surrealistic movement of André Breton. For more, see the book: "Alfred Rosmer et le mouvement révolutionnaire internationale" by Christian Gras). His colleagues were Nicolas Lazarévitch, Louis Mercier, Roger Lapeyre, Paul Chauvet, Auguste Largentier, Jean de Boë (see the article: "Nicolas Lazarévitch, Itinéraire d'un syndicaliste révolutionnaire" by Sylvain Boulouque in the review Communisme, n° 61, 2000). His main aim was to express the positive side of surrealism and existentialism, rejecting the negativity and the nihilism of André Breton. From 1943, Albert Camus had correspondence with Altiero Spinelli who founded the European Federalist Movement in Milan—see Ventotene Manifesto and the book "Unire l'Europa, superare gli stati", Altiero Spinelli nel Partito d'Azione del Nord Italia e in Francia dal 1944 al 1945-annexed a letter by Altiero Spinelli to Albert Camus. In 1944 Camus founded the "French Committee for the European Federation" (Comité Français pour la Féderation Européene -CFFE) declaring that Europe "can only evolve along the path of economic progress, democracy and peace if the nation states become a federation". From 22–25 March 1945, the first conference of the European Federalist Movement was organised in Paris with the participation of Albert Camus, George Orwell, Emmanuel Mounier, Lewis Mumford, André Philip, Daniel Mayer, François Bondy and Altiero Spinelli (see the book "The Biography of Europe" by Pan Drakopoulos). This specific branch of the European Federalist Movement disintegrated in 1957 after the domination of Winston Churchill's ideas about the European integration.
Three essays by Dr. Miho Takashima in the "International Journal of Humanities" explore the relation between the work of the French writer Albert Camus and the English writer George Orwell: "Revolt and Equilibrium: A Comparative Study of "Nineteen Eighty-Four" and "L'Homme Révolté", the Views and Struggles of Orwell and Camus", "Art and Representation: A Comparative Study of George Orwell and Albert Camus on their Literary Works", and "George Orwell and Albert Camus: A Comparative Study – Their Views and Dilemmas in the Politics of the 1930s and 40s". Takashima argues that Orwell — perhaps intentionally, in order to warn the intellectual elite — compromised with "Big Brother", while Camus confronted with "The Plague". This is observed not only in the comparison between "Nineteen Eighty-Four" and "The Rebel" but, especially, in Camus' play "The State of Siege". This theatrical play was written together with the novel "The Plague" and the essay "The Rebel". It is the work which — according to Camus himself — represents him best and is a response to George Orwell's "Nineteen Eighty-Four". The hero, Diego, opposes the totalitarian dictator named Plague, and dies in order to set a Spanish town free from the Inquisition. "The State of Siege" is a work against totalitarianism, written in the same epoch when Camus' contemporary, George Orwell, wrote "Nineteen Eighty-Four". The play includes an allegorical reference to the end of Orwell's novel. The original title of "The State of Siege" was "The Holy Inquisition in Cadix". In the French edition of the book, Camus has included an essay under the title "Why Spain?". In this polemical text, he answers his Catholic friend Gabriel Marcel who criticized him for setting the plot in Spain. Here Camus expresses his opposition to the totalitarian regimes of the West, and to the behavior of the Church in Spain which seemed to turn a blind eye to Franco's firing squads. The most important phrase of this essay is "Why Guernica, Gabriel Marcel?".
Camus died on 4 January 1960 at the age of 46 in a car accident near Sens, in a place named "Le Grand Fossard" in the small town of Villeblevin. In his coat pocket lay an unused train ticket. He had planned to travel by train, with his wife and children, but at the last minute accepted his publisher's proposal to travel with him. The driver of the Facel Vega car, Michel Gallimard — his publisher and close friend — was also killed in the accident. Camus was buried in the Lourmarin Cemetery, Lourmarin, Vaucluse, Provence-Alpes-Côte d'Azur, France. He was survived by his twin children, Catherine and Jean, who hold the copyrights to his work. Two of Camus's works were published posthumously. The first, entitled "A Happy Death", published in 1970, featured a character named Patrice Mersault, comparable to "The Strangers Meursault, but there is some debate as to the relationship between the two stories. The second posthumous publication was an unfinished novel, "The First Man", that Camus was writing before he died. The novel was an autobiographical work about his childhood in Algeria and was published in 1995.
Many writers have written on the Absurd, each with his or her own interpretation of what the Absurd actually is and their own ideas on the importance of the Absurd. For example, Sartre recognizes the absurdity of individual experience, while Kierkegaard explains that the absurdity of certain religious truths prevent us from reaching God rationally. Camus was not the originator of Absurdism and regretted the continued reference to him as a "philosopher of the absurd". He shows less and less interest in the Absurd shortly after publishing "Le Mythe de Sisyphe" ("The Myth of Sisyphus"). To distinguish Camus' ideas of the Absurd from those of other philosophers, people sometimes refer to the Paradox of the Absurd, when referring to "Camus' Absurd". His early thoughts on the Absurd appeared in his first collection of essays, "L'Envers et l'endroit" ("The Two Sides Of The Coin") in 1937. Absurd themes appeared with more sophistication in his second collection of essays, "Noces" ("Nuptials"), in 1938. In these essays Camus does not offer a philosophical account of the Absurd, or even a definition; rather he reflects on the experience of the Absurd. In 1942 he published the story of a man living an Absurd life as "L'Étranger" ("The Stranger"), and in the same year released "Le Mythe de Sisyphe" ("The Myth of Sisyphus"), a literary essay on the Absurd. He had also written a play about a Roman Emperor, Caligula, pursuing an Absurd logic. However, the play was not performed until 1945. The turning point in Camus' attitude to the Absurd occurs in a collection of four letters to an anonymous German friend, written between July 1943 and July 1944. The first was published in the "Revue Libre" in 1943, the second in the "Cahiers de Libération" in 1944, and the third in the newspaper "Libertés", in 1945. All four letters have been published as "Lettres à un ami allemand" ("Letters to a German Friend") in 1945, and have appeared in the collection "Resistance, Rebellion, and Death".
In his essays Camus presented the reader with dualisms: happiness and sadness, dark and light, life and death, etc. His aim was to emphasize the fact that happiness is fleeting and that the human condition is one of mortality. He did this not to be morbid, but to reflect a greater appreciation for life and happiness. In "Le Mythe", this dualism becomes a paradox: We value our lives and existence so greatly, but at the same time we know we will eventually die, and ultimately our endeavours are meaningless. While we can live with a dualism ("I can accept periods of unhappiness, because I know I will also experience happiness to come"), we cannot live with the paradox ("I think my life is of great importance, but I also think it is meaningless"). In "Le Mythe", Camus was interested in how we experience the Absurd and how we live with it. Our life must have meaning for us to value it. If we accept that life has no meaning and therefore no value, should we kill ourselves? Meursault, the Absurdist hero of "L'Étranger," has killed a man and is scheduled to be executed. Caligula ends up admitting his Absurd logic was wrong and is killed by an assassination he has deliberately brought about. However, while Camus possibly suggests that Caligula's Absurd reasoning is wrong, the play's anti-hero does get the last word, as the author similarly exalts Meursault's final moments. Camus' understanding of the Absurd promotes public debate; his various offerings entice us to think about the Absurd and offer our own contribution. Concepts such as cooperation, joint effort and solidarity are of key importance to Camus. Camus made a significant contribution to a viewpoint of the Absurd, and always rejected nihilism as a valid response. "If nothing had any meaning, you would be right. But there is something that still has a meaning." "Second Letter to a German Friend", December 1943. What still had meaning for Camus is that despite humans being subjects in an indifferent and "absurd" universe in which meaning is challenged by the fact that we all die, meaning can be created, however provisionally and unstably, by our own decisions and interpretations.
While writing his thesis on Plotinus and Saint Augustine of Hippo, Camus became very strongly influenced by their works, especially that of St. Augustine. In his work, "Confessions" (consisting of 13 books), Augustine promotes the idea of a connection between God and the rest of the world. Camus identified with the idea that a personal experience could become a reference point for his philosophical and literary writings. Although he blatantly considered himself an atheist, Camus later came to tout the idea that the absence of God in a person's life can simultaneously be accompanied by a longing for "salvation and meaning that only God can provide". This line of thinking presented an enormous paradox and became a major thread in defining the idea of absurdism in Camus' writings.
Camus' well-known falling out with Sartre is linked to this opposition to totalitarianism. Camus detected a reflexive totalitarianism in the mass politics espoused by Sartre in the name of radical Marxism. This was apparent in his work "L'Homme Révolté" ("The Rebel") which not only was an assault on the Soviet police state, but also questioned the very nature of mass revolutionary politics. Camus continued to speak out against the atrocities of the Soviet Union, a sentiment captured in his 1957 speech, "The Blood of the Hungarians", commemorating the anniversary of the 1956 Hungarian Revolution, an uprising crushed in a bloody assault by the Red Army.
In "The Stranger", Albert Camus characterizes his justification of the absurd through the experiences of a protagonist who simply does not conform to the system. His inherent honesty disturbs the status quo; Meursault's inability to lie cannot seamlessly integrate him within society and in turn threatens the simple fabrics of human mannerisms expected of a structurally ordered society. Consequently, the punishment for his crime is not decided on the basis of murder, but rather for the startling indifference toward his mother's recent death. Even after a conflicting spiritual discussion with a pastor inciting Meursault to consider a possible path towards redemption, the latter still refuses to take upon salvation and symbolizes his ultimatum by embracing the "gentle indifference of the world"; an act which only furthers his solidarity with a society incapable of realizing his seemingly inhumane behavior.
The plague is an undeniable part of life. As posited in "The Plague", it is omnipresent, just like death was always an impeding factor in "The Stranger". Albert Camus once again questions the meaning of the moral concepts justifying humanity and human suffering within a religious framework. For Camus, the rationale behind Christian doctrine is useless; as mortal beings, we cannot successfully rationalize the impending and inescapable death sentence forced upon every human. The plague, which befalls Oran, is a concrete and tangible facilitator of death. Ultimately, the plague enables people to understand that their individual suffering is meaningless. As the epidemic "evolves" within the seasons, so do the citizens of Oran, who instead of willfully giving up to a disease they have no control over, decide to fight against their impending death, thus unwillingly creating optimism in the midst of hopelessness. This is where Camus channels his thoughts behind the importance of solidarity: although the plague is still primarily an agent of death, it provides the uncanny opportunity for people to realize that individual suffering is absurd. In the midst of complete suffering, the challenging response adopted by the majority of the citizens of Oran demonstrates an inexplicable humanistic connection between distraught and distant characters. Only by making the choice to fight an irreversible epidemic are people able to create the ever-lacking meaning to a life destined for execution the moment of its creation.
Camus was once asked by his friend Charles Poncet which he preferred, football or the theatre. Camus is said to have replied, "Football, without hesitation." Camus played as goalkeeper for Racing Universitaire Algerois (RUA won both the North African Champions Cup and the North African Cup twice each in the 1930s) junior team from 1928–30. The sense of team spirit, fraternity, and common purpose appealed to Camus enormously. In match reports Camus would often attract positive comment for playing with passion and courage. Any aspirations in football disappeared at age 17, upon contracting tuberculosis—then incurable, Camus was bedridden for long and painful periods. Camus was referring to a sort of simplistic morality he wrote about in his early essays, the principle of sticking up for your friends, of valuing bravery and fair-play. Camus' belief was that political and religious authorities try to confuse us with over-complicated moral systems to make things appear more complex than they really are, potentially to serve their own needs.
Quite a few musical artists refer to Camus and his work in their music. The post-punk band The Fall took their name from Camus' novel "The Fall". These also include an album by Jeff Martin ("Exile and the Kingdom", 2006) and songs by Neil Diamond ("Done Too Soon", 1969), Gentle Giant ("A Cry for Everyone", 1972), The Cure ("Killing an Arab", 1978), Tuxedomoon ("The Stranger", 1979), Digable Planets ("Reachin' (A New Refutation of Time and Space)", 1993) The Magnetic Fields ("I Don't Want To Get Over You", 1999), The Manic Street Preachers ("The Masses Against The Classes", 2000), JJ72 ("Algeria", 2000), Suede ("Obsessions", 2002), Streetlight Manifesto ("Here's To Life", 2003), A Perfect Circle ("A Stranger" and "The Outsider", 2003), Angela McCluskey ("Know it All", 2004), Joanna Newsom ("This Side of the Blue", 2004), Tarkio ("Neapolitan Bridesmaid", 2006), The Independence, ("20-Ought-Almost-Talkin' Blues", 2008), Drought ("To the Benign Indifference of the Universe", 2008). The end of the song "No Future Part Two: The Days After No Future" by indie/punk band Titus Andronicus on their debut album "The Airing of Grievances" features a reading from Albert Camus's novel "The Stranger; this also occurs on the 2008 song "Albert Camus" Anti-folk singer-songwriter Jeffrey Lewis references Camus, as well as Bob Dylan and Allen Ginsberg, in a 2005 song, "Williamsburg Will Oldham Horror" in the line, "And I'm sure the thing is probably Dylan himself too, stayed up some nights wishing he was as good as Ginsberg or Camus".
Dame Agatha Christie DBE (15 September 189012 January 1976), was an English crime writer of novels, short stories and plays. She also wrote romances under the name Mary Westmacott, but is best remembered for her 80 detective novels and her successful West End theatre plays. Her works, particularly those featuring detectives Hercule Poirot and Miss Jane Marple, have given her the title the 'Queen of Crime' and made her one of the most important and innovative writers in the development of the genre. Christie has been referred to by the "Guinness Book of World Records" as the best-selling writer of books of all time and the best-selling writer of any kind, along with William Shakespeare. Only the Bible is known to have outsold her collected sales of roughly four billion copies of novels. UNESCO states that she is currently the most translated individual author in the world with only the collective corporate works of Walt Disney Productions surpassing her. Christie's books have been translated into at least 56 languages. Her stage play "The Mousetrap" holds the record for the longest initial run in the world: it opened at the Ambassadors Theatre in London on 25 November 1952 and as of 2010 is still running after more than 23,000 performances. In 1955, Christie was the first recipient of the Mystery Writers of America's highest honour, the Grand Master Award, and in the same year, "Witness for the Prosecution" was given an Edgar Award by the MWA, for Best Play. Most of her books and short stories have been filmed, some many times over ("Murder on the Orient Express", "Death on the Nile" and "4.50 From Paddington" for instance), and many have been adapted for television, radio, video games and comics. In 1968, Booker Books, a subsidiary of the agri-industrial conglomerate Booker-McConnell, bought a 51 percent stake in Agatha Christie Limited, the private company that Christie had set up for tax purposes. Booker later increased its stake to 64 percent. In 1998, Booker sold its shares to Chorion, a company whose portfolio also includes the literary estates of Enid Blyton and Dennis Wheatley. In 2004, a 5,000-word story entitled "The Incident of the Dog's Ball" was found in the attic of the author's daughter. It was published in Britain in September 2009. On November 10, 2009, Reuters announced that the story will be published by "The Strand Magazine". Early life and first marriage. Agatha Mary Clarissa Miller was born in Torquay, Devon, England. Her mother, Clarissa Margaret Boehmer, was the daughter of a British army captain, but had been sent, as a child, to live with her own mother's sister, who was the second wife of a wealthy American. Eventually Margaret married her stepfather's son from his first marriage, Frederick Alvah Miller, an American stockbroker. Thus the two women Agatha called "Grannie" were sisters. Despite her father's nationality as a "New Yorker" and her aunt's relation to the Pierpont Morgans, Agatha never claimed United States citizenship or connection. The Millers had two other children: Margaret Frary Miller (1879–1950), called Madge, who was eleven years Agatha's senior, and Louis Miller (1880–1929), called Monty, ten years older than Agatha. Later, in her autobiography, Agatha would refer to her brother as "an amiable scapegrace of a brother". During the First World War, she worked at a hospital as a nurse; she liked the profession, calling it "one of the most rewarding professions that anyone can follow". She later worked at a hospital pharmacy, a job that influenced her work, as many of the murders in her books are carried out with poison. Despite a turbulent courtship, on Christmas Eve 1914 Agatha married Archibald Christie, an aviator in the Royal Flying Corps. The couple had one daughter, Rosalind Hicks. They divorced in 1928, two years after Christie discovered her husband was having an affair. It was during this marriage that she published her first novel in 1920, "The Mysterious Affair at Styles". In 1924, she published a collection of mystery and ghost stories entitled "The Golden Ball".
In late 1926, Agatha's husband Archie revealed that he was in love with another woman, Nancy Neele, and wanted a divorce. On 8 December 1926, the couple quarrelled, and Archie Christie left their house in Sunningdale, Berkshire, to spend the weekend with his mistress at Godalming, Surrey. That same evening Agatha disappeared from her home, leaving behind a letter for her secretary saying that she was going to Yorkshire. Her disappearance caused an outcry from the public, many of whom were admirers of Agatha Christie's novels. Despite a massive manhunt, there were no results until eleven days later. Eleven days after her disappearance, Christie was identified as a guest at the Swan Hydropathic Hotel (now the Old Swan Hotel) in Harrogate, Yorkshire where she was registered as 'Mrs Teresa Neele' from Cape Town. Christie gave no account of her disappearance. Although two doctors had diagnosed her as suffering from amnesia, opinion remains divided as to the reasons for her disappearance. One suggestion is that she had suffered a nervous breakdown brought about by a natural propensity for depression, exacerbated by her mother's death earlier that year, and the discovery of her husband's infidelity. Public reaction at the time was largely negative with many believing it was all just a publicity stunt, whilst others speculated she was trying to make the police think her husband killed her as revenge for his affair. Second marriage and later life. In 1930, Christie married archaeologist Max Mallowan after joining him in an archaeological dig. Their marriage was especially happy in the early years and remained so until Christie's death in 1976. In 1977, Mallowan married his longtime associate, Barbara Parker. Christie's travels with Mallowan contributed background to several of her novels set in the Middle East. Other novels (such as "And Then There Were None") were set in and around Torquay, where she was born. Christie's 1934 novel, "Murder on the Orient Express" was written in the Hotel Pera Palace in Istanbul, Turkey, the southern terminus of the railway. The hotel maintains Christie's room as a memorial to the author. The Greenway Estate in Devon, acquired by the couple as a summer residence in 1938, is now in the care of the National Trust. Christie often stayed at Abney Hall in Cheshire, which was owned by her brother-in-law, James Watts. She based at least two of her stories on the hall: The short story "The Adventure of the Christmas Pudding", which is in the story collection of the same name, and the novel "After the Funeral". "Abney became Agatha's greatest inspiration for country-house life, with all the servants and grandeur which have been woven into her plots. The descriptions of the fictional Styles, Chimneys, Stoneygates and the other houses in her stories are mostly Abney in various forms." During the Second World War, Christie worked in the pharmacy at University College Hospital of University College, London, where she acquired a knowledge of poisons that she put to good use in her post-war crime novels. For example, the use of thallium as a poison was suggested to her by UCH Chief Pharmacist Harold Davis (later appointed Chief Pharmacist at the UK Ministry of Health), and in "The Pale Horse", published in 1961, she employed it to dispatch a series of victims, the first clue to the murder method coming from the victims’ loss of hair. So accurate was her description of thallium poisoning that on more than one occasion it helped solve cases that were baffling doctors. To honour her many literary works, she was appointed Commander of the Order of the British Empire in the 1956 New Year Honours. The next year, she became the President of the Detection Club. In the 1971 New Year Honours she was promoted Dame Commander of the Order of the British Empire, three years after her husband had been knighted for his archeological work in 1968. They were one of the few married couples where both partners were honoured in their own right. From 1971 to 1974, Christie's health began to fail; however, she continued to write. Recently, using experimental, computerized, textual tools of analysis, Canadian researchers have suggested that Christie may have begun to suffer from Alzheimer's disease or other dementia. In 1975, sensing her increasing weakness, Christie signed over the rights of her most successful play, "The Mousetrap", to her grandson. Agatha Christie died on 12 January 1976, at age 85, from natural causes, at her Winterbrook House in the north of Cholsey parish, adjoining Wallingford in Oxfordshire (formerly Berkshire). She is buried in the nearby churchyard of St Mary's, Cholsey. Christie's only child, Rosalind Margaret Hicks died, also aged 85, on 28 October 2004 from natural causes, in Torbay, Devon. Christie's grandson, Mathew Prichard, was heir to the copyright to some of his grandmother's literary work (including "The Mousetrap") and is still associated with Agatha Christie Limited. Hercule Poirot and Miss Marple. Agatha Christie's first novel "The Mysterious Affair at Styles" was published in 1920 and introduced the long-running character detective Hercule Poirot, who appeared in 33 of Christie's novels and 54 short stories. Her other well known character, Miss Marple, was introduced in "The Tuesday Night Club" in 1927 (short story), and was based on women like Christie's grandmother and her "cronies". During the Second World War, Christie wrote two novels, "Curtain" and "Sleeping Murder", intended as the last cases of these two great detectives, Hercule Poirot and Jane Marple, respectively. Both books were sealed in a bank vault for over thirty years, and were released for publication by Christie only at the end of her life, when she realized that she could not write any more novels. These publications came on the heels of the success of the film version of "Murder on the Orient Express" in 1974. Like Arthur Conan Doyle with Sherlock Holmes, Christie was to become increasingly tired of her detective, Poirot. In fact, by the end of the 1930s, Christie confided to her diary that she was finding Poirot “insufferable," and by the 1960s she felt that he was "an ego-centric creep." However, unlike Conan Doyle, Christie resisted the temptation to kill her detective off while he was still popular. She saw herself as an entertainer whose job was to produce what the public liked, and the public liked Poirot. In contrast, Christie was fond of Miss Marple. However, it is interesting to note that the Belgian detective’s titles outnumber the Marple titles by more than two to one. This is largely because Christie wrote numerous Poirot novels early in her career, while "The Murder at the Vicarage" remained the sole Marple novel until the 1940s. Christie never wrote a novel or short story featuring both Poirot and Miss Marple. In a recording, recently re-discovered and released in 2008, Christie revealed the reason for this: "Hercule Poirot, a complete egoist, would not like being taught his business or having suggestions made to him by an elderly spinster lady". Poirot is the only fictional character to have been given an obituary in "The New York Times", following the publication of "Curtain" in 1975. Following the great success of "Curtain", Christie gave permission for the release of "Sleeping Murder" sometime in 1976, but died in January 1976 before the book could be released. This may explain some of the inconsistencies compared to the rest of the [Marple series — for example, Colonel Arthur Bantry, husband of Miss Marple's friend, Dolly, is still alive and well in "Sleeping Murder" despite the fact he is noted as having died in books published earlier. It may be that Christie simply did not have time to revise the manuscript before she died. Miss Marple fared better than Poirot, since after solving the mystery in "Sleeping Murder" she returns home to her regular life in St. Mary Mead. On an edition of "Desert Island Discs" in 2007, Brian Aldiss claimed that Agatha Christie told him that she wrote her books up to the last chapter, and then decided who the most unlikely suspect was. She would then go back and make the necessary changes to "frame" that person. The evidence of Christie's working methods, as described by successive biographers, contradicts this claim.
Almost all of Agatha Christie’s books are whodunits, focusing on the English middle and upper classes. Usually, the detective either stumbles across the murder or is called upon by an old acquaintance, who is somehow involved. Gradually, the detective interrogates each suspect, examines the scene of the crime and makes a note of each clue, so readers can analyze it and be allowed a fair chance of solving the mystery themselves. Then, about halfway through, or sometimes even during the final act, one of the suspects usually dies, often because they have inadvertently deduced the killer's identity and need silencing. In a few of her novels, including "Death Comes as the End" and "And Then There Were None", there are multiple victims. Finally, the detective organizes a meeting of all the suspects and slowly denounces the guilty party, exposing several unrelated secrets along the way, sometimes over the course of thirty or so pages. The murders are often extremely ingenious, involving some convoluted piece of deception. Christie’s stories are also known for their taut atmosphere and strong psychological suspense, developed from the deliberately slow pace of her prose. Twice, the murderer surprisingly turns out to be the unreliable narrator of the story. In four stories, Christie allows the murderer to escape justice (and in the case of the last three, implicitly almost approves of their crimes); these are "The Witness for the Prosecution", "Murder on the Orient Express", "Curtain" and "The Unexpected Guest". After the dénouement of "Taken at the Flood", her sleuth Poirot has the guilty party arrested for the lesser crime of manslaughter. (When Christie adapted "Witness" into a stage play, she lengthened the ending so that the murderer was also killed.) There are also numerous instances where the killer is not brought to justice in the legal sense but instead dies (death usually being presented as a more 'sympathetic' outcome), for example "Death on the Nile", "The Murder of Roger Ackroyd", "Crooked House", "Appointment with Death" and "The Hollow". In some cases this is with the collusion of the detective involved. "Five Little Pigs", and arguably "Ordeal by Innocence", end with the question of whether formal justice will be done unresolved.
Agatha Christie was revered as a master of suspense, plotting, and characterization by most of her contemporaries and, even today, her stories have received glowing reviews in most literary circles. Fellow crime writer Anthony Berkeley Cox was an admitted fan of her work, once saying that nobody can write an Agatha Christie novel but the authoress herself. However, she does have her detractors, most notably the American novelist Raymond Chandler, who criticised her in his essay, "The Simple Art of Murder", and the American literary critic Edmund Wilson, who was dismissive of Christie and the detective fiction genre generally in his "New Yorker" essay, "Who Cares Who Killed Roger Ackroyd?".
Christie occasionally inserted stereotyped descriptions of characters into her work, particularly before the end of the Second World War (when such attitudes were more commonly expressed publicly), and particularly in regard to Italians, Jews, and non-Europeans generally. For example, in the first editions of the collection "The Mysterious Mr Quin" (1930), in the short story "The Soul of the Croupier," she described "Hebraic men with hook-noses wearing rather flamboyant jewellery"; in later editions the passage was edited to describe "sallow men" wearing same.
Christie has been portrayed on a number of occasions in film and television. Several biographical programs have been made, such as the 2004 BBC television program entitled ', in which she is portrayed by Olivia Williams, Anna Massey, and Bonnie Wright. Christie has also been portrayed fictionally. Some of these have explored and offered accounts of Christie's disappearance in 1926, including the 1979 film "Agatha" (with Vanessa Redgrave) and the "Doctor Who" episode "The Unicorn and the Wasp" (with Fenella Woolgar). Others, such as 1980 Hungarian film, "Kojak Budapesten" (not to be confused with the 1986 comedy by the same name) create their own scenarios involving Christie's criminal skill. In the 1986 TV play, "Murder by the Book", Christie herself (Peggy Ashcroft) murdered one of her fictional-turned-real characters, Poirot. Christie has also been parodied on screen, such as in the film "Murder by Indecision", which featured the character "Agatha Crispy".
"The Plague" ("Fr." "La Peste") is a novel by Albert Camus, published in 1947, that tells the story of medical workers finding solidarity in their labour as the Algerian city of Oran is swept by a plague epidemic. It asks a number of questions relating to the nature of destiny and the human condition. The characters in the book, ranging from doctors to vacationers to fugitives, all help to show the effects the plague has on a populace. The novel is believed to be based on the cholera epidemic that killed a large percentage of Oran's population in 1849 following French colonization, but the novel is placed in the 1940s. Oran and its environs were struck by disease multiple times before Camus published this novel. According to a by the Centers for Disease Control and Prevention, Oran was decimated by the plague in 1556 and 1678, but outbreaks after European colonization, in 1921 (185 cases), 1931 (76 cases), and 1944 (95 cases), were very far from the scale of the epidemic described in the novel. "The Plague" is considered an existentialist classic despite Camus' objection to the label. The narrative tone is similar to Kafka's, especially in "The Trial," where individual sentences potentially have multiple meanings, the material often pointedly resonating as stark allegory of phenomenal consciousness and the human condition. Camus included a dim-witted character misreading "The Trial" as a mystery novel as an oblique homage. The novel has been read as a metaphorical treatment of the French resistance to Nazi occupation during World War II. Although Camus's approach in the book is severe, his narrator emphasizes the ideas that we ultimately have no control, irrationality of life is inevitable, and he further illustrates the human reaction towards the ‘absurd’. "The Plague" represents how the world deals with the philosophical notion of the Absurd, a theory which Camus himself helped to define.
In the town of Oran, thousands of rats, initially going unnoticed by the populace, began to die in the streets. A hysteria develops soon after, causing the local newspapers to report the incident. Authorities responding to public pressure order the collection and cremation of the rats, unaware that the collection itself was the catalyst for the spread of the bubonic plague. The main character, Dr. Bernard Rieux, lives comfortably in an apartment building when strangely the building's concierge, M. Michel, a confidante, dies from a fever. Dr. Rieux consults his colleague, Castel, about the illness until they come to the conclusion that a plague is sweeping the town. They both approach fellow doctors and town authorities about their theory, but are eventually dismissed on the basis of one death. However, as more and more deaths quickly ensue, it becomes apparent that there is an epidemic. Authorities are slow to accept that the situation is serious and quibble over the appropriate action to take. Official notices enacting control measures are posted, but the language used is optimistic and downplays the seriousness of the situation. A "special ward" is opened at the hospital, but its 80 beds are filled within three days. As the death toll begins to rise, more desperate measures are taken. Homes are quarantined, corpses and burials are strictly supervised. A supply of plague serum finally arrives, but there is only enough to treat existing cases and the country's emergency reserves are depleted. When the daily number of deaths jumps to 30, the town is sealed and an outbreak of plague is officially declared.
The town is sealed off. The town gates are shut, rail travel is prohibited, and all mail service is suspended. The use of telephone lines is restricted only to "urgent" calls, leaving short telegrams as the only means of communicating with friends or family outside the town. The separation affects daily activity and depresses the spirit of the townspeople, who begin to feel isolated and introverted, and the plague begins to affect various characters. One character, Raymond Rambert, devises a plan to escape the city to join his lover in Paris after city officials refused his request to leave. He befriends some criminals so that they may smuggle him out of the city. Another character, Father Paneloux, uses the plague as an opportunity to advance his stature in the town by suggesting that the plague was an act of God for the citizens' sinful nature. His diatribe falls on the ears of many citizens of the town, who turned to religion in droves and who would not have done so under normal circumstances. Cottard, a criminal remorseful enough to attempt suicide yet fearful of being arrested, becomes wealthy as a major smuggler. Meanwhile, Dr. Rieux, a vacationer Jean Tarrou, and a civil servant Joseph Grand exhaustively treat patients in their homes and in the hospital. Rambert informs Tarrou of his escape plan, but when Tarrou tells him that others in the city, including Dr. Rieux, also have loved ones outside the city that they are not allowed to see, Rambert becomes sympathetic and changes his mind. He then decides to join Tarrou and Dr. Rieux to help fight the epidemic.
In mid-August, the situation continues to worsen. People try to escape the town, but some are shot by armed sentries. Violence and looting break out on a small scale, and the authorities respond by declaring martial law and imposing a curfew. Funerals are conducted with more and more speed, no ceremony, and little concern for the feelings of the families of the deceased. The inhabitants passively endure their increasing feelings of exile and separation; despondent, they waste away emotionally as well as physically.
In September and October, the town remains at the mercy of the plague. Rieux hears from the sanatorium that the condition of his wife is worsening. He also hardens his heart regarding the plague victims so that he can continue to do his work. Cottard, on the other hand, seems to flourish during the plague, because it gives him a sense of being connected to others, since everybody faces the same danger. Cottard and Tarrou attend a performance of Gluck's opera, Orpheus and Eurydice, but the actor portraying Orpheus collapses with plague symptoms during the performance. Rambert finally has a chance to escape, but he decides to stay, saying that he would feel ashamed of himself if he left. Towards the end of October, Castel's new anti-plague serum is tried for the first time, but it cannot save the life of Othon's young son, who suffers greatly, as Paneloux, Rieux, and Tarrou look on in horror. Paneloux, who has joined the group of volunteers fighting the plague, gives a second sermon. He addresses the problem of an innocent child's suffering and says it is a test of a Christian's faith, since it requires him either to deny everything or believe everything. He urges the congregation not to give up the struggle but to do everything possible to fight the plague. A few days after the sermon, Paneloux is taken ill. His symptoms do not conform to those of the plague, but the disease still proves fatal. Tarrou and Rambert visit one of the isolation camps, where they meet Othon. When Othon's period of quarantine ends, he elects to stay in the camp as a volunteer because this will make him feel less separated from his dead son. Tarrou tells Rieux the story of his life, and the two men go swimming together in the sea. Grand catches the plague and instructs Rieux to burn all his papers. But Grand makes an unexpected recovery, and deaths from the plague start to decline.
By late January, the plague is in full retreat, and the townspeople begin to celebrate the imminent opening of the town gates. Othon, however, does not escape death from the disease. Cottard is distressed by the ending of the epidemic, from which he has profited by shady dealings. Two government employees approach him, and he flees. Despite the ending of the epidemic, Tarrou contracts the plague and dies after a heroic struggle. Rieux's wife also dies. In February, the town gates open and people are reunited with their loved ones from other cities. Rambert is reunited with his wife. Rieux reveals that he is the narrator of the chronicle and that he tried to present an objective view of the events. Cottard goes mad and shoots at people from his home. He is arrested. Grand begins working on his book again. Rieux reflects on the epidemic and reaches the conclusion that there is more to admire than to despise in humans.
Cottard's personality changes after the outbreak of plague. Whereas he was aloof and mistrustful before, he now becomes agreeable and tries hard to make friends. He appears to relish the coming of the plague, and Tarrou thinks this is because he finds it easier to live with his own fears now that everyone else is in a state of fear, too. Cottard takes advantage of the crisis to make money by selling contraband cigarettes and inferior liquor. When the epidemic ends, Cottard's moods fluctuate. Sometimes he is sociable, but at other times he shuts himself up in his room. Eventually, he loses his mental balance and shoots at random at people on the street. The police arrest him. Grand is a neighbor of Cottard, and it is he who calls Rieux for help, when Cottard tries to commit suicide. When the plague takes a grip on the town, Grand joins the team of volunteers, acting as general secretary, recording all the statistics. Rieux regards him as "the true embodiment of the quiet courage that inspired the sanitary groups." Grand catches the plague himself and asks Rieux to burn his manuscript. But then he makes an unexpected recovery. At the end of the novel, Grand says he is much happier; he has written to Jeanne and made a fresh start on his book. During the epidemic, Rieux heads an auxiliary hospital and works long hours treating the victims. He injects serum and lances the abscesses, but there is little more that he can do, and his duties weigh heavily upon him. He never gets home until late, and he has to distance himself from the natural pity that he feels for the victims; otherwise, he would not be able to go on. It is especially hard for him when he visits a victim in the person's home, because he knows that he must immediately call for an ambulance and have the person removed from the house. Often the relatives plead with him not to do this, since they know they may never see the person again. Rieux works to combat the plague simply because he is a doctor and his job is to relieve human suffering. He does not do it for any grand, religious purpose, like Paneloux (Rieux does not believe in God), or as part of a high-minded moral code, like Tarrou. He is a practical man, doing what needs to be done without any fuss, even though he knows that the struggle against death is something that he can never win. It is Tarrou who first comes up with the idea of organizing teams of volunteers to fight the plague. He wants to do this before the authorities begin to conscript people, and he does not like the official plan to get prisoners to do the work. He takes action, prompted by his own code of morals; he feels that the plague is everybody's responsibility and that everyone should do his or her duty. What interests him, he tells Rieux, is how to become a saint, even though he does not believe in God. Later in the novel, Tarrou tells Rieux, with whom he has become friends, the story of his life. His father, although a kind man in private, was also an aggressive prosecuting attorney who tried death penalty cases, arguing strongly for the death penalty to be imposed. As a young boy, Tarrou attended one day of a criminal proceeding in which a man was on trial for his life. However, the idea of capital punishment disgusted him. After he left home before the age of eighteen, his main interest in life was his opposition to the death penalty, which he regarded as state-sponsored murder. When the plague epidemic is virtually over, Tarrou becomes one of its last victims, but he puts up a heroic struggle before dying.
Some, like Rambert, are exiles in double measure since they are not only cut off from those they want to be with but they do not have the luxury of being in their own homes. The feeling of exile produces many changes in attitudes and behaviors. At first, people indulge in fantasies, imagining the missing person's return, but then they start to feel like prisoners, drifting through life with nothing left but the past, since they do not know how long into the future their ordeal may last. And the past smacks only of regret, of things left undone. Living with the sense of abandonment, they find that they cannot communicate their private grief to their neighbors, and conversations tend to be superficial. Rieux returns to the theme at the end of the novel, after the epidemic is over, when the depth of the feelings of exile and deprivation is clear from the overwhelming joy with which long parted lovers and family members greet each other. For some citizens, exile was a feeling more difficult to pin down. They simply desired a reunion with something that could hardly be named but which seemed to them to be the most desirable thing on Earth. Some called it peace. Rieux numbers Tarrou among such people, although he found it only in death. This understanding of exile suggests the deeper, metaphysical implications of the term. It relates to the loss of the belief that humans live in a rational universe in which they can fulfill their hopes and desires, find meaning, and be at home. As Camus put it in The Myth of Sisyphus, "In a universe that is suddenly deprived of illusions and of light, man feels a stranger. His is an irremediable exile." The ravages of the plague in Oran vividly convey the absurdist position that humans live in an indifferent, incomprehensible universe that has no rational meaning or order, and no transcendent God. The plague comes unannounced and may strike down anyone at any time. It is arbitrary and capricious, and it leaves humans in a state of fear and uncertainty, which ends only in death. In the face of this metaphysical reality, what must be the response of individuals? Should they resign themselves to it, accept it as inevitable, and seek what solace they can as individuals, or should they join with others and fight back, even though they must live with the certainty that they cannot win? Camus's answer is clearly the latter, embodied in the characters of Rieux, Rambert, and Tarrou. Rieux's position is made clear in part II in a conversation with Tarrou. Rieux argues that one would have to be a madman to submit willingly to the plague. Rather than accepting the natural order of things — the presence of sickness and death — he believes one must fight against them. He is aware of the needs of the community; he does not live for himself alone. When Tarrou points out that "[his] victories will never be lasting," Rieux admits that he is involved in a "never ending defeat," but this does not stop him from engaging in the struggle. Rieux is also aware that working for the common good demands sacrifice; he cannot expect personal happiness. This is a lesson that Rambert learns. At first he insists that he does not belong in Oran, and his only thought is returning to the woman he loves in Paris. He thinks only of his own personal happiness and the unfairness of the situation in which he has been placed. But gradually he comes to recognize his membership of the larger human community, which makes demands on him that he cannot ignore. His personal happiness becomes less important than his commitment to helping the community. In times of calamity, people often turn to religion, and Camus examines this response in the novel. In contrast to the humanist beliefs of Rieux, Rambert, and Tarrou, the religious perspective is given in the sermons of the stern Jesuit priest, Father Paneloux. While the other main characters believe there is no rational explanation for the outbreak of plague, Paneloux believes there is. In his first sermon, given during the first month of the plague, Paneloux describes the epidemic as the "flail of God," through which God separates the wheat from the chaff, the good from the evil. Paneloux is at pains to emphasize that God did not will the calamity: "He looked on the evil-doing in the town with compassion; only when there was no other remedy did He turn His face away, in order to force people to face the truth about their life" In Paneloux's view, even the terrible suffering caused by the plague works ultimately for good. The divine light can still be seen even in the most catastrophic events, and a Christian hope is granted to all. Paneloux's argument is based on the theology of St. Augustine, on which he is an expert, and it is accepted as irrefutable by many of the townspeople, including the magistrate, Othon. But it does not satisfy Rieux. Camus carefully manipulates the plot to bring up the question of innocent suffering. Paneloux may argue that the plague is a punishment for sin, but how does he reconcile that doctrine with the death of a child? The child in question is Jacques Othon, and Paneloux, along with Rieux and Tarrou, witnesses his horrible death. Paneloux is moved with compassion for the child, and he takes up the question of innocent suffering in his second sermon. He argues that because a child's suffering is so horrible and cannot easily be ex-plained, it forces people into a crucial test of faith: either we must believe everything or we must deny everything, and who, Paneloux asks, could bear to do the latter? We must yield to the divine will, he says; we cannot pick and choose and accept only what we can understand. But we must still seek to do what good lies in our power (as Paneloux himself does as one of the volunteers who fights the plague). When Paneloux contracts the plague himself, he refuses to call a doctor. He dies according to his principles, trusting in the providence of God and not fighting against his fate. This is in contrast to Tarrou, who fights valiantly against death when his turn comes. It is clear that Camus's sympathy in this contrast of ideas lies with Rieux and Tarrou, but he also treats Paneloux with respect.
This story is told through the character Rieux. However, Rieux does not function as a first-person narrator. Rather he disguises himself, referring to himself in the third person and only at the end of the novel reveals who he is. The novel thus appears to be told by an unnamed narrator who gathers information from what he has personally seen and heard regarding the epidemic, as well as from the diary of another character, Tarrou, who makes observations about the events he witnesses. The reason Rieux does not declare himself earlier is that he wants to give an objective account of the events in Oran. He deliberately adopts the tone of an impartial observer. Rieux is like a witness who exercises restraint when called to testify about a crime; he describes what the characters said and did, without speculating about their thoughts and feelings, although he does offer generalized assessments of the shifting mood of the town as a whole. Rieux refers to his story as a chronicle, and he sees himself as an historian, which justifies his decision to stick to the facts and avoid subjectivity. This also explains why the style of The Plague often gives the impression of distance and detachment. Only rarely is the reader drawn directly into the emotions of the characters or the drama of the scene.
An allegory is a narrative with two distinct levels of meaning. The first is the literal level; the second signifies a related set of concepts and events. The Plague is in part an historical allegory, in which the plague signifies the German occupation of France from 1940 to 1944 during World War II. There are many aspects of the narrative that make the allegory plain. The town Oran, which gets afflicted by pestilence and cut off from the outside world, is the equivalent of France. Camus draws from his own experience of isolation during the war in writing "The Plague". The citizens are slow to realize the magnitude of the danger because they do not believe in pestilence or that it could happen to them, just as the French were complacent at the beginning of the war. They could not imagine that the Germans, whom they had defeated only twenty years previously, could defeat them in a mere six weeks, as happened when France fell in June 1940. The different attitudes of the characters reflect different attitudes in the French population during the occupation. Some were the equivalent of Paneloux and thought that France was to blame for the calamity that had befallen it. They believed that the only solution was to submit gracefully to an historical inevitability — the long-term dominance of Europe by Germany. Many people, however, became members of the French Resistance, and they are the allegorical equivalents of the voluntary sanitary teams in the novel, such as Tarrou, Rambert, and Grand, who fight back against the unspeakable evil (the Nazi occupiers). Some French collaborated with the Germans. In the novel, they are represented by Cottard, who welcomes the plague and uses the economic deprivation that results from it to make a fortune buying and selling on the black market. Other details in the novel can be read at the allegorical level. The plague that carries people off unexpectedly echoes the reality of the occupation, in which people could be snatched from their homes by the Gestapo and imprisoned or sent to work as slave labor in German-controlled territories or simply killed. The facts of daily life in the plague-stricken city resemble life in wartime France: the showing of reruns at the cinemas, the stockpiling of scarce goods, nighttime curfews and isolation camps (these paralleling the German internment camps). The scenes at the end of the novel, when Oran's gates are reopened, recall the jubilant scenes in Paris when the city was liberated in 1944. In some places, Camus makes the allegory explicit, as when he refers to the plague in terms that describe an enemy in war: "the epidemic was in retreat all along the line; victory was won and the enemy was abandoning his positions."
Imagery of the sea is often used in Camus's works to suggest life, vigor, and freedom. In The Plague, a key description of Oran occurs early, when it is explained that the town is built in such a way that it "turns its back on the bay, with the result that it's impossible to see the sea, you always have to go to look for it." Symbolically, Oran turns its back on life. When the plague hits, the deprivation of this symbol of freedom becomes more pronounced, as the beaches are closed, as is the port. In summer, the inhabitants lose touch with the sea altogether: "for all its nearness, the sea was out of bounds; young limbs had no longer the run of its delights." A significant episode occurs near the end of part IV, when Tarrou and Rieux sit on the terrace of a house, from which they can see far into the horizon. As he gazes seaward, Tarrou says with a sense of relief that it is good to be there. To set a seal on the friendship between the two men, they go for a swim together. This contact with the ocean is presented as a moment of renewal, harmony, and peace. It is one of the few lyrical episodes in the novel: "[T]hey saw the sea spread out before them, a gently heaving expanse of deep-piled velvet, supple and sleek as a creature of the wild." Just before Rieux enters the water, he is possessed by a "strange happiness," a feeling that is shared by Tarrou. There is a peaceful image of Rieux lying motionless on his back gazing up at the stars and moon, and then when Tarrou joins him they swim side by side, "with the same zest, the same rhythm, isolated from the world, at last free of the town and of the plague."
Applied ethics is, in the words of Brenda Almond, co-founder of the Society for Applied Philosophy, "the philosophical examination, from a moral standpoint, of particular issues in private and public life that are matters of moral judgment". It is thus a term used to describe attempts to use philosophical methods to identify the morally correct course of action in various fields of human life. Bioethics, for example, is concerned with identifying the correct approach to matters such as euthanasia, or the allocation of scarce health resources, or the use of human embryos in research. Environmental ethics is concerned with questions such as the duties of humans towards landscapes or species. Business ethics concerns questions such as the limits on managers in the pursuit of profit, or the duty of 'whistleblowers' to the general public as opposed to their employers. As such, it is a study which is supposed to involve practitioners as much as professional philosophers. Applied ethics is distinguished from normative ethics, which concerns what people should believe to be right and wrong, and from meta-ethics, which concerns the nature of moral statements.
One modern approach which attempts to overcome the seemingly impossible divide between deontology and utilitarianism is case-based reasoning, also known as casuistry. Casuistry does not begin with theory, rather it starts with the immediate facts of a real and concrete case. While casuistry makes use of ethical theory, it does not view ethical theory as the most important feature of moral reasoning. Casuists, like Albert Jonsen and Stephen Toulmin ("The Abuse of Casuistry" 1988), challenge the traditional paradigm of applied ethics. Instead of starting from theory and applying theory to a particular case, casuists start with the particular case itself and then ask what morally significant features (including both theory and practical considerations) ought to be considered for that particular case. In their observations of medical ethics committees, Jonsen and Toulmin note that a consensus on particularly problematic moral cases often emerges when participants focus on the facts of the case, rather than on ideology or theory. Thus, a Rabbi, a Catholic priest, and an agnostic might agree that, in this particular case, the best approach is to withhold extraordinary medical care, while disagreeing on the reasons that support their individual positions. By focusing on cases and not on theory, those engaged in moral debate increase the possibility of agreement.
In mathematics, the absolute value (or modulus'") |"a"| of a real number "a" is "a"s numerical value without regard to its sign. So, for example, 3 is the absolute value of both 3 and −3. Generalizations of the absolute value for real numbers occur in a wide variety of mathematical settings. For example an absolute value is also defined for the complex numbers, the quaternions, ordered rings, fields and vector spaces. The absolute value is closely related to the notions of magnitude, distance, and norm in various mathematical and physical contexts.
Jean-Robert Argand introduced the term "module" 'unit of measure' in French in 1806 specifically for the "complex" absolute value and it was borrowed into English in 1866 as the Latin equivalent "modulus". The term "absolute value" has been used in this sense since at least 1806 in French and 1857 in English. The notation | "a" | was introduced by Karl Weierstrass in 1841. Other names for "absolute value" include "the numerical value" and "the magnitude".
For any real number "a" the absolute value or modulus of "a" is denoted by | "a" | (a vertical bar on each side of the quantity) and is defined as As can be seen from the above definition, the absolute value of "a" is always either positive or zero, but never negative. The same notation is used with sets to denote cardinality; the meaning depends on context. From an analytic geometry point of view, the absolute value of a real number is that number's distance from zero along the real number line, and more generally the absolute value of the difference of two real numbers is the distance between them. Indeed the notion of an abstract distance function in mathematics can be seen to be a generalization of the absolute value of the difference (see "Distance" below). Since the square-root notation without sign represents the "positive" square root, which is sometimes even used as a definition of absolute value.
can be seen as motivating the following definition. where "x" and "y" are real numbers, the absolute value or modulus of "z" is denoted |"z"| and is defined as Similar to the geometric interpretation of the absolute value for real numbers, it follows from the Pythagorean theorem that the absolute value of a complex number is the distance in the complex plane of that complex number from the origin, and more generally, that the absolute value of the difference of two complex numbers is equal to the distance between those two complex numbers. The complex absolute value shares all the properties of the real absolute value given in (2)–(10) above. In addition, If is the complex conjugate of "z", then it is easily seen that with the last formula being the complex analogue of equation (1) mentioned above in the real case. The absolute square of "z" is defined as Since the positive reals form a subgroup of the complex numbers under multiplication, we may think of absolute value as an endomorphism of the multiplicative group of the complex numbers.
The real absolute value function is continuous everywhere. It is differentiable everywhere except for "x" = 0. It is monotonically decreasing on the interval (−∞, 0] and monotonically increasing on the interval [0, ∞). Since a real number and its negative have the same absolute value, it is an even function, and is hence not invertible. The complex absolute value function is continuous everywhere but (complex) differentiable "nowhere"; it violates the Cauchy-Riemann equations. Both the real and complex functions are idempotent.
The derivative of the real absolute value function is the signum function, sgn("x"), which is defined as for "x" ≠ 0. The absolute value function is not differentiable at "x" = 0. For applications in which a well-defined derivative may be needed, however, the subderivative is well defined at zero. Where the absolute value function of a real number returns a value without respect to its sign, the signum function returns a number's sign without respect to its value. Therefore "x" = sgn("x")abs("x"). where the value of the Heaviside function at zero is conventional. So for all nonzero points on the real number line, The absolute value function has no concavity at any point, the sign function is constant at all points. Therefore the second derivative of |"x"| with respect to "x" is zero everywhere except zero, where it is undefined. The absolute value function is also integrable. Its antiderivative is
The absolute value is closely related to the idea of distance. As noted above, the absolute value of a real or complex number is the distance from that number to the origin, along the real number line, for real numbers, or in the complex plane, for complex numbers, and more generally, the absolute value of the difference of two real or complex numbers is the distance between them. The standard Euclidean distance between two points This can be seen to be a generalization of | "a" − "b" |, since if "a" and "b" are real, then by equation (1), The above shows that the "absolute value" distance for the real numbers or the complex numbers, agrees with the standard Euclidean distance they inherit as a result of considering them as the one and two-dimensional Euclidean spaces respectively.
The fundamental properties of the absolute value for real numbers given in (2)–(5) above, can be used to generalize the notion of absolute value to an arbitrary field, as follows. Where 0 denotes the additive identity element of "F". It follows from positive-definiteness and multiplicativeness that "v"(1) = 1, where 1 denotes the multiplicative identity element of "F". The real and complex absolute values defined above are examples of absolute values for an arbitrary field. An absolute value which satisfies any (hence all) of the above conditions is said to be non-Archimedean, otherwise it is said to be Archimedean.
Again the fundamental properties of the absolute value for real numbers can be used, with a slight modification, to generalize the notion to an arbitrary vector space. For all "a" in "F", and v, u in "V", The norm of a vector is also called its "length" or "magnitude". In the case of Euclidean space R"n", the function defined by is a norm called the Euclidean norm. When the real numbers R are considered as the one-dimensional vector space R1, the absolute value is a norm, and is the "p"-norm for any "p". In fact the absolute value is the "only" norm on R1, in the sense that, for every norm || · || on R'"1, || "x" || = || 1 || · | "x" |. The complex absolute value is a special case of the norm in an inner product space. It is identical to the Euclidean norm, if the complex plane is identified with the Euclidean plane R2.
codice_1 extends the sign bit of codice_2 into codice_3. If codice_2 is nonnegative, then codice_3 becomes zero, and the latter two instructions have no effect, leaving codice_2 unchanged. If codice_2 is negative, then codice_3 becomes codice_9, or −1. The next two instructions then become a two's complement inversion, giving the absolute value of the negative value in codice_2. Note that the smallest negative value (−231 or codice_11), which has no corresponding positive encoding, returns itself, which is accurate when taken as an unsigned integer.
An Analog or analogue signal is any continuous signal for which the time varying feature (variable) of the signal is a representation of some other time varying quantity, i.e., analogous to another time varying signal. It differs from a digital signal in terms of small fluctuations in the signal which are meaningful. Analog is usually thought of in an electrical context; however, mechanical, pneumatic, hydraulic, and other systems may also convey analog signals. An analog signal uses some property of the medium to convey the signal's information. For example, an aneroid barometer uses rotary position as the signal to convey pressure information. Electrically, the property most commonly used is voltage followed closely by frequency, current, and charge. Any information may be conveyed by an analog signal; often such a signal is a measured response to changes in physical phenomena, such as sound, light, temperature, position, or pressure, and is achieved using a transducer. For example, in sound recording, fluctuations in air pressure (that is to say, sound) strike the diaphragm of a microphone which induces corresponding fluctuations in the current produced by a coil in an electromagnetic microphone, or the voltage produced by a condensor microphone. The voltage or the current is said to be an "analog" of the sound. An analog signal has a theoretically infinite resolution. In practice an analog signal is subject to noise and a finite slew rate. Therefore, both analog and digital systems are subject to limitations in resolution and bandwidth. As analog systems become more complex, effects such as non-linearity and noise ultimately degrade analog resolution to such an extent that the performance of digital systems may surpass it. Similarly, as digital systems become more complex, errors can occur in the digital data stream. A comparable performing digital system is more complex and requires more bandwidth than its analog counterpart. In analog systems, it is difficult to detect when such degradation occurs. However, in digital systems, degradation can not only be detected but corrected as well.
The main advantage is the fine definition of the analog signal which has the potential for an infinite amount of signal resolution. Compared to digital signals, analog signals are of higher density. Another advantage with analog signals is that their processing may be achieved more simply than with the digital equivalent. An analog signal may be processed directly by analog components, though some processes aren't available except in digital form.
The primary disadvantage of analog signaling is that any system has noise – i.e., random unwanted variation. As the signal is copied and re-copied, or transmitted over long distances, these apparently random variations become dominant. Electrically, these losses can be diminished by shielding, good connections, and several cable types such as coaxial or twisted pair. The effects of noise create signal loss and distortion. This is impossible to recover, since amplifying the signal to recover attenuated parts of the signal amplifies the noise (distortion/interference) as well. Even if the resolution of an analog signal is higher than a comparable digital signal, the difference can be overshadowed by the noise in the signal. Most of the analog systems also suffer from generation loss.
Another method of conveying an analog signal is to use modulation. In this, some base signal (e.g., a sinusoidal carrier wave) has one of its properties modulated: amplitude modulation involves altering the amplitude of a sinusoidal voltage waveform by the source information, frequency modulation changes the frequency. Other techniques, such as changing the phase of the base signal also work. Analog circuits do not involve quantisation of information into digital format. The concept being measured over the circuit, whether sound, light, pressure, temperature, or an exceeded limit, remains from end to end. See digital for a discussion of "digital vs. analog". Sources: Parts of an earlier version of this article were originally taken from Federal Standard 1037C in support of MIL-STD-188.
Hercule Poirot (;) is a fictional Belgian detective created by Agatha Christie. Along with Miss Marple, Poirot is one of Christie's most famous and long-lived characters, appearing in 33 novels and 51 short stories that were published between 1920 and 1975 and set in the same era. Poirot has been portrayed on screen, for films and television, by various actors including Albert Finney, Peter Ustinov, Ian Holm, Tony Randall, Alfred Molina and David Suchet.
His name was derived from two other fictional detectives of the time: Marie Belloc Lowndes' Hercule Popeau and Frank Howel Evans' Monsieur Poiret, a retired Belgian police officer living in London. Hercule Poirot's initials replicate that of the sauce which he happens to like, HP Brown Sauce as he comments in "Elephants Can Remember": "Ah yes, that is correct my initials do appear to be the same as such a fine delicacy, a good English creation." A more obvious influence on the early Poirot stories is that of Arthur Conan Doyle. In "An Autobiography" Christie admits that "I was still writing in the Sherlock Holmes tradition –eccentric detective, stooge assistant, with a Lestrade-type Scotland Yard detective, Inspector Japp." For his part Conan Doyle acknowledged basing Sherlock Holmes on the model of Edgar Allan Poe's fictional French detective C. Auguste Dupin, who in his use of "ratiocination" prefigures Poirot's reliance on his "little grey cells". Poirot also bears a striking resemblance to A. E. W. Mason's fictional detective—Inspector Hanaud of the French Sûreté—who, first appearing in the 1910 novel "At the Villa Rose," predates the writing of the first Poirot novel by six years. In chapter 4 of the second Inspector Hanaud novel, "The House of the Arrow" (1924), Hanaud declares sanctimoniously to the heroine, "You are wise, Mademoiselle…For, after all, I am Hanaud. There is only one." Christie's Poirot was Belgian. Unlike the models mentioned above, Christie's Poirot character was clearly the result of her early development of the detective in her first book written in 1916 (though only published in 1920). Not only was his Belgian nationality interesting because of Belgium's occupation by Germany (which provided a valid explanation of why such a skilled detective would be out of work and available to solve mysteries at an English country house), but also at the time of Christie's writing, it was considered patriotic to express sympathy with the Belgians, since the invasion of their country had constituted Britain's "casus belli" for entering World War I, and British wartime propaganda emphasized the "Rape of Belgium".
His first published appearance was in "The Mysterious Affair at Styles" (published 1920) and his last was in "Curtain" (published 1975, the year before Christie died). On publication of this novel, Poirot was the only fictional character to be given an obituary in the "New York Times; August 6, 1975 "Hercule Poirot is Dead; Famed Belgian Detective"." By 1930, Agatha Christie found Poirot 'insufferable' and by 1960, she felt that he was a 'detestable, bombastic, tiresome, ego-centric little creep'. Yet the public loved him, and Christie refused to kill him off, claiming that it was her duty to produce what the public liked, and what the public liked was Poirot.
In the later books, the limp is not mentioned, which suggests it may have been a temporary wartime injury. Poirot has dark hair, which he dyes later in life (though many of his screen incarnations are portrayed as bald or balding) and green eyes that are repeatedly described as shining "like a cat's" when he is struck by a clever idea. Frequent mention is made of his patent-leather shoes, damage to which is frequently a subject of (for the reader, comical) misery on his part. Poirot's appearance, regarded as fastidious during his early career, is hopelessly out of fashion later in his career. Poirot is extremely punctual and carries a turnip pocket watch almost to the end of his career. He is also fastidious about his personal finances, preferring to keep a bank balance of 444 pounds, 4 shillings, and 4 pence.
In "The Mysterious Affair at Styles", Poirot operates as a fairly conventional, clue-based detective, depending on logic, which is represented in his vocabulary by two common phrases: his use of "the little grey cells" and "order and method". Irritating to Hastings is the fact that Poirot will sometimes conceal from him important details of his plans, as in "The Big Four" where Hastings is kept in the dark throughout the climax. This aspect of Poirot is less evident in the later novels, partly because there is rarely a narrator so there is no one for Poirot to mislead. As early as "Murder on the Links", where he still largely depends on clues, Poirot mocks a rival detective who focuses on the traditional trail of clues that had been established in detective fiction by the example of Sherlock Holmes: footprints, fingerprints and cigar ash. From this point on he establishes himself as a psychological detective who proceeds not by a painstaking examination of the crime scene, but by enquiring either into the nature of the victim or the murderer. Central to his behaviour in the later novels is the underlying assumption that particular crimes are only committed by particular types of people. Poirot's methods focus on getting people to talk. Early in the novels, he frequently casts himself in the role of "Papa Poirot", a benign confessor, especially to young women. Later he lies freely in order to gain the confidences of other characters, either inventing his own reason for being interested in the case or a family excuse for pursuing a line of questioning. In the later novels Christie often uses the word "mountebank" when Poirot is being assessed by other characters, showing that he has successfully passed himself off as a charlatan or fraud. All these techniques help Poirot attain his principal target: "For in the long run, either through a lie, or through truth, people were bound to give themselves away …"
Hastings first meets Poirot during his years as a private detective in Europe and almost immediately after they both arrive in England. He becomes Poirot's lifelong friend and appears in many of the novels and stories. Poirot regards Hastings as a poor private detective, not particularly intelligent, yet helpful in his way of being fooled by the criminal and for his tendency to unknowingly "stumble" onto the truth. Hastings marries and has four children - two sons and two daughters. It must also be said that Hastings is a man who is capable of great bravery and courage when the road gets rough, facing death unflinchingly when confronted by "The Big Four" and possessing unwavering loyalty towards Poirot. However, when forced to choose between Poirot and his wife in that novel, he initially chooses to betray Poirot to the Big Four so that they would not torture and kill his wife. Later, though, he tells Poirot to draw back and escape the trap. The two are an airtight team until Hastings meets and marries Dulcie Duveen, a beautiful music hall performer half his age. They later emigrate to Argentina, leaving Poirot behind as a "very unhappy old man." Poirot and Hastings are reunited in "Curtain: Poirot's Last Case," having been earlier reunited in " The ABC Murders " and "Dumb Witness" when Hastings arrives in England for business.
The detective novelist Ariadne Oliver is Agatha Christie's humorous self-caricature. Like Agatha Christie, she isn't overly fond of the detective she is most famous for creating – in Ariadne's case the Finnish sleuth Sven Hjerson. We never learn anything about her husband, but we do know that she hates alcohol and public appearances, and has a great fondness for apples until she is put off them by the events of "Hallowe'en Party". She also has a habit of constantly changing her hairstyle, and in every appearance by her much is made of the clothes and hats she wears. She has a maid called Maria who prevents the public adoration from becoming too much of a burden on her employer, but does nothing to prevent her from becoming too much of a burden on others. She has authored over fifty-six novels and she has a great dislike of people taking and modifying her story characters. She is also the only one in Poirot's universe to have noted that "It’s not natural for five or six people to be on the spot when B is murdered and all have a motive for killing B." She first met Poirot in the story "Cards on the Table" and has been bothering him ever since.
Poirot's secretary, Miss Felicity Lemon, has few human weaknesses. The only two mistakes she is ever recorded making are a typing error during the events of "Hickory Dickory Dock" and the mis-mailing of an electric bill, although in her defence she was worried about strange events surrounding her sister at the time. Poirot described her as being "Unbelievably ugly and incredibly efficient. Anything that she mentioned as worth consideration usually was worth consideration"." She is an expert on nearly everything and plans to create the perfect filing system. She also once worked for the government agent-turned-philanthropist, Parker Pyne. Whether this was during one of Poirot’s numerous retirements or before she entered his employment is unknown. In the "Agatha Christie Hour", she was portrayed by British actress Angela Easterling, while in "Agatha Christie's Poirot", she was portrayed by Pauline Moran.
Japp is an Inspector from Scotland Yard and appears in many of the stories, trying to solve the cases Poirot is working on. Japp is an outgoing, loud and sometimes inconsiderate man by nature and his relationship with the bourgeois Belgian is one of the stranger aspects of Poirot’s world. He first met Poirot in Belgium, 1904, during the Abercrombie Forgery and later that year joined forces again to hunt down a criminal known as Baron Altara. They also meet in England where Poirot often helps Japp solve a case and lets him take the credit in return for special favours. These favours usually entail being supplied with cases that would interest him.
It is difficult to draw any concrete conclusions about Poirot's family, due to the fact that Poirot often supplies false or misleading information about himself or his background in order to assist him in obtaining information relevant to a particular case. Any evidence regarding Poirot for which Poirot himself is the source is therefore most unreliable. Achille Poirot is also mentioned by Dr. Burton in the prelude to "The Labours of Hercules". Here Hercule Poirot replies that he has had a brother called Achille "only for a short space of time", so the existence of this brother remains unconfirmed even by Hercule Poirot himself. It is strongly implied that this "quiet retreat in the Ardennes" near Spa is the Poirot family home, and thus that Poirot may have been born or at least in part grew up there. On the other hand, Poirot was working undercover at the time and may have deliberately or unconsciously misled Hastings about his family connection to the area. This is all extremely vague, as Poirot is thought to be an old man in his dotage even in the early Poirot novels, and in "An Autobiography" Christie admitted that she already imagined him to be an old man in 1920. (At the time, of course, she had no idea she would be going on writing Poirot books for many decades to come.) Much of the suggested dating for Poirot's age is therefore retconning on the part of those attempting to make sense of his extraordinarily long career. Poirot is a Roman Catholic by birth, and retains a strong sense of Catholic morality later in life. Not much is known of Poirot’s childhood other than he once claimed in "Three Act Tragedy" to have been from a large family with little wealth. In "Taken at the Flood", he further claimed to have been raised and educated by nuns, raising the possibility that he (and any siblings) were orphaned.
As an adult, Poirot joined the Belgian police force. Very little mention is made in Christie's work about this part of his life, but in "The Nemean Lion" (1939) Poirot himself refers to a Belgian case of his in which "a wealthy soap manufacturer […] poisoned his wife in order to be free to marry his secretary". We do not know whether this case resulted in a successful prosecution or not; moreover, Poirot is not above lying in order to produce a particular effect in the person to whom he is speaking, so this evidence is not reliable. Perhaps this is enough evidence to suggest that Poirot's police career was a successful one. Nevertheless, he regards the case in "The Chocolate Box", which took place in 1893, as his only actual failure of detection. Again, Poirot is not reliable as a narrator of his personal history and there is no evidence that Christie sketched it out in any depth. It was also in this period that Poirot shot a man who was firing from a roof onto the public below. Poirot had retired from the Belgian police force by the time he met Hastings in 1916 on the case retold in "The Mysterious Affair at Styles". It should be noted that Poirot is a French-speaking Belgian, i.e. probably a Walloon or Bruxellois; but there can hardly be found any occasion where he refers to himself as such, or is so referred to by others. At the time of writing, at least of the earlier books where the character was defined, non-Belgians such as Agatha Christie were far less aware than nowadays of the deep linguistic divide in Belgian society, assuming that all Belgians were French-speaking. Career as a private detective. During World War I, Poirot left Belgium for Britain as a refugee. It was here, on 16 July 1916, that he again met his lifelong friend, Captain Arthur Hastings, and solved the first of his cases to be published: "The Mysterious Affair at Styles". After that case Poirot apparently came to the attention of the British secret service, and undertook cases for the British government, including foiling the attempted abduction of the Prime Minister. After the war Poirot became a free agent and began undertaking civilian cases. He moved into what became both his home and work address, 56B Whitehaven Mansions, Charterhouse Square, Smithfield London W1. It was chosen by Poirot for its symmetry. His first case was "The Affair at the Victory Ball", which saw Poirot enter the high society and begin his career as a private detective. Between the world wars, Poirot travelled all over Europe and the Middle East investigating crimes and murders. Most of his cases happened during this period and he was at the height of his powers at this point in his life. "The Murder On the Links" saw the Belgian pit his grey cells against a French murderer. In the Middle East he solved the cases of "Death on the Nile", and "Murder in Mesopotamia" with ease and even survived "An Appointment with Death". As he passed through Eastern Europe on his return trip, he solved "The Murder on the Orient Express". However he did not travel to the Americas or Australia, probably due to his sea sickness. It was during this time he met the Countess Vera Rossakoff, a glamorous jewel thief. The history of the Countess is, like Poirot's, steeped in mystery. She claims to have been a member of the Russian aristocracy before the Russian Revolution and suffered greatly as a result, but how much of that story is true is an open question. Even Poirot acknowledges that Rossakoff has told several wildly varying accounts of her early life. Poirot later became smitten with the woman and allowed her to escape justice. Although letting the Countess escape may be morally questionable, that impulse to take the law into his own hands was far from unique. In "The Nemean Lion", he sided with the criminal, Miss Amy Carnaby, and saved her from having to face justice by blackmailing his client Sir Joseph Hoggins, who himself was plotting murder and was unwise enough to let Poirot discover this. Poirot even sent Miss Carnaby two hundred pounds as a final payoff before her dog kidnapping campaign came to an end. In "The Murder of Roger Ackroyd" he allowed the murderer to escape justice through suicide and then ensured the truth was never known to spare the feelings of the murderer's relatives. In "The Augean Stables" he helped the government to cover up vast corruption, even though it might be considered more honest to let the truth come out. It could be suggested that in "Murder on the Orient Express" Poirot allows the murderers to escape justice as well, after he discovers that twelve different people stabbed the victim - Mr. Ratchett - in his sleep. This may be because - since 12 people stabbed the victim, none was certain who delivered the killing blow. Ultimately a falsehood is made up to tell the police and the 12 perpetrators are allowed to go free. After his cases in the Middle East, Poirot returned to Britain. Apart from some of the so-called "Labours of Hercules" (see next section) he very rarely travelled abroad during his later career.
There is a great deal of confusion about Poirot's retirement. Most of the cases covered by Poirot's private detective agency take place before his retirement to grow marrows, at which time he solves "The Murder of Roger Ackroyd". It has been said that twelve cases related in "The Labours of Hercules" (1947) must refer to a different retirement, but the fact that Poirot specifically says that he intends to grow marrows indicates that these stories also take place before "Roger Ackroyd", and presumably Poirot closed his agency once he had completed them. There is specific mention in "The Capture of Cerberus" to the fact that there has been a gap of twenty years between Poirot's previous meeting with Countess Rossakoff and this one. If the "Labours" precede the events in "Roger Ackroyd", then the Roger Ackroyd case must have taken place around twenty years "later" than it was published, and so must any of the cases that refer to it. One alternative would be that having failed to grow marrows once, Poirot is determined to have another go, but this is specifically denied by Poirot himself. Also, in "The Erymanthian Boar", a character is said to have been turned out of Austria by the Nazis, implying that the events of The Labours of Hercules took place after 1937. Another alternative would be to suggest that the Preface to the "Labours" takes place at one date but that the labours are completed over a matter of twenty years. None of the explanations is especially attractive. In terms of a rudimentary chronology, Poirot speaks of retiring to grow marrows in Chapter 18 of "The Big Four" (1927), which places that novel out of published order before Roger Ackroyd. He declines to solve a case for the Home Secretary because he is retired in Chapter One of "Peril at End House" (1932). He is certainly retired at the time of "Three Act Tragedy" (1935) but he does not enjoy his retirement and comes repeatedly out of it thereafter when his curiosity is engaged. Nevertheless, he continues to employ his secretary, Miss Lemon, at the time of the cases retold in "Hickory Dickory Dock" and "Dead Man's Folly", which take place in the mid-1950s. It is therefore better to assume that Christie provided no authoritative chronology for Poirot's retirement, but assumed that he could either be an active detective, a consulting detective or a retired detective as the needs of the immediate case required.
Poirot is less active during the cases that take place at the end of his career. Beginning with "Three Act Tragedy" (1934), Christie had perfected during the inter-war years a sub-genre of Poirot novel in which the detective himself spent much of the first third of the novel on the periphery of events. In novels such as "Taken at the Flood", "After the Funeral" and "Hickory Dickory Dock" he is even less in evidence, frequently passing the duties of main interviewing detective to a subsidiary character. In "Cat Among the Pigeons" Poirot's entrance is so late as to be almost an afterthought. Whether this was a reflection of his age or of the fact that Christie was by now heartily sick of him it is difficult to assess. There is certainly a case for saying that "Crooked House" (1949) and "Ordeal by Innocence" (1957), which are not Poirot novels at all but so easily could have been, represent a logical endpoint of the general diminution of Poirot himself within the Poirot sequence. Towards the end of his career it becomes clear that Poirot's retirement is no longer a convenient fiction. He assumes a genuinely inactive lifestyle during which he concerns himself with studying famous unsolved cases of the past and reading detective novels. He even writes a book about mystery fiction in which he deals sternly with Edgar Allan Poe and Wilkie Collins. In the absence of a more appropriate puzzle, he solves such inconsequential domestic problems as the presence of three pieces of orange peel in his umbrella stand. Poirot (and, it is reasonable to suppose, his creator) becomes increasingly bemused by the vulgarism of the up and coming generation's young people. In "Hickory Dickory Dock", he investigates the strange goings on in a student hostel, while in the "Third Girl" he is forced into contact with the smart set of Chelsea youths. In the growing drug and pop culture of the sixties, he proves himself once again, but has become heavily reliant on other investigators (especially the private investigator, Mr. Goby) who provide him with the clues that he can no longer gather for himself. Notably, during this time his physical characteristics also change dramatically, and by the time Arthur Hastings meets Poirot again in Curtain, he looks very different from his previous appearances, having become thin with age and with obviously dyed hair.
Poirot dies from complications of a heart condition at the end of ', where he moves his amyl nitrite pills out of his reach, possibly out of guilt because he was forced to become the murderer in "Curtain", although it was for the benefit of others. It is revealed at the end of "Curtain" that he fakes his need for a wheelchair (he wants to fool people into believing that he is suffering from arthritis).
The Poirot books take readers through the whole of his life in England, from the first book ("The Mysterious Affair at Styles"), where he is a refugee staying at Styles, to the last Poirot book ("Curtain"), where he visits Styles once again before his death. In between, Poirot solves cases outside England as well, including his most famous case, "Murder on the Orient Express" (1934). Hercule Poirot became famous with the publication, in 1926, of "The Murder of Roger Ackroyd", whose surprising solution proved controversial. The novel is still among the most famous of all detective novels: Edmund Wilson alludes to it in the title of his well-known attack on detective fiction, "Who Cares Who Killed Roger Ackroyd?" Aside from "Roger Ackroyd", the most critically-acclaimed Poirot novels appeared from 1932 to 1942, including such acknowledged classics as "Murder on the Orient Express", "The ABC Murders" (1935), "Cards on the Table" (1936), and "Death on the Nile" (1937). The last of these, a tale of multiple homicide upon a Nile steamer, was judged by the celebrated detective novelist John Dickson Carr to be among the ten greatest mystery novels of all time. The 1942 novel "Five Little Pigs" (aka "Murder in Retrospect"), in which Poirot investigates a murder committed sixteen years before by analysing various accounts of the tragedy, is a "Rashomon"-like performance that critic and mystery novelist Robert Barnard called the best of the Christie novels.
Peter Ustinov played Poirot a total of six times, starting with "Death on the Nile" (1978). He reprised the role in "Evil Under the Sun" (1982) and "Appointment with Death" (1988). When Christie's daughter, Rosalind Hicks, observed to Ustinov that Poirot did not look like him, Ustinov quipped "He does now!" He appeared again as Poirot in three made-for-television movies: "Thirteen at Dinner" (1985), "Dead Man's Folly" (1986), and "Murder in Three Acts" (1986). The first of these was based on "Lord Edgware Dies" and was made by Warner Brothers. It also starred Faye Dunaway and David Suchet as Inspector Japp, just before he himself began to play the famous detective. (Ironically, it is reputed that David Suchet highlights his performance as Japp to be "possibly the worst performance of [his] career.")
In 2004, NHK (a Japanese TV network) produced a 39 episode anime series titled "Agatha Christie's Great Detectives Poirot and Marple", as well as a manga series under the same title released in 2005. The series, adapting several of the best-known Poirot and Marple stories, ran from July 4, 2004 through May 15, 2005, and is now being shown as re-runs on NHK and other networks in Japan. Poirot was voiced by Satomi Kōtarō and Miss Marple was voiced by Yachigusa Kaoru.
There have been a number of radio adaptations of the Poirot stories, most recently on BBC Radio 4 (and regularly repeated on BBC 7) starring John Moffatt. In 1939, "the Mercury Players dramatized "The Murder of Roger Ackroyd" on CBS's "Campbell Playhouse". A 1945 radio series of at least 11 original half-hour episodes (none of which apparently adapt any Christie stories) transferred Poirot from London to New York and starred character actor Harold Huber, perhaps better known for his appearances as a police officer in various Charlie Chan films. On February 22, 1945, "speaking from London, Agatha Christie introduced the initial broadcast of the Poirot series via shortwave."
In Neil Simon's "Murder By Death", James Coco plays a character named "Milo Perrier" who is a parody of Poirot. The film also features parodies of Charlie Chan, Sam Spade, Nick and Nora Charles, and Miss Marple. Poirot was also parodied in "The Goodies" episode "Daylight Robbery on the Orient Express". The British television show "Count Duckula" features a parody of Hercule Poirot (in passing) known as Mr. Hercules Parrot, arm in arm with a character called Miss Marbles. An episode of "Animaniacs" featured Yakko Warner as "Hercule Yakko". The episode involved the theft of a diamond on a cruise ship, involving much of the series' cast as suspects. In the movie "Spiceworld", Hercule Poirot (Hugh Laurie) is about to blame a weapons-packing Emma Bunton, but after she flashes him an innocent smile, Poirot instead accuses an innocent man of the crime. In ', Poirot appears as a young boy on the train transporting Holmes and Watson. Holmes helps the boy in opening a puzzle-box, with Watson giving the boy advice about using his "little grey cells", giving the impression that Poirot first heard the line here. Poirot would go on to use the "little grey cells" line countless times throughout Agatha Christie's fiction. In an episode of "Muppets Tonight", Jason Alexander played Hercule Poirot, believed by the Muppets to be Hercules Poirot, with superhuman powers. The Belgian brewery Brasserie Ellezelloise makes a highly rated stout called "Hercule" with a moustachioed caricature of Hercule Poirot on the label. Dave Stone has created two parodies of Poirot named Dupont. The first, Andre Dupont, appears in the "Detective-Judge Armitage" story "Dowager Duchess of Ghent". The second, Emile Dupont, appears in the Bernice Summerfield novel "Ship of Fools". The "decipherer of enigmas" in José Carlos Somoza's novel "The Athenian Murders" is named Herakles Pontor. In the English version of "Geronimo Stilton" series, the main protagonist has a friend named "Hercule Poirat". In the anime and manga series "Detective Conan", Mouri Kogoro's detective agency is located above the Poirot café. A profile summary of Hercule appears at the end of volume 3 of the manga. In the Israeli sitcom "The Pajamas", one of the characters of Kobi is a grotesque policeman named 'Marcel Fuero', as a reference to Poirot ('Poirot' and 'Fuero' are written the same way in Hebrew, פוארו). In the Portuguese New Year's special comedy show "Crime na Pensão Estrelinha" (starring Portuguese top comedian Herman José), there is a character named Hércules Pirô. Aside from the clear reference to Agatha Christie's character, 'Pirô' also is a misspelling of the word 'pirou', which is Portuguese slang for 'has gone crazy'.
Jane Marple, usually referred to as Miss Marple, is a fictional character appearing in twelve of Agatha Christie's crime novels. Miss Marple is an elderly spinster who acts as an amateur detective, and lives in the village of St. Mary Mead. She is one of the most famous of Christie's characters and has been portrayed numerous times on screen. Her first published appearance was in issue 350 of "The Royal Magazine" for December 1927 with the first printing of the short story "The Tuesday Night Club" which later became the first chapter of "The Thirteen Problems" (1932). Her first appearance in a full-length novel was in "The Murder at the Vicarage" in 1930.
Miss Jane Marple is an elderly lady who lives in the little English village St. Mary Mead. Superficially stereotypical, she is dressed neatly in tweed and is frequently seen knitting or pulling weeds in her garden. Miss Marple sometimes comes across as confused or "fluffy", but when it comes to solving mysteries, she has a sharp logical mind, and an almost unmatched understanding of human nature with all its weaknesses, strengths, quirks and foibles. In the detective story tradition, she often embarrasses the local "professional" police by solving mysteries that have them stumped. Tape recordings Christie made in the mid 1960s reveal that 'Miss Marple' was partly based on Christie's grandmother. However, there is no definitive source for the derivation of the name 'Marple'. The most common explanation suggests that the name was taken from the railway station in Marple, Stockport, through which Christie passed, with the alternative account that Christie took it from the home of a Marple family who lived at Marple Hall, near her sister Madge's home at Abney Hall. Agatha Christie attributed the inspiration for the character of Miss Marple to a number of sources: Miss Marple was "the sort of old lady who would have been rather like some of my grandmother's Ealing cronies – old ladies whom I have met in so many villages where I have gone to stay as a girl". Christie also used material from her fictional creation, spinster Caroline Sheppard, who appeared in The Murder of Roger Ackroyd. When Michael Morton adapted Roger Ackroyd for the stage, he removed the character of Caroline replacing her with a young girl. This change saddened Christie and she determined to give old maids a voice: Miss Marple was born. The character of Jane Marple in the first Miss Marple book, "The Murder at the Vicarage", is markedly different from how she appears in later books. This early version of Miss Marple is a gleeful gossip and not an especially nice woman. The citizens of St. Mary Mead like her but are often tired by her nosy nature and how she seems to expect the worst of everyone. In later books she becomes more modern and a kinder person. Miss Marple never married and has no close living relatives. "Vicarage" introduced Miss Marple's nephew, the "well-known author" Raymond West. His wife Joan (initially called Joyce), a modern artist, was introduced in 1933 in "The Thirteen Problems". Raymond tends to be overconfident in himself and underestimates Miss Marple's mental powers. In her later years, Miss Marple has a live-in companion named Cherry Baker, who was first introduced in "The Mirror Crack'd From Side To Side". Miss Marple is able to solve difficult crimes not only because of her shrewd intelligence, but because St. Mary Mead, over her lifetime, has given her seemingly infinite examples of the negative side of human nature. No crime can arise without reminding Miss Marple of some parallel incident in the history of her time. Miss Marple's acquaintances are sometimes bored by her frequent analogies to people and events from St. Mary Mead, but these analogies often lead Miss Marple to a deeper realization about the true nature of a crime. Although she looks like a sweet, frail old woman, Miss Marple is not afraid of dead bodies and is not easily intimidated. She also has a remarkable ability to latch onto a casual comment and connect it to the case at hand. Miss Marple has never worked for her living and is of independent means, although she benefits in her old age from the financial support of Raymond West, her nephew ("A Caribbean Mystery",1964). She demonstrates a remarkably thorough education, including some art courses that involved study of human anatomy through the study of human cadavers. In "They Do It with Mirrors" (1952), it is revealed that, in her distant youth, Miss Marple spent time in Europe at a finishing school. She is not herself from the aristocracy or landed gentry, but is quite at home amongst them; Miss Marple would probably have been happy to describe herself as a "gentlewoman". Miss Marple may thus be considered a female version of that staple of British detective fiction, the gentleman detective. This education, history, and background are hinted at in the Margaret Rutherford films (see below), in which Miss Marple mentions her awards at marksmanship, fencing and equestrianism (although these hints are played for comedic value). Christie wrote a concluding novel to her Marple series, "Sleeping Murder", in 1940. She locked it away in a bank vault so it would be safe should she be killed in The Blitz. The novel was not published until shortly after Christie's death in 1976, some thirty-six years after it was originally written. While Miss Marple is described as 'an old lady' in many of the stories, her age is mentioned in "At Bertram's Hotel", where it's said she visited the hotel when she was 14 and almost 60 years have passed since then. Excluding "Sleeping Murder", forty-one years passed between the first and last-written novels, and many characters grow and age. An example would be the Vicar's son. At the end of "The Murder at the Vicarage", the Vicar's wife is pregnant. In "The Mirror Crack'd from Side to Side", it is mentioned that the son is now grown, successful and has a career. The effects of aging are seen on Miss Marple, such as needing vacation after illness in "A Caribbean Mystery" or finding she can no longer knit due to poor eyesight in "The Mirror Crack'd from Side to Side". Miss Marple short story collections. Miss Marple also appears in "Greenshaw's Folly", a short story traditionally included as part of the Poirot collection "The Adventure of the Christmas Pudding" (1960). Four stories in the "Three Blind Mice" collection (1950) feature Miss Marple: "Strange Jest", "Tape-Measure Murder", "The Case of the Caretaker", and "The Case of the Perfect Maid".
Although popular from her first appearance in 1930, Jane Marple had to wait thirty-two years for her first big-screen appearance. When she made it, the results were found disappointing to Christie purists and Christie herself; nevertheless, Agatha Christie dedicated the novel "The Mirror Crack'd from Side to Side" to Margaret Rutherford. "Murder, She Said" (1961, directed by George Pollock) was the first of four British MGM productions starring Dame Margaret Rutherford. She presented the character as a bold old lady, different from the prim and birdlike character Christie created in her novels. This first film was based on the 1957 novel "4:50 from Paddington" (U.S. title, "What Mrs. McGillicuddy Saw!"), and the changes made in the plot were typical of the series. In the film, Mrs. McGillicuddy does not see anything because there is no Mrs. McGillicuddy. Miss Marple herself sees an apparent murder committed on a train running alongside hers. Likewise, it is Miss Marple herself who poses as a maid to find out the facts of the case, not a young friend of hers who has made a business of it. Joan Hickson played the part of the home help in this film and can claim to have appeared in two Miss Marple series. The other Rutherford films (all directed by George Pollock) were "Murder at the Gallop" (1963), based on the 1953 Hercule Poirot novel "After the Funeral" (In this film, she is identified as Miss JTV Marple, though there were no indication as to what the extra initials might stand for); "Murder Most Foul" (1964), based on the 1952 Poirot novel "Mrs McGinty's Dead"; and "Murder Ahoy!" (1964). The last film is not based on any Christie work but displays a few plot elements from "They Do It With Mirrors" (viz., the ship is used as a reform school for wayward boys and one of the teachers uses them as a crime force), and there is a kind of salute to "The Mousetrap". Rutherford also appeared briefly as Miss Marple in the spoof Hercule Poirot adventure "The Alphabet Murders" (1965). Rutherford, who was 70 years old when the first film was made, insisted that she wore her own clothes during the filming of the movie, as well as having her real-life husband, Stringer Davis appear alongside her as the character 'Mr Stringer'. The Rutherford films are frequently repeated on television in Germany, and in that country Miss Marple is generally identified with Rutherford's quirky portrayal.
In 1980, Angela Lansbury played Miss Marple in "The Mirror Crack'd" (EMI, directed by Guy Hamilton), based on Christie's 1962 novel. However, Lansbury is only on screen for a short time, the bulk of the film being taken up with the machinations of an all-star cast that included Elizabeth Taylor, Rock Hudson, Geraldine Chaplin, Tony Curtis, and Kim Novak. Edward Fox appeared as Inspector Craddock, who did Miss Marple's legwork. Lansbury's Marple was a crisp, intelligent woman who moved stiffly and spoke in clipped tones. Unlike most incarnations of Miss Marple, this one smoked cigarettes.
American TV was the setting for the first dramatic portrayal of Miss Marple with Gracie Fields, the legendary British actress, playing her in a 1956 episode of "Goodyear TV Playhouse" based on "A Murder Is Announced", the 1950 Christie novel. In 1970, the character of Miss Marple was portrayed by Inge Langen in a West German television adaptation of "The Murder at the Vicarage " ("Mord im Pfarrhaus"). From 1984 to 1992, the BBC adapted all of the original Miss Marple novels as a series titled "Miss Marple". Joan Hickson played the lead role. (Coincidentally, Hickson had played a cook in the first film in which Margaret Rutherford played Miss Marple.) These programs, which are actually a set of 12 feature-length TV movies rather than a TV series in the usual sense, followed the plots of the original novels more closely than previous film and television adaptations had. Hickson has come to be regarded by many as the definitive Miss Marple (indeed Agatha Christie herself once remarked years earlier that she would like Joan Hickson to play Miss Marple). Angela Lansbury, after playing Miss Marple in "The Mirror Crack'd", went on to star in the TV series "Murder, She Wrote" as Jessica Fletcher, a mystery novelist who also solves crimes. The character was based in part on Miss Marple and another Christie character, Ariadne Oliver. Beginning in 2004, ITV broadcast a series of adaptations of Agatha Christie's books under the title "Agatha Christie's Marple", usually referred to as "Marple." Geraldine McEwan starred in the first three series. Julia McKenzie took over the role in the fourth season. The adaptions are notable for changing the plots and characters of the original books (e.g. incorporating lesbian affairs, changing killer identities, re-naming or removing significant characters, and even using stories from other books where Miss Marple didn't originally feature). From 2004 to 2005, Japanese TV network NHK produced a 39 episode anime series titled "Agatha Christie's Great Detectives Poirot and Marple", which features both Miss Marple and Hercule Poirot. Miss Marple's voice is provided by Kaoru Yachigusa.
April is the fourth month of the year in the Gregorian Calendar, and one of four months with a length of 30 days. April was originally the second month of the Roman calendar, before January and February were added by King Numa Pompilius about 700 BC. It became the fourth month of the calendar year (the year when twelve months are displayed in order) during the time of the decemvirs about 450 BC, when it also was given 29 days. The derivation of the name (Latin "Aprilis") is uncertain. The traditional etymology is from the Latin "aperire", "to open," in allusion to its being the season when trees and flowers begin to "open," which is supported by comparison with the modern Greek use of ἁνοιξις (opening) for spring. Since most of the Roman months were named in honor of divinities, and as April was sacred to Venus, the "Festum Veneris et Fortunae Virilis" being held on the first day, it has been suggested that Aprilis was originally her month Aphrilis, from her Greek name Aphrodite ("Aphros"), or from the Etruscan name "Apru". Jacob Grimm suggests the name of a hypothetical god or hero, "Aper" or "Aprus". The Anglo-Saxons called April "Oster-monath" or "Eostur-monath", The Venerable Bede says that this month is the root of the word Easter. He further speculates that the month was named after a goddess "Eostre" whose feast was in that month. St George's day is the twenty-third of the month; and St Mark's Eve, with its superstition that the ghosts of those who are doomed to die within the year will be seen to pass into the church, falls on the twenty-fourth. In China the symbolic ploughing of the earth by the emperor and princes of the blood takes place in their third month, which frequently corresponds to our April. The Finns called (and still call) this month "Huhtikuu", or 'Burnwood Month', when the wood for beat and burn clearing of farmland was felled. The "days of April" ("journées d'avril") is a name appropriated in French history to a series of insurrections at Lyons, Paris and elsewhere, against the government of Louis Philippe in 1834, which led to violent repressive measures, and to a famous trial known as the "procès d'avril". The birthstone of April is the diamond, and the birth flower is typically listed as either the Daisy or the Sweet Pea. April starts on the same day of the week as July in all years, and January in leap years.
August is the eighth month of the year in the Gregorian Calendar and one of seven Gregorian months with a length of 31 days. This month was originally named "Sextilis" in Latin, because it was the sixth month in the original ten-month Roman calendar under Romulus in 753 BCE, when March was the first month of the year. About 700 BCE it became the eighth month when January and February were added to the year before March by King Numa Pompilius, who also gave it 29 days. Julius Caesar added two days when he created the Julian calendar in 45 BCE giving it its modern length of 31 days. In 8 BCE it was renamed in honor of Augustus, who did "not" take a day from February (see the debunked theory on month lengths). In common years no other month starts on the same day of the week as August, though in leap years February starts on the same day.
In the Hebrew Bible, Aaron (; "Ahărōn", "Hārūn"), sometimes called Aaron the Levite (אַהֲרֹן הַלֵוִי), was the brother of Moses, (Exodus 6:16-20) and represented the priestly functions of his tribe, becoming the first High Priest of the Israelites. While Moses was receiving his education at the Egyptian royal court and during his exile among the Midianites, Aaron and his sister remained with their kinsmen in the eastern border-land of Egypt (Goshen). Aaron there gained a name for eloquent and persuasive speech; so that when the time came for the demand upon the Pharaoh to release Israel from captivity, Aaron became his brother’s "nabi", or spokesman, to his own people (Exodus 7:1) and, after their unwillingness to hear, to the Pharaoh himself (Exodus 7:9). Various dates for his life have been proposed, ranging from approximately 1600 to 1200 B.C.
Aaron’s function included the duties of speaker and implied personal dealings with the Egyptian royal court on behalf of Moses, who was always the central moving figure. The part played by Aaron in the events that preceded the Exodus was, therefore, ministerial, and not directive. He, along with Moses, performed “signs” before his people which impressed them with a belief in the reality of the divine mission of the brothers (Exodus 4:15-16). At the command of Moses he stretched out his rod in order to bring on the first three plagues (Exodus 7:19, 8:1,12). In the infliction of the remaining plagues he appears to have acted merely as the attendant of Moses, whose outstretched rod drew the divine wrath upon the Pharaoh and his subjects (Exodus 9:23, 10:13,22). The potency of Aaron’s rod had already been demonstrated by its victory over the rods of the Egyptian magicians, which it swallowed after all the rods alike had been turned into serpents (Exodus 7:9). During the journey in the wilderness, Aaron is not always prominent or active; and he sometimes appears guilty of rebellious or treasonable conduct. At the battle with Amalek, he is chosen with Hur to support the hand of Moses that held the “rod of God” (Exodus 17:9). When the revelation was given to Moses at Mount Sinai, he headed the elders of Israel who accompanied Moses on the way to the summit. Joshua, however, was admitted with his leader to the very presence of the Lord, while Aaron and Hur remained below to look after the people Exodus 24:9-14. It was during the prolonged absence of Moses that Aaron yielded to the clamors of the people, and made a Golden Calf as a visible image of the divinity who had delivered them from Egypt (Exodus 32:1-6) (it should be noted that in the account given of the same events, in the Qur'an, Aaron is not the idol-maker and upon Moses' return begged his pardon as he had felt mortally threatened by the Israelites (Quran 7:142-152)) At the intercession of Moses, Aaron was saved from the plague which smote the people (Deuteronomy 9:20, Exodus 32:35), although it was to Aaron’s tribe of Levi that the work of punitive vengeance was committed (Exodus 32:26).
At the time when the tribe of Levi was set apart for the priestly service, Aaron was anointed and consecrated to the priesthood, arrayed in the robes of his office, and instructed in its manifold duties (Exodus 28, Exodus 29). On the very day of his consecration, his sons, Nadab and Abihu, were consumed by fire from the Lord for having offered incense in an unlawful manner (Leviticus 10). Scholarly consensus is that in Aaron's High Priesthood the sacred writer intended to describe a model, the prototype, so to say, of the Jewish High Priest. God, on Mount Sinai instituting a worship, also instituted an order of priests. According to the patriarchal customs, the firstborn son in every family used to perform the functions connected with God's worship. It might have been expected, consequently, that Reuben's family would be chosen by God for the ministry of the new altar. However, according to the biblical narrative it was Aaron who was the object of God's choice. To what jealousies this gave rise later, has been indicated above. The office of the Aaronites was at first merely to take care of the lamp which was to burn perpetually before the veil of the tabernacle Exodus 27:21. A more formal calling soon followed (Exodus 28:1). Aaron and his sons, distinguished from the Common People by their sacred functions, were also to receive holy garments suitable to their office. Aaron offered the various sacrifices and performed the many ceremonies of the consecration of the new priests, according to the divine instructions (Exodus 29), and repeated these rites for seven days, during which Aaron and his sons were entirely separated from the rest of the people. When, on the eighth day, the High Priest had inaugurated his office of sacrifice by killing the animals, he blessed the people (very likely according to the prescriptions of Numbers 6:24-26), and, with Moses, entered into the tabernacle to possess it. They "came forth and blessed the people. And the glory of the Lord appeared to all the multitude: And behold a fire, coming forth from the Lord, devoured the holocaust, and the fat that was upon the altar: which when the multitude saw, they praised the Lord, falling on their faces" (Leviticus 9:23-24). In this way the institution of the Aaronic priesthood was established.
From the time of the sojourn at Mount Sinai, where he became the anointed priest of Israel, Aaron ceased to be the minister of Moses, his place being taken by Joshua. He is mentioned in association with Miriam in a jealous complaint against the exclusive claims of Moses as the Lord’s prophet. The presumption of the murmurers was rebuked, and Miriam was smitten with "Tzaraath". Aaron entreated Moses to intercede for her, at the same time confessing the sin and folly that prompted the uprising. Aaron himself was not struck with the plague on account of sacerdotal immunity; and Miriam, after seven days’ quarantine, was healed and restored to favor (Numbers 12). Micah a prophet in Judaism, mentions Moses, Aaron, and Miriam as the leaders of Israel after the Exodus (a judgment wholly in accord with the tenor of the narratives). In the present instance it is made clear by the express words of the oracle (Numbers 12:6-8) that Moses was unique among men as the one with whom the Lord spoke face to face. The failure to recognize or concede this prerogative of their brother was the sin of Miriam and Aaron. The validity of the exclusive priesthood of the family of Aaron was attested after the ill-fated rebellion of Korah, who was a first cousin of Aaron. When the earth had opened and swallowed up the leaders of the insurgents (Numbers 16:25-35), Eleazar, the son of Aaron, was commissioned to take charge of the censers of the dead priests. And when the plague had broken out among the people who had sympathized with the rebels, Aaron, at the command of Moses, took his censer and stood between the living and the dead till the plague was stayed (Numbers 17:1-15, 16:36-50). Another memorable transaction followed. Each of the tribal princes of Israel took a rod and wrote his name upon it, and the twelve rods were laid up over night in the tent of meeting. The next morning Aaron’s rod was found to have budded and blossomed and produced ripe almonds (Numbers 17:8). The miracle proved merely the prerogative of the tribe of Levi; but now a formal distinction was made in perpetuity between the family of Aaron and the other Levites. While all the Levites (and only Levites) were to be devoted to sacred services, the special charge of the sanctuary and the altar was committed to the Aaronites alone (Numbers 18:1-7). The scene of this enactment is unknown, as is the time mentioned.
Aaron, like Moses, was not permitted to enter Canaan with the others. The reason alleged is that the two brothers showed impatience at Meribah (Kadesh) in the last year of the desert pilgrimage (Numbers 20:12-13), when Moses brought water out of a rock to quench the thirst of the people. The action was construed as displaying a want of deference to the Lord, since they had been commanded to speak to the rock, whereas Moses struck it with the wonder-working rod (Numbers 20:7-11). Of the death of Aaron we have two accounts. The principal one gives a detailed statement that soon after the incident at Meribah, Aaron, with his son Eleazar and Moses, ascended Mount Hor. There Moses stripped Aaron of his priestly garments and transferred them to Eleazar. Aaron died on the summit of the mountain, and the people mourned for him thirty days (Numbers 20:22-29; compare 33:38-39). The other account is found in Deut. 10. 6, where Moses is reported as saying that Aaron died at Mosera and was buried there. There is a significant amount of travel between these two points, as the itinerary in Numbers 33:31-37 records seven stages between Moseroth (Mosera) and Mount Hor.
The older prophets and prophetical writers beheld in their priests the representatives of a religious form inferior to the prophetic truth; men without the spirit of God and lacking the will-power requisite to resist the multitude in its idolatrous proclivities. Thus Aaron, the first priest, ranks below Moses: he is his mouthpiece, and the executor of the will of God revealed through Moses, although it is pointed out that it is said fifteen times in the Pentateuch that “the Lord spoke to Moses "and" Aaron.” Under the influence of the priesthood which shaped the destinies of the nation under Persian rule, a different ideal of the priest was formed, as is learned from Malachi 2:4-7; and the prevailing tendency was to place Aaron on a footing equal with Moses. “At times Aaron, and at other times Moses, is mentioned first in Scripture—this is to show that they were of equal rank,” says Mekilta. בא, 1; expressly infers this when introducing in his record of renowned men the glowing description of Aaron’s ministration.
In fulfilment of the promise of peaceful life, symbolized by the pouring of oil upon his head (Leviticus Rabbah x., Midrash Teh. cxxxiii. 1), Aaron's death, as described in the Haggadah, was of a wonderful tranquillity. Accompanied by Moses, his brother, and by Eleazar, his son, Aaron went to the summit of Mount Hor, where the rock suddenly opened before him and a beautiful cave lit by a lamp presented itself to his view. "Take off thy priestly raiment and place it upon thy son Eleazar!" said Moses; "and then follow me." Aaron did as commanded; and they entered the cave, where was prepared a bed around which angels stood. "Go lie down upon thy bed, my brother," Moses continued; and Aaron obeyed without a murmur. Then his soul departed as if by a kiss from God. The cave closed behind Moses as he left; and he went down the hill with Eleazar, with garments rent, and crying: "Alas, Aaron, my brother! thou, the pillar of supplication of Israel!" When the Israelites cried in bewilderment, "Where is Aaron?" angels were seen carrying Aaron's bier through the air. A voice was then heard saying: "The law of truth was in his mouth, and iniquity was not found on his lips: he walked with me in righteousness, and brought many back from sin" (Malachi 2:6). He died, according to Seder Olam Rabbah ix., R. H. 2, 3a, on the first of Ab." The pillar of cloud which proceeded in front of Israel's camp disappeared at Aaron's death (see Seder 'Olam, ix. and R. H. 2b-3a). The seeming contradiction between Numbers 20:22 et seq. and Deutronomy 10:6 is solved by the rabbis in the following manner: Aaron's death on Mount Hor was marked by the defeat of the people in a war with the king of Arad, in consequence of which the Israelites fled, marching seven stations backward to Mosera, where they performed the rites of mourning for Aaron; wherefore it is said: "There [at Mosera] died Aaron." The rabbis also dwell with special laudation on the brotherly sentiment which united Aaron and Moses. When the latter was appointed ruler and Aaron high priest, neither betrayed any jealousy; instead they rejoiced in one another's greatness. When Moses at first declined to go to Pharaoh, saying: "O my Lord, send, I pray thee, by the hand of him whom thou wilt send" (Exodus 4:13), he was unwilling to deprive Aaron, his brother, of the high position the latter had held for so many years; but the Lord reassured him, saying: "Behold, when he seeth thee, he will be glad in his heart" (). Indeed, Aaron was to find his reward, says Shimon bar Yochai; for that heart which had leaped with joy over his younger brother's rise to glory greater than his was decorated with the Urim and Thummim, which were to "be upon Aaron's heart when he goeth in before the Lord" (Canticles Rabbah i. 10). Moses and Aaron met in gladness of heart, kissing each other as true brothers (Exodus 4:27; compare Song of Songs 8:1), and of them it is written: "Behold how good and how pleasant [it is] for brethren to dwell together in unity!" (Psalms 133:1). Of them it is said: "Mercy and truth are met together; righteousness and peace have kissed [each other]" (Psalms 85:10); for Moses stood for righteousness, according to Deuteronomy 33:21, and Aaron for peace, according to. Again, mercy was personified in Aaron, according to Deuteronomy 33:8, and truth in Moses, according to Numbers 12:7. When Moses poured the oil of anointment upon the head of Aaron, Aaron modestly shrank back and said: "Who knows whether I have not cast some blemish upon this sacred oil so as to forfeit this high office." Then the Shekhinah spake the words: "Behold the precious ointment upon the head, that ran down upon the beard of Aaron, that even went down to the skirts of his garment, is as pure as the dew of Hermon" ().
According to Tanhuma, Aaron’s activity as a prophet began earlier than that of Moses. Hillel held Aaron up as an example, saying: “Be of the disciples of Aaron, loving peace and pursuing peace; love your fellow creatures and draw them nigh unto the Law!” This is further illustrated by the tradition preserved in Abot de-Rabbi Natan 12, Sanhedrin 6b, and elsewhere, according to which Aaron was an ideal priest of the people, far more beloved for his kindly ways than was Moses. While Moses was stern and uncompromising, brooking no wrong, Aaron went about as peacemaker, reconciling man and wife when he saw them estranged, or a man with his neighbor when they quarreled, and winning evil-doers back into the right way by his friendly intercourse. The mourning of the people at Aaron’s death was greater, therefore, than at that of Moses; for whereas, when Aaron died the whole house of Israel wept, including the women. Moses was bewailed by “the sons of Israel” only (). Even in the making of the Golden Calf the rabbis find extenuating circumstances for Aaron. His fortitude and silent submission to the will of God on the loss of his two sons are referred to as an excellent example to men how to glorify God in the midst of great affliction. Especially significant are the words represented as being spoken by God after the princes of the Twelve Tribes had brought their dedication offerings into the newly reared Tabernacle: “Say to thy brother Aaron: Greater than the gifts of the princes is thy gift; for thou art called upon to kindle the light, and, while the sacrifices shall last only as long as the Temple lasts, thy light shall last forever.”
Recently, the tradition that Kohanim are actually descended from a single patriarch, Aaron, was found to be apparently consistent with genetic testing. The majority of Kohanim, but not all, share a direct male lineage with a common Y chromosome, and testing was done across sectors of the Jewish population to see if there was any commonality between the Y chromosomes of Kohanim. The results were found to cluster rather closely around a specific DNA signature, found in the Semitic Haplogroup J1, which the researchers named the Cohen modal haplotype, implying that many of the Kohanim do share a distinctive common ancestry. This information was also used to support the claim that the Lemba (a sub-Saharan tribe) are in fact descendant from a group of Jewish Priests. The Cohen Modal Haplotype or CMH is found in haplogroup J1, which geneticists estimate originated in the Southern Levant (modern day Israel, Jordan; biblical Canaan) or North Africa (Egypt) approximately 10,000 - 15,000 years ago. Biblical tradition holds that Abraham and his ancestors, the Semitic tribes, originated from Southern Arabia or East Africa (Genesis 10); Aaron and Moses were 7th generation descendants from Abraham (Exodus 6). An estimated 20% of the modern Jewish community fall into haplogroup J1. The traditional date for Abraham is circa 2200-2000 BC. Behar, et al., found Kohanim in a variety of haplogroups (E3b, G2, H, I1b, J, K2, Q, R1a1, R1b), which included those which originated in the Levant (J1, J2) and those from Southern Arabia, East Africa, or another geographic region.
Aaron married Elisheba, daughter of Amminadab and sister of Nahshon (Exod 6:23). The sons of Aaron were Eleazar, Ithamar, Nadab and Abihu. A descendant of Aaron is an Aaronite, or Kohen, meaning Priest. Any non-Aaronic Levite descended from Levi but not of the priestly division would be assigned to assist the Levitical priests of the family of Aaron in the care of the tabernacle and later of the temple.
Aaron is considered a type of Christ, the high priest of the new dispensation. In the Eastern Orthodox Church and the Maronite Church he is venerated as a saint, with a feast day celebrated on September 4, together with Moses (for those churches which follow the traditional Julian Calendar, September 4 falls on September 17 of the modern Gregorian Calendar). He is also commemorated, together with other righteous saints from the Old Testament on the Sunday of the Holy Fathers (the Sunday before Christmas). He is commemorated as one of the Holy Forefathers in the Calendar of Saints of the Armenian Apostolic Church on July 30. He is commemorated on July 1 in the modern Latin calendar and in the Syriac Calendar.
Aaron is believed to be a Prophet in Islam and is known as Harun, which is the Arabic name for Aaron. His role also found an analogue in the person of Ali, to whom Muhammad said: " Will you not be pleased that you will be to me like Aaron was to Moses? " In the Quran Aaron was not involved with the creation of the Golden Calf and made efforts to dissuade the Israelites from worshiping it, a significant difference from the older texts.
Depictions of Aaron within art history are rare. Other than Aaron's inclusion in the crowd of revelers around the Golden Calf ceremony—most notably in Nicolas Poussin’s “The Adoration of the Golden Calf” (ca. 1633-34, National Gallery London)—there is little else. The recent discovery in 1991 of Pier Francesco Mola’s “Aaron, Holy to the Lord” (ca. 1650, Private Collection, New York: image available for study at Fred R. Kline Gallery Archives) adds significantly to the Aaronic mythos. The painting offers a dramatic, deeply psychological portrayal of the single figure of Aaron in his priestly garments celebrating Yom Kippur in the wilderness Tabernacle. The Mola “Aaron” is considered, quite surprisingly, the unique single figure of Aaron to have been painted by a European old master artist, circa 15th-18th centuries (A.Pigler, "Barockthemen" Vol. 1; although unknown to Pigler). The carefully rendered Judaic iconographic details in the Mola painting are rare and the subject itself may have importance in relationship to mid-17th century Jewish history, characterized by a controversial messianic movement involving a serious contender for a new Messiah, Shabtai Zvi, whose influence was felt in Jewish communities worldwide (Harris Lenowitz, "The Jewish Messiahs"). It is highly probable that the Zvi messianic phenomenon was noted by the Catholic Church as a possible threat to Jesus, their sanctified Messiah. It may be significant to note, in considering the possible influence of the Roman Catholic Church in choices of lay-commissioned religious art during this period and considering as well the importance of Aaron in the Christian tradition, that "Aaron, Holy to the Lord" was originally commissioned along with a now lost pendant of Moses (both from Mola) by the nobel Colonna family, wealthy Catholic art patrons living in Rome (Getty Museum Archives).
In chemistry, an alcohol is any organic compound in which a hydroxyl functional group (-OH) is bound to a carbon atom, usually connected to other carbon or hydrogen atoms. An important class are the simple acyclic alcohols, the general formula for which is CnH2n+1OH. Of those, ethanol (C2H5OH) is the type of alcohol found in alcoholic beverages, and in common speech the word alcohol refers specifically to ethanol. Other alcohols are usually described with a clarifying adjective, as in "isopropyl alcohol" ("propan-2-ol") or "wood alcohol" ("methyl alcohol", or methanol). The suffix "-ol" appears in the IUPAC chemical name of all alcohols.
The most commonly used alcohol is ethanol, with the ethane backbone. Ethanol has been produced and consumed by humans for millennia, in the form of fermented and distilled alcoholic beverages, and was isolated by the Persian alchemist Rāzi (Rhazes) around 900 AD. It is a clear flammable liquid that boils at 78.4 °C, which is used as an industrial solvent, car fuel, and raw material in the chemical industry. In the US and some other countries, because of legal and tax restrictions on alcohol consumption, ethanol destined for other uses often contains additives that make it unpalatable (such as Bitrex) or poisonous (such as methanol). Ethanol in this form is known generally as denatured alcohol; when methanol is used, it may be referred to as methylated spirits ("Meths") or "surgical spirits". The simplest alcohol is methanol, which was formerly obtained by the distillation of wood and therefore is called "wood alcohol". It is a clear liquid resembling ethanol in smell and properties, with a slightly lower boiling point (64.7 °C), and is used mainly as a solvent, fuel, and raw material. Unlike ethanol, methanol is extremely toxic: one sip (as little as 10 ml) can cause permanent blindness by destruction of the optic nerve and 30 ml (one fluid ounce) is potentially fatal. Two other alcohols whose uses are relatively widespread (though not so much as those of methanol and ethanol) are propanol and butanol. Like ethanol, they can be produced by fermentation processes. (However, the fermenting agent is a bacterium, "Clostridium acetobutylicum", that feeds on cellulose, not sugars like the Saccharomyces yeast that produces ethanol.)
In the IUPAC system, the name of the alkane chain loses the terminal "e" and adds "ol", e.g. "methanol" and "ethanol". When necessary, the position of the hydroxyl group is indicated by a number between the alkane name and the "ol": propan-1-ol for CH3CH2CH2OH, propan-2-ol for CH3CH(OH)CH3. Sometimes, the position number is written before the IUPAC name: 1-propanol and 2-propanol. If a higher priority group is present (such as an aldehyde, ketone or carboxylic acid), then it is necessary to use the prefix "hydroxy", for example: 1-hydroxy-2-propanone (CH3COCH2OH). The IUPAC nomenclature is used in scientific publications and where precise identification of the substance is important. In other less formal contexts, an alcohol is often called with the name of the corresponding alkyl group followed by the word "alcohol", e.g. methyl alcohol, ethyl alcohol. Propyl alcohol may be "n"-propyl alcohol or isopropyl alcohol depending on whether the hydroxyl group is bonded to the 1st or 2nd carbon on the propane chain. Alcohols are classified into "primary", "secondary" and "tertiary", based upon the number of carbon atoms connected to the carbon atom that bears the hydroxyl group. Namely, the primary alcohols have general formulas RCH2OH; secondary ones are RR'CHOH; and tertiary ones are RR'R"COH, where R, R'and R" stand for alkyl groups. Ethanol and "n"-propyl alcohol are primary alcohols; isopropyl alcohol is a secondary one. The prefixes "sec"- (or "s"-) and "tert"- (or "t"-), conventionally in italics, may be used before the alkyl group's name to distinguish secondary and tertiary alcohols, respectively, from the primary one. For example, isopropyl alcohol is occasionally called "sec"-propyl alcohol, and the tertiary alcohol (CH3)3COH, or 2-methylpropan-2-ol in IUPAC nomenclature, is commonly known as "tert"-butyl alcohol or "tert"-butanol.
The word "alcohol" appears in English in the 16th century, loaned via French from medical Latin, ultimately from the Arabic ("," "the kohl, a powder used as an eyeliner"). ' is Arabic for the definitive article, "the" in English. The current Arabic name for alcohol is ', re-introduced from western usage. ' was the name given to the very fine powder, produced by the sublimation of the natural mineral stibnite to form antimony sulfide Sb2S3 (hence the essence or "spirit" of the substance), which was used as an antiseptic and eyeliner. William Johnson in his 1657 "Lexicon Chymicum" glosses the word as "antimonium sive stibium". By extension, the word came to refer to any fluid obtained by distillation, including "alcohol of wine", the distilled essence of wine. Libavius in "Alchymia" (1594) has "vini alcohol vel vinum alcalisatum". Johnson (1657) glosses "alcohol vini" as "quando omnis superfluitas vini a vino separatur, ita ut accensum ardeat donec totum consumatur, nihilque fæcum aut phlegmatis in fundo remaneat." The word's meaning became restricted to "spirit of wine" (ethanol) in the 18th century, and was again extended to the family of substances so called in modern chemistry from 1850.
Alcohols have an odor that is often described as “biting” and as “hanging” in the nasal passages. The hydroxyl group generally makes the alcohol molecule polar. Those groups can form hydrogen bonds to one another and to other compounds (except in certain large molecules where the hydroxyl is protected by steric hindrance of adjacent groups). This hydrogen bonding means that alcohols can be used as protic solvents. Two opposing solubility trends in alcohols are: the tendency of the polar OH to promote solubility in water, and of the carbon chain to resist it. Thus, methanol, ethanol, and propanol are miscible in water because the hydroxyl group wins out over the short carbon chain. Butanol, with a four-carbon chain, is moderately soluble because of a balance between the two trends. Alcohols of five or more carbons (Pentanol and higher) are effectively insoluble in water because of the hydrocarbon chain's dominance. All simple alcohols are miscible in organic solvents. Because of hydrogen bonding, alcohols tend to have higher boiling points than comparable hydrocarbons and ethers. The boiling point of the alcohol ethanol is 78.29 °C, compared to 69 °C for the hydrocarbon Hexane (a common constituent of gasoline), and 34.6 °C for Diethyl ether. Alcohols, like water, can show either acidic or basic properties at the O-H group. With a pKa of around 16-19 they are generally slightly weaker acids than water, but they are still able to react with strong bases such as sodium hydride or reactive metals such as sodium. The salts that result are called alkoxides, with the general formula RO- M+. Alcohols can also undergo oxidation to give aldehydes, ketones or carboxylic acids, or they can be dehydrated to alkenes. They can react to form ester compounds, and they can (if activated first) undergo nucleophilic substitution reactions. The lone pairs of electrons on the oxygen of the hydroxyl group also makes alcohols nucleophiles. For more details see the reactions of alcohols section below. As one moves from primary to secondary to tertiary alcohols with the same backbone, the hydrogen bond strength, the boiling point,and the acidity typically decrease.
Alcohols can be used as a beverage (ethanol only), as fuel and for many scientific, medical, and industrial utilities. Ethanol in the form of alcoholic beverages has been consumed by humans since pre-historic times. A 50% v/v solution of ethylene glycol in water is commonly used as an antifreeze. Some alcohols, mainly ethanol and methanol, can be used as an alcohol fuel. Fuel performance can be increased in forced induction internal combustion engines by injecting alcohol into the air intake after the turbocharger or supercharger has pressurized the air. This cools the pressurized air, providing a denser air charge, which allows for more fuel, and therefore more power. Alcohols have applications in industry and science as reagents or solvents. Because of its low toxicity and ability to dissolve non-polar substances, ethanol can be used as a solvent in medical drugs, perfumes, and vegetable essences such as vanilla. In organic synthesis, alcohols serve as versatile intermediates. Ethanol can be used as an antiseptic to disinfect the skin before injections are given, often along with iodine. Ethanol-based soaps are becoming common in restaurants and are convenient because they do not require drying due to the volatility of the compound. Alcohol is also used as a preservative for specimens. Alcohol gels have become common as hand sanitizers.
Alcohols can behave as weak acids, undergoing deprotonation. The deprotonation reaction to produce an alkoxide salt is either performed with a strong base such as sodium hydride or "n"-butyllithium, or with sodium or potassium metal. It should be noted, though, that the bases used to deprotonate alcohols are strong themselves. The bases used and the alkoxides created are both highly moisture sensitive chemical reagents. The acidity of alcohols is also affected by the overall stability of the alkoxide ion. Electron-withdrawing groups attached to the carbon containing the hydroxyl group will serve to stabilize the alkoxide when formed, thus resulting in greater acidity. On the other hand, the presence of electron-donating group will result in a less stable alkoxide ion formed. This will result in a scenario whereby the unstable alkoxide ion formed will tend to accept a proton to reform the original alcohol. With alkyl halides alkoxides give rise to ethers in the Williamson ether synthesis.
The OH group is not a good leaving group in nucleophilic substitution reactions, so neutral alcohols do not react in such reactions. However, if the oxygen is first protonated to give R−OH2+, the leaving group (water) is much more stable, and the nucleophilic substitution can take place. For instance, tertiary alcohols react with hydrochloric acid to produce tertiary alkyl halides, where the hydroxyl group is replaced by a chlorine atom by unimolecular nucleophilic substitution. If primary or secondary alcohols are to be reacted with hydrochloric acid, an activator such as zinc chloride is needed. Alternatively the conversion may be performed directly using thionyl chloride.[1] In the Barton-McCombie deoxygenation an alcohol is deoxygenated to an alkane with tributyltin hydride or a trimethylborane-water complex in a radical substitution reaction.
Alcohols are themselves nucleophilic, so R−OH2+ can react with ROH to produce ethers and water in a dehydration reaction, although this reaction is rarely used except in the manufacture of diethyl ether. More useful is the E1 elimination reaction of alcohols to produce alkenes. The reaction generally obeys Zaitsev's Rule, which states that the most stable (usually the most substituted) alkene is formed. Tertiary alcohols eliminate easily at just above room temperature, but primary alcohols require a higher temperature. A more controlled elimination reaction is the Chugaev elimination with carbon disulfide and iodomethane.
In order to drive the equilibrium to the right and produce a good yield of ester, water is usually removed, either by an excess of H2SO4 or by using a Dean-Stark apparatus. Esters may also be prepared by reaction of the alcohol with an acid chloride in the presence of a base such as pyridine. Other types of ester are prepared similarly- for example tosyl (tosylate) esters are made by reaction of the alcohol with p-toluenesulfonyl chloride in pyridine.
Primary alcohols (R-CH2-OH) can be oxidized either to aldehydes (R-CHO) or to carboxylic acids (R-CO2H), while the oxidation of secondary alcohols (R1R2CH-OH) normally terminates at the ketone (R1R2C=O) stage. Tertiary alcohols (R1R2R3C-OH) are resistant to oxidation. The direct oxidation of primary alcohols to carboxylic acids normally proceeds via the corresponding aldehyde, which is transformed via an "aldehyde hydrate" (R-CH(OH)2) by reaction with water before it can be further oxidized to the carboxylic acid. Reagents useful for the transformation of primary alcohols to aldehydes are normally also suitable for the oxidation of secondary alcohols to ketones. These include Collins reagent and Dess-Martin periodinane. The direct oxidation of primary alcohols to carboxylic acids can be carried out using Potassium permanganate or the Jones reagent.
Ethanol in alcoholic beverages has been consumed by humans since prehistoric times for a variety of hygienic, dietary, medicinal, religious, and recreational reasons. The consumption of large doses of ethanol causes drunkenness (intoxication), which may lead to a hangover as its effects wear off. Depending upon the dose and the regularity of its consumption, ethanol can cause acute respiratory failure or death. Because ethanol impairs judgment in humans, it can be a catalyst for reckless or irresponsible behavior. The LD50 of ethanol in rats is 10.3 g/kg. Other alcohols are substantially more poisonous than ethanol, partly because they take much longer to be metabolized and partly because their metabolism produces substances that are even more toxic. Methanol (wood alcohol), for instance, is oxidized to formaldehyde and then to the poisonous formic acid in the liver by alcohol dehydrogenase and formaldehyde dehydrogenase enzymes respectively; accumulation of formic acid can lead to blindness or death. Similarly poisoning due to other alcohols such as ethylene glycol or diethylene glycol are due to their metabolites which are also produced by alcohol dehydrogenase. An effective treatment to prevent toxicity after methanol or ethylene glycol ingestion is to administer ethanol. Alcohol dehydrogenase has a higher affinity for ethanol, thus preventing methanol from binding and acting as a substrate. Any remaining methanol will then have time to be excreted through the kidneys. Methanol itself, while poisonous, has a much weaker sedative effect than ethanol. Some longer-chain alcohols such as n-propanol, isopropanol, n-butanol, t-butanol and 2-methyl-2-butanol do however have stronger sedative effects, but also have higher toxicity than ethanol. These longer chain alcohols are found as contaminants in some alcoholic beverages and are known as fusel alcohols, and are reputed to cause severe hangovers although it is unclear if the fusel alcohols are actually responsible. Many longer chain alcohols are used in industry as solvents and are occasionally abused by alcoholics, leading to a range of adverse health effects.
Achill Island (;) in County Mayo is the largest island off of Ireland, and is situated off the west coast. It has a population of 2,700. Its area is. Achill is attached to the mainland by Michael Davitt Bridge, between the villages of Gob an Choire (Achill Sound) and Poll Raithní (Polranny). A bridge was first completed here in 1887, and replaced by the current structure in 1949. Other centres of population include the villages of Keel, Dooagh, Dumha Éige (Dooega) and Dugort. The parish's main Gaelic football pitch and two secondary schools are on the mainland at Poll Raithní. Early human settlements are believed to have been established on Achill around 3000 BCE. A paddle dating from this period was found at the crannóg near Dookinella. The island is 87% peat bog. The parish of Achill also includes the Curraun peninsula. The people of Curraun consider themselves Achill people, and most natives of Achill refer to this area as being "in Achill". In the summer of 1996, the RNLI decided to station a lifeboat at Kildownet.
It is believed that at the end of the Neolithic Period (around 4000 BCE), Achill had a population of 500–1,000 people. The island would have been mostly forest until the Neolithic people began crop cultivation. Settlement increased during the Iron Age, and the dispersal of small forts around the coast indicate the warlike nature of the times. Granuaile maintained a castle at Kildownet in the 16th century. Achill Island lies in the Barony of Burrishoole, in the territory of ancient Umhall (Umhall Uactarach and Umhall Ioctarach), that originally encompassed an area extending from the Galway/Mayo border to Achill Head in Co. Mayo. The hereditary chieftains of Umhall were the O’Malleys, recorded in the area in 814 AD when they successfully repelled an onslaught by the Vikings in Clew Bay. The Anglo/Norman invasion of Connacht in 1235 AD saw the territory of Umhall taken over by the Butlers and later by the de Burgos. The Butler Lordship of Burrishoole continued into the late fourteenth century when Thomas le Botiller was recorded as being in possession of Akkyll & Owyll. In the 17th and 18th centuries, there was much migration to Achill from other parts of Ireland, particularly Ulster, due to the political and religious turmoil of the time. For a while there were two different dialects of Irish being spoken on Achill. This led to many townlands being recorded as having two names during the 1824 Ordnance Survey, and some maps today give different names for the same place. Achill Irish still has many traces of Ulster Irish.
Achill Archaeological Field School is based at the Achill Archaeology Centre in Dooagh, which has served as a catalyst for a wide array of archaeological investigations on the island. It was founded in 1991 and is a training school for students of archaeology and anthropology. Since 1991, several thousand students from 21 countries have come to Achill to study and participate in ongoing excavations. The school is involved in a study of the prehistoric and historic landscape at Slievemore, incorporating a research excavation at a number of sites within the deserted village of Slievemore. Slievemore is rich in archaeological monuments that span a 5,000 year period from the Neolithic to the Post Medieval. Recent archaeological research suggests the village was occupied year-round at least as early as the 19th century, though it is known to have served as a seasonally occupied booley village by the first half of the 20th century. (A booley village is a village occupied only during part of the year, such as a resort community, a lake community, or (as the case on Achill) a place to live while tending flocks or herds of ruminants during winter or summer pasturing.) Specifically, some of the people of Dooagh and Pollagh would migrate in the summer to Slievemore and then go back to Dooagh in the autumn. From 2004 to 2006, the Achill Island Maritime Archaeology Project was sponsored by the College of William and Mary, the Institute of Maritime History, the Achill Folklife Centre, and the Lighthouse Archaeological Maritime Program (LAMP). This project focused on the documentation of archaeological resources related to Achill's rich maritime heritage. Maritime archaeologists recorded 19th century fishing station, ice house, and boat house ruins, a number of anchors which had been salvaged from the sea, 19th century and more recent currach pens, a number of traditional vernacular watercraft including a possibly 100-year old Achill yawl, and the remains of four historic shipwrecks. The summer 2009 field school excavated Round House 2 on Slievemore Mountain under the direction of archaeologist Stuart Rathborne. Only the outside northwall, entrance way and inside of the Round House were completely excavated.
Despite some unsympathetic development, the island retains some striking natural beauty. The cliffs of Croaghaun on the western end of the island are the highest sea cliffs in Europe but are inaccessible by road. Near the westernmost point of Achill, Achill Head, is Keem Bay. Keel Beach is quite popular with tourists and some locals as a surfing location. South of Keem beach is Moytoge Head, which with its rounded appearance drops dramatically down to the ocean. An old British observation post, built during World War I to prevent the Germans from landing arms for the Irish Republican Army, is still standing on Moytoge. During the Second World War this post was rebuilt by the Irish Defence Forces as a Look Out Post for the Coast Watching Service wing of the Defence Forces. It operated from 1939 to 1945. The mountain Slievemore (672 m) rises dramatically in the north of the island and the Atlantic Drive (along the south/west of the island) has some dramatically beautiful views. On the slopes of Slievemore, there is an abandoned village (the "Deserted Village") The Deserted Village is traditionally thought to be a remnant village from An Gorta Mór (The Great Hunger of 1845-1849). Just west of the deserted village is an old Martello tower, again built by the British to warn of any possible French invasion during the Napoleonic Wars. The area also boasts an approximately 5000-year old Neolithic tomb. Achillbeg (', "Little Achill") is a small island just off Achill's southern tip. Its inhabitants were resettled on Achill in the 1960s. The villages of Dooniver and Askill have very picturesque scenery
While a number of attempts at setting up small industrial units on the island have been made, the economy of the island is largely dependent on tourism. Subventions from Achill people working abroad, in particular in the United Kingdom and the United States allowed many families to remain living in Achill throughout the 19th and 20th centuries. Since the advent of Ireland's "Celtic Tiger" economy fewer Achill people are forced to look for work abroad. Agriculture plays a small role and is only profitable because of European Union subsidies. The fact that the island is mostly bog means that its potential for agriculture is limited largely to sheep farming. In the past, fishing was a significant activity but this aspect of the economy is small now. At one stage, the island was known for its shark fishing, basking shark in particular was fished for its valuable liver oil. There was a big spurt of growth in tourism in the 1960s and 1970s before which life was tough and difficult on the island. Since that heyday, the common perception is that tourism in Achill has been slowly declining.
Because of the inhospitable climate, few inhabited houses date from before the 20th century, though there are many examples of abandoned stone structures dating to the 19th century. The best known of these earlier can be seen in the "Deserted Village" ruins near the graveyard at the foot of Slievemore. Even the houses in this village represent a relatively comfortable class of dwelling as, even as recently as a hundred years ago, some people still used "Beehive" style houses (small circular single roomed dwellings with a hole in ceiling to let out smoke). Many of the oldest and most picturesque inhabited cottages date from the activities of the Congested Districts Board for Ireland—a body set up around the turn of the 20th century in Ireland to improve the welfare for inhabitants of small villages and towns. Most of the homes in Achill at the time were very small and tightly packed together in villages. The CDB subsidised the building of new, more spacious (though still small by modern standards) homes outside of the traditional villages. Some of the recent building development on the island (over the last 30 years or so) has been contentious and in many cases is not as sympathetic to the landscape as the earlier style of whitewashed raised gable cottages. Because of generous tax incentives, many holiday homes have been built over the last ten years. This building boom has brought benefits but at a cost. On the one hand it has provided much-needed employment for the local people, has increased the demand and value for suitable development land and has allows the island to support more tourists. On the other hand, many of these houses have been built in prominent scenic areas and have damaged traditional views of the island while lying empty for most of the year. They may also be contributing to the declining fortunes for the traditional beneficiaries of tourism - bed and breakfasts, public houses and guesthouses.
Ginsberg was born into a Jewish family in Newark, New Jersey, and grew up in nearby Paterson. His father Louis Ginsberg was a poet and a high school teacher. Ginsberg's mother, Naomi Livergant Ginsberg, was affected by a rare psychological illness that was never properly diagnosed. She was also an active member of the Communist Party and took Ginsberg and his brother Eugene to party meetings. Ginsberg later said that his mother "made up bedtime stories that all went something like: 'The good king rode forth from his castle, saw the suffering workers and healed them.'" As a young teenager, Ginsberg began to write letters to "The New York Times" about political issues such as World War II and workers' rights. When he was in junior high school, he accompanied his mother by bus to her therapist. The trip deeply disturbed Ginsberg — he mentioned it and other moments from his childhood in his long autobiographical poem "Kaddish for Naomi Ginsberg (1894-1956)." Also while in high school, Ginsberg began reading Walt Whitman, inspired by his teacher's passionate reading. In 1943, Ginsberg graduated from Eastside High School and briefly attended Montclair State College before entering Columbia University on a scholarship from the Young Men's Hebrew Association of Paterson. In 1945, he joined the Merchant Marine to earn money to continue his education at Columbia. While at Columbia, Ginsberg contributed to the "Columbia Review" literary journal, the "Jester" humor magazine, won the Woodberry Poetry Prize and served as president of the Philolexian Society, the campus literary and debate group.
In Ginsberg's freshman year at Columbia he met fellow undergraduate Lucien Carr, who introduced him to a number of future Beat writers, including Jack Kerouac, William S. Burroughs, and John Clellon Holmes. They bonded because they saw in one another excitement about the potential of American youth, a potential that existed outside the strict conformist confines of post-World War II, McCarthy-era America. Ginsberg and Carr talked excitedly about a "New Vision" (a phrase adapted from Arthur Rimbaud) for literature and America. Carr also introduced Ginsberg to Neal Cassady, for whom Ginsberg had a long infatuation. Kerouac later described the meeting between Ginsberg and Cassady in the first chapter of his 1957 novel "On the Road". Kerouac saw them as the dark (Ginsberg) and light (Cassady) side of their "New Vision." Kerouac's perception had to do partly with Ginsberg's association with Communism. Though Ginsberg was never a member of the Communist Party, Kerouac named him "Carlo Marx" in "On the Road". This was a source of strain in their relationship, since Kerouac grew increasingly distrustful of Communism. In 1948 in an apartment in Harlem, Ginsberg had an auditory hallucination while reading the poetry of William Blake (later referred to as his "Blake vision"). At first Ginsberg claimed to have heard the voice of God, but later interpreted the voice as that of Blake himself reading "Ah, Sunflower", "The Sick Rose", and "Little Girl Lost". Ginsberg believed that he had witnessed the interconnectedness of the universe. He looked at lattice work on the fire escape and realized some hand had crafted that; he then looked at the sky and intuited that some hand had crafted that also, or rather that the sky was the hand that crafted itself. He explained that this hallucination was not inspired by drug use, but said he sought to recapture that feeling later with various drugs. Also in New York, Ginsberg met Gregory Corso in the Pony Stable Bar, one of New York's first openly lesbian bars. Corso, recently released from prison, was supported by the Pony Stable patrons and was writing poetry there the night of their meeting. Ginsberg claims he was immediately attracted to Corso, who was straight but understanding of homosexuality after three years in prison. Ginsberg was even more struck by reading Corso's poems, realizing Corso was "spiritually gifted." Ginsberg introduced Corso to the rest of his inner circle. In their first meeting at the Pony Stable, Corso showed Ginsberg a poem about a woman who lived across the street from him, and sunbathed naked in the window. Amazingly, the woman happened to be Ginsberg's girlfriend from one of his forays into heterosexuality. Ginsberg was living with the woman and took Corso over to their apartment, the woman proposed sex and Corso, still very young, fled in fear. Ginsberg introduced Corso to Kerouac and Burroughs and they began to travel together. Ginsberg and Corso remained life-long friends and collaborators. It was also during this period that Ginsberg was romantically involved with Elise Cowen.
In 1954 in San Francisco, Ginsberg met Peter Orlovsky, with whom he fell in love and who remained his life-long partner. Also in San Francisco Ginsberg met members of the San Francisco Renaissance and other poets who would later be associated with the Beat Generation in a broader sense. Ginsberg's mentor William Carlos Williams wrote an introductory letter to San Francisco Renaissance figurehead Kenneth Rexroth, who then introduced Ginsberg into the San Francisco poetry scene. There, Ginsberg also met three budding poets and Zen enthusiasts who were friends at Reed College: Gary Snyder, Philip Whalen, and Lew Welch. In 1959, along with poets John Kelly, Bob Kaufman, A. D. Winans, and William Margolis, Ginsberg was one of the founders of the "Beatitude" poetry magazine. Wally Hedrick — a painter and co-founder of the Six Gallery — approached Ginsberg in the summer of 1955 and asked him to organize a poetry reading at the Six Gallery. At first, Ginsberg refused, but once he’d written a rough draft of "Howl", he changed his "fucking mind," as he put it. Ginsberg advertised the event as "Six Poets at the Six Gallery." One of the most important events in Beat mythos, known simply as "The Six Gallery reading" took place on October 7, 1955. The event, in essence, brought together the East and West Coast factions of the Beat Generation. Of more personal significance to Ginsberg: that night was the first public reading of "Howl", a poem that brought worldwide fame to Ginsberg and to many of the poets associated with him. An account of that night can be found in Kerouac's novel "The Dharma Bums," describing how change was collected from audience members to buy jugs of wine, and Ginsberg reading passionately, drunken, with arms outstretched. A taped recording of the reading of "Howl" that Ginsberg gave at Reed College has recently been rediscovered and appeared on their from 9am PST 15 February 2008. Ginsberg's principal work, "Howl", is well-known for its opening line: "I saw the best minds of my generation destroyed by madness, starving hysterical naked..." "Howl" was considered scandalous at the time of its publication, because of the rawness of its language, which is frequently explicit. Shortly after its 1956 publication by San Francisco's City Lights Bookstore, it was banned for obscenity. The ban became a cause célèbre among defenders of the First Amendment, and was later lifted after Judge Clayton W. Horn declared the poem to possess redeeming artistic value.
Ginsberg claimed at one point that all of his work was an extended biography (like Kerouac's "Duluoz Legend"). "Howl" is not only a biography of Ginsberg's experiences before 1955 but also a history of the Beat Generation. Ginsberg also later claimed that at the core of "Howl" were his unresolved emotions about his schizophrenic mother. Though "Kaddish" deals more explicitly with his mother (so explicitly that a line-by-line analysis would be simultaneously overly-exhaustive and relatively unrevealing), "Howl" in many ways is driven by the same emotions. Though references in most of his poetry reveal much about his biography, his relationship to other members of the Beat Generation, and his own political views, "Howl", his most famous poem, is still perhaps the best place to start. To Paris and the "Beat Hotel". In 1957, Ginsberg surprised the literary world by abandoning San Francisco. After a spell in Morocco, he and Peter Orlovsky joined Gregory Corso in Paris. Corso introduced them to a shabby lodging house above a bar at 9 rue Gît-le-Coeur that was to become known as the Beat Hotel. They were soon joined by William Burroughs and others. It was a productive, creative time for all of them. There, Ginsberg finished his epic poem "Kaddish", Corso composed "Bomb" and "Marriage", and Burroughs (with help from Ginsberg and Corso) put together "Naked Lunch," from previous writings. This period was documented by the photographer Harold Chapman, who moved in at about the same time, and took pictures constantly of the residents of the "hotel" until it closed in 1963. During 1962-3, Ginsberg and Orlovsky traveled extensively across India, living half a year at a time in Benaras and Calcutta. During this time he formed friendships with some of the prominent young Bengali poets of the time including Shakti Chattopadhyay and Sunil Gangopadhyay. England and the International Poetry Incarnation. In May, 1965, Allen Ginsberg arrived at Better Books, London, and offered to read anywhere for free. Shortly after his arrival, he gave a reading at Better Books, which was described by Jeff Nuttall as "the first healing wind on a very parched collective mind". Tom McGrath wrote "This could well turn out to have been a very significant moment in the history of England - or at least in the history of English Poetry". Shortly after the reading at Better Books, plans were hatched for the International Poetry Incarnation, which was to be held at the Royal Albert Hall in London on June 11, 1965. The event attracted an audience of 7,000 people to readings and live and tape performances by a wide variety of figures, including Allen Ginsberg, Adrian Mitchell, Alexander Trocchi, Harry Fainlight, Anselm Hollo, Christopher Logue, George Macbeth, Gregory Corso, Lawrence Ferlinghetti, Michael Horovitz, Simon Vinkenoog, Spike Hawkins, Tom McGrath and William Burroughs. Peter Whitehead documented the event on film and released it as "Wholly Communion".
Though "Beat" is most accurately applied to Ginsberg and his closest friends (Corso, Orlovsky, Kerouac, Burroughs, etc.), the term "Beat Generation" has become associated with many of the other poets Ginsberg met and became friends with in the late 1950s and early 1960s. A key feature of this term seems to be a friendship with Ginsberg. Friendship with Kerouac or Burroughs might also apply, but both writers later strove to disassociate themselves from the name "Beat Generation." Part of their dissatisfaction with the term came from the mistaken identification of Ginsberg as the leader. Ginsberg never claimed to be the leader of a movement. He did, however, claim that many of the writers with whom he had become friends in this period shared many of the same intentions and themes. Some of these friends include: Bob Kaufman; LeRoi Jones before he became Amiri Baraka, who, after reading "Howl", wrote a letter to Ginsberg on a sheet of toilet paper; Diane DiPrima; Jim Cohn; poets associated with the Black Mountain College such as Robert Creeley and Denise Levertov; poets associated with the New York School such as Frank O'Hara and Kenneth Koch. Later in his life, Ginsberg formed a bridge between the beat movement of the 1950s and the hippies of the 1960s, befriending, among others, Timothy Leary, Ken Kesey, and Bob Dylan. Ginsberg gave his last ever reading at Booksmith, a bookstore located in the Haight Ashbury neighborhood of San Francisco, a few months before his death.
Ginsberg's spiritual journey began early on with his spontaneous visions, and continued with an early trip to India and a chance encounter on a New York City street with Chögyam Trungpa Rinpoche (they both tried to catch the same cab), a Tibetan Buddhist meditation master of the Kagyu and Nyingma sects, who became his friend and life-long teacher. Ginsberg helped Trungpa (and New York poet Anne Waldman) in founding the Jack Kerouac School of Disembodied Poetics at Naropa Institute in Boulder, Colorado. Ginsberg was also involved with Krishnaism. He befriended A.C. Bhaktivedanta Swami Prabhupada, the founder of the Hare Krishna movement in the Western world, a relationship that is documented by Satsvarupa dasa Goswami in his biographical account "Srila Prabhupada Lilamrta". Ginsberg donated money, materials, and his reputation to help the Swami establish the first temple, and toured with him to promote his cause. Music and chanting were both important parts of Ginsberg's live delivery during poetry readings. He often accompanied himself on a harmonium, and was often accompanied by a guitarist. When Ginsberg asked if he could sing a song in praise of Lord Krishna on William F. Buckley, Jr.'s TV show "Firing Line" on September 3, 1968, Buckley acceded and the poet chanted slowly as he played dolefully on a harmonium. According to Richard Brookhiser, an associate of Buckley's, the host commented that it was "the most unharried Krishna I've ever heard." Attendance to his poetry readings was generally standing room only for most of his career, no matter where in the world he appeared. Ginsberg came in touch with the Hungryalist poets of Bengal, especially Malay Roy Choudhury, who introduced Ginsberg to the three fishes with one head of Indian emperor Jalaluddin Mohammad Akbar. The three fishes symbolised coexistence of all thought, philosophy and religion.
Ginsberg won the National Book Award for his book "The Fall of America". In 1993, the French Minister of Culture awarded him the medal of "Chevalier des Arts et des Lettres" (the Knight of Arts and Letters). With the exception of a special guest appearance at the NYU Poetry Slam on February 20, 1997, Ginsberg gave what is thought to be his last reading at The Booksmith in San Francisco on December 16, 1996. He died April 5, 1997, surrounded by family and friends in his East Village loft in New York City, succumbing to liver cancer via complications of hepatitis. He was 70 years old. Ginsberg continued to write through his final illness, with his last poem, "Things I'll Not Do (Nostalgias)", written on March 30. Ginsberg is buried in his family plot in Gomel Chesed Cemetery.
Ginsberg's willingness to talk about taboo subjects made him a controversial figure during the conservative 1950s and a significant figure in the 1960s. But Ginsberg continued to broach controversial subjects throughout the 1970s, '80s, and '90s. When explaining how he approached controversial topics, he often pointed to Herbert Huncke: he said that when he first got to know Huncke in the 1940s, Ginsberg saw that he was sick from his heroin addiction, but at the time heroin was a taboo subject and Huncke was left with nowhere to go for help. Role in Vietnam War protests. Ginsberg also played a key role in ensuring that a 1965 protest of the Vietnam war, which took place at the Oakland-Berkeley city line and drew several thousand marchers, was not violently interrupted by the California chapter of the notorious motorcycle gang, the Hells Angels, and their leader, Sonny Barger. The day prior to the scheduled march, the Hell's Angels attacked the front line of a smaller scale protest where a confrontation between police and demonstrators was brewing. The Hell's Angels came in on motorcycles and slashed banners while yelling "Go back to Russia, you fucking communists!" at the protesters. The Hell's Angels then vowed to disrupt the larger protest the next day. Ginsberg traveled to Barger's home in Oakland to talk the situation through. It is rumored that he offered Barger and other members of the Hell's Angels LSD as a gesture of friendship and goodwill. In the end, Barger and the other Hell's Angels that were present came away deeply impressed by the courage of Ginsberg and his companion Ken Kesey. They vowed not to attack the next day's protest march and furthermore deemed Ginsberg a man who was worth helping out. It was shortly after the Tompkins Square Park riots in New York that Ginsberg was involved in a fracas with the Mentofreeist group and was assaulted by its leader, Vargus Pike. Pike was arrested, and was later released when Ginsberg, sporting a black eye, refused to press charges.
Ginsberg talked openly about his connections with Communism and his admiration for past communist heroes and the labor movement at a time when the Red Scare and McCarthyism were still raging. He admired Castro and many other quasi-Marxist figures from the 20th century. In "America" (1956), Ginsberg writes: "America I used to be a communist when I was a kid I'm not sorry..." Biographer Jonah Raskin has claimed that despite his often stark opposition to communist orthodoxy, Ginsberg held "his own idiosyncratic version of communism". On the other hand, when Donald Mains, a New York City politician, publicly accused Ginsberg of being a member of the Communist Party, Ginsberg objected: "I am not, as a matter of fact, a member of the Communist party, nor am I dedicated to the overthrow of [the U.S.] government or any government by violence.... I must say that I see little difference between the armed and violent governments both Communist and Capitalist that I have observed..." Ginsberg traveled to several Communist countries to promote free speech. He claimed that Communist countries, China for example, welcomed him because they thought he was an enemy of Capitalism but often turned against him when they saw him as a trouble maker. For example, in 1965 Ginsberg was deported from Cuba for publicly protesting Cuba's anti-marijuana stance. The Cubans sent him to Czechoslovakia, where one week after being named the King of a May Day parade, Ginsberg was labeled an "immoral menace" by the Czech government because of his free expression of radical ideas, and was then deported. Václav Havel points to Ginsberg as an important inspiration in striving for freedom.
One contribution that is often considered his most significant and most controversial was his openness about homosexuality. Ginsberg was an early proponent of freedom for gay people. In 1943 he discovered within himself "mountains of homosexuality." He expressed this desire openly and graphically in his poetry. He also struck a note for gay marriage by listing Peter Orlovsky, his lifelong companion, as his spouse in his Who's Who entry. Later gay writers saw his frank talk about homosexuality as an opening to speak more openly and honestly about something often before only hinted at or spoken of in metaphor. In writing about sexuality in graphic detail and in his frequent use of language seen as indecent he challenged—and ultimately changed—obscenity laws. He was a staunch supporter of others whose expression challenged obscenity laws (William S. Burroughs and Lenny Bruce, for example). Radio talk show host, Michael Savage befriended and traveled with Beat poets Allen Ginsberg and Lawrence Ferlinghetti. Stephen Schwartz, also an acquaintance of Savage from this time, reported Savage possessed a photograph of himself and Ginsberg swimming naked in Hawaii and used the photograph as sort of a "calling card." Savage maintained a correspondence with Ginsberg consisting of ten letters and a trio of postcards across four years, which is maintained with Ginsberg's papers at Stanford University. One letter asked for Ginsberg to do a poetry reading, so others could "hear and see and know why I adore your public image." One postcard from Michael Savage mentions his desire to photograph Ginsberg "nude, in a provocative way."
Ginsberg also talked often about drug use. Throughout the 1960s he took an active role in the demystification of LSD and with Timothy Leary worked to promote its common use. He was also for many decades an advocate of marijuana legalization, and at the same time warned his audiences against the hazards of tobacco in his "Put Down Your Cigarette Rag (Don't Smoke):" "Don't Smoke Don't Smoke Nicotine Nicotine No / No don't smoke the official Dope Smoke Dope Dope."
Though early on he had intentions to be a labor lawyer, Ginsberg wrote poetry for most of his life. Most of his very early poetry was written in formal rhyme and meter like his father or like his idol William Blake. His admiration for the writing of Jack Kerouac inspired him to take poetry more seriously. Though he took odd jobs to support himself, in 1955, upon the advice of a psychiatrist, Ginsberg dropped out of the working world to devote his entire life to poetry. Soon after, he wrote "Howl", the poem that brought him and his friends much fame and allowed him to live as a professional poet for the rest of his life. Later in life, Ginsberg entered academia, teaching poetry as Distinguished Professor of English at Brooklyn College from 1986 until his death.
Since Ginsberg's poetry is intensely personal, and since much of the vitality of those associated with the beat generation comes from mutual inspiration, much credit for style, inspiration, and content can be given to Ginsberg's friends. Ginsberg claimed throughout his life that his biggest inspiration was Kerouac's concept of "spontaneous prose". He believed literature should come from the soul without conscious restrictions. However, Ginsberg was much more prone to revise than Kerouac. For example, when Kerouac saw the first draft of "Howl" he disliked the fact that Ginsberg had made editorial changes in pencil (transposing "negro" and "angry" in the first line, for example). Kerouac only wrote out his concepts of Spontaneous Prose at Ginsberg's insistence because Ginsberg wanted to learn how to apply the technique to his poetry. The inspiration for "Howl" was Ginsberg's friend, Carl Solomon and "Howl" is dedicated to Solomon (whom Ginsberg also directly addresses in the third section of the poem). Solomon was a Dada and Surrealism enthusiast (he introduced Ginsberg to Artaud) who suffered bouts of depression. Solomon wanted to commit suicide, but he thought a form of suicide appropriate to dadaism would be to go to a mental institution and demand a lobotomy. The institution refused, giving him many forms of therapy, including electroshock therapy. Much of the final section of the first part of "Howl" is a description of this. Ginsberg used Solomon as an example of all those ground down by the machine of "Moloch." Moloch, to whom the second section is addressed, is a Levantine god to whom children were sacrificed. Ginsberg may have gotten the name from the Kenneth Rexroth poem "Thou Shalt Not Kill", a poem about the death of one of Ginsberg's heroes, Dylan Thomas. But Moloch is mentioned a few times in the Torah and references to Ginsberg's Jewish background are not infrequent in his work. Ginsberg said the image of Moloch was inspired by peyote visions he had of the Francis Drake Hotel in San Francisco which appeared to him as a skull; he took it as a symbol of the city (not specifically San Francisco, but all cities). Ginsberg later acknowledged in various publications and interviews that behind the visions of the Francis Drake Hotel were memories of the Moloch of Fritz Lang's film "Metropolis" (1927) and of the woodcut novels of Lynd Ward. Moloch has subsequently been interpreted as any system of control, including the conformist society of post-World War II America focused on material gain, which Ginsberg frequently blamed for the destruction of all those outside of societal norms. He also made sure to emphasize that Moloch is a part of all of us: the decision to "defy" socially created systems of control—and therefore go against Moloch—is a form of self-destruction. Many of the characters Ginsberg references in "Howl", such as Neal Cassady and Herbert Huncke, destroyed themselves through excessive substance abuse or a generally wild lifestyle. The personal aspects of "Howl" are perhaps as important as the political aspects. Carl Solomon, the prime example of a "best mind" destroyed by defying society, is associated with Ginsberg's schizophrenic mother: the line "with mother finally ****** (fucked)" comes after a long section about Carl Solomon, and in Part III, Ginsberg says "I'm with you in Rockland where you imitate the shade of my mother." Ginsberg later admitted that the drive to write "Howl" was fueled by sympathy for his ailing mother, an issue which he was not yet ready to deal with directly. He dealt with it directly with 1959's "Kaddish". Inspiration from mentors and idols. Ginsberg's poetry was strongly influenced by Modernism (specifically Ezra Pound, T. S. Eliot, Hart Crane, and most importantly William Carlos Williams), Romanticism (specifically Percy Shelley and John Keats), the beat and cadence of jazz (specifically that of bop musicians such as Charlie Parker), and his Kagyu Buddhist practice and Jewish background. He considered himself to have inherited the visionary poetic mantle handed down from the English poet and artist William Blake, and the American poet Walt Whitman. The power of Ginsberg's verse, its searching, probing focus, its long and lilting lines, as well as its New World exuberance, all echo the continuity of inspiration that he claimed. He studied poetry under William Carlos Williams, who was then in the middle of writing his epic poem Paterson about the industrial city near his home. Ginsberg, after attending a reading by Williams, sent the older poet several of his poems and wrote an introductory letter. Most of these early poems were rhymed and metered and included archaic pronouns like "thee." Williams hated the poems. He told Ginsberg later, "In this mode perfection is basic, and these poems are not perfect." Though he hated the early poems, Williams loved the exuberance in Ginsberg's letter. He included the letter in a later part of "Paterson." He taught Ginsberg not to emulate the old masters but to speak with his own voice and the voice of the common American. Williams taught him to focus on strong visual images, in line with Williams' own motto "No ideas but in things." His time studying under Williams led to a tremendous shift from the early formalist work to a loose, colloquial free verse style. Early breakthrough poems include "Bricklayer's Lunch Hour" and "Dream Record." Carl Solomon introduced Ginsberg to the work of Antonin Artaud ("To Have Done with the Judgement of God" and "Van Gogh: The Man Suicided by Society"), and Jean Genet ("Our Lady of the Flowers"). Philip Lamantia introduced him to other Surrealists and Surrealism continued to be an influence (for example, sections of "Kaddish" were inspired by André Breton's "Free Union"). Ginsberg claimed that the anaphoric repetition of "Howl" and other poems was inspired by Christopher Smart in such poems as "Jubilate Agno." Ginsberg also claimed other more traditional influences, such as: Franz Kafka, Herman Melville, Fyodor Dostoevsky, Edgar Allan Poe, and even Emily Dickinson. Ginsberg also made an intense study of haiku and the paintings of Paul Cézanne, from which he adapted a concept important to his work, which he called the "Eyeball Kick". He noticed in viewing Cézanne's paintings that when the eye moved from one color to a contrasting color, the eye would spasm, or "kick." Likewise, he discovered that the contrast of two seeming opposites was a common feature in haiku. Ginsberg used this technique in his poetry, putting together two starkly dissimilar images: something weak with something strong, an artifact of high culture with an artifact of low culture, something holy with something unholy. The example Ginsberg most often used was "hydrogen jukebox" (which later became the title of a song cycle composed by Philip Glass with lyrics drawn from Ginsberg's poems). Another example is Ginsberg's observation on Bob Dylan during Dylan's hectic and intense 1966 electric-guitar tour, fuelled by a cocktail of amphetamines, opiates, alcohol, and psychedelics, as a "Dexedrine Clown". The phrases "eyeball kick" and "hydrogen jukebox" both show up in "Howl", as well as a direct quote from Cézanne: "Pater Omnipotens Aeterna Deus".
From the study of his idols and mentors and the inspiration of his friends—not to mention his own experiments—Ginsberg developed an individualistic style that's easily identified as Ginsbergian. "Howl" came out during a potentially hostile literary environment less welcoming to poetry outside of tradition; there was a renewed focus on form and structure among academic poets and critics partly inspired by New Criticism. Consequently, Ginsberg often had to defend his choice to break away from traditional poetic structure, often citing Williams, Pound, and Whitman as precursors. Ginsberg's style may have seemed to critics chaotic or unpoetic, but to Ginsberg it was an open, ecstatic expression of thoughts and feelings that were naturally poetic. He believed strongly that traditional formalist considerations were archaic and didn't apply to reality. Though some, Diana Trilling for example, have pointed to Ginsberg's occasional use of meter (for example the anapest of "who came back to Denver and waited in vain"), Ginsberg denied any intention toward meter and claimed instead that meter follows the natural poetic voice, not the other way around; he said, as he learned from Williams, that natural speech is occasionally dactylic, so poetry that imitates natural speech will sometimes fall into a dactylic structure but only accidentally. Like Williams, Ginsberg's line breaks were often determined by breath: one line in "Howl", for example, should be read in one breath. Ginsberg claimed he developed such a long line because he had long breaths (saying perhaps it was because he talked fast, or he did yoga, or he was Jewish). The long line could also be traced back to his study of Walt Whitman; Ginsberg claimed Whitman's long line was a dynamic technique few other poets had ventured to develop further. Whitman is often compared to Ginsberg because their poetry sexualized aspects of the male form — though there is no direct evidence Whitman was homosexual. Many of Ginsberg's early long line experiments contain some sort of anaphoric repetition, or repetition of a "fixed base" (for example "who" in "Howl", "America" in "America"), and this has become a recognizable feature of Ginsberg's style. However, he said later this was a crutch because he lacked confidence in his style; he didn't yet trust "free flight". In the 60s, after employing it in some sections of "Kaddish" ("caw" for example) he, for the most part, abandoned the anaphoric repetition. Several of his earlier experiments with methods for formatting poems as a whole become regular aspects of his style in later poems. In the original draft of "Howl", each line is in a "stepped triadic" format reminiscent of Williams (see "Ivy Leaves", for example). He abandoned the "stepped triadic" when he developed his long line, but the stepped lines showed up later, most significantly in the travelogues of "The Fall of America." "Howl" and "Kaddish", arguably his two most important poems, are both organized as an inverted pyramid, with larger sections leading to smaller sections. In "America", he experimented with a mix of longer and shorter lines. "Lightning's blue glare fills Oklahoma plains, "An old man catching fireflies on the porch at night "watched the Herd Boy cross the Milky Way "How can we war against that?"
As an example, the field of real numbers is not algebraically closed, because the polynomial equation "x"2 + 1 = 0  has no solution in real numbers, even though all its coefficients (1 and 0) are real. The same argument proves that no subfield of the real field is algebraically closed; in particular, the field of rational numbers is not algebraically closed. Also, no finite field "F" is algebraically closed, because if "a"1, "a"2, …, "an" are the elements of "F", then the polynomial ("x" − "a"1)("x" − "a"2) ··· ("x" − "a'n") + 1 has no zero in "F". By contrast, the fundamental theorem of algebra states that the field of complex numbers is algebraically closed. Another example of an algebraically closed field is the field of (complex) algebraic numbers. The only irreducible polynomials are those of degree one. The field "F" is algebraically closed if and only if the only irreducible polynomials in the ring "F"["x"] are those of degree one. The assertion “the polynomials of degree one are irreducible” is trivially true for any field. If "F" is algebraically closed and "p"("x") is an irreducible polynomial of "F"["x"], then it has some root "a" and therefore "p"("x") is a multiple of "x" − "a". Since "p"("x") is irreducible, this means that "p"("x") = "k"("x" − "a"), for some "k" ∈ "F" \ . On the other hand, if "F" is not algebraically closed, then there is some non-constant polynomial "p"("x") in "F"["x"] without roots in "F". Let "q"("x") be some irreducible factor of "p"("x"). Since "p"("x") has no roots in "F", "q"("x") also has no roots in "F". Therefore, "q"("x") has degree greater than one, since every first degree polynomial has one root in "F". Every polynomial is a product of first degree polynomials. The field "F" is algebraically closed if and only if every polynomial "p"("x") of degree "n" ≥ 1, with coefficients in "F", splits into linear factors. In other words, there are elements "k", "x"1, "x"2, …, "xn" of the field "F" such that "p"("x") = "k"("x" − "x"1)("x" − "x"2) ··· ("x" − "xn"). If "F" has this property, then clearly every non-constant polynomial in "F"["x"] has some root in "F"; in other words, "F" is algebraically closed. On the other hand, that the property stated here holds for "F" if "F" is algebraically closed follows from the previous property together with the fact that, for any field "K", any polynomial in "K"["x"] can be written as a product of irreducible polynomials. The field has no proper algebraic extension. The field "F" is algebraically closed if and only if it has no proper algebraic extension. If "F" has no proper algebraic extension, let "p"("x") be some irreducible polynomial in "F"["x"]. Then the quotient of "F"["x"] modulo the ideal generated by "p"("x") is an algebraic extension of "F" whose degree is equal to the degree of "p"("x"). Since it is not a proper extension, its degree is 1 and therefore the degree of "p"("x") is 1. On the other hand, if "F" has some proper algebraic extension "K", then the minimal polynomial of an element in "K" \ "F" is irreducible and its degree is greater than 1. The field has no proper finite extension. The field "F" is algebraically closed if and only if it has no finite algebraic extension because if, within the previous proof, the word “algebraic” is replaced by the word “finite”, then the proof is still valid. Every endomorphism of "Fn" has some eigenvector. The field "F" is algebraically closed if and only if, for each natural number "n", every linear map from "Fn" into itself has some eigenvector. An endomorphism of "Fn" has an eigenvector if and only if its characteristic polynomial has some root. Therefore, when "F" is algebraically closed, every endomorphism of "Fn" has some eigenvector. On the other hand, if every endomorphism of "Fn" has an eigenvector, let "p"("x") be an element of "F"["x"]. Dividing by its leading coefficient, we get another polynomial "q"("x") which has roots if and only if "p"("x") has roots. But if "q"("x") = "xn" + "a'n" − 1"x'n" − 1+ ··· + "a"0, then "q"("x") is the characteristic polynomial of the companion matrix
The field "F" is algebraically closed if and only if every rational function in one variable "x", with coefficients in "F", can be written as the sum of a polynomial function with rational functions of the form "a"/("x" − "b")n, where "n" is a natural number, and "a" and "b" are elements of "F". If "F" is algebraically closed then, since the irreducible polynomials in "F"["x"] are all of degree 1, the property stated above holds by the theorem on partial fraction decomposition. On the other hand, suppose that the property stated above holds for the field "F". Let "p"("x") be an irreducible element in "F"["x"]. Then the rational function 1/"p" can be written as the sum of a polynomial function "q" with rational functions of the form "a"/("x" − "b")n. Therefore, the rational expression can be written as a quotient of two polynomials in which the denominator is a product of first degree polynomials. Since "p"("x") is irreducible, it must divide this product and, therefore, it must also be a first degree polynomial.
If "F" is an algebraically closed field and "n" is a natural number, then "F" contains all "n"th roots of unity, because these are (by definition) the "n" (not necessarily distinct) zeroes of the polynomial "xn" − 1. A field extension that is contained in an extension generated by the roots of unity is a "cyclotomic extension", and the extension of a field generated by all roots of unity is sometimes called its "cyclotomic closure". Thus algebraically closed fields are cyclotomically closed. The converse is not true. Even assuming that every polynomial of the form "xn" − "a" splits into linear factors is not enough to assure that the field is algebraically closed. If a proposition which can be expressed in the language of first-order logic is true for an algebraically closed field, then it is true for every algebraically closed field with the same characteristic. Furthermore, if such a proposition is valid for an algebraically closed field with characteristic 0, then not only is it valid for all other algebraically closed fields with characteristic 0, but there is some natural number "N" such that the proposition is valid for every algebraically closed field with characteristic "p" when "p" > N. Every field "F" has some extension which is algebraically closed. Among all such extensions there is one and (up to isomorphism) only one which is an algebraic extension of "F"; it is called the algebraic closure of "F".
Anatoly Yevgenyevich Karpov (; born May 23, 1951) is a Soviet and Russian chess grandmaster and former World Champion. He was official world champion from 1975 to 1985, played three more matches for the title from 1986 to 1990, then was FIDE World Champion from 1993 to 1999. For his decades-long standing among the world's elite, Karpov is considered one of the greatest players of all time. His tournament successes include over 160 first-place finishes. He had a peak Elo rating of 2780, and his 90 total months at world number-one are second all-time behind only Garry Kasparov since the inception of the FIDE ranking list in 1971. Since 2005, he has been a member of the Public Chamber of Russia. He has recently involved himself in several humanitarian causes, such as advocating the use of iodised salt.
Karpov was born on May 23, 1951 at Zlatoust in the Urals region of the former Soviet Union, and learned to play chess at the age of four. His early rise in chess was swift, as he became a Candidate Master by age eleven. At twelve, he was accepted into Mikhail Botvinnik's prestigious chess school, though Botvinnik made the following remark about the young Karpov: "The boy does not have a clue about chess, and there's no future at all for him in this profession." Karpov acknowledged that his understanding of chess theory was very confused at that time, and wrote later that the homework which Botvinnik assigned greatly helped him, since it required that he consult chess books and work diligently. Karpov improved so quickly under Botvinnik's tutelage that he became the youngest Soviet National Master in history at fifteen in 1966; this tied the record established by Boris Spassky in 1952.
Karpov finished first in his first international tournament in Trinec several months later, ahead of Viktor Kupreichik. In 1967, he won the annual European Junior Championship at Groningen. Karpov won a gold medal for academic excellence in high school, and entered Moscow State University in 1968 to study mathematics. He later transferred to Leningrad State University, eventually graduating from there in economics. One reason for the transfer was to be closer to his coach, Grandmaster Semyon Furman, who lived in Leningrad. In his writings, Karpov credits Furman as a major influence on his development as a world-class player. In 1969, Karpov became the first Soviet player since Spassky (1955) to win the World Junior Chess Championship, scoring an undefeated 10/11 in the finals at Stockholm. In 1970, he tied for fourth place at an international tournament in Caracas, Venezuela, and was awarded the grandmaster title.
He won the 1971 Alekhine Memorial in Moscow (equal with Leonid Stein), ahead of a star-studded field, for his first significant adult victory. His Elo rating shot from 2540 in 1971 to 2660 in 1973, when he shared second in the USSR Chess Championship, and finished equal first with Viktor Korchnoi in the Leningrad Interzonal Tournament, with the latter success qualifying him for the 1974 Candidates Matches, which would determine the challenger of the reigning world champion, Bobby Fischer.
Karpov defeated Lev Polugaevsky by the score of +3 =5 in the first Candidates' match, earning the right to face former champion Boris Spassky in the semi-final round. Karpov was on record saying that he believed Spassky would easily beat him and win the Candidates' cycle to face Fischer, and that he (Karpov) would win the following Candidates' cycle in 1977. Spassky won the first game as Black in good style, but tenacious, aggressive play from Karpov secured him overall victory by +4 -1 =6. The Candidates' final was played in Moscow with Korchnoi. Karpov took an early lead, winning the second game against the Sicilian Dragon, then scoring another victory in the sixth game. Following ten consecutive draws, Korchnoi threw away a winning position in the seventeenth game to give Karpov a 3-0 lead. In game 19, Korchnoi succeeded in winning a long endgame, then notched a speedy victory after a blunder by Karpov two games later. Three more draws, the last agreed by Karpov in a clearly better position, closed the match, as he thus prevailed +3 -2 =19, moving on to challenge Fischer for the world title.
Though a world championship match between Karpov and Fischer was highly anticipated, those hopes were never realised. Fischer insisted that the match be the first to ten wins (draws not counting), but that the champion would retain the crown if the score was tied 9—9. FIDE, the International Chess Federation, refused to allow this proviso, and FIDE declared that Fischer relinquished his crown. Karpov later attempted to set up another match with Fischer, but all the negotiations fell through. This thrust the young Karpov into the role of World Champion without having faced the reigning champion. Garry Kasparov argued that Karpov would have had good chances, because he had beaten Spassky convincingly and was a new breed of tough professional, and indeed had higher quality games, while Fischer had been inactive for three years. Spassky thought that Fischer would have won in 1975 but Karpov would have qualified again and beaten Fischer in 1978.
Karpov participated in nearly every major tournament for the next ten years. He convincingly won the very strong Milan tournament in 1975, and captured his first of three Soviet titles in 1976. He created the most phenomenal streak of tournament wins against the strongest players in the world. Karpov held the record for most consecutive tournament victories (9) until it was shattered by Garry Kasparov's (14). In 1978, Karpov's first title defence was against Korchnoi, the opponent he had defeated in the 1973-75 Candidates' cycle; the match was played at Baguio in the Philippines, with the winner needing six victories. As in 1974, Karpov took an early lead, winning the eighth game after seven draws to open the match, but Korchnoi staged a comeback late in the match, as, after the score was +5 -2 =20 in Karpov's favour, he won three of the next four games to draw level, with Karpov then winning the next game to retain the title (+6 -5 =21). Three years later Korchnoi re-emerged as the Candidates' winner against German finalist Dr. Robert Hübner to challenge Karpov in Meran, Italy. This match, however, was won handily by Karpov, the score being (11–7, +6 -2 =10) in what is remembered as the "Massacre in Merano". Karpov's tournament career reached a peak at the exceptional Montreal "Tournament of Stars" tournament in 1979, where he finished joint first (+7 -1 =10) with Mikhail Tal, ahead of a field of strong grandmasters completed by Jan Timman, Ljubomir Ljubojevic, Spassky, Vlastimil Hort, Lajos Portisch, Huebner, Bent Larsen and Lubomir Kavalek. He dominated Las Palmas 1977 with an incredible 13.5/15. He also won the prestigious Bugojno tournament in 1978 (shared) and 1980, the Linares tournament in 1981 (shared with Larry Christiansen) and 1994, the Tilburg tournament in 1977, 1979, 1980, 1982, and 1983, and the Soviet Championship in 1976, 1983, and 1988. Karpov represented the Soviet Union at six Chess Olympiads, in all of which the USSR won the team gold medal. He played first reserve at Skopje 1972, winning the board prize with 13/15. At Nice 1974, he advanced to board one and again won the board prize with 12/14. At La Valletta 1980, he was again board one and scored 9/12. At Lucerne 1982, he scored 6.5/8 on board one. At Dubai 1986, he scored 6/9 on board two. His last was Thessaloniki 1988, where on board two he scored 8/10. In Olympiad play, Karpov lost only two games out of 68 played. To illustrate Karpov's dominance over his peers as champion, his score was +11 -2 =20 versus Spassky, +5 =12 versus Robert Hübner, +6 -1 =16 versus Ulf Andersson, +3 -1 =10 versus Vasily Smyslov, +1 =16 versus Mikhail Tal, +10 -2 =13 versus Ljubojevic. Karpov had cemented his position as the world's best player and world champion by the time Garry Kasparov arrived on the scene. In their first match, the World Chess Championship 1984, held in Moscow, with the victor again being the first to win six games outright, Karpov built a commanding 4-0 lead after nine games. The next seventeen games were drawn, setting the record for world title matches, and it took Karpov until Game 27 to gain his fifth win. In Game 31, Karpov had a winning position but failed to take advantage and settled for a draw. He lost the next game, after which fourteen more draws ensued. In particular, Karpov held a solidly winning position in Game 41, but again blundered and had to settle for a draw. After Kasparov won Games 47 and 48, FIDE President Florencio Campomanes unilaterally terminated the match, citing the health of the players. The match had lasted an unprecedented five months, with five wins for Karpov, three for Kasparov, and a staggering forty draws. A rematch was set for later in 1985, also in Moscow. The events of the so-called Marathon match forced FIDE to return to the previous format, a match limited to 24 games (with Karpov remaining champion if the match should finish 12-12). In a hard fight, Karpov had to win the final game to draw the match and retain his title, but wound up losing, thus surrendering the title to his opponent. The final score was 11-13 (+3 -5 =16), in favor of Kasparov.
Karpov remained a formidable opponent (and the world #2) until the early 1990s. He fought Kasparov in three more world championship matches in 1986 (held in London and Leningrad), 1987 (held in Seville), and 1990 (held in New York City and Lyon). All three matches were extremely close: the scores were 11.5 to 12.5 (+4 -5 = 15), 12 to 12 (+4 -4 =16), and 11.5 to 12.5 (+3 -4 =17). In all three matches, Karpov had winning chances up to the very last games. In particular, the 1987 Seville match featured an astonishing blunder by Kasparov in the 23rd game. Instead, in the final game, needing only a draw to win the title, Karpov cracked under pressure from the clock at the end of the first session of play, missed a variation leading to an almost forced draw, and allowed Kasparov to adjourn the game with an extra pawn. After a further mistake in the second session, Karpov was slowly ground down and resigned on move 64, ending the match and allowing Kasparov to keep the title. In their five world championship matches, Karpov scored 19 wins, 21 losses, and 104 draws in 144 games. Karpov is on record saying that had he had the opportunity to fight Fischer for the crown in his twenties, he (Karpov) could have been a much better player as a result (in a similar way as Kasparov's constant rivalry with him helped Kasparov to achieve his full potential).
In 1992, Karpov lost a Candidates Match against Nigel Short in 1992. But in 1993, Karpov reacquired the FIDE World Champion title when Kasparov and Short split from FIDE. Karpov defeated Timman – the loser of the Candidates' final against Short. The next major meeting of Kasparov and Karpov was the 1994 Linares chess tournament. The field, in eventual finishing order, was Karpov, Kasparov, Shirov, Bareev, Kramnik, Lautier, Anand, Kamsky, Topalov, Ivanchuk, Gelfand, Illescas, Judit Polgar, and Beliavsky; with an average Elo rating of 2685, the highest ever at that time, making it the first Category XVIII tournament ever held. Impressed by the strength of the tournament, Kasparov had said several days before the tournament that the winner could rightly be called the world champion of tournaments. Perhaps spurred on by this comment, Karpov played the best tournament of his life. He was undefeated and earned 11 points out of 13 possible (the best world-class tournament winning percentage since Alekhine won San Remo in 1930), finishing 2.5 points ahead of second-place Kasparov and Shirov. Many of his wins were spectacular (in particular, his win over Topalov is considered possibly the finest of his career). This performance against the best players in the world put his Elo rating tournament performance at 2985, the highest performance rating of any player in history. Jeff Sonas considers this the best tournament result in history. Karpov defended his FIDE title against Gata Kamsky (+6 -3 =9) in 1996. However, in 1998, FIDE largely scrapped the old system of Candidates' Matches, instead having a large knock-out event in which a large number of players contested short matches against each other over just a few weeks. In the first of these events, the FIDE World Chess Championship 1998, champion Karpov was seeded straight into the final, defeating Viswanathan Anand (+2 -2 =2, rapid tiebreak 2:0). In the subsequent cycle, the format was changed, with the champion having to qualify. Karpov refused to defend his title, and ceased to be FIDE World Champion after the FIDE World Chess Championship 1999.
Karpov's outstanding classical tournament play has been seriously limited since 1995, since he prefers to be more involved in politics of his home country of Russia. He had been a member of the Supreme Soviet Commission for Foreign Affairs and the President of the Soviet Peace Fund before the Soviet Union dissolved. In addition, he had been involved in several disputes with FIDE and became increasingly disillusioned with chess. In the September 2009 FIDE rating list, he dropped out of the world's Top 100 for the first time. Karpov usually limits his play to exhibition events, and has revamped his style to specialize in rapid chess. In 2002 he won a match against Kasparov, defeating him in a rapid time control match 2.5-1.5. In 2006, he tied for first with Kasparov in a blitz tournament, ahead of Korchnoi and Judit Polgar. Karpov and Kasparov played a mixed 12-game match from September 21–24, 2009, in Valencia, Spain. It consisted of four rapid (or semi rapid) and eight blitz games and took place exactly 25 years after the two players' legendary encounter at World Chess Championship 1984. Kasparov won the match 9-3. In March 2010 Karpov anounced that he would be a candidate for the president of FIDE. The election will take place at the 39th Chess Olympiad.
Karpov's "boa constrictor" playing style is solidly positional, taking no risks but reacting mercilessly to any tiny errors made by his opponents. As a result, he is often compared to his idol, the famous José Raúl Capablanca, the third World Champion. Karpov himself describes his style as follows:Let us say the game may be continued in two ways: one of them is a beautiful tactical blow that gives rise to variations that don't yield to precise calculation; the other is clear positional pressure that leads to an endgame with microscopic chances of victory... I would choose the latter without thinking twice. If the opponent offers keen play I don't object; but in such cases I get less satisfaction, even if I win, than from a game conducted according to all the rules of strategy with its ruthless logic.
The aspect ratio of a shape is the ratio of its longer dimension to its shorter dimension. It may be applied to two characteristic dimensions of a three-dimensional shape, such as the ratio of the longest and shortest axis, or for symmetrical objects that are described by just two measurements, such as the length and diameter of a rod. The aspect ratio of a torus is the ratio of the major axis "R" to the minor axis "r".